<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>大数据之Hive(三) - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据之Hive(三)</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h2>
<a id="_0"></a>分区表</h2> 
<h3>
<a id="_1"></a>概念和常用操作</h3> 
<p>将一个大表的数据按照业务需要分散存储到多个目录，每个目录称为该表的一个分区。一般来说是按照日期来作为分区的标准。在查询时可以通过where子句来选择查询所需要的分区，这样查询效率会提高很多。</p> 
<p>①创建分区表</p> 
<pre><code class="prism language-sql">hive <span class="token punctuation">(</span><span class="token keyword">default</span><span class="token punctuation">)</span><span class="token operator">&gt;</span> 
<span class="token keyword">create</span> <span class="token keyword">table</span> dept_partition
<span class="token punctuation">(</span>
    deptno <span class="token keyword">int</span><span class="token punctuation">,</span>    <span class="token comment">--部门编号</span>
    dname  string<span class="token punctuation">,</span> <span class="token comment">--部门名称</span>
    loc    string  <span class="token comment">--部门位置</span>
<span class="token punctuation">)</span>
    partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span><span class="token keyword">day</span> string<span class="token punctuation">,</span> <span class="token keyword">hour</span> string<span class="token punctuation">)</span>
    <span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'t'</span><span class="token punctuation">;</span>
</code></pre> 
<p>查询分区表数据时，可以将分区字段看作表的伪列，可像使用其他字段一样使用分区字段。</p> 
<table>
<thead><tr>
<th>操作命令</th>
<th>作用</th>
</tr></thead>
<tbody>
<tr>
<td>desc 表名</td>
<td>查看表的信息，分辨是否为分区表</td>
</tr>
<tr>
<td>show partition 表名</td>
<td>查看所有分区信息</td>
</tr>
<tr>
<td>alter 表名 add partition(dt=‘’)</td>
<td>添加分区，多个分区不用添加分隔符</td>
</tr>
<tr>
<td>alter 表名 drop partition(),partiton2</td>
<td>删除分区, 多个分区逗号分隔</td>
</tr>
<tr>
<td>msck repair table 表名 add/drop/ sync partitions</td>
<td>没有使用hive load命令上传文件时，用来修复分区，默认是add</td>
</tr>
</tbody>
</table> 
<h3>
<a id="_27"></a>二级分区表</h3> 
<p>如果一天内的数据量也很大，可以再次将数据按照小时进行分区。适合数据量特别大的时候使用</p> 
<h3>
<a id="_30"></a>动态分区表</h3> 
<p>动态分区是指向分区表insert数据时，被写往的分区不由用户指定，而是由每行数据的最后一个字段的值来动态的决定。使用动态分区，可只用一个insert语句将数据写入多个分区。</p> 
<ol>
<li>开启动态分区功能set <code>hive.exec.dynamic.partition=true;</code>
</li>
<li>设置为动态分区非严格模式set <code>hive.exec.dynamic.partition.mode=nonstrict</code>
</li>
<li>需要先存在一张大表已经存储好了，然后转换为动态分区表。</li>
<li>按照已经存储的表的最后一列作为分区列</li>
</ol> 
<pre><code class="prism language-sql"><span class="token keyword">insert</span> <span class="token keyword">into</span> <span class="token keyword">table</span> dept_partition_dynamic 
<span class="token keyword">partition</span><span class="token punctuation">(</span>loc<span class="token punctuation">)</span>  <span class="token comment">-- 动态分区就是指这个值没有写死</span>
<span class="token keyword">select</span> 
    deptno<span class="token punctuation">,</span> 
    dname<span class="token punctuation">,</span> 
    loc 
<span class="token keyword">from</span> dept<span class="token punctuation">;</span>
</code></pre> 
<h2>
<a id="_47"></a>分桶表</h2> 
<p>分区提供一个隔离数据和优化查询的便利方式。底层是将数据放到不同目录，但是并非所有数据都可形成合理的分区。分桶是指将同一个文件的数据按照分桶数再划分为更细粒度的不同文件。数据内容是按照对应字段的哈希值对桶数取模来分配的。只在特定情况下效率会更高。</p> 
<h2>
<a id="_52"></a>分区和分桶结合使用</h2> 
<pre><code class="prism language-sql"><span class="token keyword">create</span> <span class="token keyword">table</span> stu_buck_sort_part<span class="token punctuation">(</span>
	id <span class="token keyword">int</span><span class="token punctuation">,</span>
	name string
<span class="token punctuation">)</span>
partitioned <span class="token keyword">by</span> <span class="token punctuation">(</span><span class="token keyword">day</span> string<span class="token punctuation">)</span>  <span class="token comment">-- 分区</span>
<span class="token keyword">clustered</span> <span class="token keyword">by</span> <span class="token punctuation">(</span>id<span class="token punctuation">)</span> sorted <span class="token keyword">by</span> <span class="token punctuation">(</span>id<span class="token punctuation">)</span>
<span class="token keyword">into</span> <span class="token number">4</span> buckets  <span class="token comment">-- 分桶</span>
<span class="token keyword">row</span> format delimited <span class="token keyword">fields</span> <span class="token keyword">terminated</span> <span class="token keyword">by</span> <span class="token string">'t'</span><span class="token punctuation">;</span>
</code></pre> 
<p>分区和分桶的区别：</p> 
<ol>
<li>分区是分的是目录，分桶分的是文件</li>
<li>分区的字段不能是表中字段，分桶的字段必须是表中的字段</li>
</ol> 
<h2>
<a id="_69"></a>自定义函数</h2> 
<h3>
<a id="_70"></a>用户自定义函数分类</h3> 
<p>（1）UDF：一进一出<br> （2）UDAF：多进一出<br> （3）UDTF：一进多出</p> 
<h3>
<a id="_74"></a>自定义步骤</h3> 
<ol>
<li>模仿length函数</li>
<li>导入jar包</li>
<li>编写MyUDF类，继承GenericUDF类，重写方法</li>
<li>initialize(检查器数组)，返回值为检查器。检查器类内部封装了所有可以处理的类对象。初始化用来： 
  <ul>
<li>检查参数个数，不正确时抛UDFArgumentLengthException()</li>
<li>检查参数类型, 不正确时抛UDFArgumentTypeException()</li>
<li>约定函数的返回值类型, 可以选择java的序列化对象或者hadoop的writable对象。使用工厂类（帮你把各种类的单例已经new好了）来获取返回对应的对象。</li>
</ul> </li>
<li>evaluate(函数值对象 o) ，返回值是Object 
  <ul>
<li>如果为null，返回0或-1</li>
<li>不为null, 返回 o.toString().length();</li>
</ul> </li>
<li>使用Maven打包，在target中复制到hadoop中，建议放到data目录下, 复制路径pwd。</li>
<li>在hadoop中使用add jar 路径</li>
<li>进入jdbc中创建永久函数<code>create function my_len as "方法的全类名"</code>；如果想创建临时方法，在function前面加上temporary。临时函数可以跨库使用，永久函数需要加上前缀库名后才能跨库使用。</li>
<li>由于add jar本身也是临时生效的，需要将jar包上传到HDFS中才能真正变成永久函数。然后在创建函数时添加<code>using "HDFS路径"</code>
</li>
</ol> 
<h2>
<a id="Hadoop_90"></a>Hadoop压缩</h2> 
<p>存储时选择压缩比的最好的bzip2，计算时选择速度快点压缩算法，目前天选加唯一的就是snappy。</p> 
<ol>
<li>打开参数, 这两个参数默认都为false<br> Hadoop: mapreduce.map.output.compress=true<br> Hive：hive.exec.compress.output=true</li>
<li>设置压缩方式</li>
<li>使用hadoop103:8088中的yarn来查看压缩算法是否被使用。</li>
<li>实际使用过程中并不能提升程序的运行效率，只是减少了IO，但需要额外的配置，只有在特殊场景才会配置。</li>
</ol> 
<h3>
<a id="Hive_99"></a>Hive文件格式</h3> 
<table>
<thead><tr>
<th>文件名</th>
<th>特点</th>
</tr></thead>
<tbody>
<tr>
<td>textfile</td>
<td>行式存储</td>
</tr>
<tr>
<td>orc</td>
<td>列式存储, 比较适合列式的查询，符合公司业务需求</td>
</tr>
<tr>
<td>Parquet</td>
<td>列式存储</td>
</tr>
</tbody>
</table> 
<h3>
<a id="ORC_106"></a>ORC文件结构</h3> 
<ul>
<li>Stripe0：大小等于物理块，128M 
  <ul>
<li>Index索引</li>
<li>column a</li>
<li>column b</li>
<li>column c</li>
<li>Footer编码信息</li>
</ul> </li>
<li>Stripe1：和上面一样</li>
<li>…</li>
<li>File Footer: 
  <ul><li>stripe的起始位置，索引的长度，数据的长度，Stripe footer的长度</li></ul> </li>
</ul> 
<p>使用orc列式存储时可以将原文件大小缩小到原先的40%，parquet大概是原先的70%。在数据量较大时，orc和parquet进行按列查询时查询速度会比textfile速度更快。</p> 
<h2>
<a id="_120"></a>企业优化</h2> 
<h3>
<a id="_121"></a>计算资源配置</h3> 
<ol>
<li>调整yarn内存和容器内存</li>
<li>调整map和reduce的内存和CPU核心数</li>
</ol> 
<h3>
<a id="Explain_125"></a>Explain查看执行计划</h3> 
<p>语法：<code>explain query-SQL</code></p> 
<h3>
<a id="_128"></a>分组聚合优化</h3> 
<h4>
<a id="mapside_129"></a>map-side聚合</h4> 
<p>将聚合操作从reduce阶段提前到map阶段。<br> <code>set hive.map.aggr = true.</code> 开启预聚合combiner<br> 可以将该参数关闭，比较两次查询过程的执行时间。该优化对于有数据倾斜的数据有很好的优化效果。</p> 
<h3>
<a id="join_134"></a>join优化</h3> 
<ol>
<li>common join 
  <ul><li>没有开启自动转map join</li></ul> </li>
<li>map join 
  <ul>
<li>文件大小小于25M时被称为小表</li>
<li>配置参数开启hive.auto.convert.join</li>
<li>配置参数开启无条件转map join，不考虑数据是否是小表，出错时直接OOM内存溢出。</li>
</ul> </li>
<li>bucket map join 
  <ul>
<li>将大表进行分桶，分桶是根据字段来分的，分桶时必须按照<code>连接键</code>来分。</li>
<li>左右两边分桶的个数必须是相等或倍数关系。</li>
</ul> </li>
<li>sort merge bucket join 
  <ul>
<li>在分桶的基础上，将桶内数据进行排序后再进行Join操作，将全量IO转换为部分IO。</li>
<li>设置参数为true： 
    <ul>
<li>sortedmerge</li>
<li>sortmerge.join</li>
</ul> </li>
</ul> </li>
</ol> 
<h3>
<a id="_149"></a>数据倾斜</h3> 
<h4>
<a id="reducer_150"></a>reducer倾斜</h4> 
<ol>
<li>map-side聚合：默认是开启的</li>
<li>Skew-GroupBy优化：将数据打散，不按照原先的逻辑进行分组，随机平均分散到不同的reducer中。适合倾斜量级很大时，否则优化效果不是很明显。</li>
</ol> 
<h4>
<a id="join_153"></a>join数据倾斜</h4> 
<ol>
<li>桶表join</li>
<li>map join</li>
</ol> 
<h3>
<a id="_157"></a>任务的并行度</h3> 
<ol>
<li>设置输入类为CombineInputFormat, 而不是hadoop默认的TextInputFormat。切片默认大小为256，而不是原先的128M.<br> <code>set hive.input.format = hive.ql.io.CombineInputFormat</code>
</li>
<li>reduce数量默认跟map数量一致，设定reduce个数的上限为1009个。reducer处理字节的上限为256M。</li>
<li>开启合并map reduce任务输出的小文件<code>hive.merge.mapredfiles</code>, 小文件平均大小小于平均值16M时触发合并。<strong>这个配置默认是关闭，如果出现输出时小文件很多时，建议修改配置一下。</strong>
</li>
</ol> 
<h3>
<a id="_164"></a>其他优化</h3> 
<h4>
<a id="CBO_165"></a>CBO优化</h4> 
<p>最优成本优化，<code>hive.cbo.enable = true</code> 默认是开启的。多表连接时，会自动将最小的两个表进行连接，这样处理过程中的IO和写入磁盘负担更小，整体速度更快。</p> 
<h4>
<a id="_168"></a>谓词下推（过滤提前）</h4> 
<p>在既有join操作和where操作时，先进行连接操作后再进行过滤计算量会远远大于先进行过滤后再进行连接操作。并且这个顺序变化后的结果是一样的。<br> <code>hive.optimize.ppd = false</code> ，该参数默认是开启的，设置时需要将cbo同时修改。</p> 
<h4>
<a id="Fetch_172"></a>Fetch抓取</h4> 
<ol>
<li>select * 语句</li>
<li>where语句</li>
<li>limit语句<br> 上述三个语句不需要进行MR计算操作，hive.fetch.task.conversion = more; 有三个等级，分别是, minimal, more</li>
</ol> 
<table>
<thead><tr>
<th>级别</th>
<th>作用</th>
</tr></thead>
<tbody>
<tr>
<td>none</td>
<td>不转换成IO、计数器、过滤操作</td>
</tr>
<tr>
<td>minimal</td>
<td>select * 转换，其他不转换</td>
</tr>
<tr>
<td>more</td>
<td>尽量转换成不走MR的操作</td>
</tr>
</tbody>
</table> 
<h4>
<a id="_184"></a>本地化模式</h4> 
<ol>
<li>本地模式数据不能超过128M，超过时会自动转换为128M</li>
<li>本地输入的数据总量不能超过4个</li>
</ol> 
<h4>
<a id="_189"></a>并行执行</h4> 
<p>hive.exec.parallel, <strong>默认是关闭的，有利于提高集群的稳定性，如果需要提高效率，可以开启该参数</strong>。<br> exec.parallel.thread.number=8, 设置最大并行度，如果集群比较小，反而会降低效率。</p> 
<h4>
<a id="_193"></a>严格模式</h4> 
<ol>
<li> <p><code>hive.strict.checks.no.partition.filter=false;</code> 默认是关闭的，查询数据时必须给分区参数，否则报错。日常是开启的，年终是关闭的。年终时需要进行文件的合并操作。</p> </li>
<li> <p>order by 后面必须添加limit n关键字，提高map阶段的shuffle速度，只需要排序前面n条数据。</p> </li>
<li> <p>笛卡尔乘积默认会开启，连接时必须加连接条件。</p> </li>
</ol>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>