<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>人工智能：卷积神经网络及YOLO算法 入门详解与综述（二） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">人工智能：卷积神经网络及YOLO算法 入门详解与综述（二）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p style="margin-left:.0001pt"><span style="color:#000000">        经过前六章的阅读，我从三个世界、数据法则、信息纽带、知识升华、自然智能以及人工智能六个方面对于信息科学技术与创新有了深层次的认识与了解。从对于三个世界的描述中，我了解到了物理、生物和数字世界的区别和联系。同时也明白了物质、能量与数据构成了人类所赖以生存和发展的客观和主观世界。通过这样的三个世界基本底层架构的认知，展开了之后的讨论，之后详细地了解到数据的作用，例如数据在生命的产生与演化中起着至关重要的作用，在生命体内DNA中的数据就记录了遗传的基本信息，大脑中的储存数据量与神经元细胞和它们的数量存在着正相关的关系。 数据之间的快速传导使各网络之间可以不考虑地理上的联系而重新组合在一起。信息的传递和交换也变得日益频繁。而在之后对于信息的定义及作用介绍之中，通过对于信息法则的介绍以及对于信息编码过程的展示，让我明白了信息的结构、含义与效用。信息的提取与升华成为知识，我对知识的描述性与程序性、显性与隐性、公共性与私密性有了进一步的认识。由知识的不断进化集合的过程中，自然智能也逐渐彰显出其作用，自然智能也拥有其法则。无独有偶，针对于自然智能的研究也不断启发着人工智能的发展。上一章重点讲述了人工智能的历史、概念、算法以及人工智能的面临障碍。使我对于人工智能的理解有了很大提升。本章就人工智能的应用技术进行了更深层次的分析与讲解。同时本章讨论的课题如下：</span></p> 
<p class="img-center"><img alt="" height="357" src="https://images2.imgbox.com/64/f6/KMTg3pR5_o.png" width="1200"></p> 
<p style="margin-left:.0001pt;text-align:center"><span style="color:#000000">（本章讨论的课题）</span></p> 
<p style="margin-left:.0001pt"><span style="color:#000000"><strong>一、卷积神经网络</strong></span></p> 
<p style="margin-left:.0001pt"><span style="color:#000000">        在对于人工智能的研究之中，科学家们将目光由限制在一定规则内的棋牌类竞技的算法研究转向了更加广阔的更加复杂多样的现实环境之中，作为人工智能的应用，它的数据来源多种多样，但是最为直接，也最为实际的便是视觉所获得的数据，如何收集并且处理视觉数据成为了人工智能研究的领域之一。</span></p> 
<p style="margin-left:.0001pt"><span style="color:#000000">        以人类的视角来看，在看到实在物体之后，经过多次观察与记忆便可以在再次看到相同物体时准确地分辨出来事物的外形特征，这是自然智能基于构造层面所给予我们的特性。但是与人类不同，传统机器并没有自然智能的一系列特性，因此它们对于物体图像的识别能力非常低。科学家们常常采取数学的方式描述这一问题，即：基于视觉自然感知与识别能力相当于将极高维度的图像数据空间中某些特征或模式信息经过感官和大脑处理后转化为知识映射到一个低维度的特征空间，在数学上可以用流型空间模型描述。</span></p> 
<p>        虽然人类对于生物神经系统的认知并非十分透彻，但针对于这个问题，科学家采取了人工神经网络的算法进行应对。并随着机器深度学习算法的不断进步，对于物体识别与图像处理的机器应用与研究方面也有很大的帮助。不仅如此，人工神经网络的算法也在不断地完善与创新。从最开始地手写数字的自动<span style="color:#000000">识别时开始传统神经网络的应用。但是这种图像识别首先需要大量的正负标记的物体样本数据，然后使用选定的神经网络和损失函数模型通过监督学习的方法进行训练，以确定网络中不同神经元的权重。学习过程中的知识是成功的关键。为了增强网络的表达能力，需要足够数量的隐藏层和神经网络中的神经元数量。由于一般图像中包含的像素数量非常多，相应的神经网络的参数量也很大。虽然理论上全连接神经网络可以将包含图像的高纬度像素空间映射到代表类别的低纬度特征空间，但实际应用中所需的神经元参数空间是巨大的，尤其是隐藏层增加了训练成本和难度也大大增加，在很大程度上影响了手写图片的识别准确率。</span></p> 
<p class="img-center"><img alt="" height="412" src="https://images2.imgbox.com/b6/d4/1VzqCj2c_o.png" width="905"></p> 
<p style="margin-left:.0001pt;text-align:center"><span style="color:#000000">（<span style="background-color:#ffffff"><span style="color:#333333">识别准确率不高原因）</span></span></span></p> 
<p style="margin-left:.0001pt;text-align:left"><span style="color:#000000">        相较于传统的神经网络带来的种种不便，法裔美国计算机学家杨立昆1989</span><span style="color:#000000">年发明了卷积神经网络。这项研究成果是人工智能深度学习历史上划时代的里程碑，其核心思想得益于美国神经学家大卫·休伯尔和托斯坦·威泽尔对动物视觉神经感知过程的研究成果，也受到日本计算机科学家福島邦彦的新认知子理论模型的启发。</span></p> 
<p class="img-center"><img alt="" height="519" src="https://images2.imgbox.com/b2/d3/DKY3t4Kh_o.png" width="605"></p> 
<p style="margin-left:.0001pt;text-align:center"><span style="color:#000000"><span style="background-color:#ffffff"><span style="color:#333333">（卷积神经网络的发展历程）</span></span></span></p> 
<p><span style="color:#000000">        卷积神经网络的基本结构由输入层、卷积层、池化层（也称为取样层）、全连接层及输出层构成。卷积层和池化层一般会取若干个，采用卷积层和池化层交替设置，即一个卷积层连接一个池化层，池化层后再连接一个卷积层，依此类推。由于卷积层中输出特征面的每个神经元与其输入进行局部连接，并通过对应的连接权值与局部输入进行加权求和再加上偏置值，得到该神经元输入值，该过程等同于卷积过程。[1]</span></p> 
<p class="img-center"><img alt="" height="306" src="https://images2.imgbox.com/cc/97/dbFmod6r_o.png" width="1048"></p> 
<p style="margin-left:.0001pt;text-align:center"><span style="color:#000000"><span style="background-color:#ffffff"><span style="color:#333333">（卷积神经网络的基本结构）</span></span></span></p> 
<p style="margin-left:.0001pt"><span style="color:#000000">        同样，图像识别的实质是将图像的像素数据经过某种压缩编码提取其中物体的语义信息，卷积神经网络发挥着关键的作用，它通过使用低维度卷积核或滤波器对图像的局部像素空间进行卷积，然后进入神经元激活来提取图像的特征，所产生图像被称为特征图。接下来可以对卷积后的特征图进行池化，即首先用滤波器在特征图按给定步长移动，每次对每个区域的元素通过“平均”或“最大”化操作进行数据压缩，从而产生一组维度更低的特征图。</span></p> 
<p style="margin-left:.0001pt"><span style="color:#000000">        卷积网络相邻连接的局部性导致整体连接的稀疏性，有助于减少训练过程中的过度拟合，提高的网络的泛化能力。使用同一滤波器以实现参数共享，不仅大大减少的网络的参数空间，也在一定程度上确保了特征对位移、拉伸和旋转的相对不变性，从而提高了算法的鲁棒性。在深度学习神经网络中，一般均采取多个“卷积-激活-池化”层分级提取图像的特征信息，不断对原始图像数据进行压缩编码最终通过完全连接网络实现图像分类与识别。[2]</span></p> 
<p style="margin-left:.0001pt;text-align:left"><span style="color:#000000">        卷积神经网络由浅到深、从局部到全局，通过不断提取和压缩图像的特征数据，最终实现对图像中数字识别的过程。</span></p> 
<p style="margin-left:.0001pt;text-align:left"><span style="color:#000000">        在这个过程中“编码-</span><span style="color:#000000">解码”的基本规则不可忽视。卷积神经网络通过不断卷积和池化编码提取图像的宏观特征和语义，但同时会损失图像的空间分辨率和局部特征。为了平衡全局物体分类和局部物体定位之间的矛盾，科学家采用了通过数据扩展的</span><span style="color:#000000">“</span><span style="color:#000000">上采样</span><span style="color:#000000">”</span><span style="color:#000000">提高图像的分辨率和获取图像的局部特征，这种方式很好地弥补了卷积池化这种数据压缩的</span><span style="color:#000000">“</span><span style="color:#000000">下采样</span><span style="color:#000000">”</span><span style="color:#000000">带来的问题。韩国浦项工科大学的研究者在标准卷积网络上附加一个对称的</span><span style="color:#000000">“</span><span style="color:#000000">反卷积</span><span style="color:#000000">”</span><span style="color:#000000">网络，对卷积网络产生的特征图进行对应的上采样，最终输出与输入图像同样尺寸和分辨率的语义分割图像。这种端到端</span><span style="color:#000000">“</span><span style="color:#000000">编码</span><span style="color:#000000">-</span><span style="color:#000000">解码</span><span style="color:#000000">”</span><span style="color:#000000">神经网络的下采样和上采样参数均通过训练学习获得</span><span style="color:#000000">,</span><span style="color:#000000">能够取得更高的语义分割精度。</span></p> 
<p style="margin-left:.0001pt;text-align:justify"><span style="color:#000000">        在图像识别与人工智能的处理过程中，先验知识也起到关键的作用。因为虽然通过人工智能的相关算法可以较为精准地识别出图像。但是针对于图像应该作出地判断与反应，例如识别出该图像具体是一个什么事物，这些事物之间的可能存在的关系，以及这些事物所代表的含义等等，却是需要先验知识作为前提，再用现有知识与之进行比对才能做出合适的判断。正如我们在“自然智能”章节中所讲到的有关情绪智能的部分。如果想让人工智能真正如电影《人工智能》里面一般，可以通过图像识别分辨出人的表情，从而分析出他的喜怒哀乐的情感。那么这个图像识别程序中就必须要有一定的先验知识，才能达到这样的境界。</span></p> 
<p class="img-center"><img alt="" height="652" src="https://images2.imgbox.com/e8/34/dyAxysl3_o.jpg" width="500"></p> 
<p style="margin-left:.0001pt;text-align:center"><span style="color:#000000"><span style="background-color:#ffffff"><span style="color:#333333">（电影《人工智能》海报）</span></span></span></p> 
<p style="margin-left:.0001pt"><span style="color:#000000"><strong>二、矛盾的解决</strong></span></p> 
<p><span style="color:#000000">        物体识别既要提取语义信息，又要获取位置信息，前者需要全局特征,而后者需要局部特征。这两个矛盾看似不可调和，实际上却存在非常丰富的解决办法。在讨论这个问题之前。我们要先明确计算机视觉的任务，它主要分为两大类：物体检测与实例分割。物体检测即确定包含目标物体边框的大小与位置并识别其所属类别，而实例分割即确定包含目标物的像素空间并识别其所属类别。物体检测或事件分割既需要识别目标物体的类别，又需要确定被识别物体的定位边框或所占据的像素空间。</span></p> 
<p style="margin-left:.0001pt;text-align:justify"><span style="color:#000000">        当只有单物体需要识别时，通过神经网络卷积与池化之后产生了一组特征图，通过两个完全连接网络，我们既可以对图像中的物体进行分类，也可以对物体所处的边框参数进行回归等等的方法解决。</span></p> 
<p style="margin-left:.0001pt;text-align:justify"><span style="color:#000000">        如果要进行多物体的识别，解决这两个矛盾的基本方法之一是先设法确定图像中可能包含不同物体的边框，再对每一个边框中的单一物体进行分类。这种先分割、在分类的策略需要找到可以包含被识别物体的边框。但是由于现实中图像中的物体呈现的情形非常复杂以及综合原因的影响，在分类之前识别和定位这些物体本身也是一件极具挑战性的问题。虽然理论上可以先将图像按某种方式分为不同尺寸的边框，希望这些边框能够以最小边框最大限度覆盖图像中包含的物体。针对这个问题，科学家们人工设计一种选择搜索的算法，对一幅给定图像产生若干个可能覆盖物体的边框。于是美国加州大学伯克利分校的博士后研究员 Girshick</span><span style="color:#000000">及合作者在骨干网络产生特征图之后再用选择搜索算法产生定位边框，从而大大减少了图像分类的重复计算。</span><span style="color:#000000">[3] </span><span style="color:#000000">虽然这种方法可以取得比较高的精度，但网络训练过程复杂且效率低。</span></p> 
<p style="margin-left:.0001pt;text-align:justify"><span style="color:#000000">        为了解决这个问题，科学家们提出了单步算法，美国华盛顿大学博士研究生Redmon </span><span style="color:#000000">等提出的</span><span style="color:#000000">“</span><span style="color:#000000">你只看一次</span><span style="color:#000000">”</span><span style="color:#000000">（</span><span style="color:#000000">YOLO</span><span style="color:#000000">）算法，基本思想是将物体识别问题视为针对不同定位边框和分类概率的回归问题，</span><span style="color:#000000">YOLO</span><span style="color:#000000">首先将输入图像划分为网格，然后以每个网格为中心定义若干个不同比例和尺寸的锚定边框，作为卷积神经网络的输入。在卷积神经网络的输出端对每个网格输出所有锚定边框与基础真相边框的相对位置和尺寸参数以及对应边框中不同类别的概率。我们只考虑信心系数大于一定阈值的锚定边框，最终只输出包含目标物体的边框，否则则认为边框所对应的是背景。</span><span style="color:#000000">[4]</span><span style="color:#000000">其应用非常广泛，如识别下面图像中自行车和小狗的应用：</span></p> 
<p class="img-center"><img alt="" height="379" src="https://images2.imgbox.com/58/39/iaP6qk9T_o.png" width="904"></p> 
<p style="margin-left:.0001pt;text-align:center"><span style="color:#000000"><span style="background-color:#ffffff"><span style="color:#333333">（</span></span><span style="background-color:#ffffff"><span style="color:#333333">YOLO</span></span><span style="background-color:#ffffff"><span style="color:#333333">算法的实际应用）</span></span></span></p> 
<p style="margin-left:.0001pt;text-align:justify"><span style="color:#000000">        为了更进一步的保证定位边框的精度，微软研究院何凯明等人于2017</span><span style="color:#000000">年发明了</span><span style="color:#000000">“</span><span style="color:#000000">掩膜</span><span style="color:#000000">R-CNN”</span><span style="color:#000000">，</span> <span style="color:#000000">在原</span><span style="color:#000000">Fast R-CNN</span><span style="color:#000000">基础上引入一个与物体检测平行的卷积神经网络对区域建议网络产生的物体区域提取掩膜。为了克服原来物体识别网络</span><span style="color:#000000">“</span><span style="color:#000000">关注区域池化</span><span style="color:#000000">”</span><span style="color:#000000">中输入图像与特征图图像像素之间产生的偏移误差，采取了线性插值方法进行校准，并以</span><span style="color:#000000">“</span><span style="color:#000000">关注区域对准</span><span style="color:#000000">”</span><span style="color:#000000">取而代之。</span><span style="color:#000000">[5]</span><span style="color:#000000">这种在包含物体的定位边框内生成物体所对应的像素空间的方法是一种</span><span style="color:#000000">“</span><span style="color:#000000">先检测再区分</span><span style="color:#000000">”</span><span style="color:#000000">的策略。这在极大程度上解决了这两者之间的矛盾。</span></p> 
<p style="margin-left:.0001pt"><span style="color:#000000">        通过本章人工智能的深入学习，让我对于人工智能在图像识别与物体感知方面的应用有了一个全新的感受。特别是作为图像识别的内核的卷积神经网络及其算法让我印象十分深刻，对于卷积神经网络在图像识别中的具体作用，编码解码的基本规则以及先验知识的重要作用的讨论强化了我对与卷积神经网络的认知，更顺着解决提取语义信息与获取位置信息的矛盾的思路，学习了解了二步法，YOLO算法，掩膜R-CNN算法等等。这些收获让我在脑海中构建了一个关于人工智能的大体框架，启发我今后不断学习去填充完善它。</span></p> 
<p style="margin-left:.0001pt"></p> 
<p style="margin-left:.0001pt"></p> 
<p style="margin-left:.0001pt;text-align:justify"><strong><span style="color:#000000">参考文献：</span></strong></p> 
<p style="margin-left:.0001pt;text-align:justify"><strong><span style="color:#000000">[1]</span><span style="color:#000000">周飞燕,金林鹏,董军.卷积神经网络研究综述[J].计算机学报,2017,40(06):1229-1251.</span></strong></p> 
<p style="margin-left:.0001pt;text-align:justify"><strong><span style="color:#000000">[2] [5]</span>黄卫平. 数据智能科学技术导论[M].北京：清华大学出版社,1-274.</strong></p> 
<p style="margin-left:.0001pt;text-align:justify"><strong><span style="color:#000000">[3] R. Girshick</span><span style="color:#000000">，Fast R-CNN, arXiv:1504.08083v2 [cs.CV] 27 Sep 2015.</span></strong></p> 
<p style="margin-left:.0001pt;text-align:justify"><strong><span style="color:#000000">[4] J. Redmon, et.al., You only look once: Unified, real-time object detection, Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779–788, arXiv:1506.02640v5 [cs.CV] 9 May 2016.</span></strong></p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>