<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>猿创征文｜机器学习实战（8）——随机森林 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">猿创征文｜机器学习实战（8）——随机森林</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="1%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-toc" style="margin-left:0px"><a href="#1%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">1 随机森林</a></p> 
<p id="2%20%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91-toc" style="margin-left:0px"><a href="#2%20%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91">2 极端随机树</a></p> 
<p id="3%20%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7-toc" style="margin-left:0px"><a href="#3%20%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7">3 特征重要性</a></p> 
<p id="4%20%E6%8F%90%E5%8D%87%E6%B3%95-toc" style="margin-left:0px"><a href="#4%20%E6%8F%90%E5%8D%87%E6%B3%95">4 提升法</a></p> 
<p id="4.1%20AdaBoost-toc" style="margin-left:40px"><a href="#4.1%20AdaBoost">4.1 AdaBoost</a></p> 
<p id="4.2%20%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87-toc" style="margin-left:40px"><a href="#4.2%20%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87">4.2 梯度提升</a></p> 
<hr id="hr-toc">
<p>机器学习实战（7）中我们已经提到，随机森林是决策树的集成，通常用bagging方法训练，训练集大小通过max_samples来设置。除了先构建一个 BaggingClassifier 然后将结果传输到 DecisionTreeClassifier ，还有一种方法就是使用 RandomForestClassifier 类（对于回归任务有RandomForestRegressor类），这种方法更方便。</p> 
<h1 id="1%20%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97">1 随机森林</h1> 
<p>常规模块的导入以及图像可视化的设置：（<strong>注意，本节是基于第7节集成学习的代码继续往下演示的</strong>）</p> 
<pre><code class="language-python"># Common imports
import numpy as np
import os
 
# to make this notebook's output stable across runs
np.random.seed(42)
 
# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)</code></pre> 
<p>以下代码训练一个拥有500棵树的随机森林分类器（其中每棵树限制为最多16个叶节点）：</p> 
<pre><code class="language-python">from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)
rnd_clf.fit(X_train, y_train)

y_pred_rf = rnd_clf.predict(X_test)

np.sum(y_pred == y_pred_rf) / len(y_pred)  # almost identical predictions</code></pre> 
<p>运行结果如下：</p> 
<pre><code class="language-python">0.936</code></pre> 
<p>除了少数例外（没有splitter（强制为random），没有presort（强制为False），没有max_samples（强制为1.0），没有base_estimator（强制为DecisionTreeClassifier）），RandomForestClassifier 具有 DecisionTreeClassifier 和 BaggingClassifier 的所有超参数，前者用来控制树的生长，后者用来控制集成本身。</p> 
<p>用 BaggingClassifier 实现与上述代码效果相同的效果：</p> 
<pre><code class="language-python">bag_clf = BaggingClassifier(
    DecisionTreeClassifier(splitter="random", max_leaf_nodes=16, random_state=42),
    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=42)

bag_clf.fit(X_train, y_train)
y_pred = bag_clf.predict(X_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_pred)</code></pre> 
<p>运行结果如下：</p> 
<pre><code class="language-python">0.92</code></pre> 
<h1 id="2%20%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91">2 极端随机树</h1> 
<p>随机森林里单棵树的生长过程中，每个节点在分裂时仅考虑到一个随机子集所包含的特征。如果我们对每个特征使用随机阈值，而不是搜索得出的最佳阈值，则可能让决策树生产得更随机。这种极端随机的决策树组成的森林被称为极端随机树。同样，它也是以更高的偏差换取了更低的方差。</p> 
<p>我们使用Scikit-Learn的 ExtraTreeClassifier 可以创建一个极端随机树分类器。</p> 
<p>我们很难预先知道一个 RandomForestClassifier 是否比一个 ExtraTreeClassifier 更好或是更差，只有两种都尝试一遍，然后使用交叉验证进行比较。</p> 
<h1 id="3%20%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7">3 特征重要性</h1> 
<p>我们查看单个决策树会发现，重要的特征更可能出现在靠近根节点的位置，而不重要的特征通常出现在靠近叶节点的位置。因此，通过计算一个特征在森林中所有树上的平均深度，就可以估算出一个特征的重要程度。Scikit-Learn 在训练结束后自动计算每个特征的重要性。</p> 
<p>例如下列代码：</p> 
<pre><code class="language-python">from sklearn.datasets import load_iris
iris = load_iris()
rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)
rnd_clf.fit(iris["data"], iris["target"])
for name, score in zip(iris["feature_names"], rnd_clf.feature_importances_):
    print(name, score)</code></pre> 
<p>运行结果如下：</p> 
<pre><code class="language-python">sepal length (cm) 0.11249225099876375
sepal width (cm) 0.02311928828251033
petal length (cm) 0.4410304643639577
petal width (cm) 0.4233579963547682</code></pre> 
<p>从结果来看，花瓣长度（44%）和宽度（42%）特征比较重要。</p> 
<p>我们前面曾用到过MNIST数据集，如果我们也训练一个随机森林分类器，然后绘制每个像素的重要性：</p> 
<pre><code class="language-python">from sklearn.datasets import fetch_openml
#加载数据
mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)
mnist.target = mnist.target.astype(np.int64)

#训练
rnd_clf = RandomForestClassifier(n_estimators=10, random_state=42)
rnd_clf.fit(mnist["data"], mnist["target"])

def plot_digit(data):
    image = data.reshape(28, 28)
    plt.imshow(image, cmap = mpl.cm.hot,
               interpolation="nearest")
    plt.axis("off")

#可视化    
plot_digit(rnd_clf.feature_importances_)

cbar = plt.colorbar(ticks=[rnd_clf.feature_importances_.min(), rnd_clf.feature_importances_.max()])
cbar.ax.set_yticklabels(['Not important', 'Very important'])

plt.show()</code></pre> 
<p>运行结果如下：</p> 
<p class="img-center"><img alt="" height="244" src="https://images2.imgbox.com/59/41/dx35cl0D_o.png" width="356"></p> 
<p>因此，我们可以通过随机森林对重要特征进行选择。</p> 
<h1 id="4%20%E6%8F%90%E5%8D%87%E6%B3%95">4 提升法</h1> 
<p>提升法是指可以将几个弱学习结合成一个强学习器的任意集成方法。</p> 
<h2 id="4.1%20AdaBoost">4.1 AdaBoost</h2> 
<p>新预测器对其前序进行纠正的办法之一，就是更多地关注前序拟合不足的训练实例。从而使新的预测器不断地越来越专注于难缠的问题，这就是 AdaBoost 使用的技术。</p> 
<p>例如，要构建一个 AdaBoost 分类器，首先需要训练一个基础分类器，用它对训练集进行预测，然后对错误分类的训练实例增加其相对权重，接着，使用这个最新的权重对第二个分类器进行训练，然后再次对训练集进行预测，继续更新权重，不断循环向前。</p> 
<p>下面代码实现卫星数据集的例子：（再次强调，本节是基于第七节集成学习，因此卫星数据集已经存在）</p> 
<pre><code class="language-python">m = len(X_train)

plt.figure(figsize=(11, 4))
for subplot, learning_rate in ((121, 1), (122, 0.5)):
    sample_weights = np.ones(m)
    plt.subplot(subplot)
    for i in range(5):
        svm_clf = SVC(kernel="rbf", C=0.05, gamma="auto", random_state=42)
        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)
        y_pred = svm_clf.predict(X_train)
        sample_weights[y_pred != y_train] *= (1 + learning_rate)
        plot_decision_boundary(svm_clf, X, y, alpha=0.2)
        plt.title("learning_rate = {}".format(learning_rate), fontsize=16)
    if subplot == 121:
        plt.text(-0.7, -0.65, "1", fontsize=14)
        plt.text(-0.6, -0.10, "2", fontsize=14)
        plt.text(-0.5,  0.10, "3", fontsize=14)
        plt.text(-0.4,  0.55, "4", fontsize=14)
        plt.text(-0.3,  0.90, "5", fontsize=14)

plt.show()</code></pre> 
<p>运行结果如下：</p> 
<p><img alt="" height="291" src="https://images2.imgbox.com/e1/4a/cZcSFiz0_o.png" width="678"></p> 
<p>图中显示了在卫星数据集上5个连续的预测器的决策边界（每个预测器都是使用RBF核函数高度正则化的SVM分类器） 。第一个分类器产生了许多错误实例，所以这些实例的权重得到提升。因此第二个分类器在这些实例上的表现有所提升，然后第三个，第四个······ 右图绘制的是相同预测器序列，唯一的差别在于学习率减半（即每次迭代仅提升一半错误分类的实例的权重）。</p> 
<p>这种依序学习技术有一个重要的缺陷就是无法并行，因为每个预测器只能在前一个预测器的训练完成并评估之后才能开始训练。</p> 
<p>下面，我们来看一下 AdaBoost 算法。每个实例的权重 <img alt="omega ^{left ( i right )}" class="mathcode" src="https://images2.imgbox.com/93/02/0bla6euB_o.gif"> 最初设置为 <img alt="frac{1}{m}" class="mathcode" src="https://images2.imgbox.com/dc/67/Hq8zSqCR_o.gif"> 。第一个预测器训练后，计算其加权误差率 <img alt="r_{1}" class="mathcode" src="https://images2.imgbox.com/d4/1f/kkDOTpZJ_o.gif">  , 公式如下：</p> 
<div class="img-center"> 
 <figure class="image">
  <img alt="" height="128" src="https://images2.imgbox.com/7e/11/jDaf0rjB_o.png" width="529">
  <figcaption>
   第j个预测器的加权误差率
  </figcaption>
 </figure>
</div> 
<p>预测器的权重 <img alt="alpha _{j}" class="mathcode" src="https://images2.imgbox.com/17/f8/aeo7bcav_o.gif"> 通过下面的公式计算，其中 <img alt="eta" class="mathcode" src="https://images2.imgbox.com/ca/9d/VaMPun2x_o.gif"> 是学习率超参数（默认为1）。预测器的准确率越高，其权重越高。</p> 
<p class="img-center"><img alt="" height="79" src="https://images2.imgbox.com/6c/ef/4xpx67sp_o.png" width="142"></p> 
<p>下面的公式对实例的权重进行更新，提升被错误分类的实例的权重。</p> 
<p class="img-center"><img alt="" height="114" src="https://images2.imgbox.com/e2/11/OqiR0UbR_o.png" width="236"></p> 
<p>然后将所有实例的权重归一化（除以<img alt="sum_{i=1}^{m} omega ^{left ( i right )}" class="mathcode" src="https://images2.imgbox.com/9e/cb/pY3lNogG_o.gif">） 。最后，使用更新后的权重训练一个新的预测器，重复该过程，直到完美的预测器出现，算法停止。</p> 
<p>预测的时候，AdaBoost 就是简单地计算所有预测器的预测结果，并使用预测器的权重 <img alt="alpha _{j}" class="mathcode" src="https://images2.imgbox.com/17/f8/aeo7bcav_o.gif">对它们进行加权。最后，得到大多数加权投票的类别就是预测器给出的预测类别。</p> 
<p class="img-center"><img alt="" height="81" src="https://images2.imgbox.com/31/b4/oOoHOe1x_o.png" width="394"></p> 
<p>下面的代码使用Scikit-Learn 的 AdaBoostClassifier 训练了一个 AdaBoost 分类器，它基于200个单层决策树。单层决策树就是 max_depth=1 的决策树，就是一个决策节点加两个叶节点。</p> 
<pre><code class="language-python">from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=1), n_estimators=200,
    algorithm="SAMME.R", learning_rate=0.5, random_state=42)
ada_clf.fit(X_train, y_train)

plot_decision_boundary(ada_clf, X, y)</code></pre> 
<p>运行结果如下：</p> 
<p class="img-center"><img alt="" height="278" src="https://images2.imgbox.com/44/03/du5uI9HY_o.png" width="408"></p> 
<h2 id="4.2%20%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87">4.2 梯度提升</h2> 
<p>与 AdaBoost 一样，梯度提升也是逐步在集成中添加预测器，每一个都对前序做出改正。不同之处在于，它不是像 AdaBoost 那样在每个迭代中调整实例权重，而是让新的预测器针对前一个预测器的残差进行拟合。</p> 
<p>我们来看一个简单的回归示例，使用决策树作为基础预测器，这种称为梯度树提升或者是梯度提升回归树。首先，我们在训练集上拟合一个 DecisionTreeRegressor ：</p> 
<p>带噪声的二次训练集：</p> 
<pre><code class="language-python">np.random.seed(42)
X = np.random.rand(100, 1) - 0.5
y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)</code></pre> 
<pre><code class="language-python">from sklearn.tree import DecisionTreeRegressor

tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg1.fit(X, y)</code></pre> 
<p>针对第一个预测器的残差，训练第二个 DecisionTreeRegressor ：</p> 
<pre><code class="language-python">y2 = y - tree_reg1.predict(X)
tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg2.fit(X, y2)</code></pre> 
<p>针对第二个预测器的残差，训练第三个 DecisionTreeRegressor ：</p> 
<pre><code class="language-python">y3 = y2 - tree_reg2.predict(X)
tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg3.fit(X, y3)</code></pre> 
<p>现在我们就有了一个包含三棵树的集成，它将所有树的预测相加，从而对新实例进行预测：</p> 
<pre><code class="language-python">X_new = np.array([[0.8]])
y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
y_pred</code></pre> 
<p>运行结果如下：</p> 
<pre><code class="language-python">array([0.75026781])</code></pre> 
<p>下面我们可视化进行分析：</p> 
<pre><code class="language-python">def plot_predictions(regressors, X, y, axes, label=None, style="r-", data_style="b.", data_label=None):
    x1 = np.linspace(axes[0], axes[1], 500)
    y_pred = sum(regressor.predict(x1.reshape(-1, 1)) for regressor in regressors)
    plt.plot(X[:, 0], y, data_style, label=data_label)
    plt.plot(x1, y_pred, style, linewidth=2, label=label)
    if label or data_label:
        plt.legend(loc="upper center", fontsize=16)
    plt.axis(axes)

plt.figure(figsize=(11,11))

plt.subplot(321)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h_1(x_1)$", style="g-", data_label="Training set")
plt.ylabel("$y$", fontsize=16, rotation=0)
plt.title("Residuals and tree predictions", fontsize=16)

plt.subplot(322)
plot_predictions([tree_reg1], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1)$", data_label="Training set")
plt.ylabel("$y$", fontsize=16, rotation=0)
plt.title("Ensemble predictions", fontsize=16)

plt.subplot(323)
plot_predictions([tree_reg2], X, y2, axes=[-0.5, 0.5, -0.5, 0.5], label="$h_2(x_1)$", style="g-", data_style="k+", data_label="Residuals")
plt.ylabel("$y - h_1(x_1)$", fontsize=16)

plt.subplot(324)
plot_predictions([tree_reg1, tree_reg2], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1) + h_2(x_1)$")
plt.ylabel("$y$", fontsize=16, rotation=0)

plt.subplot(325)
plot_predictions([tree_reg3], X, y3, axes=[-0.5, 0.5, -0.5, 0.5], label="$h_3(x_1)$", style="g-", data_style="k+")
plt.ylabel("$y - h_1(x_1) - h_2(x_1)$", fontsize=16)
plt.xlabel("$x_1$", fontsize=16)

plt.subplot(326)
plot_predictions([tree_reg1, tree_reg2, tree_reg3], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="$h(x_1) = h_1(x_1) + h_2(x_1) + h_3(x_1)$")
plt.xlabel("$x_1$", fontsize=16)
plt.ylabel("$y$", fontsize=16, rotation=0)

plt.show()</code></pre> 
<p>运行结果如下：</p> 
<p><img alt="" height="670" src="https://images2.imgbox.com/d6/ef/gUU4awiM_o.png" width="696"></p> 
<p>上图中，左侧表示这三棵树单独的预测，右侧表示集成的预测。第一行，集成只有一棵树，所以它的预测与第一棵树的预测完全相同。第二行是在第一行的残差上训练的一棵新树，右侧可见，集成的预测等于前面两棵树的预测之和。</p> 
<p>训练梯度提升回归树集成有个简单的方法，就是使用 Scikit-Learn 的 GradientBoostingRegressor 类。与 RandomForestRegressor 类似，它具有控制决策树生长的超参数以及控制集成训练的超参数。下面的代码可以创建上面的集成：</p> 
<pre><code class="language-python">from sklearn.ensemble import GradientBoostingRegressor

gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
gbrt.fit(X, y)

gbrt_slow = GradientBoostingRegressor(max_depth=2, n_estimators=200, learning_rate=0.1, random_state=42)
gbrt_slow.fit(X, y)

plt.figure(figsize=(11,4))

plt.subplot(121)
plot_predictions([gbrt], X, y, axes=[-0.5, 0.5, -0.1, 0.8], label="Ensemble predictions")
plt.title("learning_rate={}, n_estimators={}".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)

plt.subplot(122)
plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])
plt.title("learning_rate={}, n_estimators={}".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)

plt.show()</code></pre> 
<p>运行结果如下：</p> 
<p><img alt="" height="268" src="https://images2.imgbox.com/71/9b/RB0aVBYd_o.png" width="664"></p> 
<p>上图显示了低学习率的两个集成：左侧拟合训练集的树数量不足，右侧拟合训练集的树数量过度而导致过度拟合。超参数 learning_rate 对每棵树的贡献进行缩放。如果我们设置为低值，如0.1，那么就需要更多的树来拟合训练集，但是预测的泛化效果更好，这也是一种被称为收缩的正则化技术。</p> 
<p>要找到树的最佳数量，我们可以使用早期停止法。简单的实现方法就是使用 staged_predict() 方法：它在训练的每个阶段都对集成的预测返回一个迭代器。</p> 
<p>以下代码训练了一个拥有120棵树的梯度提升回归树集成，然后测量每个训练阶段的验证误差，从而找到树的最优数量，最后使用最优树重新训练一个梯度提升回归树集成：</p> 
<pre><code class="language-python">import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=49)

gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120, random_state=42)
gbrt.fit(X_train, y_train)

errors = [mean_squared_error(y_val, y_pred)
          for y_pred in gbrt.staged_predict(X_val)]
bst_n_estimators = np.argmin(errors) + 1

gbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators, random_state=42)
gbrt_best.fit(X_train, y_train)

min_error = np.min(errors)

plt.figure(figsize=(11, 4))

plt.subplot(121)
plt.plot(errors, "b.-")
plt.plot([bst_n_estimators, bst_n_estimators], [0, min_error], "k--")
plt.plot([0, 120], [min_error, min_error], "k--")
plt.plot(bst_n_estimators, min_error, "ko")
plt.text(bst_n_estimators, min_error*1.2, "Minimum", ha="center", fontsize=14)
plt.axis([0, 120, 0, 0.01])
plt.xlabel("Number of trees")
plt.title("Validation error", fontsize=14)

plt.subplot(122)
plot_predictions([gbrt_best], X, y, axes=[-0.5, 0.5, -0.1, 0.8])
plt.title("Best model (%d trees)" % bst_n_estimators, fontsize=14)

plt.show()</code></pre> 
<p>运行结果如下：</p> 
<p><img alt="" height="286" src="https://images2.imgbox.com/3d/da/skghNe7O_o.png" width="669"></p> 
<p>左图是验证误差，右图是最后的预测模型。</p> 
<p><span style="color:#4da8ee"><strong>学习笔记——《机器学习实战：基于Scikit-Learn和TensorFlow》</strong></span></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>