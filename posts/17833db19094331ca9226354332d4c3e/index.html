<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Hive语法及进阶 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hive语法及进阶</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="-toc" style="margin-left:80px"></p> 
<p id="1%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E2%80%94%E2%80%94%E5%BB%BA%E8%A1%A8-toc" style="margin-left:80px"><a href="#1%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E2%80%94%E2%80%94%E5%BB%BA%E8%A1%A8" title="1、Hive基本操作——建表">1、Hive基本操作——建表</a></p> 
<p id="2%E3%80%81Hive%E2%80%94%E2%80%94%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE(%E4%B8%8A%E4%BC%A0%E5%88%B0hive%E8%A1%A8)-toc" style="margin-left:80px"><a href="#2%E3%80%81Hive%E2%80%94%E2%80%94%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%28%E4%B8%8A%E4%BC%A0%E5%88%B0hive%E8%A1%A8%29" title="2、Hive——加载数据(上传到hive表)">2、Hive——加载数据(上传到hive表)</a></p> 
<p id="3%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E5%8C%BA-toc" style="margin-left:80px"><a href="#3%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E5%8C%BA" title="3、Hive基本语法——分区">3、Hive基本语法——分区</a></p> 
<p id="4%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA-toc" style="margin-left:80px"><a href="#4%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA" title="4、Hive基本语法——动态分区">4、Hive基本语法——动态分区</a></p> 
<p id="5%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E6%A1%B6-toc" style="margin-left:80px"><a href="#5%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E6%A1%B6" title="5、Hive基本语法——分桶">5、Hive基本语法——分桶</a></p> 
<p id="6%E3%80%81Hive%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-toc" style="margin-left:80px"><a href="#6%E3%80%81Hive%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B" title="6、Hive的数据类型">6、Hive的数据类型</a></p> 
<p id="7%E3%80%81Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0-toc" style="margin-left:80px"><a href="#7%E3%80%81Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0" title="7、Hive高级函数">7、Hive高级函数</a></p> 
<p id="8%E3%80%81Hive%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0-toc" style="margin-left:80px"><a href="#8%E3%80%81Hive%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0" title="8、Hive开窗函数">8、Hive开窗函数</a></p> 
<p id="9%E3%80%81Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0-toc" style="margin-left:80px"><a href="#9%E3%80%81Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0" title="9、Hive自定义函数">9、Hive自定义函数</a></p> 
<p id="10%E3%80%81Hive-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96%C2%A0-toc" style="margin-left:80px"><a href="#10%E3%80%81Hive-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96%C2%A0" title="10、Hive-数据倾斜优化 ">10、Hive-数据倾斜优化 </a></p> 
<hr id="hr-toc">
<h3 id="1%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%E2%80%94%E2%80%94%E5%BB%BA%E8%A1%A8">1、Hive基本操作——建表</h3> 
<blockquote> 
 <p>内部表基础建表语句一:(默认指定文件类型为TextFile,HDFS路径为/user/hive/warehouse/库/下)<br>     格式:</p> 
 <pre><code class="language-sql"> create table 表名
      (
         字段名1 字段类型1,
         字段名2 字段类型2,
           ...
      )
      row format delimited fields terminated by '分隔符';  // 必选指定列之间的分隔符</code></pre> 
</blockquote> 
<blockquote> 
 <p><br> 内部表基础建表语句二:(HDFS路径为/user/hive/warehouse/库/下)<br>     格式:<br>  </p> 
 <pre><code class="language-sql">create table 表名
     (
        字段名1 字段类型1,
        字段名2 字段类型2,
         ...
     )
      row format delimited fields terminated by '分隔符'// 必选指定列之间的分隔符
      stored as file_format；
概述:
     stored as:指定具体的从HDFS获取数据的格式,格式不匹配无法获取(默认为TextFile)</code></pre> 
</blockquote> 
<blockquote> 
 <p>内部表基础建表语句三:(HDFS路径为/user/hive/warehouse/库/下)<br>     格式:</p> 
 <pre><code class="language-sql">create table 表名
     (
         字段名1 字段类型1,
         字段名2 字段类型2,
          ...
     )
      row format delimited fields terminated by '分隔符'  // 必选指定列之间的分隔符
      stored as file_format
      location 'HDFS路径';
  概述:   
      location:表示指定hive数据在hdfs的路径, 如果路径不存在会自动创建,存在就直接绑定,不能通过 
      hdfs路径判断是否是hive表
  注意:
     以上创建的都是内部表， 默认情况下 ，删除表会把数据也删除</code></pre> 
</blockquote> 
<blockquote> 
 <p><strong>外部表external       </strong><br>   格式:<br>  </p> 
 <pre><code class="language-sql"> create external table 表名
     (
         字段名1 字段类型1,
         字段名2 字段类型2,
          ...
      )
       row format delimited fields terminated by '分隔符'  // 必选指定列之间的分隔符
       stored as file_format
       location 'HDFS路径';
 注意:
    删除外部表不会删除HDFS上的数据</code></pre> 
</blockquote> 
<p></p> 
<h3 id="2%E3%80%81Hive%E2%80%94%E2%80%94%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE(%E4%B8%8A%E4%BC%A0%E5%88%B0hive%E8%A1%A8)">2、Hive——加载数据(上传到hive表)<br>  </h3> 
<blockquote> 
 <p>上传数据方式1:<br>     格式:</p> 
 <pre><code class="language-sql">​hadoop dfs -put linux路径 hdfs路径;</code></pre> 
</blockquote> 
<blockquote> 
 <p>  上传数据方式2:(直接在hive命令行操作)<br>     格式:</p> 
 <pre><code class="language-sql">​dfs -put linux路径 hdfs路径;
优点:比在hadoop操作hdfs快的多</code></pre> 
</blockquote> 
<blockquote> 
 <p> 上传数据方式3:(直接在hive命令行操作)<br>     格式:</p> 
 <pre><code class="language-sql">​load data inpath '/HDFS路径' into 表名(自动找到hdfs的路径);
注意:对于hive来说是加载,对于HDFS来说是移动</code></pre> 
</blockquote> 
<blockquote> 
 <p>  上传数据方式4:(直接在hive命令行操作)<br>     格式:</p> 
 <pre><code class="language-sql">​load data local inpath '/HDFS路径' into 表名(自动找到hdfs的路径);
注意:从本地上传数据</code></pre> 
</blockquote> 
<blockquote> 
 <p>  上传数据方式5:(直接在hive命令行操作)<br>     格式:</p> 
 <pre><code class="language-sql">create table  表名 as 查询语句
create table student as select * from studentsText;
注意:只能创建内部表</code></pre> 
</blockquote> 
<blockquote> 
 <p>  上传数据方式6:(直接在hive命令行操作)<br>     格式1:</p> 
 <pre><code class="language-sql">insert  into 表名 查询语句
insert into students select * from studentsText;</code></pre> 
 <p>  格式2:</p> 
 <pre><code class="language-sql">insert  overwrite table  表名 查询语句
insert overwrite table students select * from studentsText;
注意:数据对数据,会有格式的转化</code></pre> 
</blockquote> 
<p></p> 
<h3 id="3%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E5%8C%BA">3、Hive基本语法——分区</h3> 
<blockquote> 
 <p>分区:避免全表扫描<br>     格式:</p> 
 <pre><code class="language-sql"> create external table 表名
       (
           字段名1 字段类型1,
            字段名2 字段类型2,
                ...
        )pratition by (分区字段 字段类型)
         row format delimited fields terminated by '分隔符'// 必选指定列之间的分隔符
         stored as file_format
         location 'HDFS路径';
 注意: 分区字段和普通字段没区别(不能重复)</code></pre> 
</blockquote> 
<blockquote> 
 <p>    添加分区:</p> 
 <pre><code class="language-sql"> alter table 表名 add partition(分区字段='值');</code></pre> 
 <p> 删除分区:</p> 
 <pre><code class="language-sql">alter table 表名 drop partition(分区字段='值');</code></pre> 
 <p> 查看分区：</p> 
 <pre><code class="language-sql">select distinct 分区字段 from 表名;
show partitions 表; </code></pre> 
 <p>  插入数据（需指定分区）:</p> 
 <pre><code class="language-sql">load data local inpath '路径' into table 表名 partiton(分组字段='值');
注意：分区不存在自动创建</code></pre> 
</blockquote> 
<p></p> 
<h3 id="4%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%8A%A8%E6%80%81%E5%88%86%E5%8C%BA">4、Hive基本语法——动态分区</h3> 
<blockquote> 
 <p>  set hive.exec.dynamic.partition=true; //开启hive动态分区,hive默认不支持动态分区<br>   set hive.exec.dynamic.partition.mode=nostrict; //动态分区模式  动静结合<br>   set hive.exec.max.dynamic.partitions.pernode=1000; //hive最大分区数</p> 
</blockquote> 
<blockquote> 
 <p>  insert into 分区表 分区字段(字段名称) select * from 表<br>   注意:分区字段不会按照名字匹配，而是按照位置(匹配查询到的最后一个字段)<br>   例如：</p> 
 <pre><code class="language-sql">​insert into 分区表 partition(dt_year,dt_month) 

           select id,name,age,gender,clazz,year,month from 表</code></pre> 
</blockquote> 
<p>    </p> 
<h3 id="5%E3%80%81Hive%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95%E2%80%94%E2%80%94%E5%88%86%E6%A1%B6">5、Hive基本语法——分桶</h3> 
<p></p> 
<p>分桶(动态的,hive默认不支持分桶)</p> 
<p>  set hive.enforce.bucketing=true</p> 
<blockquote> 
 <p>  格式:     </p> 
 <pre><code class="language-sql">​create external table 表名

       (
            字段名1 字段类型1,
            字段名2 字段类型2,
            ...

       )clustered by (分桶字段) into 分桶数量 buckets

        row format delimited fields terminated by '分隔符'// 必选指定列之间的分隔符

        stored as file_format

        location 'HDFS路径';</code></pre> 
</blockquote> 
<p>  注意:</p> 
<p>    分桶字段来源普通字段,分同数量是明确的</p> 
<p>分桶加载数据:</p> 
<p>  load:不会触发分桶</p> 
<p>  insert into:自动触发分桶</p> 
<p>注意:</p> 
<p>  分桶:不常用,因为对文件进行切分产生过多的小文件  </p> 
<p>  分区:最多三级分区</p> 
<p><br>  </p> 
<h3 id="6%E3%80%81Hive%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">6、Hive的数据类型</h3> 
<p>1）基本数据类型</p> 
<blockquote> 
 <p>数值型</p> 
 <p>TINYINT — 微整型，只占用1个字节，只能存储0-255的整数。</p> 
 <p>SMALLINT– 小整型，占用2个字节，存储范围–32768 到 32767。</p> 
 <p>INT– 整型，占用4个字节，存储范围-2147483648到2147483647。</p> 
 <p><strong>BIGINT– 长整型</strong>，占用8个字节，存储范围-2^63到2^63-1。</p> 
 <p>布尔型</p> 
 <p>BOOLEAN — TRUE/FALSE</p> 
 <p>浮点型</p> 
 <p>FLOAT– 单精度浮点数。</p> 
 <p>DOUBLE– 双精度浮点数。</p> 
 <p>字符串型</p> 
 <p><strong>STRING– 不设定长度。</strong></p> 
</blockquote> 
<p>2）日期类型：</p> 
<blockquote> 
 <p>1，Timestamp 格式“YYYY-MM-dd HH:mm:ss.fffffffff”（9位小数位精度）</p> 
 <p>2，Date DATE值描述特定的年/月/日，格式为"YYYY-MM-dd"。</p> 
 <p>时间戳——&gt;日期</p> 
 <pre><code class="language-sql">select from unixtime(时间戳,'YYYY-MM-dd');</code></pre> 
 <p>日期——&gt;时间戳</p> 
 <pre><code class="language-sql">​select unix_timestamp('2021年9月26日 16时00分21秒','YYYY年MM月dd日 HH时mm分ss秒');
注意：前后格式要一样，后面格式如果没给，前面只能给默认格式：'2021-9-26 16:00:21'</code></pre> 
 <pre><code class="language-sql">create table testDate(
     ts timestamp,
     dt date
)row format delimited fields terminated by ',';

//2021-01-14 14:24:57.200, 2021-01-11</code></pre> 
</blockquote> 
<p>3）复杂数据类型： Structs，Maps，Arrays</p> 
<blockquote> 
 <p>**array</p> 
 <pre><code class="language-sql">create table testArray(
    name string,
    weight array&lt;string&gt;
)row format delimited fields terminated by 't'
collection items terminated by ',';

//zs    120,130,140
//ls    160,170,180</code></pre> 
 <pre><code class="language-sql">select * from testArray;

zs    ["110","120","130"]
ls    ["170","160","180"]</code></pre> 
 <pre><code class="language-sql">select weight[0] from testArray;

110
170</code></pre> 
</blockquote> 
<blockquote> 
 <p>*map</p> 
 <pre><code class="language-sql">create table scoreMap(
    name string,
    score map&lt;string,int&gt;
)row format delimited fields terminated by 't'
collection items terminated by ','
map keys terminated by ':';

//zs     score:80,teacher:xm,money:1000
//ls     score:66,teacher:xh,money:10000</code></pre> 
 <pre><code class="language-sql">​select * from scoreMap;

zs    {"score":80,"teacher":null,"money":1000}
ls    {"score":66,"teacher":null,"money":10000}</code></pre> 
 <pre><code class="language-sql">​select score['money'] from scoreMap;

1000
10000    </code></pre> 
</blockquote> 
<blockquote> 
 <p>​*struct </p> 
 <pre><code class="language-sql">create table scoreStruct(
    name string,
    score struct&lt;course:string,score:int,course_id:int,teacher:string&gt;
)row format delimited fields terminated by 't'
 collection items terminated by ',';

//zs    数学,80,1001,小明
//ls    语文,90,1002,小红​</code></pre> 
 <pre><code class="language-sql">select * from scoreStruct;

zs    {"course":"数学","score":80,"course_id":1001,"teacher":"小明"}
ls    {"course":"语文","score":90,"course_id":1002,"teacher":"小红"}</code></pre> 
 <pre><code class="language-sql">select score.score from scoreStruct;

80
90</code></pre> 
</blockquote> 
<p>hive转json格式:</p> 
<blockquote> 
 <p><strong>get_json_object</strong></p> 
 <pre><code class="language-sql">select get_json_object('{"course":"数学","score":80,"course_id":1001,"teacher":"小明"}','$.course');

数学
$:表示当前json转化之后的对象</code></pre> 
</blockquote> 
<p></p> 
<h3 id="7%E3%80%81Hive%E9%AB%98%E7%BA%A7%E5%87%BD%E6%95%B0">7、Hive高级函数</h3> 
<p><strong> 1）行转列</strong><br> lateral view用于和split, explode等UDTF一起使用，它能够将一行数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p> 
<blockquote> 
 <p>对数组展开<br> zs    ["110","120","130"]</p> 
 <pre><code class="language-sql">select explode(weight) from testArray;</code></pre> 
 <p>110<br> 120<br> 130<br> 170<br> 160<br> 180</p> 
 <pre><code class="language-sql">select name,new_col from testArray lateral view explode(weight) sum as new_col; </code></pre> 
 <p>zs    110<br> zs    120<br> zs    130<br> ls    170<br> ls    160<br> ls    180</p> 
 <p>对map展开<br> zs    {"score":80,"teacher":null,"money":1000}</p> 
 <pre><code class="language-sql">select explode(score) from scoreMap;</code></pre> 
 <p>score    80<br> teacher    NULL<br> money    1000<br> score    66<br> teacher    NULL<br> money    10000</p> 
 <pre><code class="language-sql">select name,col1,col2 from scoreMap lateral view explode(score) t1 as col1,col2;</code></pre> 
 <p>zs    score    80<br> zs    teacher    NULL<br> zs    money    1000<br> ls    score    66<br> ls    teacher    NULL<br> ls    money    10000</p> 
</blockquote> 
<p><strong>2）列转行</strong><br> collect_list</p> 
<blockquote> 
 <p>zs    110<br> zs    120<br> zs    130<br> ls    160<br> ls    170<br> ls    180</p> 
 <pre><code class="language-sql">create table test(
    name string,
    age string
)row format delimited fields terminated by ',';</code></pre> 
 <pre><code class="language-sql">select name,collect_list(age) from test group by name;

ls    ["160","170","180"]
zs    ["110","120","130"]</code></pre> 
</blockquote> 
<h3 id="8%E3%80%81Hive%E5%BC%80%E7%AA%97%E5%87%BD%E6%95%B0">
<br><strong>8、Hive开窗函数</strong>
</h3> 
<p><strong>1）窗口函数</strong></p> 
<blockquote> 
 <pre><code class="language-sql">create external table students(
    id string,
    name string,
    age string,
    gender string,
    clazz string
)
row format delimited fields terminated by ',';</code></pre> 
 <p>1）row_number()：无并列排名<br> 通过某些操作对数据一一打上行号，over(重写的规则，按性别打行号，按年龄从大到小排序)</p> 
 <pre><code class="language-sql">select * ,row_number() over(partition by gender order by age desc) as num from students;</code></pre> 
 <p>分别求男女年龄前三<strong>（一般用于分组中求 TopN）</strong></p> 
 <pre><code class="language-sql">select * from (select * ,row_number() over(partition by gender order by age desc) as num from students) as s where s.num&lt;=3;</code></pre> 
 <p>2）desc_rank:有并列排名，依次递增<br> 3）rank:有并列排名，不依次递增<br> 4）percent_rank:(rank的结果-1)/(分区内数据的个数-1)</p> 
 <pre><code class="language-sql">select *

         ,row_number() over(partition by gender order by age desc) as num

         ,desc_rank() over(partition by gender order by age desc) as rank1

         ,rank() over(partition by gender order by age desc) as rank2

         ,percent_rank() over(partition by gender order by age desc) as rank3

from students;</code></pre> 
</blockquote> 
<p><strong>2）窗口帧：用于从分区中选择指定的多条记录，供窗口函数处理</strong></p> 
<p>current row :当前行</p> 
<p>unbounded:无界限</p> 
<blockquote> 
 <p>1、按行号划分 rows</p> 
 <p>1）rows格式1：前一行+当前行+后一行</p> 
 <pre><code class="language-sql">avg(score) over (partition by clazz order by score desc rows between 1 preceding and 1 following) as avg1,</code></pre> 
 <p>2）rows格式2：前两行+当前行</p> 
 <pre><code class="language-sql">max(score) over (partition by clazz order by score desc rows between 2 preceding and current row) as max1,</code></pre> 
 <p>2、按值范围划分，窗口大小不固定 range</p> 
 <p>range格式：如果当前值在80，取值就会落在范围在80-2=78和80+2=82组件之内的行</p> 
 <pre><code class="language-sql">avg(score) over (partition by clazz order by score desc range between 2 preceding and 2 following) as avg2,
</code></pre> 
</blockquote> 
<blockquote> 
 <pre><code class="language-sql">create table testwos(

        id string,

        score string,

        clazz string

)row format delimited fields terminated by ',';</code></pre> 
 <p>id score clazz</p> 
 <p>111,69,class1</p> 
 <p>113,74,class1</p> 
 <p>216,74,class1</p> 
 <p>112,80,class1</p> 
 <p>215,82,class1</p> 
 <p>212,83,class1</p> 
 <p>211,93,class1</p> 
 <p>115,93,class1</p> 
 <p>213,94,class1</p> 
 <p>114,94,class1</p> 
 <p>214,94,class1</p> 
 <p>124,70,class2</p> 
 <p>121,74,class2</p> 
 <p>223,74,class2</p> 
 <p>222,78,class2</p> 
 <p>123,78,class2</p> 
 <p>224,80,class2</p> 
 <p>225,85,class2</p> 
 <p>221,99,class2</p> 
</blockquote> 
<p><strong>3）Windows as 的使用</strong></p> 
<p>格式：Windows 变量名 as (变量)；将经常使用的字段定义</p> 
<p>原代码</p> 
<pre><code class="language-sql">select *,

row_number() over (partition by clazz order by score desc) as num1,

rank() over (partition by clazz order by score desc) as num2

from testwos;</code></pre> 
<p> 改进后 </p> 
<pre><code class="language-sql">select *,

row_number() over w as num1,

rank() over w as num2

from testwos

Window w as (partition by clazz order by score desc) ;</code></pre> 
<p><strong>4）with as的使用</strong></p> 
<p><strong>格式：with 表名 as (查询语句)</strong></p> 
<p>例子：想要求学生总分，且显示学生信息</p> 
<p>对students和score两张表做一个关联</p> 
<pre><code class="language-sql">create table score(

id string,

cou_id string,

score bigint

)row format delimited fields terminated by ',';</code></pre> 
<blockquote> 
 <p>4个job任务</p> 
 <p>//mysql版本的原始语句</p> 
 <pre><code class="language-sql">select * from (select id,sum(score)as score from (select students.*,score.score from students left join score on students.id= score.id)as stu group by id) as s1

left join (select students.*,score.score from students left join score on students.id= score.id) as s2

on s1.id=s2.id;</code></pre> 
 <p>//hive版本的使用with as后（定义一条查询语句）</p> 
 <pre><code class="language-sql">with stu as (select students.*,score.score from students left join score on students.id=score.id)

select * from (select id,sum(score)as score from stu group by id) as s1

left join stu as s2

on s1.id=s2.id;</code></pre> 
 <p>//hive版本的使用with as后（定义两条查询语句）</p> 
 <pre><code class="language-sql">with stu as (select students.*,score.score from students left join score on students.id=score.id),

s1 as (select id,sum(score) as score from stu group by id)

select * from s1

left join stu

on s1.id=stu.id;</code></pre> 
</blockquote> 
<p></p> 
<h3 id="9%E3%80%81Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0">9、Hive自定义函数</h3> 
<p>1) UDF 一进一出</p> 
<blockquote> 
 <p>* 创建maven项目，并加入依赖</p> 
 <p>        &lt;dependency&gt;</p> 
 <p>            &lt;groupId&gt;org.apache.hive&lt;/groupId&gt;</p> 
 <p>            &lt;artifactId&gt;hive-exec&lt;/artifactId&gt;</p> 
 <p>            &lt;version&gt;1.2.1&lt;/version&gt;</p> 
 <p>        &lt;/dependency&gt;</p> 
</blockquote> 
<p>参数传入字符串 </p> 
<pre><code class="language-java">​
package com.shujia.udf;

import org.apache.hadoop.hive.ql.exec.UDF;

/**
 * 一进一出
 * "hello"——&gt;"$hello#"
 * 1001,zs
 * select id,fun01(name) from students;
 * 1001,$zs#
 */
public class TestUDF extends UDF {
    public String evaluate(String str){
        String newstr="$"+str+"#";
        return newstr;
    }
}

​</code></pre> 
<p>参数传入数组</p> 
<pre><code class="language-java">​

package com.shujia.udf;

import org.apache.hadoop.hive.ql.exec.UDF;

import java.util.ArrayList;

/**
 * [120,130,140]
 * 120-130-140
 *
 * select fun02(weight) from testArray;
 * 110-120-130
 * 170-160-180
 */

public class UDFArray extends UDF {
    public String evaluate(ArrayList&lt;String&gt; arr){
        String newStr="";
        for (String s : arr) {
            newStr=newStr+"-"+s;
        }
        return newStr.substring(1);
    }
}</code></pre> 
<p> 参数传入可变参数</p> 
<pre><code class="language-java">package com.shujia.udf;

import org.apache.hadoop.hive.ql.exec.UDF;

import java.util.ArrayList;

/**
 *  UDF可以进多个参数，但只能一出
 *  select fun04('123','456','789');
 *  ["123","456","789"]
 */

public class MyUDF04 extends UDF {
    //可变参数实际上是一个数组String[]
    public ArrayList&lt;String&gt; evaluate(String ...args){
        ArrayList&lt;String&gt; list = new ArrayList&lt;String&gt;();
        for (String s : args) {
            list.add(s);
        }
        return list;
    }
}</code></pre> 
<blockquote> 
 <p>代码实现自定义hive函数 UDF函数</p> 
 <p>1）导包</p> 
 <p>2）当前类继承hive自带的UDF类</p> 
 <p>3）重写evaluate函数</p> 
 <p>4）打包上传，hive需要加载jar资源</p> 
 <p> * hive shell中，使用 add jar 路径 jar包作为资源添加到hive环境中</p> 
 <pre><code class="language-sql">   add jar /usr/local/soft/jars/HiveUDF2-1.0.jar;</code></pre> 
 <p>5）hive中注册UDF函数</p> 
 <p> * 使用jar包资源注册一个临时函数，fun01是你的函数名，'com.shujia.udf.TestUDF'是主类名</p> 
 <pre><code class="language-sql">   create temporary function fun01 as 'com.shujia.udf.TestUDF';</code></pre> 
 <p></p> 
 <p> * 使用函数名处理数据</p> 
 <pre><code class="language-sql"> select fun01('123');</code></pre> 
 <p> $123#</p> 
 <pre><code class="language-sql"> select id,fun01(name) from students;</code></pre> 
</blockquote> 
<p>2）UDTF一进多出</p> 
<pre><code class="language-java">package com.shujia.udtf;

/**
 * 一进多出
 * [zs,18]
 * col1 col2
 * String int
 */
public class MyUDTF01 extends GenericUDTF {
    //指定进来的数据类型，返回的数据类型，返回的数据列名
    @Override
    public StructObjectInspector initialize(StructObjectInspector argOIs) throws UDFArgumentException {
        //以数组存储多出的字段名
        ArrayList&lt;String&gt; filedNames = new ArrayList&lt;String&gt;();
        //以数组存储多出的字段类型
        ArrayList&lt;ObjectInspector&gt; filedObj = new ArrayList&lt;ObjectInspector&gt;();
        filedNames.add("col1");
        filedObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        filedNames.add("col2");
        filedObj.add(PrimitiveObjectInspectorFactory.javaStringObjectInspector);
        return ObjectInspectorFactory.getStandardStructObjectInspector(filedNames, filedObj);
    }

    @Override
    /**
     * 写逻辑的
     * 接受复杂数据 Java没办法解析 通过GenericUDTF自带的方法去解析
     * select fudtf01("key1:value1,key2:value2");
     * key1	value1
     * key2	value2
     */
    public void process(Object[] objects) throws HiveException {
        for (int i = 0; i &lt;objects.length ; i++) {
            String s = objects[i].toString();
            String[] split = s.split(",");//["zs:18","ls:25"]
            for (String s1 : split) {
                String[] split1 = s1.split(":");
                //多出 不再走Java返回值（因为Java返回值只有一个）
                forward(split1);//hive 中没有object,需要指定返回的类型
            }
        }
    }

}
</code></pre> 
<p></p> 
<h3 id="10%E3%80%81Hive-%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E4%BC%98%E5%8C%96%C2%A0">10、Hive-数据倾斜优化 </h3> 
<p>数据倾斜解决 ：看下key的分布 处理集中的key</p> 
<p>原因：</p> 
<p>1）key分布不均匀（实际上还是重复） 比如 group by 或者 distinct的时候</p> 
<p>2）数据重复，join 笛卡尔积 数据膨胀</p> 
<p>表现：</p> 
<p>任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。 单一reduce的记录数与平均记录数差异过大，通常可能达到3倍甚至更多。 最长时长远大于平均时长。</p> 
<p>解决方案：</p> 
<p>1）看下业务上，数据源头能否对数据进行过滤，比如 key为 null的，业务层面进行优化。</p> 
<p>2）找到key重复的具体值，进行拆分，hash。异步求和。</p> 
<blockquote> 
 <p>异步求和</p> 
 <p>1、数据打散</p> 
 <p>2、正常求和</p> 
 <p>3、和相加</p> 
 <p></p> 
 <p>遇见了count(1)这种写法，什么意思？</p> 
 <p><strong>count(*) </strong>统计所有行，不会忽略列值为NULL  </p> 
 <p><strong>count(1)</strong> 也是查询所有行总数，与count(*) 结果一样，在统计结果的时候，不会忽略列值为NULL  </p> 
 <p><strong>count(列名) </strong>只包括列名那一列，在统计结果的时候，会忽略列值为null的计数</p> 
 <p></p> 
 <p>0~1 rand()</p> 
 <p>0~9 rand()*10</p> 
 <p>向下取整 ceil(rand()*10)</p> 
</blockquote> 
<p>原来的数据</p> 
<pre><code class="language-sql"> select id,count(1) from bigtest group by id;</code></pre> 
<p><img alt="" height="190" src="https://images2.imgbox.com/52/fc/AJ97nsiV_o.png" width="208"></p> 
<p></p> 
<p>数据打散</p> 
<p>将id为84401和null的字段打散</p> 
<pre><code class="language-sql">with bd as (select *,if(id='84401' or id = 'null',ceil(rand()*10),id) as index from bigtest)

select id,index,count(1) as c from bd group by id,index;</code></pre> 
<p><img alt="" height="451" src="https://images2.imgbox.com/da/84/MVXSHt9p_o.png" width="220"></p> 
<p></p> 
<p>正常求和</p> 
<pre><code class="language-sql">with bd as (select *,if(id='84401' or id = 'null',ceil(rand()*10),id) as index from bigtest)

      ,bd1 as (select id,index,count(1) as c from bd group by id,index)

select id,sum(c) from bd1 group by id;</code></pre> 
<p> <img alt="" height="172" src="https://images2.imgbox.com/9a/d1/xJoMbIDm_o.png" width="182"></p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>