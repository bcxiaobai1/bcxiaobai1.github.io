<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>基于BP神经网络识别手写字体MINST字符集 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于BP神经网络识别手写字体MINST字符集</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <p><a href="https://download.csdn.net/download/sheziqiong/85771938">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85771938<br> <a href="https://download.csdn.net/download/sheziqiong/85771938">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85771938</p> 
<h2>
<a id="BP_2"></a>BP神经网络识别手写字体</h2> 
<h3>
<a id="_4"></a>导言</h3> 
<ol>
<li> <p><strong>问题描述：</strong><br>   本次实验所要解决的问题是使用人工神经网络实现识别手写字体。实验采用MINST手写字符集作为识别对象。其中60000张作为训练集，剩余10000张作为测试集。实验采用python语言进行编程，使用到一些python的第三方库。使用的神经网络模型为BP神经网络，这是一种按照误差逆向传播算法训练的多层前馈神经网络。而其逆向传播过程使用了小批量梯度下降法（MBGD）。本次实验中使用的是含隐藏层的784*30*10的网络模型，在此模型下，学习率为3的情况下，大概30次学习后可以取得95%的正确率。其余结果详解结果分析。</p> </li>
<li> <p><strong>背景介绍：</strong><br>   <strong>1）识别手写字体</strong>：字符识别是图像识别领域中的一个非常活跃的分支，一方面是由于问题本身的难度使之成为一个极具挑战性的课题，另一方面，是因为字符识别不是一门孤立的应用技术，其中包含了模式识别领域的其它分支都会遇到的一些基本的、共性的问题。也正是由于字符识别技术的飞速发展，才促使模式识别和图像分析发展成为一个成熟的科学领域。<br>   <strong>2）人工神经网络</strong>：人工神经网络是20世纪80 年代以来人工智能领域兴起的研究热点。它从信息处理角度对人脑神经元网络进行抽象， 建立某种简单模型，按不同的连接方式组成不同的网络。神经网络是一种运算模型，由大量的节点（或称神经元）之间相互联接构成。每个节点代表一种特定的输出函数，称为激励函数。每两个节点间的连接都代表一个对于通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则因网络的连接方式，权重值和激励函数的不同而不同。而网络自身通常都是对自然界某种算法或者函数的逼近，也可能是对一种逻辑策略的表达。人工神经网络是由大量处理单元互联组成的非线性、自适应信息处理系统。它是在现代神经科学研究成果的基础上提出的，试图通过模拟大脑神经网络处理、记忆信息的方式进行信息处理。<br>   <strong>3）BP神经网络</strong>：BP神经网络是一种按误差逆传播算法训练的多层前馈网络，是目前应用最广泛的神经网络模型之一。BP网络能学习和存贮大量的输入-输出模式映射关系，而无需事前揭示描述这种映射关系的数学方程。它的学习规则是使用最速下降法，通过反向传播来不断调整网络的权值和阈值，使网络的误差平方和最小。BP神经网络模型拓扑结构包括输入层（input）、隐层(hide layer)和输出层(output layer)。<br>   <strong>4）梯度下降法</strong>：梯度下降法是一个最优化算法，通常也称为最速下降法。梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）。梯度下降法又有批量梯度下降法BGD、随机梯度下降法SGD、小批量梯度下降法MBGD。</p> </li>
</ol> 
<h3>
<a id="_14"></a>实验过程</h3> 
<ol>
<li> <p><strong>算法理论部分</strong><br>   在本次实验中的神经网络，我们对节点的激励函数采用常用的<strong>sigmoid函数</strong>。这个函数如下所示：<br> <img src="https://images2.imgbox.com/cd/62/s7S8Dp4r_o.png" alt=""><br>   它的导数可求，且可以表示为如下所示：<br> <img src="https://images2.imgbox.com/53/e5/fCTEC2Rw_o.png" alt=""><br>   我们使用误差函数来描述正确结果与网络输出的差值，并对这个误差函数求偏导作为每一个神经网络中更新权重以及偏移量的依据。这里使用如下的误差函数：<br> <img src="https://images2.imgbox.com/55/03/EvSHR6J2_o.png" alt=""><br>   其中T为正确结果，O为神经网络输出的结果。<br>   假设BP神经网络模型如下所示：<br> <img src="https://images2.imgbox.com/ae/84/mofVN5mY_o.png" alt=""><br>   给定隐藏层或输出层的单元 j，单位j的净输入Ij为：<br> <img src="https://images2.imgbox.com/43/c2/wCVTJqS2_o.png" alt=""><br>   wij是从上一层单元i到单元j的连接权重;，Oi是上一层单元i的输出，θj是j单元的偏置.<br>   给定单元j的输入Ij， 则单位j的输出Oj的公式如下，即使用sigmoid函数作为激励函数.<br> <img src="https://images2.imgbox.com/ae/b7/EmTJrCaV_o.png" alt=""><br>   为了调整权重 wjk,我们首先计算在E关于wjk的偏导数，这里可以使用求偏导的链式法则。然后利用这个值修正权重。<br> <img src="https://images2.imgbox.com/9d/50/vPsKeI77_o.png" alt=""><br>   同样的，如果要调整偏移量，也是计算误差函数E关于其的偏导数。<br>   最终，由计算可得，对于输出层的单元k，误差梯度可表示如下：<br> <img src="https://images2.imgbox.com/81/7d/f6qlmNrL_o.png" alt=""><br>   隐藏层单元 j 的误差梯度为：<br> <img src="https://images2.imgbox.com/14/26/wc9M2tzA_o.png" alt=""><br>   而权重和偏移量的更新可表示如下：<br> <img src="https://images2.imgbox.com/4d/ab/qFgbvrYV_o.png" alt=""><br>   根据以上三个公式，即可反向传播误差，结合梯度下降算法，修正神经网络的权重和偏移量。<br>   对于梯度下降算法，导言中有所提及，这里我们使用的是小批量梯度下降算法（MBGD）。MBGD在每次更新参数时使用b个样本（b一般为10），求得这b个样本的误差平均值之后，更新一次权重和偏移量。它的伪代码表现如下：<br> <img src="https://images2.imgbox.com/d7/03/NRNhMxvI_o.png" alt=""></p> </li>
<li> <p><strong>代码实现部分</strong><br>   代码实现部分的流程如下所示。<br> <img src="https://images2.imgbox.com/c4/0b/Q9UCUNu5_o.png" alt=""><br> <strong>Main函数部分：</strong><br>   Main函数如下所示。</p> <pre><code class="prism language-python">train_images <span class="token operator">=</span> decode_idx3_ubyte<span class="token punctuation">(</span>train_images_idx3_ubyte_file<span class="token punctuation">)</span>
train_labels <span class="token operator">=</span> decode_idx1_ubyte<span class="token punctuation">(</span>train_labels_idx1_ubyte_file<span class="token punctuation">)</span>
test_images <span class="token operator">=</span> decode_idx3_ubyte<span class="token punctuation">(</span>test_images_idx3_ubyte_file<span class="token punctuation">)</span>
test_labels <span class="token operator">=</span> decode_idx1_ubyte<span class="token punctuation">(</span>test_labels_idx1_ubyte_file<span class="token punctuation">)</span>

trainingimages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>im <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span> <span class="token keyword">for</span> im <span class="token keyword">in</span> train_images<span class="token punctuation">]</span>  <span class="token comment"># 归一化</span>
traininglabels <span class="token operator">=</span> <span class="token punctuation">[</span>vectorized_result<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> train_labels<span class="token punctuation">]</span>

testimages <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>im <span class="token operator">/</span> <span class="token number">255</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span> <span class="token keyword">for</span> im <span class="token keyword">in</span> test_images<span class="token punctuation">]</span>
testlabels <span class="token operator">=</span> <span class="token punctuation">[</span>l <span class="token keyword">for</span> l <span class="token keyword">in</span> test_labels<span class="token punctuation">]</span>
net <span class="token operator">=</span> NueraLNet<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">28</span> <span class="token operator">*</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

net<span class="token punctuation">.</span>train_net<span class="token punctuation">(</span>trainingimages<span class="token punctuation">,</span> traininglabels<span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> testimages<span class="token punctuation">,</span> testlabels<span class="token punctuation">)</span>
net<span class="token punctuation">.</span>save_training<span class="token punctuation">(</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>read_training<span class="token punctuation">(</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>test_net<span class="token punctuation">(</span>testimages<span class="token punctuation">,</span> testlabels<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"end"</span><span class="token punctuation">)</span>
</code></pre> <p>  首先是从字符集中读取出训练集和验证集的图片及其数字的数据，以list的形式存储。而对于每一个图片则是为numpy数组，对于每一个标签则为浮点数。由于读取出来的是像素的灰度值，我们需要将其归一化才能使用，并将其每一张图片重构为1*784的数组。而对于训练集的标签，我们还需要为其每一个标签重新构造1*10的数组，其中下标为标签值的地方值为1，原因是为了与我们神经网络10的输出节点相对应。读取字符集以及转化标签的函数均为自定义函数，由于与神经网络无关，这里不作解释。<br>   接着，我们声明了一个神经网络对象，这个对象的类是我们自定义的。初始化是一个列表，表明了每一层各有多少个节点。其中输入层为784个节点，对应图片的784个像素点，输出层为10个节点，对应输出0-9.隐层可以自定义。Train_net函数为训练神经网络，save_training函数为报错训练后的神经网络参数到本地。read_training函数为读取本地已保存过的网络参数。test_net是使用验证集，验证神经网络的结果。以上几个函数均在类NueraLNet中定义，其具体用法稍后解释。</p> <p><strong>自定义神经网络类NueraLNet部分：</strong><br>   在实现自定义类之前，我们先使用sigmoid函数定义了激励函数及其导数，以供类中使用。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
 <span class="token keyword">return</span> np<span class="token punctuation">.</span>longfloat<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">sigmoid_prime</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> <p>  首先是对类的初始化，传入一个列表代表网络每一层的节点数。然后保存层数，以及这个列表。并根据这个列表随机生成神经网络的权重及偏移量。两者都是列表，其中偏移量bias中的是数组，每一个数组代表从第二层开始的每一层的节点的偏移量。权重weights中的是二维数组，其中行代表前一层的节点位置，列代表后一层的节点位置。对应的即为前一层某个节点到后一层某个节点的权重。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> sizes<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>sizes<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>sizes <span class="token operator">=</span> sizes
    self<span class="token punctuation">.</span>bias <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">for</span> y <span class="token keyword">in</span> sizes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>weights <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">for</span> x<span class="token punctuation">,</span> y <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>sizes<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sizes<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> <p>  如下函数体现了神经网络的前向传播部分，即通过神经网络得出结果。最终得到的result是一个numpy数组，其中有输出层每一个节点的输出。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_result</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> images<span class="token punctuation">)</span><span class="token punctuation">:</span>
    result <span class="token operator">=</span> images
    <span class="token keyword">for</span> b<span class="token punctuation">,</span> w <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">)</span><span class="token punctuation">:</span>
        result <span class="token operator">=</span> sigmoid<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>result<span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>
   <span class="token keyword">return</span> result

</code></pre> <p>  下边的函数是训练神经网络，其中trainimage是训练集的图片，trainresult是训练集的结果，traintime是使用次训练集训练的次数。Rate是学习率，默认为1，minbatch为小批梯度下降中，每一批量的样本个数，默认为10。test_image为验证集图片，test_result为验证集结果，默认为空。<br>   接下来就是对于每一次训练。我们先将训练集分组，分成小批量。对于每一批量，我们调用update_net函数更新网络参数。更新结束后 ，输出第几次学习结束。最后判断有没有验证集，如果有的话，我们将使用test_net函数，查看这一次训练后，神经网络检测字体的正确率。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train_net</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> trainimage<span class="token punctuation">,</span> trainresult<span class="token punctuation">,</span> traintime<span class="token punctuation">,</span> rate<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> minibatch<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> test_image<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> test_result<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>traintime<span class="token punctuation">)</span><span class="token punctuation">:</span>
        minibatchimage <span class="token operator">=</span> <span class="token punctuation">[</span>trainimage<span class="token punctuation">[</span>k<span class="token punctuation">:</span>k<span class="token operator">+</span>minibatch<span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>trainimage<span class="token punctuation">)</span><span class="token punctuation">,</span> minibatch<span class="token punctuation">)</span><span class="token punctuation">]</span>
        minibatchresult <span class="token operator">=</span> <span class="token punctuation">[</span>trainresult<span class="token punctuation">[</span>k<span class="token punctuation">:</span>k<span class="token operator">+</span>minibatch<span class="token punctuation">]</span> <span class="token keyword">for</span> k <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>trainimage<span class="token punctuation">)</span><span class="token punctuation">,</span> minibatch<span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> image<span class="token punctuation">,</span> result <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>minibatchimage<span class="token punctuation">,</span> minibatchresult<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>update_net<span class="token punctuation">(</span>image<span class="token punctuation">,</span> result<span class="token punctuation">,</span> rate<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"第{0}次学习结束"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> test_image <span class="token keyword">and</span> test_result<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>test_net<span class="token punctuation">(</span>test_image<span class="token punctuation">,</span> test_result<span class="token punctuation">)</span>
</code></pre> <p>  下边的函数就是刚刚提到过的更新网络的update_net函数。在这个函数中使一批中所有样本误差梯度的平均值更新一次神经网络的参数。首先我们按照类中存储权重和偏移量的变量格式相应声明存取权重和偏移量误差梯度的变量。接下来，对于批次中的每一个样本，我们通过get_error函数得到测试这个样本之后权重和偏移量的误差梯度。这个get_error函数也就是我们的反向传播误差的过程。这个函数稍后讲解。然后我们将这一批次中所有样本的误差梯度累加到先前声明的存取误差梯度的类中。最后我们根据这一批次所有样本的误差梯度平均值，乘上学习率来调整神经网络的权重和偏移量。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">update_net</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> training_image<span class="token punctuation">,</span> training_result<span class="token punctuation">,</span> rate<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_b_error <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> b <span class="token keyword">in</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">]</span>
    batch_w_error <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">]</span>
    <span class="token keyword">for</span> image<span class="token punctuation">,</span> result <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>training_image<span class="token punctuation">,</span> training_result<span class="token punctuation">)</span><span class="token punctuation">:</span>
        b_error<span class="token punctuation">,</span> w_error <span class="token operator">=</span> self<span class="token punctuation">.</span>get_error<span class="token punctuation">(</span>image<span class="token punctuation">,</span> result<span class="token punctuation">)</span>
        batch_b_error <span class="token operator">=</span> <span class="token punctuation">[</span>bbe <span class="token operator">+</span> be <span class="token keyword">for</span> bbe<span class="token punctuation">,</span> be <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>batch_b_error<span class="token punctuation">,</span> b_error<span class="token punctuation">)</span><span class="token punctuation">]</span>
        batch_w_error <span class="token operator">=</span> <span class="token punctuation">[</span>bwe <span class="token operator">+</span> we <span class="token keyword">for</span> bwe<span class="token punctuation">,</span> we <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>batch_w_error<span class="token punctuation">,</span> w_error<span class="token punctuation">)</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>bias <span class="token operator">=</span> <span class="token punctuation">[</span>b <span class="token operator">-</span> <span class="token punctuation">(</span>rate<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>training_image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span>bbe <span class="token keyword">for</span> b<span class="token punctuation">,</span> bbe <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> batch_b_error<span class="token punctuation">)</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>weights <span class="token operator">=</span> <span class="token punctuation">[</span>w <span class="token operator">-</span> <span class="token punctuation">(</span>rate<span class="token operator">/</span><span class="token builtin">len</span><span class="token punctuation">(</span>training_image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span>bwe <span class="token keyword">for</span> w<span class="token punctuation">,</span> bwe <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>weights<span class="token punctuation">,</span> batch_w_error<span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> <p>  接下来就是反向传播误差的函数get_error。这个函数的参数即为一个样本及其结构。首先也是按照类中存储权重和偏移量的变量格式相应声明存取权重和偏移量误差梯度的变量。由于反向传播误差的过程中，我们需要神经网络中间的输入输出数据。所以定义列表out_data存储每一个节点的输出值，其中第一个元素是输入层的输出值也就是测试的样本。定义列表in_data存储每一个节点的输入值。<br>   接下来我们使用一个for循环进行了前向传播输入的过程并记录了节点的输入输出值。在列表in_data后边添加上一层输出（out_data[-1]）乘以权重加上偏移量，作为这一层的输入。在列表out_data后边添加刚刚的输入（in_data[-1]）通过激励函数后的值，作为这一层节点的输出。这样就保存测试过程中，每一层节点的输入和输出值。<br>   接下来是计算整个网络的权重及偏移量的误差梯度。这里计算的完全按照之前理论部分推导出来的公式，只要之前理解，这里也好理解。首先是计算输出层的偏移量和最后一层的权重的误差梯度。按照公式来。接下来是使用for循环进行反向传播误差。计算之前的每一层的偏移量和权重的误差梯度。这个也是按照公式来的计算即可。<br>   最后则返回计算好的偏移量和权重的误差梯度。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_error</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> image<span class="token punctuation">,</span> result<span class="token punctuation">)</span><span class="token punctuation">:</span>
    b_error <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> b <span class="token keyword">in</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">]</span>
    w_error <span class="token operator">=</span> <span class="token punctuation">[</span>np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>w<span class="token punctuation">.</span>shape<span class="token punctuation">)</span> <span class="token keyword">for</span> w <span class="token keyword">in</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">]</span>
    out_data <span class="token operator">=</span> <span class="token punctuation">[</span>image<span class="token punctuation">]</span>
    in_data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> b<span class="token punctuation">,</span> w <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">)</span><span class="token punctuation">:</span>
        in_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>out_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> w<span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>
        out_data<span class="token punctuation">.</span>append<span class="token punctuation">(</span>sigmoid<span class="token punctuation">(</span>in_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    b_error<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> sigmoid_prime<span class="token punctuation">(</span>in_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>out_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> result<span class="token punctuation">)</span>
    w_error<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>out_data<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> b_error<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> l <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
        b_error<span class="token punctuation">[</span><span class="token operator">-</span>l<span class="token punctuation">]</span> <span class="token operator">=</span> sigmoid_prime<span class="token punctuation">(</span>in_data<span class="token punctuation">[</span><span class="token operator">-</span>l<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">*</span> 
                      np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>b_error<span class="token punctuation">[</span><span class="token operator">-</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>weights<span class="token punctuation">[</span><span class="token operator">-</span>l<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        w_error<span class="token punctuation">[</span><span class="token operator">-</span>l<span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>out_data<span class="token punctuation">[</span><span class="token operator">-</span>l<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> b_error<span class="token punctuation">[</span><span class="token operator">-</span>l<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> b_error<span class="token punctuation">,</span> w_error
</code></pre> <p>  接下来这个函数是测试神经网络正确率的函数，先前在训练神经网络时也调用过。这个函数的参数是验证集图片及其结果。我们首先使用get_result函数得到验证集图片中的每一个结果，并取其结果中最大值的索引，与验证集结果打包成tuple。接着得到所有的tuple中，两者相等的个数，即为识别正确的个数，然后输出识别的结果。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">test_net</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> test_image<span class="token punctuation">,</span> test_result<span class="token punctuation">)</span><span class="token punctuation">:</span>
    results <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>get_result<span class="token punctuation">(</span>image<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> result<span class="token punctuation">)</span> 
               <span class="token keyword">for</span> image<span class="token punctuation">,</span> result <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>test_image<span class="token punctuation">,</span> test_result<span class="token punctuation">)</span><span class="token punctuation">]</span>
    right <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>x <span class="token operator">==</span> y<span class="token punctuation">)</span> <span class="token keyword">for</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> results<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"正确率：{0}/{1}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>right<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>test_result<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> results
</code></pre> <p>  接下来是将神经网络的权重和偏移量保存的本地的函数，这样我们在识别时可以直接读取位于本地的参数，而不需要重新训练神经网络。使用numpy的savez函数将weights和bias中的numpy数组，打包存到本地的npz文件中。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">save_training</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    np<span class="token punctuation">.</span>savez<span class="token punctuation">(</span><span class="token string">'./datafile/weights.npz'</span><span class="token punctuation">,</span> <span class="token operator">*</span>self<span class="token punctuation">.</span>weights<span class="token punctuation">)</span>
    np<span class="token punctuation">.</span>savez<span class="token punctuation">(</span><span class="token string">'./datafile/bias.npz'</span><span class="token punctuation">,</span> <span class="token operator">*</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
</code></pre> <p>  下边的函数是将本地的参数读取到神经网络中。使用load函数加载后，将加载后的数组依次添加到神经网络的weights和bias中去。即可不用训练神经网络就能进行识别。</p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">read_training</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    length <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>sizes<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span>
    file_weights <span class="token operator">=</span> np<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'./datafile/weights.npz'</span><span class="token punctuation">)</span>
    file_bias <span class="token operator">=</span> np<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'./datafile/bias.npz'</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>weights <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    self<span class="token punctuation">.</span>bias <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>length<span class="token punctuation">)</span><span class="token punctuation">:</span>
        index <span class="token operator">=</span> <span class="token string">"arr_"</span> <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weights<span class="token punctuation">.</span>append<span class="token punctuation">(</span>file_weights<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>append<span class="token punctuation">(</span>file_bias<span class="token punctuation">[</span>index<span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> <p>  这样，整个与神经网络有关的代码就讲解结束了。</p> </li>
</ol> 
<h3>
<a id="_173"></a>结果分析</h3> 
<p>  本次实验的编程语言为python3.6，使用的IDE为pycharm。使用到python的第三方库有numpy和PIL<br>   本次实验中，我们可以通过调节迭代次数，学习率，隐藏层节点个数等参数，获取不同的神经网络训练结果。接下来的结果分析将从这三个参数出发。<br>   首先是训练结果与学习率的关系。在这里我们控制隐藏层节点数为30不变。训练样本数为10000.（详细结果数据可见excel文件《结果分析-学习率》）从图中可以看出，在适当学习率的情况下，30次学习后的正确率在94%-95%左右。除了学习率为0.1和0.5的情况下，其余基本在10次学习之后就达到了相对稳定的状态。<br>   另外分析图表我们还可以得出以下结论：<br>   （1）学习率越高，训练结果的震荡越大。<br>   （2）在一定范围内，学习率越高，初次迭代后的正确率越高，超过此范围后，初次迭代后的正确率下降。<br>   （3）学习率越高，越快趋于稳定，但随着学习率增高，训练结果震荡越大。<br> <img src="https://images2.imgbox.com/8f/88/SWn7fetv_o.png" alt=""><br>   接下来是训练结果与隐藏层节点数的关系。这里控制学习率为上次结果中较好的5.训练样本数仍未10000。<br>   从图中可知：在30次迭代学习，学习率为5的情况下，40-70层隐藏层的网络成功率基本可以达到96%。另外，随着隐藏层数的增加，成功率基本是随之增高的。但是同样的，训练网络所耗费的时间也会大大增加。不知为何，70层隐藏层的前两次迭代正确率非常低。另外，同一学习率，在隐藏层数低的网络引起的震荡，要高于隐藏层数高的网络。<br> <img src="https://images2.imgbox.com/5e/61/m8mMsIL2_o.png" alt=""></p> 
<p><a href="https://download.csdn.net/download/sheziqiong/85771938">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85771938<br> <a href="https://download.csdn.net/download/sheziqiong/85771938">资源下载地址</a>：https://download.csdn.net/download/sheziqiong/85771938</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>