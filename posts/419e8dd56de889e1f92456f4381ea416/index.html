<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>深入理解PyTorch中的nn.Embedding - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深入理解PyTorch中的nn.Embedding</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-tomorrow-night">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>目录</h3>
 <ul>
<li><a href="#_3">一、前置知识</a></li>
<li>
<ul>
<li><a href="#11_Corpus_5">1.1 语料库（Corpus）</a></li>
<li><a href="#12_Token_13">1.2 词元（Token）</a></li>
<li><a href="#13_Vocabulary_33">1.3 词表（Vocabulary）</a></li>
</ul>
  </li>
<li><a href="#nnEmbedding__75">二、nn.Embedding 基础</a></li>
<li>
<ul>
<li><a href="#21__embedding_77">2.1 为什么要 embedding？</a></li>
<li><a href="#22__81">2.2 基础参数</a></li>
<li><a href="#23_nnEmbedding__nnLinear__157">2.3 nn.Embedding 与 nn.Linear 的区别</a></li>
<li><a href="#24_nnEmbedding__207">2.4 nn.Embedding 的更新问题</a></li>
</ul>
  </li>
<li><a href="#nnEmbedding__247">三、nn.Embedding 进阶</a></li>
<li>
<ul>
<li><a href="#31__250">3.1 全部参数</a></li>
<li><a href="#32__354">3.2 使用预训练的词嵌入</a></li>
</ul>
  </li>
<li><a href="#_380">四、最后</a></li>
</ul>
</div>
<p></p> 
<h1>
<a id="_3"></a>一、前置知识</h1> 
<h2>
<a id="11_Corpus_5"></a>1.1 语料库（Corpus）</h2> 
<p><strong>太长不看版：</strong> NLP任务所依赖的语言数据称为语料库。</p> 
<p><strong>详细介绍版：</strong> 语料库（Corpus，复数是Corpora）是组织成数据集的真实文本或音频的集合。 此处的真实是指由该语言的母语者制作的文本或音频。 语料库可以由从报纸、小说、食谱、广播到电视节目、电影和推文的所有内容组成。 在自然语言处理中，语料库包含可用于训练 AI 的文本和语音数据。</p> 
<h2>
<a id="12_Token_13"></a>1.2 词元（Token）</h2> 
<p>为简便起见，假设我们的语料库只有三个英文句子并且均已经过处理（全部小写+去掉标点符号）：</p> 
<pre><code class="prism language-python">corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"he is an old worker"</span><span class="token punctuation">,</span> <span class="token string">"english is a useful tool"</span><span class="token punctuation">,</span> <span class="token string">"the cinema is far away"</span><span class="token punctuation">]</span>
</code></pre> 
<p>我们往往需要将其词元化（tokenize）以成为一个序列，这里只需要简单的 <code>split</code> 即可：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">tokenize</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">[</span>sentence<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">for</span> sentence <span class="token keyword">in</span> corpus<span class="token punctuation">]</span>


tokens <span class="token operator">=</span> tokenize<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>
<span class="token comment"># [['he', 'is', 'an', 'old', 'worker'], ['english', 'is', 'a', 'useful', 'tool'], ['the', 'cinema', 'is', 'far', 'away']]</span>
</code></pre> 
<blockquote> 
 <p>? 这里我们是以单词级别进行词元化，还可以以字符级别进行词元化。</p> 
</blockquote> 
<h2>
<a id="13_Vocabulary_33"></a>1.3 词表（Vocabulary）</h2> 
<p>词表<strong>不重复</strong>地包含了语料库中的所有词元，其实现方式十分容易：</p> 
<pre><code class="prism language-python">vocab <span class="token operator">=</span> <span class="token builtin">set</span><span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>vocab<span class="token punctuation">)</span>
<span class="token comment"># {'is', 'useful', 'an', 'old', 'far', 'the', 'away', 'a', 'he', 'tool', 'cinema', 'english', 'worker'}</span>
</code></pre> 
<p>词表在NLP任务中往往并不是最重要的，我们需要为词表中的每一个单词分配唯一的索引并构建单词到索引的映射：<code>word2idx</code>。这里我们按照单词出现的频率来构建 <code>word2idx</code>。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter

word2idx <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    word<span class="token punctuation">:</span> idx
    <span class="token keyword">for</span> idx<span class="token punctuation">,</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> freq<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>
        <span class="token builtin">sorted</span><span class="token punctuation">(</span>Counter<span class="token punctuation">(</span><span class="token builtin">sum</span><span class="token punctuation">(</span>tokens<span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span>
<span class="token comment"># {'is': 0, 'he': 1, 'an': 2, 'old': 3, 'worker': 4, 'english': 5, 'a': 6, 'useful': 7, 'tool': 8, 'the': 9, 'cinema': 10, 'far': 11, 'away': 12}</span>
</code></pre> 
<p>反过来，我们还可以构建 <code>idx2word</code>：</p> 
<pre><code class="prism language-python">idx2word <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>idx<span class="token punctuation">:</span> word <span class="token keyword">for</span> word<span class="token punctuation">,</span> idx <span class="token keyword">in</span> word2idx<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>idx2word<span class="token punctuation">)</span>
<span class="token comment"># {0: 'is', 1: 'he', 2: 'an', 3: 'old', 4: 'worker', 5: 'english', 6: 'a', 7: 'useful', 8: 'tool', 9: 'the', 10: 'cinema', 11: 'far', 12: 'away'}</span>
</code></pre> 
<p>对于 1.2 节中的 <code>tokens</code>，也可以转化为索引的表示：</p> 
<pre><code class="prism language-python">encoded_tokens <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span>token<span class="token punctuation">]</span> <span class="token keyword">for</span> token <span class="token keyword">in</span> line<span class="token punctuation">]</span> <span class="token keyword">for</span> line <span class="token keyword">in</span> tokens<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>encoded_tokens<span class="token punctuation">)</span>
<span class="token comment"># [[1, 0, 2, 3, 4], [5, 0, 6, 7, 8], [9, 10, 0, 11, 12]]</span>
</code></pre> 
<p>这种表示方式将在后续讲解 <code>nn.Embedding</code> 时提到。</p> 
<h1>
<a id="nnEmbedding__75"></a>二、nn.Embedding 基础</h1> 
<h2>
<a id="21__embedding_77"></a>2.1 为什么要 embedding？</h2> 
<p>RNN无法直接处理单词，因此需要通过某种方法把单词变成数字形式的向量才能作为RNN的输入。这种把单词映射到向量空间中的一个向量的做法称为<strong>词嵌入</strong>（word embedding），对应的向量称为<strong>词向量</strong>（word vector）。</p> 
<h2>
<a id="22__81"></a>2.2 基础参数</h2> 
<p>我们首先讲解 <code>nn.Embedding</code> 中的基础参数，了解它的基本用法后，再讲解它的全部参数。</p> 
<p>基础参数如下：</p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">)</span>
</code></pre> 
<p>其中 <code>num_embeddings</code> 是词表的大小，即 <code>len(vocab)</code>；<code>embedding_dim</code> 是词向量的维度。</p> 
<p>我们使用第一章节的例子，此时词表大小为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        12
       
      
      
       12
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">1</span><span class="mord">2</span></span></span></span></span>，不妨设嵌入后词向量的维度是 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        3
       
      
      
       3
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>（即将单词嵌入到三维向量空间中），则 embedding 层应该这样创建：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 为了复现性</span>
emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
</code></pre> 
<p>embedding 层中只有一个参数 <code>weight</code>，在创建时它会从<strong>标准正态分布</strong>中进行初始化：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>emb<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([[-1.1258, -1.1524, -0.2506],</span>
<span class="token comment">#         [-0.4339,  0.8487,  0.6920],</span>
<span class="token comment">#         [-0.3160, -2.1152,  0.3223],</span>
<span class="token comment">#         [-1.2633,  0.3500,  0.3081],</span>
<span class="token comment">#         [ 0.1198,  1.2377,  1.1168],</span>
<span class="token comment">#         [-0.2473, -1.3527, -1.6959],</span>
<span class="token comment">#         [ 0.5667,  0.7935,  0.4397],</span>
<span class="token comment">#         [ 0.1124,  0.6408,  0.4412],</span>
<span class="token comment">#         [-0.2159, -0.7425,  0.5627],</span>
<span class="token comment">#         [ 0.2596,  0.5229,  2.3022],</span>
<span class="token comment">#         [-1.4689, -1.5867,  1.2032],</span>
<span class="token comment">#         [ 0.0845, -1.2001, -0.0048]], requires_grad=True)</span>
</code></pre> 
<p>这里我们可以把 <code>weight</code> 当作 embedding 层的一个权重。</p> 
<hr> 
<p>接下来再来看一下 <code>nn.Embedding</code> 的输入。直观来看，给定一个已经词元化的句子，将其中的单词输入到 embedding 层应该得到相应的词向量。事实上，<code>nn.Embedding</code> 接受的输入并不是词元化后的句子，而是它的索引形式，即第一章节中提到的 <code>encoded_tokens</code>。</p> 
<p><code>nn.Embedding</code> 可以接受<strong>任何形状</strong>的张量作为输入，但因为传入的是索引，所以张量中的每个数字都不应超过 <code>len(vocab) - 1</code>，否则就会报错。接下来，<code>nn.Embedding</code> 的作用就像一个<strong>查找表</strong>（Lookup Table）一样，通过这些索引在 <code>weight</code> 中查找并返回相应的词向量。</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>emb<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token comment"># tensor([[-1.1258, -1.1524, -0.2506],</span>
<span class="token comment">#         [-0.4339,  0.8487,  0.6920],</span>
<span class="token comment">#         [-0.3160, -2.1152,  0.3223],</span>
<span class="token comment">#         [-1.2633,  0.3500,  0.3081],</span>
<span class="token comment">#         [ 0.1198,  1.2377,  1.1168],</span>
<span class="token comment">#         [-0.2473, -1.3527, -1.6959],</span>
<span class="token comment">#         [ 0.5667,  0.7935,  0.4397],</span>
<span class="token comment">#         [ 0.1124,  0.6408,  0.4412],</span>
<span class="token comment">#         [-0.2159, -0.7425,  0.5627],</span>
<span class="token comment">#         [ 0.2596,  0.5229,  2.3022],</span>
<span class="token comment">#         [-1.4689, -1.5867,  1.2032],</span>
<span class="token comment">#         [ 0.0845, -1.2001, -0.0048]], requires_grad=True)</span>
sentence <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>encoded_tokens<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 一共有三个句子，这里只使用第一个句子</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sentence<span class="token punctuation">)</span>
<span class="token comment"># tensor([1, 0, 2, 3, 4])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>emb<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># tensor([[-0.4339,  0.8487,  0.6920],</span>
<span class="token comment">#         [-1.1258, -1.1524, -0.2506],</span>
<span class="token comment">#         [-0.3160, -2.1152,  0.3223],</span>
<span class="token comment">#         [-1.2633,  0.3500,  0.3081],</span>
<span class="token comment">#         [ 0.1198,  1.2377,  1.1168]], grad_fn=&lt;EmbeddingBackward0&gt;)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>emb<span class="token punctuation">.</span>weight<span class="token punctuation">[</span>sentence<span class="token punctuation">]</span> <span class="token operator">==</span> emb<span class="token punctuation">(</span>sentence<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># tensor([[True, True, True],</span>
<span class="token comment">#         [True, True, True],</span>
<span class="token comment">#         [True, True, True],</span>
<span class="token comment">#         [True, True, True],</span>
<span class="token comment">#         [True, True, True]])</span>
</code></pre> 
<h2>
<a id="23_nnEmbedding__nnLinear__157"></a>2.3 nn.Embedding 与 nn.Linear 的区别</h2> 
<p>细心的读者可能已经看出 <code>nn.Embedding</code> 和 <code>nn.Linear</code> 似乎很像，那它们到底有什么区别呢？</p> 
<p>回顾 <code>nn.Linear</code>，若不开启 <code>bias</code>，设输入向量为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        x
       
      
      
       x
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">x</span></span></span></span></span>，<code>nn.Linear.weight</code> 对应的矩阵为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        A
       
      
      
       A
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault">A</span></span></span></span></span>（形状为 <code>hidden_size × input_size</code>），则计算方式为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         y
        
        
         =
        
        
         x
        
        
         
          A
         
         
          T
         
        
       
       
         y=xA^{text T} 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.891331em;vertical-align: 0em"></span><span class="mord mathdefault">x</span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">T</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>其中 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        x
       
       
        ,
       
       
        y
       
      
      
       x,y
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span></span></span> 均为<strong>行向量</strong>。</p> 
<p>假如 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        x
       
      
      
       x
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">x</span></span></span></span></span> 是one-hot向量，第 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span> 个位置是 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        1
       
      
      
       1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">1</span></span></span></span></span>，那么 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        y
       
      
      
       y
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span></span></span> 就是 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         A
        
        
         T
        
       
      
      
       A^{text T}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.841331em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">A</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">T</span></span></span></span></span></span></span></span></span></span></span></span></span></span> 的第 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span> 行。</p> 
<p>现给定一个单词 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        w
       
      
      
       w
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span></span></span></span></span>，假设它在 <code>word2idx</code> 中的索引就是 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span>，在 <code>nn.Embedding</code> 中，我们根据这个索引 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span> 去查找 <code>emb.weight</code> 的第 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span> 行。而在 <code>nn.Linear</code> 中，我们则是将这个索引 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span> 编码成一个one-hot向量，再去乘上对应的权重矩阵得到矩阵的第 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span> 行。</p> 
<p>请看下例：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

vocab_size <span class="token operator">=</span> <span class="token number">4</span>  <span class="token comment"># 词表大小为4</span>
embedding_dim <span class="token operator">=</span> <span class="token number">3</span>  <span class="token comment"># 词向量维度为3</span>
weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 随机初始化权重矩阵</span>

<span class="token comment"># 保持线性层和嵌入层具有相同的权重</span>
linear_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
linear_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> weight<span class="token punctuation">.</span>T  <span class="token comment"># 注意转置</span>
emb_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
emb_layer<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">=</span> weight

idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 假设某个单词在word2idx中的索引为2</span>
word <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>  <span class="token comment"># 上述单词的one-hot表示</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>emb_layer<span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># tensor([ 0.4033,  0.8380, -0.7193], grad_fn=&lt;EmbeddingBackward0&gt;)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>linear_layer<span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># tensor([ 0.4033,  0.8380, -0.7193], grad_fn=&lt;SqueezeBackward3&gt;)</span>
</code></pre> 
<p>从中我们可以总结出：</p> 
<ul>
<li>
<code>nn.Linear</code> 接受向量作为输入，而 <code>nn.Embedding</code> 则是接受离散的索引作为输入；</li>
<li>
<code>nn.Embedding</code> 实际上就是输入为one-hot向量，且不带bias的 <code>nn.Linear</code>。</li>
</ul> 
<p>此外，<code>nn.Linear</code> 在运算过程中做了矩阵乘法，而 <code>nn.Embedding</code> 是直接根据索引查表，因此在该情景下 <code>nn.Embedding</code> 的效率显然更高。</p> 
<blockquote> 
 <p>? <strong>进一步阅读：</strong> <a href="https://stackoverflow.com/questions/65445174/what-is-the-difference-between-an-embedding-layer-with-a-bias-immediately-afterw/65448744#65448744">[Stack Overflow] What is the difference between an Embedding Layer with a bias immediately afterwards and a Linear Layer in PyTorch?</a></p> 
</blockquote> 
<h2>
<a id="24_nnEmbedding__207"></a>2.4 nn.Embedding 的更新问题</h2> 
<p>在查阅了PyTorch官方论坛和Stack Overflow的一些帖子后，发现有不少人对 <code>nn.Embedding</code> 中的权重 <code>weight</code> 是怎么更新的感到非常困惑。</p> 
<blockquote> 
 <p>? <code>nn.Embedding</code> 的权重实际上就是词嵌入本身</p> 
</blockquote> 
<p>事实上，<code>nn.Embedding.weight</code> 在更新的过程中既没有采用 Skip-gram 也没有采用 CBOW。回顾最简单的多层感知机，其中的 <code>nn.Linear.weight</code> 会随着反向传播自动更新。当我们把 <code>nn.Embedding</code> 视为一个特殊的 <code>nn.Linear</code> 后，其更新机制就不难理解了，无非就是按照梯度进行更新罢了。</p> 
<p>训练结束后，得到的词嵌入是最适合当前任务的词嵌入，而非像word2vec，GloVe这种更为通用的词嵌入。</p> 
<p>当然我们也可以在训练开始之前使用预训练的词嵌入，例如上述提到的word2vec，但此时应该考虑针对当前任务重新训练或进行微调。</p> 
<hr> 
<p>假如我们已经使用了预训练的词嵌入并且不想让它在训练过程中自我更新，那么可以尝试冻结梯度，即：</p> 
<pre><code class="prism language-python">emb<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
</code></pre> 
<blockquote> 
 <p>? <strong>进一步阅读：</strong></p> 
 <ol>
<li><a href="https://discuss.pytorch.org/t/how-nn-embedding-trained/32533">[PyTorch Forums] How nn.Embedding trained?</a></li>
<li><a href="https://discuss.pytorch.org/t/how-does-nn-embedding-work/88518">[PyTorch Forums] How does nn.Embedding work?</a></li>
<li><a href="https://stackoverflow.com/questions/50747947/embedding-in-pytorch">[Stack Overflow] Embedding in pytorch</a></li>
<li><a href="https://stackoverflow.com/questions/58718612/what-exactly-happens-inside-embedding-layer-in-pytorch">[Stack Overflow] What “exactly” happens inside embedding layer in pytorch?</a></li>
</ol> 
</blockquote> 
<h1>
<a id="nnEmbedding__247"></a>三、nn.Embedding 进阶</h1> 
<p>在这一章节中，我们会讲解 <code>nn.Embedding</code> 的所有参数并介绍如何使用预训练的词嵌入。</p> 
<h2>
<a id="31__250"></a>3.1 全部参数</h2> 
<p>官方文档：</p> 
<p><img src="https://images2.imgbox.com/68/84/W2yXs4Nb_o.png" alt="在这里插入图片描述"></p> 
<hr> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         padding_idx
        
       
      
      
       textcolor{blue}{text{padding_idx}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00444em;vertical-align: -0.31em"></span><span class="mord text" style="color: blue"><span class="mord" style="color: blue">padding_idx</span></span></span></span></span></span></p> 
<p>我们知道，<code>nn.Embedding</code> 虽然可以接受任意形状的张量作为输入，但绝大多数情况下，其输入的形状为 <code>batch_size × sequence_length</code>，<strong>这要求同一个 batch 中的所有序列的长度相同</strong>。</p> 
<p>回顾1.2节中的例子，语料库中的三个句子的长度相同（拥有相同的单词个数），但事实上这是博主特意选取的三个句子。现实任务中，很难保证同一个 batch 中的所有句子长度都相同，因此我们需要对那些长度较短的句子进行填充。因为输入到 <code>nn.Embedding</code> 中的都是索引，所以我们也需要用索引进行填充，那使用哪个索引最好呢？</p> 
<p>假设语料库为：</p> 
<pre><code class="prism language-python">corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"he is an old worker"</span><span class="token punctuation">,</span> <span class="token string">"time tries truth"</span><span class="token punctuation">,</span> <span class="token string">"better late than never"</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span>
<span class="token comment"># {'he': 0, 'is': 1, 'an': 2, 'old': 3, 'worker': 4, 'time': 5, 'tries': 6, 'truth': 7, 'better': 8, 'late': 9, 'than': 10, 'never': 11}</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>encoded_tokens<span class="token punctuation">)</span>
<span class="token comment"># [[0, 1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11]]</span>
</code></pre> 
<p>我们可以在 <code>word2idx</code> 中新增一个词元 <code>&lt;pad&gt;</code>（代表填充词元），并为其分配新的索引：</p> 
<pre><code class="prism language-python">word2idx<span class="token punctuation">[</span><span class="token string">'&lt;pad&gt;'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">12</span>
</code></pre> 
<p>对 <code>encoded_tokens</code> 进行填充：</p> 
<pre><code class="prism language-python">max_length <span class="token operator">=</span> <span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span> <span class="token keyword">for</span> seq <span class="token keyword">in</span> encoded_tokens<span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>encoded_tokens<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    encoded_tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token punctuation">[</span>word2idx<span class="token punctuation">[</span><span class="token string">'&lt;pad&gt;'</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token punctuation">(</span>max_length <span class="token operator">-</span> <span class="token builtin">len</span><span class="token punctuation">(</span>encoded_tokens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>encoded_tokens<span class="token punctuation">)</span>
<span class="token comment"># [[0, 1, 2, 3, 4], [5, 6, 7, 12, 12], [8, 9, 10, 11, 12]]</span>
</code></pre> 
<p>创建 embedding 层并指定 <code>padding_idx</code>：</p> 
<pre><code class="prism language-python">emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word2idx<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">)</span>  <span class="token comment"># 假设词向量维度是3</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>emb<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token comment"># tensor([[ 1.5017, -1.1737,  0.1742],</span>
<span class="token comment">#         [-0.9511, -0.4172,  1.5996],</span>
<span class="token comment">#         [ 0.6306,  1.4186,  1.3872],</span>
<span class="token comment">#         [-0.1833,  1.4485, -0.3515],</span>
<span class="token comment">#         [ 0.2474, -0.8514, -0.2448],</span>
<span class="token comment">#         [ 0.4386,  1.3905,  0.0328],</span>
<span class="token comment">#         [-0.1215,  0.5504,  0.1499],</span>
<span class="token comment">#         [ 0.5954, -1.0845,  1.9494],</span>
<span class="token comment">#         [ 0.0668,  1.1366, -0.3414],</span>
<span class="token comment">#         [-0.0260, -0.1091,  0.4937],</span>
<span class="token comment">#         [ 0.4947,  1.1701, -0.5660],</span>
<span class="token comment">#         [ 1.1717, -0.3970, -1.4958],</span>
<span class="token comment">#         [ 0.0000,  0.0000,  0.0000]], requires_grad=True)</span>
</code></pre> 
<p>可以看出填充词元对应的词向量是<strong>零向量</strong>，并且在训练过程中填充词元对应的词向量不会进行更新（<strong>始终</strong>是零向量）。</p> 
<p><code>padding_idx</code> 默认为 <code>None</code>，即不进行填充。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         max_norm
        
       
      
      
       textcolor{blue}{text{max_norm}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.74056em;vertical-align: -0.31em"></span><span class="mord text" style="color: blue"><span class="mord" style="color: blue">max_norm</span></span></span></span></span></span></p> 
<p>如果词向量的范数超过了 <code>max_norm</code>，则将其按范数归一化至 <code>max_norm</code>：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         w
        
        
         :
        
        
         =
        
        
         max_norm
        
        
         ⋅
        
        
         
          w
         
         
          
           ∥
          
          
           w
          
          
           ∥
          
         
        
       
       
         w:=text{max_norm}cdotfrac{w}{Vert wVert} 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height: 0.36687em;vertical-align: 0em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.75445em;vertical-align: -0.31em"></span><span class="mord text"><span class="mord">max_norm</span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 2.04356em;vertical-align: -0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.10756em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">∥</span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="mord">∥</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p><code>max_norm</code> 默认为 <code>None</code>，即不进行归一化。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         norm_type
        
       
      
      
       textcolor{blue}{text{norm_type}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.92508em;vertical-align: -0.31em"></span><span class="mord text" style="color: blue"><span class="mord" style="color: blue">norm_type</span></span></span></span></span></span></p> 
<p>当指定了 <code>max_norm</code> 时，<code>norm_type</code> 决定采用何种范数去计算。默认是2-范数。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         scale_grad_by_freq
        
       
      
      
       textcolor{blue}{text{scale_grad_by_freq}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.00444em;vertical-align: -0.31em"></span><span class="mord text" style="color: blue"><span class="mord" style="color: blue">scale_grad_by_freq</span></span></span></span></span></span></p> 
<p>若将该参数设置为 <code>True</code>，则对词向量 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        w
       
      
      
       w
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span></span></span></span></span> 进行更新时，会根据它在一个 batch 中出现的频率对相应的梯度进行缩放：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          
           ∂
          
          
           Loss
          
         
         
          
           ∂
          
          
           w
          
         
        
        
         :
        
        
         =
        
        
         
          1
         
         
          
           frequency
          
          
           (
          
          
           w
          
          
           )
          
         
        
        
         ⋅
        
        
         
          
           ∂
          
          
           Loss
          
         
         
          
           ∂
          
          
           w
          
         
        
       
       
         frac{partial text{Loss}}{partial w}:=frac{1}{text{frequency}(w)}cdotfrac{partial text{Loss}}{partial w} 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 2.05744em;vertical-align: -0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.37144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord" style="margin-right: 0.05556em">∂</span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord" style="margin-right: 0.05556em">∂</span><span class="mord text"><span class="mord">Loss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height: 0.36687em;vertical-align: 0em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 2.25744em;vertical-align: -0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord text"><span class="mord">frequency</span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 2.05744em;vertical-align: -0.686em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.37144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord" style="margin-right: 0.05556em">∂</span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord" style="margin-right: 0.05556em">∂</span><span class="mord text"><span class="mord">Loss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p>默认为 <code>False</code>。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         sparse
        
       
      
      
       textcolor{blue}{text{sparse}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord text" style="color: blue"><span class="mord" style="color: blue">sparse</span></span></span></span></span></span></p> 
<p>若设置为 <code>True</code>，则与 <code>Embedding.weight</code> 相关的梯度将变为稀疏张量，此时优化器只能选择：<code>SGD</code>、<code>SparseAdam</code> 和 <code>Adagrad</code>。默认为 <code>False</code>。</p> 
<h2>
<a id="32__354"></a>3.2 使用预训练的词嵌入</h2> 
<p>有些情况下我们需要使用预训练的词嵌入，这时候可以使用 <code>from_pretrained</code> 方法，如下：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
pretrained_embeddings <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>pretrained_embeddings<span class="token punctuation">)</span>
<span class="token comment"># tensor([[ 1.5410, -0.2934, -2.1788],</span>
<span class="token comment">#         [ 0.5684, -1.0845, -1.3986],</span>
<span class="token comment">#         [ 0.4033,  0.8380, -0.7193],</span>
<span class="token comment">#         [-0.4033, -0.5966,  0.1820]])</span>
emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>pretrained_embeddings<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>emb<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token comment"># tensor([[ 1.5410, -0.2934, -2.1788],</span>
<span class="token comment">#         [ 0.5684, -1.0845, -1.3986],</span>
<span class="token comment">#         [ 0.4033,  0.8380, -0.7193],</span>
<span class="token comment">#         [-0.4033, -0.5966,  0.1820]])</span>
</code></pre> 
<p>如果要避免预训练的词嵌入在后续的训练过程中更新，可将 <code>freeze</code> 参数设置为 <code>True</code>：</p> 
<pre><code class="prism language-python">emb <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span>pretrained_embeddings<span class="token punctuation">,</span> freeze<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<h1>
<a id="_380"></a>四、最后</h1> 
<blockquote> 
 <p>ℹ️ <strong>博主对 <code>nn.Embedding</code> 的理解可能仍不到位，如有错误欢迎在评论区指出。</strong><br> ? <strong>如果这篇文章有帮助到你，可以关注❤️ + 点赞? + 收藏⭐ + 留言?，您的支持将是我创作的最大动力?</strong></p> 
</blockquote>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>