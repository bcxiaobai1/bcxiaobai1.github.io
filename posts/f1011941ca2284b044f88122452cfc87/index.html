<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>深度学习入门（三十八）计算性能——多GPU训练 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习入门（三十八）计算性能——多GPU训练</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>深度学习入门（三十八）计算性能——多GPU训练</h3>
 <ul>
<li><a href="#_1">前言</a></li>
<li><a href="#GPU_5">计算性能——多GPU训练</a></li>
<li>
<ul>
<li><a href="#_6">课件</a></li>
<li>
<ul>
<li><a href="#GPU_7">多GPU并行</a></li>
<li><a href="#VS_14">数据并行VS模型并行</a></li>
<li><a href="#_20">数据并行</a></li>
<li><a href="#_23">总结</a></li>
</ul>
   </li>
<li><a href="#_27">教材</a></li>
<li>
<ul>
<li><a href="#1__29">1 问题拆分</a></li>
<li><a href="#2__46">2 数据并行性</a></li>
<li><a href="#3__72">3 简单网络</a></li>
<li><a href="#4__106">4 数据同步</a></li>
<li><a href="#5__156">5 数据分发</a></li>
<li><a href="#6__188">6 训练</a></li>
<li><a href="#7__256">7 小结</a></li>
</ul>
   </li>
<li><a href="#GPU_262">多GPU的简洁实现</a></li>
<li>
<ul>
<li><a href="#1_270">1简单网络</a></li>
<li><a href="#2__302">2 网络初始化</a></li>
<li><a href="#3__311">3 训练</a></li>
<li><a href="#4__373">4 小结</a></li>
</ul>
  </li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<h1>
<a id="_1"></a>前言</h1> 
<p>核心内容来自<a href="https://tangshusen.me/Dive-into-DL-PyTorch/#/">博客链接1</a><a href="https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#id7">博客连接2</a>希望大家多多支持作者<br> 本文记录用，防止遗忘</p> 
<h1>
<a id="GPU_5"></a>计算性能——多GPU训练</h1> 
<h2>
<a id="_6"></a>课件</h2> 
<h3>
<a id="GPU_7"></a>多GPU并行</h3> 
<p>一台机器可以安装多个GPU (1-16)<br> 在训练和预测时，我们将一个小批量计算切分到多个GPU上来达到加速目的<br> 常用切分方案有</p> 
<ul>
<li>数据并行</li>
<li>模型并行 </li>
<li>通道并行(数据+模型并行)</li>
<li>
<h3>
<a id="VS_14"></a>数据并行VS模型并行</h3> 
<p>数据并行：将小批量分成n块，每个GPU拿到完整参数计算—块数据的梯度</p> 
</li>
<li>通常性能更好</li>
<li> 模型并行:将模型分成n块，每个GPU拿到一块模型计算它的前向和方向结果 
</li>
<li>通常用于模型大到单GPU放不下</li>
<li>
<h3>
<a id="_20"></a>数据并行</h3> 
<p><img src="https://images2.imgbox.com/ad/da/uYMrFWdZ_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="_23"></a>总结</h3> 
<p>1、当一个模型能用单卡计算时，通常使用数据并行拓展到多卡上<br> 2、模型并行则用在超大模型上</p> 
<h2>
<a id="_27"></a>教材</h2> 
<p>到目前为止，我们讨论了如何在CPU和GPU上高效地训练模型，同时在自动并行节中展示了深度学习框架如何在CPU和GPU之间自动地并行化计算和通信，还在GPU节中展示了如何使用nvidia-smi命令列出计算机上所有可用的GPU。 但是我们没有讨论如何真正实现深度学习训练的并行化。 是否存在一种方法，以某种方式分割数据到多个设备上，并使其能够正常工作呢？ 本节将详细介绍如何从零开始并行地训练网络， 这里需要运用小批量随机梯度下降算法。</p> 
<h3>
<a id="1__29"></a>1 问题拆分</h3> 
<p>我们从一个简单的计算机视觉问题和一个稍稍过时的网络开始。 这个网络有多个卷积层和汇聚层，最后可能有几个全连接的层，看起来非常类似于LeNet或AlexNet。 假设我们有多个GPU。 我们希望以一种方式对训练进行拆分，为实现良好的加速比，还能同时受益于简单且可重复的设计选择。 毕竟，多个GPU同时增加了内存和计算能力。 简而言之，对于需要分类的小批量训练数据，我们有以下选择。</p> 
<p><code>不推荐</code>第一种方法，在多个GPU之间拆分网络。 也就是说，每个GPU将流入特定层的数据作为输入，跨多个后续层对数据进行处理，然后将数据发送到下一个GPU。 与单个GPU所能处理的数据相比，我们可以用更大的网络处理数据。 此外，每个GPU占用的显存（memory footprint）可以得到很好的控制，虽然它只是整个网络显存的一小部分。</p> 
<p>然而，GPU的接口之间需要的密集同步可能是很难办的，特别是层之间计算的工作负载不能正确匹配的时候， 还有层之间的接口需要大量的数据传输的时候（例如：激活值和梯度，数据量可能会超出GPU总线的带宽）。 此外，计算密集型操作的顺序对于拆分来说也是非常重要的，其本质仍然是一个困难的问题，目前还不清楚研究是否能在特定问题上实现良好的线性缩放。 综上所述，除非存框架或操作系统本身支持将多个GPU连接在一起，否则不建议这种方法。</p> 
<p><code>不推荐</code>第二种方法，拆分层内的工作。 例如，将问题分散到4个GPU，每个GPU生成16个通道的数据，而不是在单个GPU上计算64个通道。 对于全连接的层，同样可以拆分输出单元的数量。 下图描述了这种设计，其策略用于处理显存非常小（当时为2GB）的GPU。 当通道或单元的数量不太小时，使计算性能有良好的提升。 此外，由于可用的显存呈线性扩展，多个GPU能够处理不断变大的网络。<br> <img src="https://images2.imgbox.com/92/63/HAlLn9s2_o.png" alt="在这里插入图片描述"><br> 然而，我们需要大量的同步或屏障操作（barrier operation），因为每一层都依赖于所有其他层的结果。 此外，需要传输的数据量也可能比跨GPU拆分层时还要大。 因此，基于带宽的成本和复杂性，我们同样不推荐这种方法。</p> 
<p>最后一种方法，跨多个GPU对数据进行拆分。 这种方式下，所有GPU尽管有不同的观测结果，但是执行着相同类型的工作。 在完成每个小批量数据的训练之后，梯度在GPU上聚合。 这种方法最简单，并可以应用于任何情况，同步只需要在每个小批量数据处理之后进行。 也就是说，当其他梯度参数仍在计算时，完成计算的梯度参数就可以开始交换。 而且，GPU的数量越多，小批量包含的数据量就越大，从而就能提高训练效率。 但是，添加更多的GPU并不能让我们训练更大的模型。</p> 
<p><img src="https://images2.imgbox.com/67/a1/BdyAdGvg_o.png" alt="在这里插入图片描述"></p> 
<p>上图比较了多个GPU上不同的并行方式。 总体而言，只要GPU的显存足够大，数据并行是最方便的。</p> 
<h3>
<a id="2__46"></a>2 数据并行性</h3> 
<p>假设一台机器有k个GPU。 给定需要训练的模型，虽然每个GPU上的参数值都是相同且同步的，但是每个GPU都将独立地维护一组完整的模型参数。 例如，下图演示了在k=2时基于数据并行方法训练模型。<br> <img src="https://images2.imgbox.com/b0/a1/pWP6UCK7_o.png" alt="在这里插入图片描述"><br> 一般来说，k个GPU并行训练过程如下：</p> 
</li>
<li>在任何一次训练迭代中，给定的随机的小批量样本都将被分成k个部分，并均匀地分配到GPU上。 </li>
<li>每个GPU根据分配给它的小批量子集，计算模型参数的损失和梯度。 </li>
<li>将k个GPU中的局部梯度聚合，以获得当前小批量的随机梯度。 </li>
<li>聚合梯度被重新分发到每个GPU中。 </li>
<li>每个GPU使用这个小批量随机梯度，来更新它所维护的完整的模型参数集。</li>
<li>
<p>在实践中请注意，当在k个GPU上训练时，需要扩大小批量的大小为k的倍数，这样每个GPU都有相同的工作量，就像只在单个GPU上训练一样。 因此，在16-GPU服务器上可以显著地增加小批量数据量的大小，同时可能还需要相应地提高学习率。</p> 
<p>下面我们将使用一个简单网络来演示多GPU训练。</p> 
<pre><code class="prism language-python"><span class="token operator">%</span>matplotlib inline
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">import</span> functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l
</code></pre> 
<h3>
<a id="3__72"></a>3 简单网络</h3> 
<p>我们使用之前的（稍加修改的）LeNet， 从零开始定义它，从而详细说明参数交换和同步</p> 
<pre><code class="prism language-python"><span class="token comment"># 初始化模型参数</span>
scale <span class="token operator">=</span> <span class="token number">0.01</span>
W1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale
b1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span>
W2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale
b2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">50</span><span class="token punctuation">)</span>
W3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">800</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale
b3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">)</span>
W4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> scale
b4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
params <span class="token operator">=</span> <span class="token punctuation">[</span>W1<span class="token punctuation">,</span> b1<span class="token punctuation">,</span> W2<span class="token punctuation">,</span> b2<span class="token punctuation">,</span> W3<span class="token punctuation">,</span> b3<span class="token punctuation">,</span> W4<span class="token punctuation">,</span> b4<span class="token punctuation">]</span>

<span class="token comment"># 定义模型</span>
<span class="token keyword">def</span> <span class="token function">lenet</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> params<span class="token punctuation">)</span><span class="token punctuation">:</span>
    h1_conv <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>X<span class="token punctuation">,</span> weight<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    h1_activation <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>h1_conv<span class="token punctuation">)</span>
    h1 <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>h1_activation<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    h2_conv <span class="token operator">=</span> F<span class="token punctuation">.</span>conv2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>h1<span class="token punctuation">,</span> weight<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>params<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    h2_activation <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>h2_conv<span class="token punctuation">)</span>
    h2 <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token operator">=</span>h2_activation<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    h2 <span class="token operator">=</span> h2<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>h2<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    h3_linear <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>h2<span class="token punctuation">,</span> params<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> params<span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span>
    h3 <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>h3_linear<span class="token punctuation">)</span>
    y_hat <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>h3<span class="token punctuation">,</span> params<span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> params<span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> y_hat

<span class="token comment"># 交叉熵损失函数</span>
loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span>reduction<span class="token operator">=</span><span class="token string">'none'</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="4__106"></a>4 数据同步</h3> 
<p>对于高效的多GPU训练，我们需要两个基本操作。 首先，我们需要向多个设备分发参数并附加梯度（<code>get_params</code>）。 如果没有参数，就不可能在GPU上评估网络。 第二，需要跨多个设备对参数求和，也就是说，需要一个<code>allreduce</code>函数。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_params</span><span class="token punctuation">(</span>params<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span>
    new_params <span class="token operator">=</span> <span class="token punctuation">[</span>p<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> params<span class="token punctuation">]</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> new_params<span class="token punctuation">:</span>
        p<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> new_params
</code></pre> 
<p>通过将模型参数复制到一个GPU。</p> 
<pre><code class="prism language-python">new_params <span class="token operator">=</span> get_params<span class="token punctuation">(</span>params<span class="token punctuation">,</span> d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b1 权重:'</span><span class="token punctuation">,</span> new_params<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'b1 梯度:'</span><span class="token punctuation">,</span> new_params<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">b1 权重<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b1 梯度<span class="token punctuation">:</span> <span class="token boolean">None</span>
</code></pre> 
<p>由于还没有进行任何计算，因此权重参数的梯度仍然为零。 假设现在有一个向量分布在多个GPU上，下面的<code>allreduce</code>函数将所有向量相加，并将结果广播给所有GPU。 请注意，我们需要将数据复制到累积结果的设备，才能使函数正常工作。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">allreduce</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">+=</span> data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>data<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>device<span class="token punctuation">)</span>
</code></pre> 
<p>通过在不同设备上创建具有不同值的向量并聚合它们。</p> 
<pre><code class="prism language-python">data <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token operator">=</span>d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'allreduce之前：n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
allreduce<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'allreduce之后：n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'n'</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">allreduce之前：
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
allreduce之后：
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">3.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="5__156"></a>5 数据分发</h3> 
<p>我们需要一个简单的工具函数，将一个小批量数据均匀地分布在多个GPU上。 例如，有两个GPU时，我们希望每个GPU可以复制一半的数据。 因为深度学习框架的内置函数编写代码更方便、更简洁，所以在<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        4
       
       
        ×
       
       
        5
       
      
      
       4 times 5
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">5</span></span></span></span></span>矩阵上使用它进行尝试。</p> 
<pre><code class="prism language-python">data <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
devices <span class="token operator">=</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
split <span class="token operator">=</span> nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>data<span class="token punctuation">,</span> devices<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'input :'</span><span class="token punctuation">,</span> data<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'load into'</span><span class="token punctuation">,</span> devices<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'output:'</span><span class="token punctuation">,</span> split<span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python"><span class="token builtin">input</span> <span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
load into <span class="token punctuation">[</span>device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
output<span class="token punctuation">:</span> <span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:1'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>为了方便以后复用，我们定义了可以同时拆分数据和标签的<code>split_batch</code>函数。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">split_batch</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> devices<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""将X和y拆分到多个设备上"""</span>
    <span class="token keyword">assert</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">==</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>X<span class="token punctuation">,</span> devices<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>scatter<span class="token punctuation">(</span>y<span class="token punctuation">,</span> devices<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="6__188"></a>6 训练</h3> 
<p>现在我们可以在一个小批量上实现多GPU训练。 在多个GPU之间同步数据将使用刚才讨论的辅助函数<code>allreduce</code>和<code>split_and_load</code>。 我们不需要编写任何特定的代码来实现并行性。 因为计算图在小批量内的设备之间没有任何依赖关系，因此它是“自动地”并行执行。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train_batch</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> device_params<span class="token punctuation">,</span> devices<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X_shards<span class="token punctuation">,</span> y_shards <span class="token operator">=</span> split_batch<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> devices<span class="token punctuation">)</span>
    <span class="token comment"># 在每个GPU上分别计算损失</span>
    ls <span class="token operator">=</span> <span class="token punctuation">[</span>loss<span class="token punctuation">(</span>lenet<span class="token punctuation">(</span>X_shard<span class="token punctuation">,</span> device_W<span class="token punctuation">)</span><span class="token punctuation">,</span> y_shard<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
          <span class="token keyword">for</span> X_shard<span class="token punctuation">,</span> y_shard<span class="token punctuation">,</span> device_W <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>
              X_shards<span class="token punctuation">,</span> y_shards<span class="token punctuation">,</span> device_params<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> l <span class="token keyword">in</span> ls<span class="token punctuation">:</span>  <span class="token comment"># 反向传播在每个GPU上分别执行</span>
        l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 将每个GPU的所有梯度相加，并将其广播到所有GPU</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>device_params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            allreduce<span class="token punctuation">(</span>
                <span class="token punctuation">[</span>device_params<span class="token punctuation">[</span>c<span class="token punctuation">]</span><span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>grad <span class="token keyword">for</span> c <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>devices<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token comment"># 在每个GPU上分别更新模型参数</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> device_params<span class="token punctuation">:</span>
        d2l<span class="token punctuation">.</span>sgd<span class="token punctuation">(</span>param<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> X<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 在这里，我们使用全尺寸的小批量</span>
</code></pre> 
<p>现在，我们可以定义训练函数。 与前几章中略有不同：训练函数需要分配GPU并将所有模型参数复制到所有设备。 显然，每个小批量都是使用<code>train_batch</code>函数来处理多个GPU。 我们只在一个GPU上计算模型的精确度，而让其他GPU保持空闲，尽管这是相对低效的，但是使用方便且代码简洁。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>num_gpus<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    devices <span class="token operator">=</span> <span class="token punctuation">[</span>d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_gpus<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token comment"># 将模型参数复制到num_gpus个GPU</span>
    device_params <span class="token operator">=</span> <span class="token punctuation">[</span>get_params<span class="token punctuation">(</span>params<span class="token punctuation">,</span> d<span class="token punctuation">)</span> <span class="token keyword">for</span> d <span class="token keyword">in</span> devices<span class="token punctuation">]</span>
    num_epochs <span class="token operator">=</span> <span class="token number">10</span>
    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> <span class="token string">'test acc'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">)</span>
    timer <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Timer<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
            <span class="token comment"># 为单个小批量执行多GPU训练</span>
            train_batch<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> device_params<span class="token punctuation">,</span> devices<span class="token punctuation">,</span> lr<span class="token punctuation">)</span>
            torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span>
        timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># 在GPU0上评估模型</span>
        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>evaluate_accuracy_gpu<span class="token punctuation">(</span>
            <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> lenet<span class="token punctuation">(</span>x<span class="token punctuation">,</span> device_params<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> devices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'测试精度：</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>animator<span class="token punctuation">.</span>Y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">，</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>timer<span class="token punctuation">.</span>avg<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">}</span></span><span class="token string">秒/轮，'</span></span>
          <span class="token string-interpolation"><span class="token string">f'在</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">str</span><span class="token punctuation">(</span>devices<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>让我们看看在单个GPU上运行效果得有多好。 首先使用的批量大小是256，学习率是0.2。</p> 
<pre><code class="prism language-python">train<span class="token punctuation">(</span>num_gpus<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">测试精度：<span class="token number">0.80</span>，<span class="token number">2.7</span>秒<span class="token operator">/</span>轮，在<span class="token punctuation">[</span>device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/02/a4/MkVsMVj5_o.png" alt="在这里插入图片描述"></p> 
<p>保持批量大小和学习率不变，并增加为2个GPU，我们可以看到测试精度与之前的实验基本相同。 不同的GPU个数在算法寻优方面是相同的。 不幸的是，这里没有任何有意义的加速：模型实在太小了；而且数据集也太小了，在这个数据集中，我们实现的多GPU训练的简单方法受到了巨大的Python开销的影响。 在未来，我们将遇到更复杂的模型和更复杂的并行化方法。 尽管如此，让我们看看Fashion-MNIST数据集上会发生什么。</p> 
<pre><code class="prism language-python">train<span class="token punctuation">(</span>num_gpus<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出</p> 
<pre><code class="prism language-python">测试精度：<span class="token number">0.84</span>，<span class="token number">2.8</span>秒<span class="token operator">/</span>轮，在<span class="token punctuation">[</span>device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/96/d1/QfMSh3gt_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="7__256"></a>7 小结</h3> 
<p>1、有多种方法可以在多个GPU上拆分深度网络的训练。拆分可以在层之间、跨层或跨数据上实现。前两者需要对数据传输过程进行严格编排，而最后一种则是最简单的策略。<br> 2、数据并行训练本身是不复杂的，它通过增加有效的小批量数据量的大小提高了训练效率。<br> 3、在数据并行中，数据需要跨多个GPU拆分，其中每个GPU执行自己的前向传播和反向传播，随后所有的梯度被聚合为一，之后聚合结果向所有的GPU广播。<br> 4、小批量数据量更大时，学习率也需要稍微提高一些。</p> 
<h2>
<a id="GPU_262"></a>多GPU的简洁实现</h2> 
<p>每个新模型的并行计算都从零开始实现是无趣的。此外，优化同步工具以获得高性能也是有好处的。下面我们将展示如何使用深度学习框架的高级API来实现这一点。数学和算法与上面的相同。不出所料，你至少需要两个GPU来运行代码。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> d2l <span class="token keyword">import</span> torch <span class="token keyword">as</span> d2l
</code></pre> 
<h3>
<a id="1_270"></a>1简单网络</h3> 
<p>让我们使用一个比LeNet更有意义的网络，它依然能够容易地和快速地训练。我们选择的是ResNet-18。因为输入的图像很小，所以稍微修改了一下。与之前的区别在于，我们在开始时使用了更小的卷积核、步长和填充，而且删除了最大汇聚层。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">resnet18</span><span class="token punctuation">(</span>num_classes<span class="token punctuation">,</span> in_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""稍加修改的ResNet-18模型"""</span>
    <span class="token keyword">def</span> <span class="token function">resnet_block</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> num_residuals<span class="token punctuation">,</span>
                     first_block<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        blk <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_residuals<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">and</span> <span class="token keyword">not</span> first_block<span class="token punctuation">:</span>
                blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Residual<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span>
                                        use_1x1conv<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> strides<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                blk<span class="token punctuation">.</span>append<span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>Residual<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>blk<span class="token punctuation">)</span>

    <span class="token comment"># 该模型使用了更小的卷积核、步长和填充，而且删除了最大汇聚层</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
        nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block1"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span>
        <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> first_block<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block2"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block3"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"resnet_block4"</span><span class="token punctuation">,</span> resnet_block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"global_avg_pool"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"fc"</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                       nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> net
</code></pre> 
<h3>
<a id="2__302"></a>2 网络初始化</h3> 
<p>我们将在训练回路中初始化网络。</p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> resnet18<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span>
<span class="token comment"># 获取GPU列表</span>
devices <span class="token operator">=</span> d2l<span class="token punctuation">.</span>try_all_gpus<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 我们将在训练代码实现中初始化网络</span>
</code></pre> 
<h3>
<a id="3__311"></a>3 训练</h3> 
<p>如前所述，用于训练的代码需要执行几个基本功能才能实现高效并行：</p> 
<p>1、需要在所有设备上初始化网络参数。</p> 
<p>2、在数据集上迭代时，要将小批量数据分配到所有设备上。</p> 
<p>3、跨设备并行计算损失及其梯度。</p> 
<p>4、聚合梯度，并相应地更新参数。</p> 
<p>最后，并行地计算精确度和发布网络的最终性能。除了需要拆分和聚合数据外，训练代码与前几章的实现非常相似。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> num_gpus<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> d2l<span class="token punctuation">.</span>load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    devices <span class="token operator">=</span> <span class="token punctuation">[</span>d2l<span class="token punctuation">.</span>try_gpu<span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_gpus<span class="token punctuation">)</span><span class="token punctuation">]</span>
    <span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">]</span><span class="token punctuation">:</span>
            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
    net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>
    <span class="token comment"># 在多个GPU上设置模型</span>
    net <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>net<span class="token punctuation">,</span> device_ids<span class="token operator">=</span>devices<span class="token punctuation">)</span>
    trainer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    timer<span class="token punctuation">,</span> num_epochs <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Timer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">10</span>
    animator <span class="token operator">=</span> d2l<span class="token punctuation">.</span>Animator<span class="token punctuation">(</span><span class="token string">'epoch'</span><span class="token punctuation">,</span> <span class="token string">'test acc'</span><span class="token punctuation">,</span> xlim<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> num_epochs<span class="token punctuation">]</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        timer<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>
            trainer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>devices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>devices<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            trainer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        timer<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>
        animator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>d2l<span class="token punctuation">.</span>evaluate_accuracy_gpu<span class="token punctuation">(</span>net<span class="token punctuation">,</span> test_iter<span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'测试精度：</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>animator<span class="token punctuation">.</span>Y<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span><span class="token format-spec">.2f</span><span class="token punctuation">}</span></span><span class="token string">，</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>timer<span class="token punctuation">.</span>avg<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">.1f</span><span class="token punctuation">}</span></span><span class="token string">秒/轮，'</span></span>
          <span class="token string-interpolation"><span class="token string">f'在</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">str</span><span class="token punctuation">(</span>devices<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>让我们看看这在实践中是如何运作的。我们先在单个GPU上训练网络进行预热。</p> 
<pre><code class="prism language-python">train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> num_gpus<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">测试精度：<span class="token number">0.92</span>，<span class="token number">13.7</span>秒<span class="token operator">/</span>轮，在<span class="token punctuation">[</span>device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ab/58/q7WQ0DmS_o.png" alt="在这里插入图片描述"><br> 接下来我们使用2个GPU进行训练。与上文中评估的LeNet相比，ResNet-18的模型要复杂得多。这就是显示并行化优势的地方，计算所需时间明显大于同步参数需要的时间。因为并行化开销的相关性较小，因此这种操作提高了模型的可伸缩性。</p> 
<pre><code class="prism language-python">train<span class="token punctuation">(</span>net<span class="token punctuation">,</span> num_gpus<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
</code></pre> 
<p>输出：</p> 
<pre><code class="prism language-python">测试精度：<span class="token number">0.89</span>，<span class="token number">8.4</span>秒<span class="token operator">/</span>轮，在<span class="token punctuation">[</span>device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> device<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">'cuda'</span><span class="token punctuation">,</span> index<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/c5/d4/eEyZULeb_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="4__373"></a>4 小结</h3> 
<p>1、神经网络可以在（可找到数据的）单GPU上进行自动评估。<br> 2、每台设备上的网络需要先初始化，然后再尝试访问该设备上的参数，否则会遇到错误。<br> 3、优化算法在多个GPU上自动聚合。</p>
</li>
</ul>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>