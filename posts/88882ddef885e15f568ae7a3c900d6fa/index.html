<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>NNDL 实验二 pytorch入门 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NNDL 实验二 pytorch入门</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <h2>文章目录</h2> 
<div> 
 <p id="main-toc-toc" style="margin-left:0px"><a href="#main-toc">一. 概念：张量、算子</a></p> 
 <p id="1%E3%80%81%E5%BC%A0%E9%87%8F-toc" style="margin-left:40px"><a href="#1%E3%80%81%E5%BC%A0%E9%87%8F">1、张量</a></p> 
 <p id="2%E3%80%81%E7%AE%97%E5%AD%90%C2%A0-toc" style="margin-left:40px"><a href="#2%E3%80%81%E7%AE%97%E5%AD%90%C2%A0">2、算子 </a></p> 
 <p id="%E4%BA%8C.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97-toc" style="margin-left:0px"><a href="#%E4%BA%8C.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97">二. 使用pytorch实现张量运算</a></p> 
 <p id="1.2%20%E5%BC%A0%E9%87%8F-toc" style="margin-left:40px"><a href="#1.2%20%E5%BC%A0%E9%87%8F">1.2 张量</a></p> 
 <p id="1.2.1%20%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F-toc" style="margin-left:80px"><a href="#1.2.1%20%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">1.2.1 创建张量</a></p> 
 <p id="1.2.2%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7-toc" style="margin-left:80px"><a href="#1.2.2%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7">1.2.2 张量的属性</a></p> 
 <p id="1.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BF%E9%97%AE-toc" style="margin-left:80px"><a href="#1.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BF%E9%97%AE">1.2.4 张量的访问</a></p> 
 <p id="1.2.5%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%BF%90%E7%AE%97-toc" style="margin-left:80px"><a href="#1.2.5%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%BF%90%E7%AE%97">1.2.5 张量的运算</a></p> 
 <p id="%E4%B8%89.%C2%A0%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-toc" style="margin-left:40px"><a href="#%E4%B8%89.%C2%A0%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">三. 数据预处理</a></p> 
 <p id="%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A-toc" style="margin-left:0px"><a href="#%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A">实验体会</a></p> 
 <hr id="hr-toc">
 <img alt="" height="318" src="https://images2.imgbox.com/d3/5e/Gw5GDuRX_o.jpg" width="1200">
</div> 
<hr>
<p></p> 
<p></p> 
<p></p> 
<h1><a id="pandas_16"></a></h1> 
<h1 id="main-toc"><strong>一. 概念：张量、算子</strong></h1> 
<p><strong>抱歉各位可能有点废话，但是真的查了好多资料，实在不想浪费了</strong></p> 
<h2 id="1%E3%80%81%E5%BC%A0%E9%87%8F">1、张量</h2> 
<p>张量的概念有很多种，首先我们来看一下<span style="color:#fe2c24"><strong>pytorch官方</strong></span>给出的定义：</p> 
<p>Tensors are a specialized data structure that are very similar to arrays and matrices.In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model’s parameters.</p> 
<p>翻译过来就是：</p> 
<p>张量是一种特殊的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用张量对模型的输入和输出以及模型的参数进行编码。</p> 
<p>并且提到：</p> 
<p>Tensors are similar to<a href="https://numpy.org/" title="NumPy’s">NumPy’s</a>ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors andNumPy arrays can often share the same underlying memory, eliminating the need to copy data (see<a href="https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-to-np-label" title="Bridge with NumPy">Bridge with NumPy</a>). Tensorsare also optimized for automatic differentiation (we’ll see more about that later in the<a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html" title="Autograd">Autograd</a>section). If you’re familiar with ndarrays, you’ll be right at home with the Tensor API. If not, follow along!</p> 
<p>这一大段的主要意思是张量与numpy数组的区别就是张量可以在<strong>GPU</strong>上运行，其他的几乎相同。</p> 
<p>在看一下网上大佬们的解析之后跟可以理解到，张量主要通过<strong>三个要素</strong>来确定：</p> 
<p><strong>1、轴的个数（阶）。</strong>例如，3D 张量有 3 个轴，矩阵有 2 个轴。这在 Numpy 等 Python 库中也叫张量的 ndim 。<br><strong>2、形状。</strong>这是一个整数元组，表示张量沿每个轴的维度大小（元素个数）。例如，前面矩阵示例的形状为 (3, 5) ，3D 张量示例的形状为 (3, 3, 5) 。向量的形状只包含一个元素，比如 (5,) ，而标量的形状为空，即 () 。(张量的形状)<br><strong>3、数据类型（在 Python 库中通常叫作 dtype ）。</strong>这是张量中所包含数据的类型，例如，张量的类型可以是 float32 、 uint8 、 float64 等。在极少数情况下，你可能会遇到字符（ char ）张量。注意：Numpy（以及大多数其他库）中不存在字符串张量，因为张量存储在预先分配的连续内存段中，而字符串的长度是可变的，无法用这种方式存储。</p> 
<p><strong>我感觉，张量这一概念的核心在于，它是一个数据容器。它包含的数据几乎总是数值数据，因此它是数字的容器。可能咱们对矩阵很熟悉，它是二维张量。张量是矩阵向任意维度的推广［注意，张量的维度（dimension）通常叫作轴（axis）］。</strong></p> 
<p>关于张量的维度，可以由下面几张图来表示</p> 
<p><img alt="" src="https://images2.imgbox.com/25/1a/nKGMG7Hh_o.jpg"><img alt="" src="https://images2.imgbox.com/0a/db/zGpRJFjT_o.jpg"><img alt="在这里插入图片描述" src="https://images2.imgbox.com/f2/16/xZ4rg2EM_o.jpg"><img alt="在这里插入图片描述" src="https://images2.imgbox.com/56/3c/9Iw2g0sg_o.jpg"></p> 
<p><img alt="在这里插入图片描述" src="https://images2.imgbox.com/d8/c2/TwsEt90A_o.jpg"> <img alt="在这里插入图片描述" src="https://images2.imgbox.com/00/02/24iotViq_o.jpg"><img alt="在这里插入图片描述" src="https://images2.imgbox.com/47/61/rH8e680b_o.jpg"></p> 
<h2 id="2%E3%80%81%E7%AE%97%E5%AD%90%C2%A0">2、算子 </h2> 
<p>英语的算子是<strong>Operator</strong>，含义为操作、运算等等。</p> 
<p>汉语采用算子作为翻译词，可能与算筹有关。</p> 
<p>算子可以理解为，把一个函数变成另一个函数的东西。</p> 
<p>函数是从数到数的映射。</p> 
<p>泛函是从函数到数的映射。</p> 
<p>算子是从函数到函数的映射。</p> 
<p>当然，有的时候这几个词可以混用，比如可以可以把数当作常函数，那么普通的函数也可以看作泛函或算子；再比如考虑从算子到算子的映射，你仍然可以叫它算子。</p> 
<p>具体地讲，比如微分算子（或者叫求导算子）等等等</p> 
<p><strong>我认为，</strong>在深度学习中深度学习算法由一个个计算单元组成，我们称这些计算单元为算子（Operator，简称OP）。在网络模型中，算子对应层中的计算逻辑，例如：卷积层（Convolution Layer）是一个算子；全连接层（Fully-connected Layer， FC layer）中的权值求和过程，是一个算子。</p> 
<h1 id="%E4%BA%8C.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97">
<a id="_19"></a><strong>二. 使用pytorch实现张量运算</strong>
</h1> 
<h2 id="1.2%20%E5%BC%A0%E9%87%8F">
<a id="1_20"></a>1.2 张量</h2> 
<h3 id="1.2.1%20%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">1.2.1 创建张量</h3> 
<p>1.2.1.1 指定数据创建张量</p> 
<pre><code class="language-c language-python"># coding=gbk
import torch
#创建以为Tensor
x = torch.arange(12)  #也是创建以为数组，这个是按序分布的，这个不包括最后一个数
y=torch.tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])   #这个是指定的一维数组
print(x)
print(y)
</code></pre> 
<p>运行结果为 ：</p> 
<blockquote> 
 <p><a id="2_34"></a>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])<br> tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</p> 
</blockquote> 
<p><strong>上边的coding是规定解码方式，要不在pycahrm写注释可能报错，是最好注意这一点。</strong></p> 
<p><strong>.arange(12)  #也是创建以为数组，这个是按序分布的，这个不包括最后一个数</strong></p> 
<p><strong>.tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])   #这个是指定的一维数组。这个函数是输入的元素是啥类型，它输出就是啥类型</strong></p> 
<pre><code class="language-python">#创建二位Tensor
x=torch.tensor([[1.0, 2.0, 3.0],[4.0, 5.0, 6.0]])  #创建指定张量
print(x)</code></pre> 
<p> 运行结果为：</p> 
<blockquote> 
 <p>tensor([[1., 2., 3.],<br>         [4., 5., 6.]])</p> 
</blockquote> 
<pre><code class="language-python"># 创建多维Tensor
ndim_3_Tensor = torch.tensor([[[1, 2, 3, 4, 5],
                                   [6, 7, 8, 9, 10]],
                                  [[11, 12, 13, 14, 15],
                                   [16, 17, 18, 19, 20]]])
print(ndim_3_Tensor)</code></pre> 
<p>运行结果为： </p> 
<blockquote> 
 <p>tensor([[[ 1,  2,  3,  4,  5],<br>          [ 6,  7,  8,  9, 10]],</p> 
 <p>        [[11, 12, 13, 14, 15],<br>          [16, 17, 18, 19, 20]]])</p> 
</blockquote> 
<p><strong>从后两个可以看出，它是以来'[]'区分 是几维，哪个属于哪一维的，可以从数开头的[来确定是几维。</strong></p> 
<p>1.2.1.2 指定形状创建</p> 
<pre><code class="language-python">#创建指定形状的张量
m, n = 2, 3
# 使用paddle.zeros创建数据全为0，形状为[m, n]的Tensor
zeros_Tensor = torch.zeros([m, n])
# 使用paddle.ones创建数据全为1，形状为[m, n]的Tensor
ones_Tensor = torch.ones([m, n])
# 使用paddle.full创建数据全为指定值，形状为[m, n]的Tensor，这里我们指定数据为10
full_Tensor = torch.full([m, n], 10)
print('zeros_Tensor:',zeros_Tensor)
print('ones_Tensor:',ones_Tensor)
print('full_Tensor:',full_Tensor)</code></pre> 
<p>运行结果为： </p> 
<blockquote> 
 <p> zeros_Tensor: tensor([[0., 0., 0.],<br>         [0., 0., 0.]])<br> ones_Tensor: tensor([[1., 1., 1.],<br>         [1., 1., 1.]])<br> full_Tensor: tensor([[10, 10, 10],<br>         [10, 10, 10]])</p> 
</blockquote> 
<p><strong>这几个函数在规定形状时，要注意形状是一个参数，要拿[]括起来，要不然可能会报错</strong></p> 
<p>1.2.1.3 指定区间创建</p> 
<pre><code class="language-python">#指定区间创建
# 使用paddle.arange创建以步长step均匀分隔数值区间[start, end)的一维Tensor
arange_Tensor = torch.arange(start=1, end=5, step=1)  #不包括结尾
# 使用paddle.arange创建以步长step均匀分隔数值区间[start, end)的一维Tensor
range_Tensor = torch.range(start=1, end=5, step=1)    #包括结尾
# 使用paddle.linspace创建以元素个数num均匀分隔数值区间[start, stop]的Tensor
linspace_Tensor = torch.linspace(start=1, end=5, steps=5)
print('arange Tensor: ', arange_Tensor)
print('range Tensor: ', range_Tensor)
print('linspace Tensor: ', linspace_Tensor)</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>arange Tensor:  tensor([1, 2, 3, 4])<br> range Tensor:  tensor([1., 2., 3., 4., 5.])<br> linspace Tensor:  tensor([1., 2., 3., 4., 5.]) </p> 
</blockquote> 
<p>这三个方法，分别是</p> 
<p><strong><code>·torch.arange()是不包含结尾的，</code>为左闭右开，也就是不包含右边的最后一个数，[1,5)。</strong></p> 
<p><strong><code>·torch.range是包含结尾的，</code>为左闭右闭，也就是包含右边的最后一个数，[1,5]。这个方法会报一个警告也就是</strong></p> 
<blockquote> 
 <p><span style="color:#fe2c24">UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).<br>   range_Tensor = torch.range(start=1, end=5, step=1)    #包括结尾</span></p> 
</blockquote> 
<p>这是这个方法要被弃用了，所以大家不要担心 </p> 
<p><strong><code>·torch.linspace()</code>，这个方法也是左闭右闭的，但是这个方法和前面的不太一样，它这里的步长其实和前面的不是一个意思，linspace中的steps与咱们上面两个理解的step不同，steps就是将(start, end)分成包含start和end的多少个数，steps是多少，返回的tensor中就有多少个元素</strong></p> 
<h3 id="1.2.2%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7">1.2.2 张量的属性</h3> 
<p>1.2.2.1 张量的形状</p> 
<pre><code class="language-python">#张量的形状
ndim_4_Tensor = torch.ones([2, 3, 4, 5])

print("Number of dimensions:", ndim_4_Tensor.dim())  #此处ndim=dim（），都是输出向量的维度
print("Shape of Tensor:", ndim_4_Tensor.shape)
print("Elements number along axis 0 of Tensor:", ndim_4_Tensor.shape[0])
print("Elements number along the last axis of Tensor:", ndim_4_Tensor.shape[-1])
print('Number of elements in Tensor: ', ndim_4_Tensor.numel())</code></pre> 
<p> 运行结果为：</p> 
<blockquote> 
 <p>Number of dimensions: 4<br> Shape of Tensor: torch.Size([2, 3, 4, 5])<br> Elements number along axis 0 of Tensor: 2<br> Elements number along the last axis of Tensor: 5<br> Number of elements in Tensor:  120</p> 
</blockquote> 
<p><strong>.din()=.ndim,这两个方法是等价的。</strong></p> 
<p><strong>shape[n],这个直观理解就是n是几就是第几维上有几个元素。说直观点就是n是几就数到几，看那个数是多少</strong></p> 
<p><strong>.numel是统计总共有多少个元素</strong></p> 
<p>1.2.2.2 形状的改变</p> 
<pre><code class="language-python"># 定义一个shape为[3,2,5]的三维Tensor
ndim_3_Tensor = torch.tensor([[[1, 2, 3, 4, 5],
                                   [6, 7, 8, 9, 10]],
                                  [[11, 12, 13, 14, 15],
                                   [16, 17, 18, 19, 20]],
                                  [[21, 22, 23, 24, 25],
                                   [26, 27, 28, 29, 30]]])
print("the shape of ndim_3_Tensor:", ndim_3_Tensor.shape)
# torch.reshape 可以保持在输入数据不变的情况下，改变数据形状。这里我们设置reshape为[2,5,3]
reshape_Tensor = torch.reshape(ndim_3_Tensor, [2, 5, 3])   #此时有参数可以为-1，因为-1位置的参数可以由系统计算得出，0是复制对应维度
print("After reshape:", reshape_Tensor)</code></pre> 
<p>运行结果为： </p> 
<blockquote> 
 <p>the shape of ndim_3_Tensor: torch.Size([3, 2, 5])<br> After reshape: tensor([[[ 1,  2,  3],<br>          [ 4,  5,  6],<br>          [ 7,  8,  9],<br>          [10, 11, 12],<br>          [13, 14, 15]],</p> 
 <p>        [[16, 17, 18],<br>          [19, 20, 21],<br>          [22, 23, 24],<br>          [25, 26, 27],<br>          [28, 29, 30]]])</p> 
</blockquote> 
<p><strong>使用reshape时存在一些技巧，比如：</strong></p> 
<ul>
<li><strong>-1表示这个维度的值是从张量的元素总数和剩余维度推断出来的。因此，有且只有一个维度可以被设置为-1。</strong></li>
<li><strong>0表示实际的维数是从张量的对应维数中复制出来的，因此shape中0所对应的索引值不能超过张量的总维度。</strong></li>
</ul>
<p>1.2.2.3 张量的数据类型</p> 
<pre><code class="language-python"># 使用torch.tensor通过已知数据来创建一个Tensor
print("Tensor dtype from Python integers:", torch.tensor(1).dtype)
print("Tensor dtype from Python floating point:", torch.tensor(1.0).dtype)</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>Tensor dtype from Python integers: torch.int64<br> Tensor dtype from Python floating point: torch.float32 </p> 
</blockquote> 
<p><strong> pytorch只能有这八种类型，并且规定类型时只能严格用这八个，<span style="color:#fe2c24">不能只写int，经过尝试之后会报错。</span></strong></p> 
<p> <strong>dtype：</strong>张量的数据类型，分成浮点，整型和布尔三大类，共9种数据类型，如下表所示（引自<a href="https://pytorch.org/docs/stable/tensors.html" title="pytorch官网">pytorch官网</a>），其中32位浮点和64位整型最为常用。图像预处理结果默认：<code>torch.float32，</code>图像标签默认：<code>torch.int64.</code><br><img alt="" height="528" src="https://images2.imgbox.com/22/51/uChRyg0Z_o.png" width="1200"></p> 
<p>1.2.2.4 张量的设备位置</p> 
<pre><code class="language-python"># 创建CPU上的Tensor
cpu_Tensor = torch.tensor(1, device='cpu:0')
# 通过Tensor.place查看张量所在设备位置
print('cpu Tensor: ', cpu_Tensor.device)
# 创建GPU上的Tensor
gpu_Tensor = torch.tensor(1, device='cuda') #可以用to方法来回转化
print('gpu Tensor: ', gpu_Tensor.device)
# 创建固定内存上的Tensor
pin_memory_Tensor = torch.tensor(1, pin_memory=True)  #会影响电脑运行速度
print('pin memory Tensor: ', pin_memory_Tensor.device)</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>cpu Tensor:  cpu</p> 
 <p>gpu Tensor:  cuda:0<br> pin memory Tensor:  cpu </p> 
</blockquote> 
<p><strong>正常情况下，常规的是现在cpu上生成张量，然后判断是否有GPU，若有用to方法将它转到GPU上，也可以直接在GPU上生成，但是这样通用不太好。 </strong></p> 
<p><strong> pin_memory=True，是在，固定内存上存储，但是这个要注意定义太多会影响运行速度。</strong></p> 
<p>1.2.3 张量与Numpy数组转换</p> 
<pre><code class="language-python">ndim_1_Tensor = torch.tensor([1., 2.])
# 将当前 Tensor 转化为 numpy.ndarray
print('Tensor to convert: ', ndim_1_Tensor.numpy())</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>Tensor to convert:  [1. 2.] </p> 
</blockquote> 
<p><strong>.numpy() .tolist() .tensor()可以实现转化numpy、tensor、list之间互相转化 </strong></p> 
<h3 id="1.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BF%E9%97%AE">1.2.4 张量的访问</h3> 
<p>1.2.4.1 索引和切片</p> 
<pre><code class="language-python"># 定义1个一维Tensor
ndim_1_Tensor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])

print("Origin Tensor:", ndim_1_Tensor)
print("First element:", ndim_1_Tensor[0])
print("Last element:", ndim_1_Tensor[-1])
print("All element:", ndim_1_Tensor[:])
print("Before 3:", ndim_1_Tensor[:3])
print("Interval of 3:", ndim_1_Tensor[::3])
print("Reverse:", torch.flip(ndim_1_Tensor,dims=[0]))   #pytorch没有复数步长，故无法通过切片操作达到逆序输出</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p> Origin Tensor: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])<br> First element: tensor(0)<br> Last element: tensor(8)<br> All element: tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])<br> Before 3: tensor([0, 1, 2])<br> Interval of 3: tensor([0, 3, 6])<br> Reverse: tensor([8, 7, 6, 5, 4, 3, 2, 1, 0])</p> 
</blockquote> 
<p><strong>我们可以通过索引或切片方便地访问或修改张量。与Numpy索引规则，具有以下特点： </strong></p> 
<ul>
<li><strong>基于0−n0−n的下标进行索引，如果下标为负数，则从尾部开始计算。</strong></li>
<li><strong>通过冒号“:”分隔切片参数start:stop:step来进行切片操作，也就是访问start到stop范围内的部分元素并生成一个新的序列。其中start为切片的起始位置，stop为切片的截止位置，step是切片的步长，这三个参数均可缺省。</strong></li>
</ul>
<p><strong> 同时注意唯一的不同是pytorch没有负数步长，<span style="color:#fe2c24">所以会报错</span>，所以可以用torch.flip(ndim_1_Tensor,dims=[0])直接将张量逆序再输出。但是TensorFlow和numpy都有负数的步长。</strong></p> 
<p>1.2.4.2 访问张量</p> 
<pre><code class="language-python"># 定义1个二维Tensor
ndim_2_Tensor = torch.tensor([[0, 1, 2, 3],
                                  [4, 5, 6, 7],
                                  [8, 9, 10, 11]])
print("Origin Tensor:", ndim_2_Tensor)
print("First row:", ndim_2_Tensor[0])
print("First row:", ndim_2_Tensor[0, :])
print("First column:", ndim_2_Tensor[:, 0])
print("Last column:", ndim_2_Tensor[:, -1])
print("All element:", ndim_2_Tensor[:])
print("First row and second column:", ndim_2_Tensor[0, 1])</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>Origin Tensor: tensor([[ 0,  1,  2,  3],<br>         [ 4,  5,  6,  7],<br>         [ 8,  9, 10, 11]])<br> First row: tensor([0, 1, 2, 3])<br> First row: tensor([0, 1, 2, 3])<br> First column: tensor([0, 4, 8])<br> Last column: tensor([ 3,  7, 11])<br> All element: tensor([[ 0,  1,  2,  3],<br>         [ 4,  5,  6,  7],<br>         [ 8,  9, 10, 11]])<br> First row and second column: tensor(1) </p> 
</blockquote> 
<p><strong>注意-1代表的是 列数从右往左数第一个，这个切边与索引与numpy一样，只是注意没有负数的步长。</strong></p> 
<p>1.2.4.3 修改张量</p> 
<pre><code class="language-python"># 定义1个二维Tensor
ndim_2_Tensor = torch.ones(size=(2,3),dtype=torch.float32)
print('Origin Tensor: ', ndim_2_Tensor)
# 修改第1维为0
ndim_2_Tensor[0] = 0
print('change Tensor: ', ndim_2_Tensor)
# 修改第1维为2.1
ndim_2_Tensor[0:1] = 2.1
print('change Tensor: ', ndim_2_Tensor)
# 修改全部Tensor
ndim_2_Tensor[...] = 3
print('change Tensor: ', ndim_2_Tensor)</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>Origin Tensor:  tensor([[1., 1., 1.],<br>         [1., 1., 1.]])<br> change Tensor:  tensor([[0., 0., 0.],<br>         [1., 1., 1.]])<br> change Tensor:  tensor([[2.1000, 2.1000, 2.1000],<br>         [1.0000, 1.0000, 1.0000]])<br> change Tensor:  tensor([[3., 3., 3.],<br>         [3., 3., 3.]]) </p> 
</blockquote> 
<p><strong>这个注意“...”这个用法是所有的意思，tensor[...,1] 表示的就是所有行，第一列，在列上用“...”用法与在行上用一致，其他的与numpy的切边与索引的方式一致</strong></p> 
<p><strong>同时注意之前说过的问题，不能直接用int等定义，要严格的按pytorch的规定，torch.dtype</strong></p> 
<h3 id="1.2.5%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%BF%90%E7%AE%97">1.2.5 张量的运算</h3> 
<p>1.2.5.1 数学运算</p> 
<pre><code class="language-python"># 定义两个Tensor
x = torch.tensor([[1.1, 2.2], [3.3, 4.4]], dtype=torch.float64)
y = torch.tensor([[5.5, 6.6], [7.7, 8.8]], dtype=torch.float64)
# 第一种调用方法，paddle.add逐元素相加算子，并将各个位置的输出元素保存到返回结果中
print('Method 1: ', torch.add(x, y))
# 第二种调用方法
print('Method 2: ', x.add(y))</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>Method 1:  tensor([[ 6.6000,  8.8000],<br>         [11.0000, 13.2000]], dtype=torch.float64)<br> Method 2:  tensor([[ 6.6000,  8.8000],<br>         [11.0000, 13.2000]], dtype=torch.float64) </p> 
</blockquote> 
<p><strong> PyTorch算术函数包含简单的加减乘除运算: add()，subtract()，multiply() 和 divide()。</strong></p> 
<p><strong>算术运算后得到的结果依然是是相同维度的数组。</strong></p> 
<p><strong><a href="https://so.csdn.net/so/search?q=Tensor&amp;spm=1001.2101.3001.7020" title="Tensor">Tensor</a>求和以及按索引求和：torch.sum() torch.Tensor.indexadd()</strong></p> 
<p><strong>Tensor元素乘积：torch.prod(input)</strong></p> 
<p><strong>对Tensor求均值、方差、极值：</strong></p> 
<p><strong>torch.mean() torch.var()</strong></p> 
<p><strong>torch.max() torch.min()</strong></p> 
<p><strong>最后还有在NLP领域经常用到的：</strong></p> 
<p><strong>求Tensor的平方根倒数、线性插值、双曲正切</strong></p> 
<p><strong>torch.rsqrt(input) torch.lerp(star,end,weight)</strong></p> 
<p><strong>torch.tanh(input, out=None)</strong></p> 
<p><strong>这是一些常用的运算函数</strong></p> 
<p>1.2.5.2 逻辑运算</p> 
<pre><code class="language-python">x = torch.tensor([[1.1, 2.2], [3.3, 4.4]], dtype=torch.float64)
y = torch.tensor([[5.5, 6.6], [7.7, 8.8]], dtype=torch.float64)
print(x.isfinite())                  # 判断Tensor中元素是否是有限的数字，即不包括inf与nan
print(x.equal(y))                    # 判断两个Tensor的全部元素是否相等，并返回形状为[1]的布尔类Tensor
print(x.eq(y))                       # 判断两个Tensor的每个元素是否相等，并返回形状相同的布尔类Tensor
print(x.not_equal(y))                # 判断两个Tensor的每个元素是否不相等
print(x.lt(y))                # 判断Tensor x的元素是否小于Tensor y的对应元素    #less的别名
print(x.less_equal(y))               # 判断Tensor x的元素是否小于或等于Tensor y的对应元素     #le的别名
print(x.gt(y))             # 判断Tensor x的元素是否大于Tensor y的对应元素  #greater的别名
print(x.greater_equal(y))            # 判断Tensor x的元素是否大于或等于Tensor y的对应元素   #ge的别名
print(x.allclose(y))                 # 判断两个Tensor的全部元素是否接近</code></pre> 
<p> 运行结果为：</p> 
<blockquote> 
 <p>tensor([[True, True],<br>         [True, True]])<br> False<br> tensor([[False, False],<br>         [False, False]])<br> tensor([[True, True],<br>         [True, True]])<br> tensor([[True, True],<br>         [True, True]])<br> tensor([[True, True],<br>         [True, True]])<br> tensor([[False, False],<br>         [False, False]])<br> tensor([[False, False],<br>         [False, False]])<br> False</p> 
</blockquote> 
<p><img alt="" height="938" src="https://images2.imgbox.com/1c/cf/dI5skurh_o.png" width="650"></p> 
<p> 1.2.5.3 矩阵运算</p> 
<pre><code class="language-python">x = torch.tensor([[1.1, 2.2], [3.3, 4.4]], dtype=torch.float64)
y = torch.tensor([[5.5, 6.6], [7.7, 8.8]], dtype=torch.float64)
print(x.T)                         # 矩阵转置
print(x.transpose(1, 0))           # 交换第 0 维与第 1 维的顺序
print(x.norm('fro'))                 # 矩阵的弗罗贝尼乌斯范数
print(x.dist(y, p=2))                # 矩阵（x-y）的2范数
print(x.matmul(y))                   # 矩阵乘法   #这个若矩阵维度不相同，也能相乘</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>tensor([[1.1000, 3.3000],<br>         [2.2000, 4.4000]], dtype=torch.float64)<br> tensor([[1.1000, 3.3000],<br>         [2.2000, 4.4000]], dtype=torch.float64)<br> tensor(6.0249, dtype=torch.float64)<br> tensor(8.8000, dtype=torch.float64)<br> tensor([[22.9900, 26.6200],<br>         [52.0300, 60.5000]], dtype=torch.float64) </p> 
</blockquote> 
<p><strong><a href="https://so.csdn.net/so/search?q=torch&amp;spm=1001.2101.3001.7020" title="注意torch">注意torch</a>.transpose(Tensor,dim0,dim1)是pytorch中的ndarray矩阵进行转置的操作 </strong></p> 
<p><strong>transpose()一次只能在两个<a href="https://so.csdn.net/so/search?q=%E7%BB%B4%E5%BA%A6&amp;spm=1001.2101.3001.7020" title="维度">维度</a>间进行转置（也可以理解为维度转换）</strong></p> 
<p><strong>tranpose（）也能实现矩阵转置但是.T更方便。</strong></p> 
<p><strong>.matmul()需要符合矩阵运算的规则。</strong></p> 
<p>1.2.5.4 广播机制</p> 
<pre><code class="language-python"># 当两个Tensor的形状一致时，可以广播
x = torch.ones((2, 3, 4))
y = torch.ones((2, 3, 4))
z = x + y
print('broadcasting with two same shape tensor: ', z.shape)

x = torch.ones((2, 3, 1, 5))
y = torch.ones((3, 4, 1))
# 从后往前依次比较：
# 第一次：y的维度大小是1
# 第二次：x的维度大小是1
# 第三次：x和y的维度大小相等，都为3
# 第四次：y的维度不存在
# 所以x和y是可以广播的
z = x + y
print('broadcasting with two different shape tensor:', z.shape)
'''
x = torch.ones((2, 3, 4))
y = torch.ones((2, 3, 6))
z = x + y
'''</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>broadcasting with two same shape tensor:  torch.Size([2, 3, 4])<br> broadcasting with two different shape tensor: torch.Size([2, 3, 4, 5]) </p> 
</blockquote> 
<p><strong>定义：</strong></p> 
<p><strong>如果一个<code>PyTorch</code>操作支持广播，则其<code>Tensor</code>参数可以自动扩展为相等大小（不需要复制数据）。通常情况下，小一点的<a href="https://so.csdn.net/so/search?q=%E6%95%B0%E7%BB%84&amp;spm=1001.2101.3001.7020" title="数组">数组</a>会被 <code>broadcast</code> 到大一点的，这样才能保持大小一致。 </strong></p> 
<p><strong>如果遵守以下规则，则两个tensor是“可广播的”：<br>    1、每个tensor至少有一个维度；<br>    2、遍历tensor所有维度时，从末尾随开始遍历，两个tensor存在下列情况：<br>        1.tensor维度相等。<br>        2.tensor维度不等且其中一个维度为1。<br>        3.tensor维度不等且其中一个维度不存在。<br> 如果两个tensor是“可广播的”，则计算过程遵循下列规则：<br>     1、如果两个tensor的维度不同，则在维度较小的tensor的前面增加维度，使它们维度相等。<br>     2、对于每个维度，计算结果的维度值取两个tensor中较大的那个值。<br>     3、两个tensor扩展维度的过程是将数值进行复制。</strong><br><span style="color:#fe2c24">但是就像我'''  ''' 的那部分。那部分不符合传播机制的要求，故无法传播要特别注意</span></p> 
<h2 id="%E4%B8%89.%C2%A0%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">三. <strong>数据预处理</strong>
</h2> 
<p>1. 读取数据集 house_tiny.csv、boston_house_prices.csv、Iris.csv</p> 
<pre><code class="language-python">import pandas as pd</code></pre> 
<p><strong>这个要单独说一下，是因为，之前我就说了，我更喜欢用pandas来读带标签的数据，并且read_csv不仅csv可以读，txt也可以读，所以比较方便。</strong> </p> 
<pre><code class="language-c language-python">house_tiny=pd.read_csv('house_tiny.csv',encoding='gbk')
boston_house_prices=pd.read_csv('boston_house_prices.csv',encoding='gbk')
Iris = pd.read_csv('Iris.csv', encoding='gbk')
print(house_tiny)
print(boston_house_prices)
print(Iris)</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p> NumRooms Alley   Price<br> 0       NaN  Pave  127500<br> 1       2.0   NaN  106000<br> 2       4.0   NaN  178100<br> 3       NaN   NaN  140000<br>         CRIM    ZN  INDUS  CHAS    NOX  ...  RAD  TAX  PTRATIO  LSTAT  MEDV<br> 0    0.00632  18.0   2.31     0  0.538  ...    1  296     15.3   4.98  24.0<br> 1    0.02731   0.0   7.07     0  0.469  ...    2  242     17.8   9.14  21.6<br> 2    0.02729   0.0   7.07     0  0.469  ...    2  242     17.8   4.03  34.7<br> 3    0.03237   0.0   2.18     0  0.458  ...    3  222     18.7   2.94  33.4<br> 4    0.06905   0.0   2.18     0  0.458  ...    3  222     18.7   5.33  36.2<br> ..       ...   ...    ...   ...    ...  ...  ...  ...      ...    ...   ...<br> 501  0.06263   0.0  11.93     0  0.573  ...    1  273     21.0   9.67  22.4<br> 502  0.04527   0.0  11.93     0  0.573  ...    1  273     21.0   9.08  20.6<br> 503  0.06076   0.0  11.93     0  0.573  ...    1  273     21.0   5.64  23.9<br> 504  0.10959   0.0  11.93     0  0.573  ...    1  273     21.0   6.48  22.0<br> 505  0.04741   0.0  11.93     0  0.573  ...    1  273     21.0   7.88  11.9</p> 
 <p>[506 rows x 13 columns]<br>       Id  SepalLengthCm  ...  PetalWidthCm         Species<br> 0      1            5.1  ...           0.2     Iris-setosa<br> 1      2            4.9  ...           0.2     Iris-setosa<br> 2      3            4.7  ...           0.2     Iris-setosa<br> 3      4            4.6  ...           0.2     Iris-setosa<br> 4      5            5.0  ...           0.2     Iris-setosa<br> ..   ...            ...  ...           ...             ...<br> 145  146            6.7  ...           2.3  Iris-virginica<br> 146  147            6.3  ...           1.9  Iris-virginica<br> 147  148            6.5  ...           2.0  Iris-virginica<br> 148  149            6.2  ...           2.3  Iris-virginica<br> 149  150            5.9  ...           1.8  Iris-virginica</p> 
 <p>[150 rows x 6 columns]</p> 
 <p></p> 
</blockquote> 
<p><strong>读的时候要注意解码方式，enconding就是解码方式，<span style="color:#fe2c24">并且要把数据复制到根目录下这样不用考虑路径问题</span>，这个数据NAN表示的数据缺失 </strong></p> 
<p>2. 处理缺失值</p> 
<pre><code class="language-python">inputs, outputs = house_tiny.iloc[:, 0:2], house_tiny.iloc[:, 2]
print(inputs)
print(outputs)
inputs = inputs.fillna(inputs.mean())
inputs = pd.get_dummies(inputs, dummy_na=True)
data=pd.concat([inputs,outputs],axis=1)     #inputs['Price']=house_tiny['Price']
print(data)
</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>   NumRooms Alley<br> 0       NaN  Pave<br> 1       2.0   NaN<br> 2       4.0   NaN<br> 3       NaN   NaN<br> 0    127500<br> 1    106000<br> 2    178100<br> 3    140000<br> Name: Price, dtype: int64   </p> 
 <p>NumRooms  Alley_Pave  Alley_nan   Price<br> 0       3.0           1          0  127500<br> 1       2.0           0          1  106000<br> 2       4.0           0          1  178100<br> 3       3.0           0          1  140000 </p> 
</blockquote> 
<ul>
<li> <p><strong>dropna：</strong>根据每个标签的值是否是缺失数据来筛选轴标签，并根据允许丢失的数据量来确定阈值</p> </li>
<li> <p><strong>fillna：</strong>用某些值填充缺失的数据或使用插值方法(如“ffill”或“bfill”)。</p> </li>
<li> <p><strong>isnull：</strong>返回表明哪些值是缺失值的布尔值</p> </li>
<li> <p><strong>notnull：</strong>isnull的反作用函数</p> </li>
</ul>
<p> 这几个函数够可以用来处理缺失值但是，常用还是fillna，所以重点说一下fillna的用法</p> 
<p><strong>fillna(value=None, method=None, <a href="https://so.csdn.net/so/search?q=axis&amp;spm=1001.2101.3001.7020" title="axis">axis</a>=None, inplace=False, limit=None, downcast=None, **kwargs)</strong></p> 
<p><strong>value：</strong>用于填充的空值的值。</p> 
<p><strong>method：</strong> {'backfill', 'bfill', 'pad', 'ffill', None}, default None。定义了填充空值的方法， pad / ffill表示用前面行/列的值，填充当前行/列的空值， backfill / bfill表示用后面行/列的值，填充当前行/列的空值。</p> 
<p><strong>axis：</strong>轴。0或'index'，表示按行删除；1或'columns'，表示按列删除。</p> 
<p><strong>inplace：</strong>是否原地替换。布尔值，默认为False。如果为True，则在原DataFrame上进行操作，返回值为None。</p> 
<p><strong>这里必须要说一下的是，我写注释的那是连接起来的部分，我用的是conact函数，但是更简便的是用pandas创建列的方法，<span style="color:#fe2c24">但是我一开始用的是outputs，所以有问题，我就用了conact，但是为什么又问实在是有点不太明白。</span></strong></p> 
<p>3. 转换为张量格式</p> 
<pre><code class="language-python">X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
print(X, y)
Iris['Species'].replace({'Iris-setosa':1,'Iris-versicolor':2,'Iris-virginica':3},inplace=True)
Iris=torch.tensor(Iris.values)  #.values方法不能转化带有字符串的
print(Iris)
boston_house=torch.tensor(boston_house_prices.values)
print(boston_house)</code></pre> 
<p>运行结果为：</p> 
<blockquote> 
 <p>tensor([[3., 1., 0.],<br>         [2., 0., 1.],<br>         [4., 0., 1.],<br>         [3., 0., 1.]], dtype=torch.float64) tensor([127500, 106000, 178100, 140000])</p> 
 <p>tensor([[1.0000e+00, 5.1000e+00, 3.5000e+00, 1.4000e+00, 2.0000e-01, 1.0000e+00],<br>         [2.0000e+00, 4.9000e+00, 3.0000e+00, 1.4000e+00, 2.0000e-01, 1.0000e+00],<br>         [3.0000e+00, 4.7000e+00, 3.2000e+00, 1.3000e+00, 2.0000e-01, 1.0000e+00],<br>         [4.0000e+00, 4.6000e+00, 3.1000e+00, 1.5000e+00, 2.0000e-01, 1.0000e+00],<br>         [5.0000e+00, 5.0000e+00, 3.6000e+00, 1.4000e+00, 2.0000e-01, 1.0000e+00],<br>         [6.0000e+00, 5.4000e+00, 3.9000e+00, 1.7000e+00, 4.0000e-01, 1.0000e+00],<br>         [1.5000e+02, 5.9000e+00, 3.0000e+00, 5.1000e+00, 1.8000e+00, 3.0000e+00]],<br>        dtype=torch.float64) </p> 
 <p>tensor([[6.3200e-03, 1.8000e+01, 2.3100e+00,  ..., 1.5300e+01, 4.9800e+00,<br>          2.4000e+01],<br>         [2.7310e-02, 0.0000e+00, 7.0700e+00,  ..., 1.7800e+01, 9.1400e+00,<br>          2.1600e+01],<br>         [2.7290e-02, 0.0000e+00, 7.0700e+00,  ..., 1.7800e+01, 4.0300e+00,<br>          3.4700e+01],<br>         ...,<br>         [6.0760e-02, 0.0000e+00, 1.1930e+01,  ..., 2.1000e+01, 5.6400e+00,<br>          2.3900e+01],<br>         [1.0959e-01, 0.0000e+00, 1.1930e+01,  ..., 2.1000e+01, 6.4800e+00,<br>          2.2000e+01],<br>         [4.7410e-02, 0.0000e+00, 1.1930e+01,  ..., 2.1000e+01, 7.8800e+00,<br>          1.1900e+01]], dtype=torch.float64)</p> 
</blockquote> 
<p><strong>鸢尾花那个有点多就取得开头和结尾</strong></p> 
<p><strong>这里注意一点，.values无法转换字符串，要是直接转换会报错，会提示可以转换的有啥，其中不包括字符串，所以要用1,2,3，来替换她，用的是replace方法，这个方法还必须用字典的那种才可以，可以看看我写的代码。 </strong></p> 
<hr>
<h1 id="%E5%AE%9E%E9%AA%8C%E4%BD%93%E4%BC%9A">
<a id="_45"></a>实验体会</h1> 
<p>第一部分，是明确了，张量，算子的概念，之前根本没考虑过明确的概念，就只是直接用，现在终于明确了概念，张量看了好多文献有点明白了，<strong><span style="color:#fe2c24">但是算子的概念在pytorch的官方文献上是没有的，所有又去看了好多，还是有点不太明白</span></strong>，感谢老师，感觉明白概念以后，看好多文献才更加明白了，概念是一切的基础。</p> 
<p>第二部分，是对照了两本书把paddlepaddle的代码，改成pytorch的代码，这部分明白了好多以前有点模糊的用法吗，更加系统了，之前是用着啥查啥，这次之后更加系统了更加清楚了，<span style="color:#fe2c24"><strong>尤其是对张量设备位置的代码，以前都是用固定的语句，就是判断的那种，有GPU再转化过去，尤其是固定内存那还是不太明白</strong></span>，传播机制以前就是认为它叫自动填充，但是这次之后明白了它的使用条件。</p> 
<p>第三部分是之前我说过的预处理部分，我用的pandas但是，再处理缺失值之后，连接的那，<strong>我用的是conact函数，但是更简便的是用pandas创建列的方法，<span style="color:#fe2c24">但是我一开始用的是outputs，所以有问题，inputs['Price']=outputs['Price']是报错的，我输出之后不知道为什么outputs是那样的，我就用了conact，但是为什么是这样，实在是有点不太明白，之后就是正常的转化。</span></strong></p> 
<p><span style="color:#0d0016"><strong>最后，感觉这次实验学到了很多，谢谢老师</strong></span></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>