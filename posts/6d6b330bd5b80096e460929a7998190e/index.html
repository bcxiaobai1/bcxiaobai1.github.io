<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>《机器学习实战：基于Scikit-Learn、Keras和TensorFlow第2版》-学习笔记（8）：降维 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《机器学习实战：基于Scikit-Learn、Keras和TensorFlow第2版》-学习笔记（8）：降维</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <blockquote> 
 <p>· Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, by Aurélien Géron (O’Reilly). Copyright 2019 Aurélien Géron, 978-1-492-03264-9.<br> · 《机器学习》周志华<br> · 环境：Anaconda（Python 3.8） + Pycharm<br> · 学习时间：2022.05.07~2022.05.08</p> 
</blockquote> 
<h1>
<a id="__6"></a>第八章 降维</h1> 
<p>许多机器学习问题涉及每个训练实例的成千上万甚至数百万个特征。正如我们将看到的那样，所有这些特征不仅使训练变得极其缓慢，而且还会使找到好的解决方案变得更加困难。这个问题通常称为维度的诅咒。</p> 
<p>幸运的是，在实际问题中，通常可以大大减少特征的数量，从而将棘手的问题转变为易于解决的问题。例如，考虑MNIST图像（在第3章中介绍）：图像边界上的像素几乎都是白色，因此你可以从训练集中完全删除这些像素而不会丢失太多信息。图7-6确认了这些像素对于分类任务而言完全不重要。另外，两个相邻的像素通常是高度相关的，如果将它们合并为一个像素（例如，通过取两个像素的平均值），不会丢失太多信息。</p> 
<blockquote> 
 <p>数据降维确实会丢失一些信息（就好比将图像压缩为JPEG会降低其质量一样），所以，它虽然能够加速训练，但是也会轻微降低系统性能。同时它也让流水线更为复杂，维护难度上升。因此，如果训练太慢，你首先应该尝试的还是继续使用原始数据，然后再考虑数据降维。不过在某些情况下，降低训练数据的维度可能会滤除掉一些不必要的噪声和细节，从而导致性能更好（但通常来说不会，它只会加速训练）。</p> 
 <p>除了加快训练，降维对于数据可视化（或称DataViz）也非常有用。将维度降到两个（或三个），就可以在图形上绘制出高维训练集，通过视觉来检测模式，常常可以获得一些十分重要的洞察，比如聚类。此外，DataViz对于把你的结论传达给非数据科学家至关重要，尤其是将使用你的结果的决策者。</p> 
</blockquote> 
<p>本章将探讨维度的诅咒，简要介绍高维空间中发生的事情。然后，我们将介绍两种主要的数据降维方法（投影和流形学习），并学习现在最流行的三种数据降维技术：PCA、Kernal PCA以及LLE。</p> 
<p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#__6">第八章 降维</a></li>
<li>
<ul>
<li><a href="#81__19">8.1 维度的诅咒</a></li>
<li><a href="#82__31">8.2 降维的主要方法</a></li>
<li>
<ul>
<li><a href="#821__37">8.2.1 投影</a></li>
<li><a href="#822__61">8.2.2 流形学习</a></li>
</ul>
   </li>
<li><a href="#83__PCA_82">8.3 主成分分析 PCA</a></li>
<li>
<ul>
<li><a href="#831__88">8.3.1 保留差异性</a></li>
<li><a href="#832__100">8.3.2 主要成分</a></li>
<li><a href="#833_d_141">8.3.3 向下投影到d维度</a></li>
<li><a href="#834_ScikitLearn_160">8.3.4 使用Scikit-Learn</a></li>
<li><a href="#835__209">8.3.5 可解释方差比</a></li>
<li><a href="#836__225">8.3.6 选择正确的维度</a></li>
<li><a href="#837_PCA_264">8.3.7 PCA压缩</a></li>
<li><a href="#838_PCA_291">8.3.8 随机PCA</a></li>
<li><a href="#839_PCAIPCA_305">8.3.9 增量PCA（IPCA）</a></li>
</ul>
   </li>
<li><a href="#84_PCAkPCA_334">8.4 内核PCA（kPCA）</a></li>
<li>
<ul><li><a href="#_356">选择内核并调整超参数</a></li></ul>
   </li>
<li><a href="#85_LLE_416">8.5 局部线性嵌入LLE</a></li>
<li><a href="#86__448">8.6 其他降维技术</a></li>
<li><a href="#87__478">8.7 练习题</a></li>
<li>
<ul>
<li><a href="#_480">问题</a></li>
<li><a href="#_502">答案</a></li>
</ul>
   </li>
<li><a href="#88__534">8.8 阅读材料</a></li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<h2>
<a id="81__19"></a>8.1 维度的诅咒</h2> 
<p>我们太习惯三维空间的生活，所以当我们试图去想象一个高维空间时，直觉思维很难成功。即使是一个基本的四维超立方体，我们也很难在脑海中想象出来，更不用说在一个千维空间中弯曲的二百维椭圆体。</p> 
<p>事实证明，在高维空间中，许多事物的行为都迥然不同。例如，如果你在一个单位平面（1×1的正方形）内随机选择一个点，那么这个点离边界的距离小于0.001的概率只有约0.4%（也就是说，一个随机的点不大可能刚好位于某个维度的“极端”）。但是，在一个10 000维的单位超立方体（1×1…×1立方体，一万个1）中，这个概率大于99.99999%。高维超立方体中大多数点都非常接近边界。</p> 
<p>还有一个更麻烦的区别：如果你在单位平面中随机挑两个点，这两个点之间的平均距离大约为0.52。如果在三维的单位立方体中随机挑两个点，两点之间的平均距离大约为0.66。但是，如果在一个100万维的超立方体中随机挑两个点呢？不管你相信与否，平均距离大约为408.25（约等于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          1000000
         
         
          /
         
         
          6
         
        
       
      
      
       sqrt{1000000/6}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.24em;vertical-align: -0.305em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.935em"><span class="svg-align"><span class="pstrut" style="height: 3.2em"></span><span class="mord" style="padding-left: 1em"><span class="mord">1</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord">0</span><span class="mord">/</span><span class="mord">6</span></span></span><span class=""><span class="pstrut" style="height: 3.2em"></span><span class="hide-tail" style="min-width: 1.02em;height: 1.28em">
           
            
           </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.305em"><span class=""></span></span></span></span></span></span></span></span></span>）！这是非常违背直觉的：位于同一个单位超立方体中的两个点，怎么可能距离如此之远？这个事实说明高维数据集有很大可能是非常稀疏的：大多数训练实例可能彼此之间相距很远。当然，这也意味着新的实例很可能远离任何一个训练实例，导致跟低维度相比，预测更加不可靠，因为它们基于更大的推测。简而言之，训练集的维度越高，过拟合的风险就越大。</p> 
<p>理论上来说，通过增大训练集，使训练实例达到足够的密度，是可以解开维度的诅咒的。然而不幸的是，实践中，要达到给定密度，所需要的训练实例数量随着维度的增加呈指数式上升。仅仅100个特征下（远小于MNIST问题），要让所有训练实例（假设在所有维度上平均分布）之间的平均距离小于0.1，你需要的训练实例数量就比可观察宇宙中的原子数量还要多。</p> 
<h2>
<a id="82__31"></a>8.2 降维的主要方法</h2> 
<p>在深入研究特定的降维算法之前，让我们看一下减少维度的两种主要方法：<strong>投影</strong>和<strong>流形学习</strong>。</p> 
<h3>
<a id="821__37"></a>8.2.1 投影</h3> 
<p>在大多数实际问题中，训练实例并不是均匀地分布在所有维度上。许多特征几乎是恒定不变的，而其他特征则是高度相关的（如之前针对MNIST所述）。结果，所有训练实例都位于（或接近于）高维空间的低维子空间内。这听起来很抽象，所以让我们看一个示例。在下图中，你可以看到由圆圈表示的3D数据集。</p> 
<p><img src="https://images2.imgbox.com/b3/5d/DiX3YudM_o.png" alt="在这里插入图片描述"></p> 
<p>请注意，所有训练实例都位于一个平面附近：这是高维（3D）空间的低维（2D）子空间。如果我们将每个训练实例垂直投影到该子空间上（如实例连接到平面的短线所示），我们将获得如下图所示的新2D数据集——我们刚刚将数据集的维度从3D减少到2D。注意，轴对应于新特征z1和z2（平面上投影的坐标）。</p> 
<p><img src="https://images2.imgbox.com/a6/c8/PWGlF38h_o.png" alt="在这里插入图片描述"></p> 
<p>但是，投影并不总是降低尺寸的最佳方法。在许多情况下，子空间可能会发生扭曲和转动，例如在下图中所示的著名的瑞士卷小数据集中。</p> 
<p><img src="https://images2.imgbox.com/a7/d4/2WSrVeQy_o.png" alt="在这里插入图片描述"></p> 
<p>如下图左侧所示，简单地投影到一个平面上（例如，去掉x3维度）会将瑞士卷的不同层挤压在一起。你真正想要的是展开瑞士卷，得到下图右侧的2D数据集。</p> 
<p><img src="https://images2.imgbox.com/cc/ee/l95avSGN_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="822__61"></a>8.2.2 流形学习</h3> 
<p>瑞士卷是2D流形的一个示例。简而言之，2D流形是可以在更高维度的空间中弯曲和扭曲的2D形状。更一般而言，d维流形是n维空间（其中d&lt;n）的一部分，局部类似于d维超平面。在瑞士卷的情况下，d=2且n=3时，它局部类似于2D平面，但在第三维中弯曲。</p> 
<p>许多降维算法通过对训练实例所在的流形进行建模来工作。这称为流形学习。它依赖于流形假设（也称为流形假说），该假设认为大多数现实世界的高维数据集都接近于低维流形。通常这是根据经验观察到的这种假设。</p> 
<p>再次考虑一下MNIST数据集：所有手写数字图像都有一些相似之处。它们由连接的线组成，边界为白色，并且或多或少居中。如果你随机生成图像，那么其中只有一小部分看起来像手写数字。换句话说，如果你试图创建数字图像，可用的自由度大大低于允许你生成任何图像的自由度。这些约束倾向于将数据集压缩为低维流形。</p> 
<p>流形假设通常还伴随着另一个隐式假设：如果用流形的低维空间表示，手头的任务（例如分类或回归）将更加简单。例如，在下图的上面一行中，瑞士卷分为两类：在3D空间（左侧）中，决策边界会相当复杂，而在2D展开流形空间中（右侧），决策边界是一条直线。</p> 
<p><img src="https://images2.imgbox.com/67/f7/oOkSAoCU_o.png" alt="在这里插入图片描述"></p> 
<p>但是，这种隐含假设并不总是成立。例如，在上图的下面一行中，决策边界位于x1=5处。此决策边界在原始3D空间（垂直平面）中看起来非常简单，但在展开流形中看起来更加复杂（四个独立线段的集合）。</p> 
<p>简而言之，在训练模型之前降低训练集的维度肯定可以加快训练速度，但这并不总是会导致更好或更简单的解决方案，它取决于数据集。</p> 
<p>希望现在你对于维度的诅咒有了一个很好的理解，也知道降维算法是怎么解决它的，特别是当流形假设成立的时候应该怎么处理。本章剩余部分将逐一介绍几个最流行的算法。</p> 
<h2>
<a id="83__PCA_82"></a>8.3 主成分分析 PCA</h2> 
<p>主成分分析（PCA）是迄今为止最流行的降维算法。首先，它识别最靠近数据的超平面，然后将数据投影到其上。</p> 
<h3>
<a id="831__88"></a>8.3.1 保留差异性</h3> 
<p>将训练集投影到低维超平面之前需要选择正确的超平面。例如下图的左图代表一个简单的2D数据集，沿三条不同的轴（即一维超平面）。右图是将数据集映射到每条轴上的结果。正如你所见，在实线上的投影保留了最大的差异性，而点线上的投影只保留了非常小的差异性，虚线上的投影的差异性居中。</p> 
<p><img src="https://images2.imgbox.com/e3/16/sqQ5hV1h_o.png" alt="在这里插入图片描述"></p> 
<p>选择保留最大差异性的轴看起来比较合理，因为它可能比其他两种投影丢失的信息更少。要证明这一选择，还有一种方法，即比较原始数据集与其轴上的投影之间的均方距离，使这个均方距离最小的轴是最合理的选择，也就是实线代表的轴。这也正是PCA背后的简单思想。</p> 
<h3>
<a id="832__100"></a>8.3.2 主要成分</h3> 
<p>主成分分析可以在训练集中识别出哪条轴对差异性的贡献度最高。在上图中是由实线表示的轴。同时它也找出了第二条轴，与第一条轴垂直，它对剩余差异性的贡献度最高。因为这个示例是二维的，所以除了这条点线再没有其他。如果是在更高维数据集中，PCA还会找到与前两条都正交的第三条轴，以及第四条、第五条，等等——轴的数量与数据集维度数量相同。</p> 
<p>第i个轴称为数据的第i个主要成分（PC）。在上图中，第一个PC是向量c1所在的轴，第二个PC是向量c2所在的轴。</p> 
<blockquote> 
 <p>对于每个主要成分，PCA都找到一个指向PC方向的零中心单位向量。由于两个相对的单位向量位于同一轴上，因此PCA返回的单位向量的方向不稳定：如果稍微扰动训练集并再次运行PCA，则单位向量可能会指向原始向量的相反方向。但是，它们通常仍位于相同的轴上。在某些情况下，一对单位向量甚至可以旋转或交换（如果沿这两个轴的方差接近），但是它们定义的平面通常保持不变。</p> 
</blockquote> 
<p>那么如何找到训练集的主要成分呢？幸运的是，有一种称为奇异值分解（SVD）的标准矩阵分解技术，该技术可以将训练集矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        X
       
      
      
       X
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span></span></span></span></span>分解为三个矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        U
       
       
        ⋅
       
       
        Σ
       
       
        
         V
        
        
         T
        
       
      
      
       U·ΣV^T
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.841331em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">U</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">Σ</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.22222em">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span></span></span></span></span>的矩阵乘法，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        V
       
      
      
       V
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.22222em">V</span></span></span></span></span>包含定义所有主要成分的单位向量。如公式所示。</p> 
<p><img src="https://images2.imgbox.com/5c/50/DIFUDMdV_o.png" alt="在这里插入图片描述"></p> 
<p>以下Python代码使用NumPy的<code>svd()</code>函数来获取训练集的所有主要成分，然后提取定义前两个PC的两个单位向量：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 构建数据集</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
m <span class="token operator">=</span> <span class="token number">60</span>
w1<span class="token punctuation">,</span> w2 <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span>
noise <span class="token operator">=</span> <span class="token number">0.1</span>
angles <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>pi <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">-</span> <span class="token number">0.5</span>
X <span class="token operator">=</span> np<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>angles<span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>angles<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span> <span class="token operator">+</span> noise <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>
X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>angles<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.7</span> <span class="token operator">+</span> noise <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>
X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> w1 <span class="token operator">+</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> w2 <span class="token operator">+</span> noise <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">)</span>

<span class="token comment"># 调用奇异值分解SVD</span>
X_centered <span class="token operator">=</span> X <span class="token operator">-</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
U<span class="token punctuation">,</span> s<span class="token punctuation">,</span> Vt <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>X_centered<span class="token punctuation">)</span>
c1 <span class="token operator">=</span> Vt<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
c2 <span class="token operator">=</span> Vt<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
</code></pre> 
<blockquote> 
 <p>PCA假定数据集以原点为中心。正如我们将看到的，Scikit-Learn的PCA类负责为你居中数据。如果你自己实现PCA（如上例所示），或者使用其他库，请不要忘记首先<strong>将数据居中</strong>（标准化）。</p> 
</blockquote> 
<h3>
<a id="833_d_141"></a>8.3.3 向下投影到d维度</h3> 
<p>一旦确定了所有主要成分，你就可以将数据集投影到前d个主要成分定义的超平面上，从而将数据集的维度降低到d维。选择这个超平面可确保投影将保留尽可能多的差异性。例如，在上图（8.2.1）中，将3D数据集投影到由前两个主成分定义的2D平面上，从而保留了数据集大部分的差异性。最终，2D投影看起来非常类似于原始3D数据集。</p> 
<p>要将训练集投影到超平面上并得到维度为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        d
       
      
      
       d
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault">d</span></span></span></span></span>的简化数据集<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         
          d
         
         
          −
         
         
          p
         
         
          r
         
         
          o
         
         
          j
         
        
       
      
      
       X_{d-proj}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight" style="margin-right: 0.02778em">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span></span></span></span></span>，计算训练集矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        X
       
      
      
       X
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span></span></span></span></span>与矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         W
        
        
         d
        
       
      
      
       W_d
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.13889em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的矩阵相乘，矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         W
        
        
         d
        
       
      
      
       W_d
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.13889em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>定义为包含<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        V
       
      
      
       V
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.22222em">V</span></span></span></span></span>的前<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        d
       
      
      
       d
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault">d</span></span></span></span></span>列的矩阵，如公式所示。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          X
         
         
          
           d
          
          
           −
          
          
           p
          
          
           r
          
          
           o
          
          
           j
          
         
        
        
         =
        
        
         X
        
        
         
          W
         
         
          d
         
        
       
       
         X_{d-proj} = XW_d 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.969438em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight" style="margin-right: 0.02778em">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.83333em;vertical-align: -0.15em"></span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.13889em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br> 以下Python代码将训练集投影到由前两个主要成分定义的平面上：</p> 
<pre><code class="prism language-python">W2 <span class="token operator">=</span> Vt<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>
X2D <span class="token operator">=</span> X_centered<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>W2<span class="token punctuation">)</span>
</code></pre> 
<p>你现在知道了如何将任何数据集的维度减少到任意数量的维度，同时保留尽可能多的差异性。</p> 
<h3>
<a id="834_ScikitLearn_160"></a>8.3.4 使用Scikit-Learn</h3> 
<p>就像我们在本章前面所做的那样，Scikit-Learn的PCA类使用SVD分解来实现PCA。以下代码应用PCA将数据集的维度降到二维（请注意，它会<strong>自动处理数据居中</strong>的问题）：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> PCA

pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>  <span class="token comment"># 将数据集的维度降到2维</span>
X2D <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
</code></pre> 
<p>将PCA转换器拟合到数据集后，其<code>components_</code>属性是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         W
        
        
         d
        
       
      
      
       W_d
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.13889em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的转置（例如，定义第一个主成分的单位向量等于<code>pca.components.T[:, 0]</code>）。</p> 
<blockquote> 
 <p>完整代码如下：</p> 
 <pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token comment"># 构建数据集</span>
np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>seed<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span>
m <span class="token operator">=</span> <span class="token number">60</span>
w1<span class="token punctuation">,</span> w2 <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.3</span>
noise <span class="token operator">=</span> <span class="token number">0.1</span>
angles <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">3</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>pi <span class="token operator">/</span> <span class="token number">2</span> <span class="token operator">-</span> <span class="token number">0.5</span>
X <span class="token operator">=</span> np<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>angles<span class="token punctuation">)</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>angles<span class="token punctuation">)</span><span class="token operator">/</span><span class="token number">2</span> <span class="token operator">+</span> noise <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>
X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>angles<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">0.7</span> <span class="token operator">+</span> noise <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">2</span>
X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> w1 <span class="token operator">+</span> X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> w2 <span class="token operator">+</span> noise <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">)</span>

<span class="token comment"># 调用奇异值分解SVD</span>
X_centered <span class="token operator">=</span> X <span class="token operator">-</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
U<span class="token punctuation">,</span> s<span class="token punctuation">,</span> Vt <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>X_centered<span class="token punctuation">)</span>
c1 <span class="token operator">=</span> Vt<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span>
c2 <span class="token operator">=</span> Vt<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'X: '</span><span class="token punctuation">,</span> X<span class="token punctuation">)</span>

W2 <span class="token operator">=</span> Vt<span class="token punctuation">.</span>T<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span>
<span class="token comment"># X2D = X_centered.dot(W2)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'X_centered:'</span><span class="token punctuation">,</span> X_centered<span class="token punctuation">)</span>

<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> PCA

pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
X2D <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'X2D: '</span><span class="token punctuation">,</span> X2D<span class="token punctuation">)</span>
</code></pre> 
</blockquote> 
<h3>
<a id="835__209"></a>8.3.5 可解释方差比</h3> 
<p>另一个有用的信息是每个主成分的可解释方差比，可以通过<code>explained_variance_ratio_</code>变量来获得。该比率表示沿每个成分的数据集方差的比率。例如，让我们看一下图8-2中表示的3D数据集的前两个成分的可解释方差比：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'输出：n'</span><span class="token punctuation">,</span> pca<span class="token punctuation">.</span>explained_variance_ratio_<span class="token punctuation">)</span>
<span class="token comment"># 输出：</span>
<span class="token comment">#  [0.84248607 0.14631839]</span>
</code></pre> 
<p>此输出告诉你，数据集方差的84.2%位于第一个PC上，而14.6%位于第二个PC上。对于第三个PC，这还不到1.2%，因此可以合理地假设第三个PC携带的信息很少。</p> 
<blockquote> 
 <p>即用来解释成分的重要性占比。</p> 
</blockquote> 
<h3>
<a id="836__225"></a>8.3.6 选择正确的维度</h3> 
<p>与其任意选择要减小到的维度，不如选择相加足够大的方差部分（例如95%）的维度。当然，如果你是为了数据可视化而降低维度，这种情况下，需要将维度降低到2或3。</p> 
<p>以下代码在不降低维度的情况下执行PCA，然后计算保留95%训练集方差所需的最小维度：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>datasets <span class="token keyword">import</span> fetch_openml
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

<span class="token comment"># 导入MINST数据集</span>
mnist <span class="token operator">=</span> fetch_openml<span class="token punctuation">(</span><span class="token string">'mnist_784'</span><span class="token punctuation">,</span> version<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
mnist<span class="token punctuation">.</span>target <span class="token operator">=</span> mnist<span class="token punctuation">.</span>target<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span>
<span class="token comment"># 划分训练集和测试集</span>
X <span class="token operator">=</span> mnist<span class="token punctuation">[</span><span class="token string">"data"</span><span class="token punctuation">]</span>
y <span class="token operator">=</span> mnist<span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span>
X_train<span class="token punctuation">,</span> X_test<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

<span class="token comment"># 应用（在不降低维度的情况下执行PCA，然后计算保留95%训练集方差所需的最小维度）</span>
pca<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>
cumsum <span class="token operator">=</span> np<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span>pca<span class="token punctuation">.</span>explained_variance_ratio_<span class="token punctuation">)</span>
d <span class="token operator">=</span> np<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>cumsum <span class="token operator">&gt;=</span> <span class="token number">0.95</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span>
</code></pre> 
<p>然后，你可以设置n_components=d并再次运行PCA。但是还有一个更好的选择：<strong>将<code>n_components</code>设置为0.0到1.0之间的浮点数来表示要保留的方差率，而不是指定要保留的主成分数</strong>：</p> 
<pre><code class="prism language-python">pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">0.95</span><span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>
</code></pre> 
<p>另一个选择是将可解释方差绘制成维度的函数（简单地用cumsum绘制，见下图）。曲线上通常会出现一个拐点，其中可解释方差会停止快速增大。在这种情况下，你可以看到将维度降低到大约100而不会损失太多的可解释方差。</p> 
<p><img src="https://images2.imgbox.com/99/b3/Gz8PPRUP_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="837_PCA_264"></a>8.3.7 PCA压缩</h3> 
<p>降维后，训练集占用的空间要少得多。例如，<u>将PCA应用于MNIST数据集，同时保留其95%的方差。你会发现每个实例将具有150多个特征，而不是原始的784个特征。因此，尽管保留了大多数方差，但数据集现在不到其原始大小的20%</u>！这是一个合理的压缩率，你可以看到这种维度减小极大地加速了分类算法（例如SVM分类器）。</p> 
<p><u>通过应用PCA投影的逆变换，还可以将缩减后的数据集解压缩回784维</u>。由于投影会丢失一些信息（在5%的方差被丢弃），因此这<strong>不会给你原始的数据，但可能会接近原始数据</strong>。原始数据与重构数据（压缩后再解压缩）之间的均方距离称为<strong>重构误差</strong>。</p> 
<p>以下代码将MNIST数据集压缩为154个维度，然后使用<code>inverse_transform()</code>方法将其解压缩回784个维度：</p> 
<pre><code class="prism language-python">pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">154</span><span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>
<span class="token comment"># 逆变换: 解压缩</span>
X_recovered <span class="token operator">=</span> pca<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">(</span>X_reduced<span class="token punctuation">)</span>
</code></pre> 
<p>下图显示了原始训练集的一些数字（左侧），以及压缩和解压缩后的相应数字。你会看到图像质量略有下降，但是数字仍然大部分保持完好。</p> 
<p><img src="https://images2.imgbox.com/4f/33/BY5WhaUG_o.png" alt="在这里插入图片描述"></p> 
<p>逆变换的公式如下所示。</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          X
         
         
          
           r
          
          
           e
          
          
           c
          
          
           o
          
          
           v
          
          
           e
          
          
           r
          
          
           e
          
          
           d
          
         
        
        
         =
        
        
         
          X
         
         
          
           d
          
          
           −
          
          
           p
          
          
           r
          
          
           o
          
          
           j
          
         
        
        
         
          W
         
         
          d
         
        
       
       
         X_{recovered} = X_{d-proj}W_d 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em">r</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">c</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right: 0.03588em">v</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight" style="margin-right: 0.02778em">r</span><span class="mord mathdefault mtight">e</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.969438em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">p</span><span class="mord mathdefault mtight" style="margin-right: 0.02778em">r</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.13889em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<h3>
<a id="838_PCA_291"></a>8.3.8 随机PCA</h3> 
<p>如果将超参数<code>svd_solver</code>设置为"randomized"，则Scikit-Learn将使用一种称为Randomized PCA的随机算法，该算法可以快速找到前d个主成分的近似值。它的计算复杂度为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        O
       
       
        (
       
       
        m
       
       
        ×
       
       
        
         d
        
        
         2
        
       
       
        )
       
       
        +
       
       
        O
       
       
        (
       
       
        
         d
        
        
         3
        
       
       
        )
       
      
      
       O(m×d^2)+O(d^3)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">O</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 1.06411em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 1.06411em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，而不是完全SVD方法的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        O
       
       
        (
       
       
        m
       
       
        ×
       
       
        
         n
        
        
         2
        
       
       
        )
       
       
        +
       
       
        O
       
       
        (
       
       
        
         n
        
        
         3
        
       
       
        )
       
      
      
       O(m×n^2)+O(n^3)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">O</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 1.06411em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 1.06411em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，因此，当<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        d
       
      
      
       d
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault">d</span></span></span></span></span>远远小于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        n
       
      
      
       n
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">n</span></span></span></span></span>时，它比完全的SVD快得多：</p> 
<pre><code class="prism language-python"><span class="token comment"># 随机PCA</span>
rnd_pca <span class="token operator">=</span> PCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">154</span><span class="token punctuation">,</span> svd_solver<span class="token operator">=</span><span class="token string">"randomized"</span><span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> rnd_pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>
</code></pre> 
<p>默认情况下，<code>svd_solver</code>实际上设置为"auto"：<strong>如果m或n大于500并且d小于m或n的80%，则Scikit-Learn自动使用随机PCA算法，否则它将使用完全的SVD方法</strong>。如果要强制Scikit-Learn使用完全的SVD，可以将<code>svd_solver</code>超参数设置为"full"。</p> 
<h3>
<a id="839_PCAIPCA_305"></a>8.3.9 增量PCA（IPCA）</h3> 
<p>前面的PCA实现的一个问题是，它们要求整个训练集都放入内存才能运行算法。幸运的是已经开发了增量PCA（IPCA）算法，它们可以使你把训练集划分为多个小批量，并<strong>一次将一个小批量送入IPCA算法</strong>。这对于大型训练集和在线（即<strong>在新实例到来时动态运行</strong>）应用PCA很有用。</p> 
<p>以下代码将MNIST数据集拆分为100个小批量（使用NumPy的array_split（）函数），并将其馈送到Scikit-Learn的<strong>IncrementalPCA类</strong>，来把MNIST数据集的维度降低到154（就像之前做的那样）。请注意，你必须在每个小批量中调用<code>partial_fit()</code>方法，而不是在整个训练集中调用<code>fit()</code>方法：</p> 
<pre><code class="prism language-python"><span class="token comment"># IPCA</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> IncrementalPCA

n_batches <span class="token operator">=</span> <span class="token number">100</span>
inc_pca <span class="token operator">=</span> IncrementalPCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">154</span><span class="token punctuation">)</span>
<span class="token keyword">for</span> X_batch <span class="token keyword">in</span> np<span class="token punctuation">.</span>array_split<span class="token punctuation">(</span>X_train<span class="token punctuation">,</span> n_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
    inc_pca<span class="token punctuation">.</span>partial_fit<span class="token punctuation">(</span>X_batch<span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> inc_pca<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>
</code></pre> 
<p>另外，你可以使用NumPy的memmap类，该类使你可以将存储在磁盘上的二进制文件中的大型数组当作完全是在内存中一样来操作，该类仅在需要时才将数据加载到内存中。由于IncrementalPCA类在任何给定时间仅使用数组的一小部分，因此内存使用情况处于受控状态。如以下代码所示，这使得调用通常的<code>fit()</code>方法成为可能：</p> 
<pre><code class="prism language-python">filename <span class="token operator">=</span> <span class="token string">"my_mnist.data"</span>
X_mm <span class="token operator">=</span> np<span class="token punctuation">.</span>memmap<span class="token punctuation">(</span>filename<span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token string">"float32"</span><span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"readonly"</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token punctuation">(</span>m<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">)</span>
batch_size <span class="token operator">=</span> m <span class="token operator">//</span> n_batches
inc_pca <span class="token operator">=</span> IncrementalPCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">154</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
inc_pca<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_mm<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="84_PCAkPCA_334"></a>8.4 内核PCA（kPCA）</h2> 
<p>在第5章中，我们讨论了内核，这是一种数学技术，它可以<strong>将实例隐式映射到一个高维空间</strong>（称为特征空间），从而可以使用支持向量机来进行非线性分类和回归。回想一下，<u>高维特征空间中的线性决策边界对应于原始空间中的复杂非线性决策边界</u>。</p> 
<p>事实证明，可以将相同的技术应用于PCA，从而可以执行复杂的非线性投影来降低维度。这叫作<strong>内核PCA（kPCA）</strong>。它通常擅长在投影后保留实例的聚类，有时甚至可以展开位于扭曲流形附近的数据集。</p> 
<p>下面的代码使用Scikit-Learn的<strong>KernelPCA类</strong>以及用<strong>RBF内核</strong>来执行kPCA（有关RBF内核和其他内核的更多详细信息，请参见第5章）：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> KernelPCA

rbf_pca <span class="token operator">=</span> KernelPCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> kernel<span class="token operator">=</span><span class="token string">"rbf"</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.04</span><span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> rbf_pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
</code></pre> 
<p>下图显示了瑞士卷，它使用线性内核（相当于简单地使用PCA类）、RBF内核和sigmoid内核减小为二维。</p> 
<p><img src="https://images2.imgbox.com/87/68/oRkQEYre_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="_356"></a>选择内核并调整超参数</h3> 
<p>由于kPCA是一种无监督学习算法，因此没有明显的性能指标可以帮助你选择最好的内核和超参数值。也就是说，降维通常是有监督学习任务（例如分类）的准备步骤，因此你可以使用网格搜索来选择在该任务上能获得最佳性能的内核和超参数。以下代码创建了一个两步流水线，首先使用kPCA将维度减少到二维，然后使用逻辑回归来分类。它使用GridSearchCV来查找kPCA的最佳内核和gamma值，以便在流水线的最后得到最好的分类准确率：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> GridSearchCV
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>pipeline <span class="token keyword">import</span> Pipeline

clf <span class="token operator">=</span> Pipeline<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">(</span><span class="token string">"kpca"</span><span class="token punctuation">,</span> KernelPCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">(</span><span class="token string">"log_reg"</span><span class="token punctuation">,</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">]</span><span class="token punctuation">)</span>
param_grid <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">{<!-- --></span>
    <span class="token string">"kpca__gamma"</span><span class="token punctuation">:</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0.03</span><span class="token punctuation">,</span> <span class="token number">0.05</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token string">"kpca__kernel"</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"rbf"</span><span class="token punctuation">,</span> <span class="token string">"sigmoid"</span><span class="token punctuation">]</span>
<span class="token punctuation">}</span><span class="token punctuation">]</span>
grid_search <span class="token operator">=</span> GridSearchCV<span class="token punctuation">(</span>clf<span class="token punctuation">,</span> param_grid<span class="token punctuation">,</span> cv<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
grid_search<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre> 
<p>然后，可以通过<code>best_params_</code>变量来得到最佳内核和超参数：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>grid_search<span class="token punctuation">.</span>best_params_<span class="token punctuation">)</span>
<span class="token comment"># {'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}</span>
</code></pre> 
<p>另一种完全无监督的方法是<strong>选择产生最低重构误差的内核和超参数</strong>。请注意，重建并不像使用线性PCA那样容易。以下是原因。</p> 
<p>下图显示了原始的瑞士卷3D数据集（左上）和使用RBF内核应用kPCA之后得到的2D数据集（右上）。多亏了内核技术，此变换在数学上等效于使用特征图φ将训练集映射到无限维特征空间（右下），然后使用线性PCA将变换后的训练集投影到2D。</p> 
<p><img src="https://images2.imgbox.com/ee/92/RTtWGBRL_o.png" alt="在这里插入图片描述"></p> 
<p>请注意，如果我们可以对一个给定的实例在缩小的空间中反转线性PCA，则重构点将位于特征空间中，而不是原始空间中（例如，如图中的X所示）。由于特征空间是无限维的，因此我们无法计算重构点，无法计算真实的重构误差。幸运的是，有可能在原始空间中找到一个点，该点将映射到重建点附近，这一点称为重建原像。一旦你有了原像，就可以测量其与原始实例的平方距离。然后可以选择内核和超参数，最大限度地减少此重构原像误差。</p> 
<p>你可能想知道如何执行这个重构。一种解决方案是训练有监督的回归模型，其中将投影实例作为训练集，将原始实例作为目标值。如果设置fit_inverse_transform=True，Scikit-Learn会自动执行此操作，如以下代码所示：</p> 
<pre><code class="prism language-python">rbf_pca <span class="token operator">=</span> KernelPCA<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> kernel<span class="token operator">=</span><span class="token string">"rbf"</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.0433</span><span class="token punctuation">,</span> fit_inverse_transform<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> rbf_pca<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
X_preimage <span class="token operator">=</span> rbf_pca<span class="token punctuation">.</span>inverse_transform<span class="token punctuation">(</span>X_reduced<span class="token punctuation">)</span>
</code></pre> 
<p>默认情况下，<code>fit_inverse_transform=False</code>，并且KernelPCA没有<code>inverse_transform()</code>方法。仅当你设置<code>fit_inverse_transform=True</code>时，才会创建此方法。</p> 
<p>计算重建原像误差：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error

<span class="token keyword">print</span><span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>X<span class="token punctuation">,</span> X_preimage<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 输出：32.786308795766132</span>
</code></pre> 
<p>现在，你可以使用网格搜索与交叉验证来找到可最大限度减少此错误的内核和超参数。</p> 
<h2>
<a id="85_LLE_416"></a>8.5 局部线性嵌入LLE</h2> 
<p>局部线性嵌入（LLE）是另一种强大的<u>非线性降维（NLDR）技术</u>。它是一种<strong>流形学习技术</strong>，不像以前的算法那样依赖于投影。简而言之，LLE的工作原理是首先测量每个训练实例如何与其最近的邻居（c.n.）线性相关，然后寻找可以最好地保留这些局部关系的训练集的低维表示形式（稍后会详细介绍）。这种方法特别适合于展开扭曲的流形，尤其是在没有太多噪声的情况下。</p> 
<p>以下代码使用Scikit-Learn的<strong>LocallyLinearEmbedding类</strong>来展开瑞士卷：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>manifold <span class="token keyword">import</span> LocallyLinearEmbedding

lle <span class="token operator">=</span> LocallyLinearEmbedding<span class="token punctuation">(</span>n_components<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> n_neighbors<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
X_reduced <span class="token operator">=</span> lle<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
</code></pre> 
<p>生成的2D数据集下图所示。如你所见，瑞士卷已完全展开，并且实例之间的距离在局部得到了很好的保留。但是，距离并没有在更大范围内保留：展开的瑞士卷的左侧部分被拉伸，而右侧部分被挤压。尽管如此，LLE在流形建模方面做得很好。</p> 
<p><img src="https://images2.imgbox.com/06/05/eCQuOLk6_o.png" alt="在这里插入图片描述"></p> 
<p>LLE的工作方式如下：对于每个训练实例<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       x^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>，算法都会识别出其k个最近的邻居（在前面的代码k=10中），然后尝试将<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       x^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>重构为这些邻居的线性函数。更具体地说，它找到权重<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         
          i
         
         
          ，
         
         
          j
         
        
       
      
      
       w_{i，j}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.716668em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord cjk_fallback mtight">，</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span></span></span></span></span>，使得<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       x^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>与<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         ∑
        
        
         
          j
         
         
          =
         
         
          1
         
        
        
         m
        
       
       
        
         w
        
        
         
          i
         
         
          ,
         
         
          j
         
        
       
       
        
         x
        
        
         j
        
       
      
      
       sum^m_{j=1}w_{i,j}x^{j}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.26048em;vertical-align: -0.435818em"></span><span class="mop"><span class="mop op-symbol small-op">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.804292em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.435818em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span></span></span></span></span></span></span></span></span>之间的平方距离尽可能小，并假设如果<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         
          (
         
         
          j
         
         
          )
         
        
       
      
      
       x^{(j)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>不是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       x^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>的k个最接近的邻居之一则<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         
          i
         
         
          ，
         
         
          j
         
        
       
       
        =
       
       
        0
       
      
      
       w_{i，j}=0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.716668em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord cjk_fallback mtight">，</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">0</span></span></span></span></span>。因此，LLE的第一步是下面公式中描述的约束优化问题，其中W是包含所有权重<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         
          i
         
         
          ，
         
         
          j
         
        
       
      
      
       w_{i，j}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.716668em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mord cjk_fallback mtight">，</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span></span></span></span></span>的权重矩阵。第二个约束条件只是简单地将每个训练实例<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       x^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>的权重归一化。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          W
         
         
          ′
         
        
        
         =
        
        
         a
        
        
         r
        
        
         g
        
        
         m
        
        
         i
        
        
         n
        
        
         
          ∑
         
         
          
           i
          
          
           =
          
          
           1
          
         
         
          m
         
        
        
         (
        
        
         
          x
         
         
          
           (
          
          
           i
          
          
           )
          
         
        
        
         −
        
        
         
          ∑
         
         
          
           j
          
          
           =
          
          
           1
          
         
         
          m
         
        
        
         
          w
         
         
          
           i
          
          
           ,
          
          
           j
          
         
        
        
         
          x
         
         
          
           (
          
          
           j
          
          
           )
          
         
        
        
         
          )
         
         
          2
         
        
       
       
         W' = argminsum^m_{i=1}(x^{(i)}-sum^m_{j=1}w_{i,j}x^{(j)})^2 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.801892em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 2.92907em;vertical-align: -1.27767em"></span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right: 0.02778em">r</span><span class="mord mathdefault" style="margin-right: 0.03588em">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em"><span class=""></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 3.06517em;vertical-align: -1.41378em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.41378em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span><br> 在此步骤之后，权重矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         W
        
        
         ′
        
       
      
      
       W'
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.751892em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span>（包含权重<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         
          i
         
         
          ,
         
         
          j
         
        
        
         ′
        
       
      
      
       w'_{i,j}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.14666em;vertical-align: -0.394772em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.394772em"><span class=""></span></span></span></span></span></span></span></span></span></span>）对训练实例之间的局部线性关系进行编码。第二步是将训练实例映射到d维空间（其中d&lt;n），同时尽可能保留这些局部关系。如果<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         z
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       z^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>是此d维空间中<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       x^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>的图像，则我们希望<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         z
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       z^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>与<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         ∑
        
        
         
          j
         
         
          =
         
         
          1
         
        
        
         m
        
       
       
        
         w
        
        
         
          i
         
         
          ,
         
         
          j
         
        
        
         ′
        
       
       
        
         z
        
        
         j
        
       
      
      
       sum^m_{j=1}w'_{i,j}z^{j}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.26048em;vertical-align: -0.435818em"></span><span class="mop"><span class="mop op-symbol small-op">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.804292em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.435818em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.394772em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.824664em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span></span></span></span></span></span></span></span></span>之间的平方距离尽可能小。</p> 
<p>这种想法导致了下面公式中描述的无约束优化问题。它看起来与第一步非常相似，但是我们没有保持实例固定并找到最佳权重，而是相反：保持权重固定并找到实例图像在低维空间中的最佳位置。注意<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Z
       
      
      
       Z
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.07153em">Z</span></span></span></span></span>是包含所有<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         z
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
      
      
       z^{(i)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.888em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>的矩阵。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          Z
         
         
          ′
         
        
        
         =
        
        
         a
        
        
         r
        
        
         g
        
        
         m
        
        
         i
        
        
         n
        
        
         
          ∑
         
         
          
           i
          
          
           =
          
          
           1
          
         
         
          m
         
        
        
         (
        
        
         
          z
         
         
          
           (
          
          
           i
          
          
           )
          
         
        
        
         −
        
        
         
          ∑
         
         
          
           j
          
          
           =
          
          
           1
          
         
         
          m
         
        
        
         
          w
         
         
          
           i
          
          
           ,
          
          
           j
          
         
        
        
         
          z
         
         
          
           (
          
          
           j
          
          
           )
          
         
        
        
         
          )
         
         
          2
         
        
       
       
         Z' = argminsum^m_{i=1}(z^{(i)}-sum^m_{j=1}w_{i,j}z^{(j)})^2 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.801892em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07153em">Z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 2.92907em;vertical-align: -1.27767em"></span><span class="mord mathdefault">a</span><span class="mord mathdefault" style="margin-right: 0.02778em">r</span><span class="mord mathdefault" style="margin-right: 0.03588em">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">i</span><span class="mord mathdefault">n</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em"><span class=""></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 3.06517em;vertical-align: -1.41378em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.41378em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mpunct mtight">,</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.04398em">z</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span><br> Scikit-Learn的LLE实现具有以下计算复杂度：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        O
       
       
        (
       
       
        m
       
       
        ⋅
       
       
        l
       
       
        o
       
       
        
         g
        
        
         m
        
       
       
        ⋅
       
       
        n
       
       
        ⋅
       
       
        l
       
       
        o
       
       
        
         g
        
        
         k
        
       
       
        )
       
      
      
       O(m·log^m·n·log^k)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.09911em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">O</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.664392em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span></span></span></span></span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault">n</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.849108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em">k</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>用于找到k个最近的邻居，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        O
       
       
        (
       
       
        m
       
       
        ⋅
       
       
        n
       
       
        ⋅
       
       
        
         k
        
        
         3
        
       
       
        )
       
      
      
       O(m·n·k^3)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06411em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">O</span><span class="mopen">(</span><span class="mord mathdefault">m</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault">n</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03148em">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>用于优化权重，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        O
       
       
        (
       
       
        d
       
       
        ⋅
       
       
        
         m
        
        
         2
        
       
       
        )
       
      
      
       O(d·m^2)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.06411em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">O</span><span class="mopen">(</span><span class="mord mathdefault">d</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>用于构造低维表示。不幸的是，最后一项中的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         m
        
        
         2
        
       
      
      
       m^2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.814108em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>使该算法很难扩展到非常大的数据集。</p> 
<h2>
<a id="86__448"></a>8.6 其他降维技术</h2> 
<p>还有许多其他降维技术，Scikit-Learn中提供了其中几种。以下是一些很受欢迎的降维技术：</p> 
<p><strong>随机投影</strong></p> 
<ul><li>顾名思义，使用随机线性投影将数据投影到较低维度的空间。这听起来可能很疯狂，但事实证明，这样的随机投影实际上很有可能很好地保持距离，就如William B.Johnson和Joram Lindenstrauss在著名引理中的数学证明。降维的质量取决于实例数目和目标维度，令人惊讶的不取决于初始维度。请查看<code>sklearn.random_projection</code>软件包的文档以获取更多详细信息。</li></ul> 
<p><strong>多维缩放（MDS）</strong></p> 
<ul><li>当尝试保留实例之间的距离时降低维度。</li></ul> 
<p><strong>等度量映射Isomap</strong></p> 
<ul>
<li>等度量映射（Isornetric Mapping，简称 Isomap）【Tenenbaum et al，2000】 的基本出发点，是认为低维流形嵌入到高维空问之后，直接在高维空间中计算直线距离具有误导性，对为高维空间中的直线距离在低维嵌入流形上是不可达的。</li>
<li>通过将每个实例与其最近的邻居连接来创建一个图，然后在尝试保留实例之间的测地距离的同时降低维度。</li>
</ul> 
<p><strong>t分布随机近邻嵌入（t-SNE）</strong></p> 
<ul><li>降低了维度，同时使相似实例保持接近，异类实例分开。它主要用于可视化，特别是在高维空间中可视化实例的聚类（例如，以2D可视化MNIST图像）。</li></ul> 
<p><strong>线性判别分析（LDA）</strong></p> 
<ul><li>LDA是一种分类算法，但是在训练过程中，它会学习各类之间最有判别力的轴，然后可以使用这些轴来定义要在其上投影数据的超平面。这种方法的好处是投影将使类保持尽可能远的距离，因此LDA是在运行其他分类算法（例如SVM分类器）之前降低维度的好技术。</li></ul> 
<p><img src="https://images2.imgbox.com/21/df/m2peJ4yR_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="87__478"></a>8.7 练习题</h2> 
<h3>
<a id="_480"></a>问题</h3> 
<ol>
<li> <p>减少数据集维度的主要动机是什么？主要缺点是什么？</p> </li>
<li> <p>维度的诅咒是什么？</p> </li>
<li> <p>一旦降低了数据集的维度，是否可以逆操作？如果可以，怎么做？如果不能，为什么？</p> </li>
<li> <p>可以使用PCA来减少高度非线性的数据集的维度吗？</p> </li>
<li> <p>假设你在1000维的数据集上执行PCA，将可解释方差比设置为95%。结果数据集将具有多少个维度？</p> </li>
<li> <p>在什么情况下，你将使用常规PCA、增量PCA、随机PCA或内核PCA？</p> </li>
<li> <p>如何评估数据集中的降维算法的性能？</p> </li>
<li> <p>链接两个不同的降维算法是否有意义？</p> </li>
<li> <p>加载MNIST数据集（在第3章中介绍），并将其分为训练集和测试集（使用前60 000个实例进行训练，其余10 000个进行测试）。在数据集上训练随机森林分类器，花费多长时间，然后在测试集上评估模型。接下来，使用PCA来减少数据集的维度，可解释方差率为95%。在精简后的数据集上训练新的随机森林分类器，查看花费了多长时间。训练速度提高了吗？接下来，评估测试集上的分类器。与之前的分类器相比如何？</p> </li>
<li> <p>使用t-SNE将MNIST数据集降至两个维度，然后用Matplotlib绘制结果。你可以通过散点图用10个不同的颜色来代表每个图像的目标类，或者也可以用对应实例的类（从0到9的数字）替换散点图中的每个点，甚至你还可以绘制数字图像本身的缩小版（如果你绘制所有数字，视觉效果会太凌乱，所以你要么绘制一个随机样本，要么选择单个实例，但是这个实例的周围最好没有其他绘制的实例）。现在你应该得到了一个很好的可视化结果及各自分开的数字集群。尝试使用其他降维算法，如PCA、LLE或MDS等，比较可视化结果。</p> </li>
</ol> 
<h3>
<a id="_502"></a>答案</h3> 
<ol>
<li> <p>降维的主要动机是：</p> 
  <ul>
<li>为了加速后续的训练算法（在某些情况下，也可能为了消除噪声和冗余特征，使训练算法性能更好）。</li>
<li>为了将数据可视化，并从中获得洞见，了解最重要的特征。</li>
<li>为了节省空间（压缩）。</li>
</ul> <p>主要的弊端是：</p> 
  <ul>
<li>丢失部分信息，可能使后续训练算法的性能降低。</li>
<li>可能是计算密集型的。</li>
<li>为机器学习流水线增添了些许复杂度。</li>
<li>转换后的特征往往难以解释。</li>
</ul> </li>
<li> <p>维度的诅咒是指许多在低维空间中不存在的问题，在高维空间中发生。在机器学习领域，一个常见的现象是随机抽样的高维向量通常非常稀疏，提升了过拟合的风险，同时也使得在没有充足训练数据的情况下，要识别数据中的模式非常困难。</p> </li>
<li> <p>一旦使用我们讨论的任意算法减少了数据集的维度，就几乎不可能再将操作完美地逆转，因为在降维过程中必然丢失了一部分信息。此外，虽然有一些算法（例如PCA）拥有简单的逆转换过程，可以重建出与原始数据集相似的数据集，但是也有一些算法不能实现逆转（例如T-SNE）。</p> </li>
<li> <p>对大多数数据集来说，PCA可以用来进行显著降维，即便是高度非线性的数据集，因为它至少可以消除无用的维度。但是如果不存在无用的维度（例如瑞士卷），那么使用PCA降维将会损失太多信息。你希望的是将瑞士卷展开，而不是将其压扁。</p> </li>
<li> <p>这是个不好回答的问题，它取决于数据集。我们来看看两个极端的示例。首先，假设数据集是由几乎完全对齐的点组成的，在这种情况下，PCA可以将数据集降至一维，同时保留95%的方差。现在，试想数据集由完全随机的点组成，分散在1000个维度上，在这种情况下，需要在950个维度上保留95%的方差。所以，这个问题的答案是：取决于数据集，它可能是1到950之间的任何数字。将解释方差绘制成关于维度数量的函数，可以对数据集的内在维度获得一个粗略的概念。</p> </li>
<li> <p><strong>常规PCA是默认选择，但是它仅适用于内存足够处理训练集的时候。增量PCA对于内存无法支持的大型数据集非常有用，但是它比常规PCA要慢一些，所以如果内存能够支持，还是应该使用常规PCA。当你需要随时应用PCA来处理每次新增的实例时，增量PCA对于在线任务同样有用。当你想大大降低维度数量，并且内存能够支持数据集时，使用随机PCA非常有效，它比常规PCA快得多。最后，对于非线性数据集，使用核化PCA非常有效。</strong></p> </li>
<li> <p>直观来说，如果降维算法能够消除许多维度并且不会丢失太多信息，那么这就算一个好的降维算法。进行衡量的方法之一是<u><strong>应用逆转换然后测量重建误差</strong></u>。然而并不是所有的降维算法都提供了逆转换。还有另一种选择，如果你将降维当作一个预处理过程，用在其他机器学习算法（比如随机森林分类器）之前，那么可以通过简单测量第二个算法的性能来进行评估。如果降维过程没有损失太多信息，那么第二个算法的性能应该跟使用原始数据集一样好。</p> </li>
<li> <p>链接两个不同的降维算法绝对是有意义的。常见的示例是使用PCA快速去除大量无用的维度，然后应用另一种更慢的降维算法，如LLE。这种两步走的策略产生的结果可能与仅使用LLE相同，但是时间要短得多。</p> </li>
</ol> 
<p>有关练习9和10的解答，请参见https://github.com/ageron/handson-ml2上提供的Jupyternotebook。</p> 
<h2>
<a id="88__534"></a>8.8 阅读材料</h2> 
<p>主成分分析是一种无监督的绒性降维方法，监督线性降维方法最著名的是线性判别分析（LDA）Fisher，1936】，其核化版本KLDA 【Baudatand Anouar，2000】。通过最大化两个变量集合之间的相关性，则可得到"典型相关分析"（Canonical Correlation Analysis，简称 CCA）Hotelling，1936】 及其核化版本KCCA 【Harden et al.，2004，该方法在多视图学习（multi-view learning）中有广泛应用.在模式识别领域人们发现，直接对矩阵对象（例如一幅图像）进行降维操作会比将其拉伸为向量（例如把图像逐行拼接成一个向量）再进行降维操作有更好的性能，于是产生了2DPCA Yang et al.，204】、2DLDA 【Ye et al，2005】、（2D）PCA 【Zhang and Zhou，2005】 等方法，以及基于张量（ten3or）的方法【Kolda and Bader，2009】。</p> 
<p>除了 Isomap和LLE，常见的流形学习方法还有拉普拉斯特征映射 （Laplcian Eigenmaps，简称LE）【【Belkin and Niyogi，2003】、局部切空间对齐（LocalTangent Space Alignment，简称 LTSA）【Zhang and Zha，2004等】。局部保持投影（Locality Preserving Projections，简称 LPP）【He and Niyogi，2004】 是基于LE的线性降维方法.对监督学习而言，根据类别信息扭曲后的低维空间常比本真低维空间更有利【Geng et al.，2005】。值得注意的是，流形学习欲有效进行邻域保持则需样本密采样、而这恰是高维情形下面临的重大障碍，因此流形学习方法在实践中的降维性能往往没有预期的好：但邻域保持的想法对机器学习的其他分支产生了重要影响，例如半监督学习中有著名的流形假设、流形正则化【Belkin et al，2006】。【Yan et al，2007】从图嵌入的角度给出了降维方法的一个统一框架。</p> 
<p>将必连关系、勿连关系作为学习任务优化目标的约束，在半监督聚类的研究中使用得更早【Wagstaf et al，2001】。在度量学习中，由于这些约束是对所有样本同时发生作用 【Xing et al，2003】，因此相应的方法被称为全局度量学习方法。人们也尝试利用局部约束（例如邻域内的三元关系），从而产生了局部距离度量学习方法【Weinberger and Saul，2009】，甚至有一些研究试图为每个样本产生最合适的距离度量Frome et al，2007;Zhan et a，2009】。在具体的学习与优化求解方面，不同的度量学习方法往往采用了不同的技术，例如 【Yang etal.，2006】将度量学习转化为判别式概率模型框架下基于样本对的二分类问题求解，【Davis et al，2007】将度量学习转化为信息论框架下的 Bregman 优化问题，能方便地进行在线学习。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>