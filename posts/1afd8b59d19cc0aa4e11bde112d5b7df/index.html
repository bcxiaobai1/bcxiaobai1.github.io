<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【开源】树莓派&#43;OAK相机，打造家庭自动化系统。 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【开源】树莓派&#43;OAK相机，打造家庭自动化系统。</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <blockquote> 
 <p>编辑：OAK中国<br> 首发：<a href="https://www.oakchina.cn/2022/09/27/home-automation-rpi-oak/">oakchina.cn</a><br> 喜欢的话，请多多?⭐️✍<br> 内容可能会不定期更新，官网内容都是最新的，请查看首发地址链接。</p> 
</blockquote> 
<h2>
<a id="_5"></a>▌前言</h2> 
<p>Hello，大家好，这里是OAK中国，我是助手君。</p> 
<p>这个项目也是我网上冲浪的时候看到的，虽然没有详细说代码步骤，但是作者的思路还是很有借鉴意义的，所以给大家分享一下。</p> 
 
<figure class="wp-block-table is-style-stripes">
 <table><tbody><tr><td>来源：<a href="https://medium.com/interaction-dynamics/how-to-use-machine-learning-for-home-automation-with-luxonis-depth-camera-uxtech-1-765418665b5">medium.com</a><br>编辑：<a href="https://www.oakchina.cn/">OAK中国</a>
</td></tr></tbody></table>
</figure> 
 
 
<p>大多数家庭自动化系统是基于二进制传感器，如接近传感器、时间传感器、光传感器等。对于每个要检测的新活动，你需要安装一个新的传感器——比如用Withings Sleep来检测人们的睡眠，这就导致安装和维护非常困难。</p> 
 
 
<p>作为一名用户体验工程师，我总是探索新的用户体验方式。所以我尝试了更具扩展性的方式——基于OAK相机、计算机视觉、机器学习和活动识别的方法。</p> 
 
 
<p id="9924">为了使设置有效，它必须消耗比它应该控制的灯更少的能量，所以机器学习硬件应该是低功耗的，然后我决定试试<a href="https://www.oakchina.cn/product/oak-d/">OAK-D</a>相机，这一功能强大、易于上手的产品。</p> 
 
 
 
<p id="7b09"><em>如果你是机器学习的初学者，这篇文章可能是一个很好的介绍。或者你已经知道机器学习，这篇文章可能有助于了解更多关于DepthAI和Myriad architecture的知识，因为它提出了一种与OpenCV非常不同的方法。</em></p> 
 
 
<p class="has-pale-cyan-blue-background-color has-background" id="3a25">本文不会详述代码，但是你仍然可以在<a href="https://github.com/interaction-dynamics/smart-home-with-luxonis">这里</a>找到所有代码。</p> 
 
 
<h2 id="616e">检测的活动</h2> 
 
 
<p id="02c3">基于我的日常工作，我选择了一项复杂的活动：<strong>一个人坐在沙发上</strong>。使用传统方式（如传感器）来检测这种活动将需要沙发中的压力传感器，而操作相当复杂。</p> 
 
 
<p id="9dbd">而用OAK-D来做这种任务就简单多了，此外，我的设备还允许同时检测多个活动。</p> 
 
 
<h2 id="3d24">硬件</h2> 
 
 
<p id="5d8b">我很喜欢拿树莓派做原型验证，但是这个项目需要的算力远远超过我的树莓派4，所以我选择了OAK-D。该设备嵌入了一个4K相机，两个720p相机以通过立体视觉捕捉深度，以及一个基于<a href="https://www.oakchina.cn/2022/09/15/oak-rvc/#RVC2">RVC2</a>的计算芯片。它足够强大，可以计算机器学习算法，并让树莓派负责剩下的工作。</p> 
 
 
<figure class="wp-block-table is-style-stripes has-small-font-size">
 <table><tbody><tr><td>编者注：OAK-D双目最高支持800P，RGB最高1200万像素（超4K）。</td></tr></tbody></table>
</figure> 
 
 
<figure class="wp-block-image size-large">
 <img src="https://images2.imgbox.com/db/6b/crSo9SKe_o.png" alt="" class="wp-image-7225">
 <figcaption>
  Luxonis OAK-D
 </figcaption>
</figure> 
 
 
<h2 id="69b2">设置</h2> 
 
 
<p id="9d68">我对这个项目的设置包括：</p> 
 
 
<ul>
<li>OAK-D捕捉视频、深度，并运行使用指定的机器学习算法<code>depthAI</code>库</li>
<li>树莓派4运行python代码，初始化<code>depthAI</code>并且流式传输视频（使用MJPG格式）</li>
<li>一台可视化结果的计算机（一旦树莓派真正接入家庭自动化系统，它应该在现实生活中就不需要了）</li>
</ul> 
 
 
<figure class="wp-block-image size-large">
 <img src="https://images2.imgbox.com/e7/c6/vhFz4Baz_o.png" alt="" class="wp-image-7226">
</figure> 
 
 
<h2 id="4ffd">算法</h2> 
 
 
<p id="e27a">活动“一个人坐在沙发上”可以分成几个部分，每一部分都需要特定的算法。</p> 
 
 
<figure class="wp-block-image size-large">
 <img src="https://images2.imgbox.com/14/b8/dg11jloQ_o.png" alt="" class="wp-image-7227">
</figure> 
 
 
<p id="efe8">例如，为了检测人和物体，比如沙发，我们需要一个<strong>目标检测</strong>算法。然后检测人是否坐着，需要一个<strong>姿态识别</strong>算法。最后，我们必须确保人靠近沙发，以确认他正坐在沙发上。仅仅基于2D图像的定位是不准确的，所以这里我们还需要基于深度捕捉的<strong>距离估计</strong>算法。</p> 
 
 
<p id="7d42">此外，我正在使用python，所以算法也必须基于python实现。</p> 
 
 
<h3 class="has-medium-font-size" id="ac95">目标检测</h3> 
 
 
<p id="258f">在机器学习领域，有3种著名的算法用于检测物体：</p> 
 
 
<ul>
<li>
<a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener noreferrer">MobileNets</a>：快但不太准确</li>
<li>
<a href="https://arxiv.org/pdf/1506.02640v5.pdf" target="_blank" rel="noopener noreferrer">YOLO</a>：一点慢但更准确一点</li>
<li>
<a href="https://arxiv.org/pdf/1311.2524.pdf" target="_blank" rel="noopener noreferrer">RCNN</a>(和更快的RCNN)：较慢但准确</li>
</ul> 
 
 
<p id="02e5">对我来说，MobileNets已经足够了，参考下图效果，让我们开始吧！</p> 
 
 
<figure class="wp-block-image size-full">
 <img src="https://images2.imgbox.com/1c/9d/82My90ug_o.gif" alt="" class="wp-image-7228">
 <figcaption>
  检测到沙发和人体
 </figcaption>
</figure> 
 
 
<p id="891d">使用计算机视觉可以让你非常快速地升级你的系统，以添加新的活动来识别。在当前用例中，我决定过滤要检测的对象，但你可以使用其他神经网络模型来识别其他类型的家具，或者你也可以建议用户将位置指向pin。</p> 
 
 
<h3 class="has-medium-font-size" id="539d">姿态识别</h3> 
 
 
<p id="5e27">经过快速研究，有两种算法非常有名：</p> 
 
 
<ul>
<li><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose" target="_blank" rel="noopener noreferrer">OpenPose</a></li>
<li><a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener noreferrer">PifPaf</a></li>
</ul> 
 
 
<p id="408d">起初，我因为简单的python API而选择了OpenPose，但最后，我受OpenPose的启发使用了<a href="https://docs.openvinotoolkit.org/latest/omz_models_model_human_pose_estimation_0001.html">自定义方式</a>，它基于MobileNet，因为它也兼容OAK-D，所以它也非常快。</p> 
 
 
<figure class="wp-block-image size-full">
 <img src="https://images2.imgbox.com/d5/9c/FbTMBkci_o.png" alt="" class="wp-image-7229" title="骨架估计">
 <figcaption>
  骨架估计
 </figcaption>
</figure> 
 
 
<h3 class="has-medium-font-size" id="90b0">距离估计</h3> 
 
 
<p id="5963">这是我选择OAK-D深度相机的主要原因。虽然也有其他算法来估计距离，但深度传感器带来更高的精度。Luxonis提供了参考的<a href="https://docs.oakchina.cn/projects/api/components/nodes/spatial_location_calculator.html" target="_blank" rel="noopener noreferrer">代码</a>。你只需要在图像上提供一个感兴趣的区域（ROI），算法就会返回一个<strong>3D坐标位置</strong>。原点是随机的，但在我的用例中这不是问题，因为我只是对物体之间的<strong>距离</strong>感兴趣。</p> 
 
 
<figure class="wp-block-image size-full">
 <img src="https://images2.imgbox.com/82/e1/sa17Svzx_o.png" alt="" class="wp-image-7232">
</figure> 
 
 
<h2 id="7fa7">结果</h2> 
 
 
<p id="88fd">视频自圆其说</p> 
 
 
<div></div> 
 
 
<h2 id="5be2">深挖工作原理</h2> 
 
 
<p id="d9bf">到目前为止，我给了你我使用的算法，但我没有解释如何为OAK-D相机编码，以及如何在这么小的设备上运行所有这些算法。</p> 
 
 
<h3 class="has-medium-font-size" id="7702">RVC2</h3> 
 
 
<p id="86fa">首先，OAK-D相机计算芯片是基于<a href="https://www.oakchina.cn/2022/09/15/oak-rvc/#RVC2" target="_blank" rel="noopener noreferrer">RVC2</a>，使用这种架构的一个著名产品是英特尔的神经计算棒。</p> 
 
 
<figure class="wp-block-image size-full">
 <img src="https://images2.imgbox.com/a8/27/Hh594yEb_o.png" alt="" class="wp-image-7233">
</figure> 
 
 
<p id="3be8">该架构允许将计算部署在一个插入USB的专用设备上，以保持你的电脑自由。Myriad X芯片被设计用于运行机器学习算法，因此非常高效。</p> 
 
 
<p id="9fe5">作为比较，一个MobileNet算法在OAK-D上的运行速度比在我的MacBook pro（2.6GHz 6核intel core i7，内存32GB 2666MHz DDR 4）上快两倍。</p> 
 
 
<p id="da8d">在Myriad X上运行机器学习算法需要将它们<a href="https://docs.oakchina.cn/en/latest/pages/Advanced/convert.html">转换成openvino格式</a>。</p> 
 
 
<h3 class="has-medium-font-size" id="fcaa">DepthAI</h3> 
 
 
<p id="bc50">但是来自Luxonis的工程师们有了一个更好的主意，他们创建了自己的python API来控制Myriad X芯片和相机——<a href="https://docs.luxonis.com/en/latest/pages/tutorials/first_steps/#first-steps-with-depthai" target="_blank" rel="noopener noreferrer">DepthAI</a>。这个想法很有说服力！从硬件的角度来看，摄像头靠近计算芯片，所以为什么不直接将信息发送到芯片，而不是将图像发送到主机，它将减少<strong>输入/输出</strong>（I/O）。</p> 
 
 
<p id="65b6">开发者指定一个带有输入（相机流）、节点（机器学习算法）和输出（主机上的端点）的管道。</p> 
 
 
<figure class="wp-block-image size-large">
 <img src="https://images2.imgbox.com/5b/4a/WDk7bCGu_o.png" alt="" class="wp-image-7234">
 <figcaption>
  depthai上的特定管道
 </figcaption>
</figure> 
 
 
<p id="8e6f">例如，我选择的算法和模型需要不同的图像大小，所以我定义了一个节点来调整对象检测神经网络的图像大小。一切都是在设备上运行，使用这种体系结构，主机性能不需要很好，因为它不需要管理大量的数据，也不需要大量的I/O。</p> 
 
 
<h3 class="has-medium-font-size" id="7299">稳定性和鲁棒性</h3> 
 
 
<p id="a112">你应该不希望由于算法错误而导致灯意外打开。因此，尽管计算机视觉算法存在不一致的错误，你的家庭自动化系统应该非常靠谱。</p> 
 
 
<p id="a39a">首先，创建了一个校准步骤来检测沙发，以便用户验证这些位置。它避免了来自MobileNet的错误。它还限制了遮挡，从而提高了位置的准确性，特别是深度。</p> 
 
 
<p id="de83">为了提高其他算法的鲁棒性，创建了一个队列系统。估计的位置必须足够接近最后5帧中检测到的位置，才能被认为是有效的。这可能会增加大幅移动的延迟，但它确实让结果更稳定，特别是对于深度传感器测量。</p> 
 
 
<h3 class="has-medium-font-size" id="f7bd">视频流</h3> 
 
 
<p id="7aa1">以控制校准步骤并通过Flask web服务器实时检查基于MPEG格式的视频流系统的结果。</p> 
 
 
<figure class="wp-block-image size-full">
 <img src="https://images2.imgbox.com/c4/4e/4EGnayJX_o.gif" alt="" class="wp-image-7235">
</figure> 
 
 
<h2 id="1253">反馈</h2> 
 
 
<h3 class="has-medium-font-size" id="904d">多重性</h3> 
 
 
<p id="7d2a">在用OAK-D相机进行了几个月的测试和原型制作后，我对这款设备的多重性非常满意。它重量轻，体积小，集成了不同的功能，如高质量图像，深度传感器，Myriad X芯片，h264编码器，空间位置计算器等。它加速了原型验证。</p> 
 
 
<h3 class="has-medium-font-size" id="ceef">算力</h3> 
 
 
<p id="8065">我害怕计算能力不够或不稳定，但即使在芯片上并行运行3个神经网络后，仍然有<strong>2FPS</strong>，这对许多家庭自动化用例来说已经足够了。对于一个5V运行的小设备来说，非常值得赞赏。</p> 
 
 
<h3 class="has-medium-font-size" id="519b">文档</h3> 
 
 
<p id="77ca">虽然<code>depthAI</code>文档是新的，但仍然非常友好，有丰富的例子。它很容易启动，API非常简单。你也可以从社区中找到很多例子来使用已经编译好的模型，比如我用来制作人体姿势的模型。</p> 
 
 
<h3 class="has-medium-font-size" id="b9ba">模块化的</h3> 
 
 
<p id="8d6f">我从OAK-D相机开始，这是Luxonis的一款多功能设备，但是也可以基于模块构建自己的设备。</p> 
 
 
<h2 id="02eb">结论</h2> 
 
 
<p id="322c">总之，虽然OAK-D相机不会取代Nvidia Geforce RTX 3080来运行机器学习算法，但它是一个非常多功能和有前景的设备，可以原型化一些用例，如家庭自动化系统。这个生态系统也非常舒适，你不会浪费时间试图让它工作。</p> 
 
 
<p id="0b3a">我们将在<a href="https://interaction-dynamics.github.io/"> Interaction Dynamics项目里</a>使用这款设备，利用最先进的技术来研究、设计和构建身临其境的用户体验。</p> 
 
<h2>
<a id="_296"></a>▌参考资料</h2> 
<p>https://docs.oakchina.cn/en/latest/<br> https://www.oakchina.cn/selection-guide/</p> 
<hr class="expb"> 
<p><font color="#00a5d5"><strong>OAK中国</strong><br> | OpenCV AI Kit在中国区的官方代理商和技术服务商<br> | 追踪AI技术和产品新动态</font></p> 
<p>戳「<strong>+关注</strong>」获取最新资讯↗↗</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>