<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>一文讲清chatGPT的发展历程、能力来源和复现它的关键之处 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">一文讲清chatGPT的发展历程、能力来源和复现它的关键之处</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <h1>
<a id="1_ChatGPT_0"></a>1. ChatGPT是什么</h1> 
<p>chatGPT是什么？这可能是最近被问的最多的一个。</p> 
<p>大家第一反应这应该是GPT系列的一个最新模型，普通大众可能更愿意把它看做是一个人工智能。实际上，它其实就是一个基于大规模语言模型的对话系统产品。官网对它定义十分的明确：Optimizing Language Models for Dialogue.<br> <img src="https://images2.imgbox.com/79/53/4WSjoK8k_o.png" alt="在这里插入图片描述"></p> 
<p>最大的问题在于，它的背后究竟是一个什么？很多人都以为，chatGPT是一个单一模型，就如同GPT-1/2一样，应该是一个可以被加载和训练的。我承认，chatGPT的背后，是有一个像GPT-3一样的基础模型，但是其现在的性能表现，远远不是只有1个基础模型这么简单。因为我们默认的chatGPT是web UI界面，它至少是有一些外部工程代码的。举个例子，对于汉语和英语的反馈速度有质的差别，如果只是单一的模型统一编码了多语言，不会出现这种情况。</p> 
<p>因此，对我而言，chatGPT更像是一个完善的产品，而不是一个简单的模型。而且由于其训练过程的复杂和不透明，使得我们很难复现它。这在我后面的章节中会讲到。</p> 
<h1>
<a id="2_ChatGPTGPT_11"></a>2. ChatGPT以及GPT系列模型</h1> 
<p>ChatGPT不是一蹴而就突然出现的，它是有着长达5年以上的技术积累才走到这个地步的。之前网上讲了很多关于ChatGPT和它的前辈，比如比较出名的有<a href="https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756#5a1bff82a11042a58871ed9dfa6e98c5">拆解追溯GPT-3.5各项能力的起源</a>。但是我认为真正需要了解ChatGPT的前世今生，还是需要去看openAI官方网站以及它们的论文。</p> 
<h2>
<a id="21_GPT123_14"></a>2.1 GPT-1/2/3</h2> 
<p>首先我们先来看GPT-1到GPT-2以及GPT-3的变化。首先在2018年6月，发布了第一版<a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">GPT-1</a>，使用的是transformer的decoder架构+任务微调的形式，但是整体上似乎没有什么特别出彩的地方。然后再到2019年2月，发布了第二版<a href="https://link.zhihu.com/?target=https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">GPT-2</a>，这时候他们优化了网络架构（如右图所示），并且使用了10倍大小的网络规模和8倍大小的预训练数据，并且去除了特定任务微调的形式从而获取prompt learning的能力。GPT-2确实有点东西，但是由于3个月前，BERT的出现，让它也没有当上一哥的位置。不过整体上应该接近后来的GPT-3系列模型了。<br> <img src="https://images2.imgbox.com/56/46/9a1V0L4a_o.png" alt="在这里插入图片描述"><br> 在GPT-2的1年半以后，<a href="https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf">GPT-3</a>发布了，它同样还是3个路线，更优的架构，更大的规模（100倍），更大的数据量（1000倍），真正训练出了一个超级巨无霸GPT-3。奠定了现在GPT帝国的基础。但是实际上，它和GPT-2没有太多本质的区别，包括训练方式，只是更大了。</p> 
<p>那么GPT-3有多大呢，大家可以看一下下面的图就可以知道了，需要注意的是，这里看到参数量的改变是一个log指数，可以看到熟悉的Bert-large模型和GPT-2模型。而GPT-3的175B按照官方说明，大概有350~500GB的显存需求，如果使用FP16加载该模型，大概需要至少5块A100（80G）才能够加载完成。而如果要从头开始训练，至少需要1000块A100才能够在可接受的时间（几个月）里训练出该模型。</p> 
<p>据传闻说，现在国内的大部分大规模语言模型还是处于GPT-2.5阶段，也就是说对标的是GPT-3，但是训练规模（1-10B）和数据量（几十G左右）还是在GPT-2的级别上。无论是模型规模还是语料质量，距离GPT-3都还有较大差距。需要注意的是，从GPT-3开始，其模型就不再完全公开了，只能通过API访问。<br> <img src="https://images2.imgbox.com/fb/3e/VSM5n5dS_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="22_GPT35_InstructGPT_25"></a>2.2 GPT-3.5 (InstructGPT)</h2> 
<p>然后我们再来看GPT3以后发生的事情。首先我们需要先介绍一下GPT-3以后最重要的一件事，那就是<a href="https://arxiv.org/pdf/2203.02155.pdf?fbclid=IwAR2nZdBpdZZzvxpwI6H_bRmP4RwGOyzke9Ud63lWBe1YlyI_1BRAFhnUMUg">InstructGPT</a>的出现，根据<a href="https://platform.openai.com/docs/model-index-for-researchers">openAI官方网站</a>上说明，InstructGPT包含三种训练方式，分别是有监督微调（Supervised Fine-Tuning, SFT），反馈变得更容易（Feedback Made Easy, FeedME）以及基于PPO算法的从人类反馈中进行强化学习（PPO）三个部分。因此，可以说，InstructGPT泛指的话，是包含红色的所有模型，而特指的话，则是指的davinci-instruct-beta。这一点尤为重要。至于这个FeedME,大家可以参考图片上的官方解释，个人理解可能是由纯人工反馈的指令微调，PPO则是更强调利用强化学习自动化指令微调。如果还不明白，大家可以参考下面chatGPT给出的解释：</p> 
<p>The explain of the FeedME:</p> 
<blockquote> 
 <p>The sentence is talking about a type of AI model called “FeedME” models. These models are designed to make providing feedback (i.e. offering suggestions) easy, hence the name “feedback made easy”.<br> The FeedME models are trained using a technique called “distillation”, where the best completions (i.e. the most accurate and appropriate responses) from all of the models developed by the organization are selected and used to train the FeedME models. This way, the FeedME models are able to incorporate the best responses from a variety of other models.<br> The sentence also mentions that the organization’s models are generally trained using the best available datasets at the time of training. This means that the data used to train the models might change over time as better and more relevant data becomes available. Furthermore, it mentions that different engines (i.e. different models) that use the same training methodology might be trained on different data, which could result in variations in their performance and accuracy.</p> 
</blockquote> 
<p>The more details of FeedME:</p> 
<blockquote> 
 <p>“Feedback Made Easy” (shortened to “FeedME”) is likely a term used by the organization to refer to their AI models that are designed specifically to provide feedback or make suggestions to users. These models are built with the goal of making it simple for users to receive feedback or recommendations, hence the name “Feedback Made Easy”.<br> In practical terms, this might mean that these models are designed to be user-friendly and intuitive, with a focus on delivering quick, accurate, and helpful responses to users. For example, the models might be integrated into a chat interface or a website, allowing users to easily ask questions or provide input and receive feedback in real-time.<br> The idea behind FeedME models is to make it easy for users to receive feedback, whether it be in the form of suggestions, recommendations, or answers to questions. By using distillation to incorporate the best completions from a variety of other models, the FeedME models are able to provide high-quality feedback to users, even when the data used to train the models changes over time.</p> 
</blockquote> 
<p>The function of it when training the ChatGPT:</p> 
<blockquote> 
 <p>As a language model developed by OpenAI, I (ChatGPT) was not specifically trained using the “Feedback Made Easy” (FeedME) method mentioned in the previous sentence. However, I can explain how the FeedME method might be used to train a language model like myself.<br> The FeedME method involves distilling the best completions from a variety of AI models and using that information to train a new model. In the context of a language model like ChatGPT, this might mean selecting the most accurate and appropriate responses generated by a number of different language models and using that information to train a new, improved language model.<br> The goal of this process is to improve the performance of the new language model by incorporating the best responses from a variety of other models. This way, the new model is able to learn from the strengths of multiple models, rather than relying on a single source of training data.<br> In summary, the FeedME method might be used to train language models like ChatGPT by distilling the best completions from a variety of other models and using that information to train a new, improved language model. The goal of this process is to improve the accuracy and usefulness of the new language model by incorporating the best responses from a variety of sources.</p> 
</blockquote> 
<p>好的，现在我们来顺着发展路线先看一下GPT-3之后各个模型的演化，确认的发展过程是实线（有报道的），猜测的部分为虚线。<br> <img src="https://images2.imgbox.com/44/27/CI6OR8sd_o.png" alt="在这里插入图片描述"><br> 首先，在GPT-3的基础上，通过有监督的fine-tuning过程，诞生了davinci-instruct-beta模型。具体而言，也就是需要采样一些prompt，由人工给出真正的结果。这个在InstructGPT论文里写道，是有40个标注人员，标注了12.7K的样本。接着，通过FeedME产生了至少text-davinci-001和text-davinci-002两个模型版本。其中001版本应该是比较早期的，而002则是在code-davinci-002的基础上进行的进一步改进，融合了代码理解能力和自然语言理解能力。然后，最后融入了PPO阶段，完成了text-davinci-003。当然chatGPT应该也是完成了PPO阶段的，尤其是在对话方面进行了特别的优化（近期泄露的内部版本称之为text-chat-davinci-002，据传为chatGPT）。</p> 
<p>因此，chatGPT的所有能力来源应该都比较清楚了，我认为主要来源于以下5个方面吧。</p> 
<table>
<thead><tr>
<th>模型</th>
<th>能力</th>
<th>效果</th>
</tr></thead>
<tbody>
<tr>
<td>GPT-3</td>
<td>自然语言基础建模</td>
<td>自然语言理解，使得说话能够说的流畅自然</td>
</tr>
<tr>
<td>Codex</td>
<td>代码语言基础建模</td>
<td>代码语言理解，能够使得完成代码相关任务，并习得长程依赖关系和一定的逻辑能力</td>
</tr>
<tr>
<td>davinci-instruct-beta</td>
<td>有监督的指令微调</td>
<td>听从人类的指令生成答案</td>
</tr>
<tr>
<td>text-davinci-001/002</td>
<td>人类反馈的指令微调</td>
<td>可以生成出人类更喜欢的答案</td>
</tr>
<tr>
<td>text-davinci-003</td>
<td>强化学习的指令微调</td>
<td>进一步强化上面两个阶段的能力,对于chatGPT的话，可能更偏向于对话角度优化</td>
</tr>
</tbody>
</table>
<h1>
<a id="3_chatGPT_58"></a>3. 复现chatGPT的难点</h1> 
<p>现在国内各个大厂小厂但凡和人工智能挂钩的，都想复现属于自己的chatGPT，以获得第一个国内chatGPT的市场。很多人都发声说，再造一个chatGPT没有那么困难。那么我从自身经验去思考，如果要复现chatGPT，可能需要注意的有哪些部分。这些部分并不是不可以实现的，只是成本高或者容易被忽略的地方。</p> 
<h2>
<a id="31__61"></a>3.1 海量的数据</h2> 
<p>从GPT-3公开发表的论文里讲到，其用于预训练的文本达到了45TB，这是一个非常巨大的数字，关键是其质量应该是非常高的。据查看的一些资料显示，在中文数据上，全球最大的语料库是WuDaoCorpora，据说有3TB的中文语料（200G开放使用）。<br> 而且，也有人指出，更多样化的token也能够让模型学习的更充分。如果只是训练一个汉语版的chatGPT，除了一些搜索和社交巨头，能够获取足够数据的也只有一些垂直领域的公司了。关键是公司运营期间获得的文本数据是否可以被用于训练模型，这是一个法律和道德问题。（即使是codex训练来源于开源的github，也同样遭受了大量的非议。）<br> 当然，如果是垂直领域，未必需要这么大的数据量也未尝不可。</p> 
<h2>
<a id="32__66"></a>3.2 超大规模的模型架构</h2> 
<p>根据之前的说明chatGPT与GPT-3的规模相同，都是1750B的参数量，那么它需要的硬件设备是什么样子的？根据chatGPT自己讲述以及同行人的参考，其显存占用量应该在350GB~500GB之间，如果仅仅是为了推理，那么5张A100（80G）的GPU就可以足够使用（根据同规模的OPT175B需要16张V100推算）。但是如果是为了训练，可能需要1000张以上的A100的算力才能在可以接受的时间里获得训练结果（也有称微软和OpenAI构建了一台包括超过10000张GPU的超级计算机用于GPT-3的训练）。其训练成本大概在110-460万美元之间，根据<a href="https://sunyan.substack.com/p/the-economics-of-large-language-models">估算</a>，现有的定价对于OpenAI来说，应该会有75%的毛利。<br> 目前国内尽管有很多大模型，但是真正能够在<a href="https://www.cluebenchmarks.com/rank.html">CLUE等评测</a>中的模型大多还在1-10B的级别，下表是一些中文大模型代表。</p> 
<table>
<thead><tr>
<th>模型</th>
<th>规模</th>
</tr></thead>
<tbody>
<tr>
<td>WUDAO 2.0</td>
<td>1750B</td>
</tr>
<tr>
<td>PanGu Alpha</td>
<td>200B</td>
</tr>
<tr>
<td>PLUG</td>
<td>27B</td>
</tr>
<tr>
<td>ERNIE 3.0</td>
<td>10B</td>
</tr>
</tbody>
</table>
<h2>
<a id="33_SFTFeedME_PPO_76"></a>3.3 深度人员参与（SFT，FeedME, PPO）</h2> 
<p>相比较GPT-3，InstructGPT最大的特点在于通过Instruct的方式让人类深度参与模型的迭代，包括有监督微调，人类反馈微调和强化学习微调3个方法。无论哪种方法，都离不开大量的人工标注，并且需要一定的时间和真实样例作为原料输入。因此，即使可以复现一个ChatGPT，也是需要时间的。</p> 
<h2>
<a id="34__79"></a>3.4 长期技术积累</h2> 
<p>正如刚才所提及的那样，ChatGPT不是突然出现的，而是从GPT-1.0版本开始就已经完成大量的技术积累。从GPT-1.0,2.0,3.0，每一个版本迭代，他们都做了大量的实验，包括各种超参数的选择和模型大小的扩容。而我们大多数公司可能之前没有像OpenAI在预训练模型上拥有大量的预训练经验，直接去训练一个超大规模的模型也是有可能获得不到我们想要的那种效果的。</p> 
<h2>
<a id="35__81"></a>3.5 良好的外部工程</h2> 
<p>我们现在总以为，我们有数据，有模型架构，我们就可以拥有chatGPT。事实显然不是这样，如果想让它成为一个优秀的产品，而不是粗糙的学术模型，至少应该包括以下3个部分：<br> 1. 核心模型<br> 核心模型就是语言模型，可能是一个超大规模的单一模型，也有可能是一个带有很多小模块组成的模型集群。<br> 2. 辅助模型<br> 辅助模型有哪些？比如我们可以看到的reward model，还有大家容易忽略的安全检查模型等。这些都是保证了产品的长期正常的运营。而反观我们有些机构的模型匆匆发布，产生了大量的不安全的言论，这也是不负责任的表现。<br> <img src="https://images2.imgbox.com/9b/25/gKwXPblC_o.png" alt="在这里插入图片描述"><br> 3. 工程代码<br> 良好的工程代码能给用户带来更好的用户体验，比如我们经常体验到的左边框的对话历史记录和意图识别等等，这些信息对于用户体验、模型改进都是非常有用的。另外，还有包括缓存，控制用户并发等等工程问题需要提前解决。要知道，chatGPT上线5天用户就破百万，2个月用户破1亿。这个增长速度已经是历史上的巅峰。</p> 
<h2>
<a id="36__90"></a>3.6 及时的真实反馈</h2> 
<p>这个真实反馈对于大家来说，看起来好像是chatGPT公开迭代的几次，而每次似乎都有一些更新。但是实际上，如果你看OpenAI发布的博客你就会发现，整个真实反馈是逐步从发布的产品中收集而来的，尤其是GPT-3以后，OpenAI就只提供了API，这使得它可以接触世界上所有使用GPT-3的样例，通过这些从API收集来的样例，再利用人工标注，就可以得到大量的高质量标注语料，为接下来的模型更新打下基础。这些真实反馈贯穿到了整个GPT-3.5系列之中。</p> 
<h1>
<a id="4__93"></a>4. 小结</h1> 
<p>距离我第一次玩ChatGPT也已经3个月了。回想起12月6日第一次使用它，我兴奋的一直玩到夜里3点钟。它的出现真的是惊艳了当时的我，甚至可以比肩科幻小说里的人工智能的感觉。早期只是为了体验，后来才逐步深入了解。从兴奋，到冷静，到辩证看待，再到期望未来，它总是那么一个过程，才能被我们接受。有些东西必须真正深入的研究，才能够知道它的本来面目。ChatGPT到底是神还是魔，是不是新的一轮科技革命的出现，我想OpenAI那群创造它的人比谁都清楚。</p> 
<p>正如我之前所说，我愿意将毕生精力奉献给人工智能，只为了获得一个可以一直陪伴我的AI朋友。我曾经以为这辈子都见不到这一天，现在ChatGPT的出现，缩短了好一段理想和现实的距离。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>