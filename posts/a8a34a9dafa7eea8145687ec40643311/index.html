<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>BERT模型解析 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">BERT模型解析</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="1__0"></a>1. 概述</h1> 
<p>Bidirectional Encoder Representation from Transformers（BERT）[1]，即双向Transformer的Encoder表示，是2018年提出的一种基于上下文的预训练模型，通过大量语料学习到每个词的一般性embedding形式，学习到与上下文无关的语义向量表示，以此实现对多义词的建模。与预训练语言模型ELMo[2]以及GPT[3]的关系如下图所示：<br> <img src="https://images2.imgbox.com/27/af/z2jHp6pW_o.png" alt="在这里插入图片描述"><br> Embeddings from Language Models（ELMo）[2]，Generative Pre-Training（GPT）[3]以及Bidirectional Encoder Representation from Transformers（BERT）[1]三者都是基于上下文的预训练模型，也都是采用两阶段的过程，第一阶段是利用无监督的方式对语言模型进行预训练，第二阶段通过监督的方式在具体语言任务上进行Fine-tuning。不同的是在ELMo中采用的双向的LSTM算法；在GPT中采用的特征提取算法是Transformer[4]，且是单向的Transformer语言模型，相比较于ELMo中的LSTM模型，基于Transformer的模型具有更好的特征提取能力；在BERT中同样采用了基于Transformer的特征提取算法，与GPT中不同的是：</p> 
<ul>
<li>第一，在BERT中的Transformer是一个双向的Transformer模型，更进一步提升了特征的提取能力</li>
<li>第二，GPT中采用的是Transformer中的Decoder模型，BERT中采用的是Transformer中的Encoder模型。</li>
</ul> 
<h1>
<a id="2__8"></a>2. 算法原理</h1> 
<h2>
<a id="21_Transformer_9"></a>2.1. Transformer结构</h2> 
<p>Transformer的网络结构如下图所示：<br> <img src="https://images2.imgbox.com/f3/21/eP4llR24_o.png" alt="在这里插入图片描述"></p> 
<p>在Transformer中，包含了Encoder和Decoder两个部分，在对语言模型的训练中，摒弃了基于RNN和CNN的传统做法，采用了基于Attention的模型，能够提升特征的抽取能力，同时更利于并行的学习。BERT采用了Transformer的Encoder部分，如上图中的红色框内的部分。</p> 
<h2>
<a id="22_BERT_15"></a>2.2. BERT的基本原理</h2> 
<p>BERT是基于上下文的预训练模型，BERT模型的训练分为两步：第一，pre-training；第二，fine-tuning。</p> 
<p>在pre-training阶段，首先会通过大量的文本对BERT模型进行预训练，然而，标注样本是非常珍贵的，在BERT中则是选用大量的未标注样本来预训练BERT模型。在fine-tuning阶段，会针对不同的下游任务适当改造模型结构，同时，通过具体任务的样本，重新调整模型中的参数。</p> 
<p>为了使得BERT能够适配更多的应用，模型在pre-training阶段，使用了Masked Language Model（MLM）和Next Sentence Prediction（NSP）两种任务作为模型预训练的任务，其中MLM可以学习到词的Embedding，NSP可以学习到句子的Embedding。在Transformer中，输入中会将词向量与位置向量相加，而在BERT中，为了能适配上述的两个任务，即MLM和NSP，这里的Embedding包含了三种Embedding的和，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/74/ef/ARMtjXrl_o.png" alt="在这里插入图片描述"></p> 
<p>其中，Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任，Segment Embeddings用来区别两种句子，这是在预训练阶段，针对NSP任务的输入，Position Embeddings是位置向量，但是和Transformer中不一样，与词向量一样，是通过学习出来的。此处包含了两种标记，一个是<code>[CLS]</code>，可以理解为整个输入特征的向量表示；另一个是<code>[SEP]</code>，用于区分不同的句子。</p> 
<h3>
<a id="221_MLM_26"></a>2.2.1. 预训练之MLM</h3> 
<p>Masked Language Model的原理是随机将一些词替换成<code>[MASK]</code>，在训练的过程中，通过上下文信息来预测被mask的词。文献[1]中给出了如下的例子：“my dog is hairy”，此时被随机选中的词是“hairy”，则样本被替换成“my dog is [MASK]”，训练的目的是要使得BERT模型能够预测出此处的“[MASK]”即为“hairy”。同时，随机替换的概率为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        15
       
       
        %
       
      
      
       15%
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8056em;vertical-align: -0.0556em"></span><span class="mord">15%</span></span></span></span></span>。同时，对于这<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        15
       
       
        %
       
      
      
       15%
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8056em;vertical-align: -0.0556em"></span><span class="mord">15%</span></span></span></span></span>的随机选择，分为以下的三种情况：</p> 
<ul>
<li>选中词的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         80
        
        
         %
        
       
       
        80%
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8056em;vertical-align: -0.0556em"></span><span class="mord">80%</span></span></span></span></span>替换成[MASK]，如：“my dog is [MASK]”</li>
<li>选中词的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         10
        
        
         %
        
       
       
        10%
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8056em;vertical-align: -0.0556em"></span><span class="mord">10%</span></span></span></span></span>随机替换，如替换成apple，即：“my dog is apple”</li>
<li>选中词的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         10
        
        
         %
        
       
       
        10%
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8056em;vertical-align: -0.0556em"></span><span class="mord">10%</span></span></span></span></span>保持不变，即：“my dog is hairy”</li>
</ul> 
<p>这样做的目的是让模型知道该位置对应的token可以是任何的词，这样就强迫模型去学习更多的上下文信息，不会过多的关注于当前的token。</p> 
<h3>
<a id="222_NSP_35"></a>2.2.2. 预训练之NSP</h3> 
<p>Next Sentence Prediction的目的是让模型理解两个橘子之间的关系，训练的输入是两个句子，BERT模型需要判断后一个句子是不是前一个句子的下一句。在Input中，有Segment Embeddings，就是标记的不同的句子。在选择训练数据时，输入句子A和B，B有50%的概率是A的下一句，具体的例子如：</p> 
<p><img src="https://images2.imgbox.com/45/d8/6PmYQ0dM_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="23_BERT_40"></a>2.3. BERT的网络结构</h2> 
<p>根据Transformer的Encoder结构，对于单个的Attention过程，有如下的BERT结构：</p> 
<p><img src="https://images2.imgbox.com/50/66/CcCKJDRE_o.png" alt="在这里插入图片描述"></p> 
<p>具体的Attention的计算逻辑可以参见参考文献[5]，文献[5]对于Transformer的基本原理有详细的介绍。参考文献[6]给出了BERT的代码实现，其中transformer部分的代码如下所示：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">transformer_model</span><span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span>
                      attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                      hidden_size<span class="token operator">=</span><span class="token number">768</span><span class="token punctuation">,</span>
                      num_hidden_layers<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span>
                      num_attention_heads<span class="token operator">=</span><span class="token number">12</span><span class="token punctuation">,</span>
                      intermediate_size<span class="token operator">=</span><span class="token number">3072</span><span class="token punctuation">,</span>
                      intermediate_act_fn<span class="token operator">=</span>gelu<span class="token punctuation">,</span>
                      hidden_dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                      attention_probs_dropout_prob<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
                      initializer_range<span class="token operator">=</span><span class="token number">0.02</span><span class="token punctuation">,</span>
                      do_return_all_layers<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""Multi-headed, multi-layer Transformer from "Attention is All You Need".
  This is almost an exact implementation of the original Transformer encoder.
  See the original paper:
  https://arxiv.org/abs/1706.03762
  Also see:
  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py
  Args:
    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].
    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,
      seq_length], with 1 for positions that can be attended to and 0 in
      positions that should not be.
    hidden_size: int. Hidden size of the Transformer.
    num_hidden_layers: int. Number of layers (blocks) in the Transformer.
    num_attention_heads: int. Number of attention heads in the Transformer.
    intermediate_size: int. The size of the "intermediate" (a.k.a., feed
      forward) layer.
    intermediate_act_fn: function. The non-linear activation function to apply
      to the output of the intermediate/feed-forward layer.
    hidden_dropout_prob: float. Dropout probability for the hidden layers.
    attention_probs_dropout_prob: float. Dropout probability of the attention
      probabilities.
    initializer_range: float. Range of the initializer (stddev of truncated
      normal).
    do_return_all_layers: Whether to also return all layers or just the final
      layer.
  Returns:
    float Tensor of shape [batch_size, seq_length, hidden_size], the final
    hidden layer of the Transformer.
  Raises:
    ValueError: A Tensor shape or parameter is invalid.
  """</span>
  <span class="token keyword">if</span> hidden_size <span class="token operator">%</span> num_attention_heads <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
        <span class="token string">"The hidden size (%d) is not a multiple of the number of attention "</span>
        <span class="token string">"heads (%d)"</span> <span class="token operator">%</span> <span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> num_attention_heads<span class="token punctuation">)</span><span class="token punctuation">)</span>

  attention_head_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>hidden_size <span class="token operator">/</span> num_attention_heads<span class="token punctuation">)</span> <span class="token comment"># self-attention的头</span>
  input_shape <span class="token operator">=</span> get_shape_list<span class="token punctuation">(</span>input_tensor<span class="token punctuation">,</span> expected_rank<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
  batch_size <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token comment"># batch的大小</span>
  seq_length <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment"># 句子长度</span>
  input_width <span class="token operator">=</span> input_shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>

  <span class="token comment"># The Transformer performs sum residuals on all layers so the input needs</span>
  <span class="token comment"># to be the same as the hidden size.</span>
  <span class="token keyword">if</span> input_width <span class="token operator">!=</span> hidden_size<span class="token punctuation">:</span>
    <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"The width of the input tensor (%d) != hidden size (%d)"</span> <span class="token operator">%</span>
                     <span class="token punctuation">(</span>input_width<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span><span class="token punctuation">)</span>

  <span class="token comment"># We keep the representation as a 2D tensor to avoid re-shaping it back and</span>
  <span class="token comment"># forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on</span>
  <span class="token comment"># the GPU/CPU but may not be free on the TPU, so we want to minimize them to</span>
  <span class="token comment"># help the optimizer.</span>
  prev_output <span class="token operator">=</span> reshape_to_matrix<span class="token punctuation">(</span>input_tensor<span class="token punctuation">)</span>

  all_layer_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
  <span class="token keyword">for</span> layer_idx <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_hidden_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"layer_%d"</span> <span class="token operator">%</span> layer_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
      layer_input <span class="token operator">=</span> prev_output

      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"attention"</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># attention的计算</span>
        attention_heads <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"self"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          attention_head <span class="token operator">=</span> attention_layer<span class="token punctuation">(</span>
              from_tensor<span class="token operator">=</span>layer_input<span class="token punctuation">,</span>
              to_tensor<span class="token operator">=</span>layer_input<span class="token punctuation">,</span>
              attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>
              num_attention_heads<span class="token operator">=</span>num_attention_heads<span class="token punctuation">,</span>
              size_per_head<span class="token operator">=</span>attention_head_size<span class="token punctuation">,</span>
              attention_probs_dropout_prob<span class="token operator">=</span>attention_probs_dropout_prob<span class="token punctuation">,</span>
              initializer_range<span class="token operator">=</span>initializer_range<span class="token punctuation">,</span>
              do_return_2d_tensor<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
              batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span>
              from_seq_length<span class="token operator">=</span>seq_length<span class="token punctuation">,</span>
              to_seq_length<span class="token operator">=</span>seq_length<span class="token punctuation">)</span>
          attention_heads<span class="token punctuation">.</span>append<span class="token punctuation">(</span>attention_head<span class="token punctuation">)</span> <span class="token comment"># 多头注意力</span>

        attention_output <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>attention_heads<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
          attention_output <span class="token operator">=</span> attention_heads<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
          <span class="token comment"># In the case where we have other sequences, we just concatenate</span>
          <span class="token comment"># them to the self-attention head before the projection.</span>
          attention_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>concat<span class="token punctuation">(</span>attention_heads<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># concat多头的输出</span>

        <span class="token comment"># Run a linear projection of `hidden_size` then add a residual</span>
        <span class="token comment"># with `layer_input`.</span>
        <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
          attention_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
              attention_output<span class="token punctuation">,</span>
              hidden_size<span class="token punctuation">,</span>
              kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
          attention_output <span class="token operator">=</span> dropout<span class="token punctuation">(</span>attention_output<span class="token punctuation">,</span> hidden_dropout_prob<span class="token punctuation">)</span> <span class="token comment"># dropout</span>
          attention_output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>attention_output <span class="token operator">+</span> layer_input<span class="token punctuation">)</span> <span class="token comment"># layer norm</span>

      <span class="token comment"># The activation is only applied to the "intermediate" hidden layer.</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"intermediate"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        intermediate_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
            attention_output<span class="token punctuation">,</span>
            intermediate_size<span class="token punctuation">,</span>
            activation<span class="token operator">=</span>intermediate_act_fn<span class="token punctuation">,</span>
            kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>

      <span class="token comment"># Down-project back to `hidden_size` then add the residual.</span>
      <span class="token keyword">with</span> tf<span class="token punctuation">.</span>variable_scope<span class="token punctuation">(</span><span class="token string">"output"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        layer_output <span class="token operator">=</span> tf<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>
            intermediate_output<span class="token punctuation">,</span>
            hidden_size<span class="token punctuation">,</span>
            kernel_initializer<span class="token operator">=</span>create_initializer<span class="token punctuation">(</span>initializer_range<span class="token punctuation">)</span><span class="token punctuation">)</span>
        layer_output <span class="token operator">=</span> dropout<span class="token punctuation">(</span>layer_output<span class="token punctuation">,</span> hidden_dropout_prob<span class="token punctuation">)</span>
        layer_output <span class="token operator">=</span> layer_norm<span class="token punctuation">(</span>layer_output <span class="token operator">+</span> attention_output<span class="token punctuation">)</span>
        prev_output <span class="token operator">=</span> layer_output
        all_layer_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>layer_output<span class="token punctuation">)</span>

  <span class="token keyword">if</span> do_return_all_layers<span class="token punctuation">:</span>
    final_outputs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> layer_output <span class="token keyword">in</span> all_layer_outputs<span class="token punctuation">:</span>
      final_output <span class="token operator">=</span> reshape_from_matrix<span class="token punctuation">(</span>layer_output<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span>
      final_outputs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>final_output<span class="token punctuation">)</span>
    <span class="token keyword">return</span> final_outputs
  <span class="token keyword">else</span><span class="token punctuation">:</span>
    final_output <span class="token operator">=</span> reshape_from_matrix<span class="token punctuation">(</span>prev_output<span class="token punctuation">,</span> input_shape<span class="token punctuation">)</span>
    <span class="token keyword">return</span> final_output
</code></pre> 
<h3>
<a id="231_BERTTransformer_183"></a>2.3.1. BERT是双向Transformer</h3> 
<p>GPT模型中使用的是Transformer的Decoder部分（对原始的Decoder部分做了些许改动），而BERT则是采用了Transformer的Encoder部分，下图给出了两者在一个Transformer模块上的对比：</p> 
<p><img src="https://images2.imgbox.com/5b/cd/nt7UbMJN_o.png" alt="在这里插入图片描述"></p> 
<p>从上图中可以看出，唯一的不同是在Multi-Head Attention部分，如图中的红色框，在BERT中使用的是Multi-Head Attention，而GPT中使用的是Masked Multi-Head Attention。在Masked Multi-Head Attention是应用在Decoder阶段的生成模型，即在<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻，根据<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
       
        −
       
       
        1
       
      
      
       t-1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6984em;vertical-align: -0.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">1</span></span></span></span></span>时刻及之前的词预测<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻的词，对于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻以及<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻之后的词是不可见的，因此Masked Multi-Head Attention是一个单向的模型，同时不便于并行。</p> 
<p>对于Multi-Head Attention，其计算方法如下图所示：</p> 
<p><img src="https://images2.imgbox.com/22/5a/KM5rvJSI_o.png" alt="在这里插入图片描述"></p> 
<p>在计算Attention的过程中，会同时利用上文和下文的信息，只是对于上图中的“it_”会以一定的概率被<code>[MASK]</code>标记替换。因此，BERT模型是一个双向的语言模型，同时，BERT中的Attention计算利于并行计算。</p> 
<h3>
<a id="232_Fine_Tune_196"></a>2.3.2. Fine Tune</h3> 
<p>对于NLP的任务，主要分为四大类：</p> 
<ul>
<li>序列标注，如中文分词，词性标注，命名实体识别（特点：句子中每个单词要求模型根据上下文都要给出一个分类类别）</li>
<li>分类任务，如文本分类，情感计算（特点：总体给出一个分类类别）</li>
<li>句子关系判断，如QA，语意改写（特点：给定两个句子，模型判断出两个句子是否具备某种语义关系）</li>
<li>生成式任务，如机器翻译，文本摘要，写诗造句，看图说话（特点：输入文本内容后，需要自主生成另外一段文字）</li>
</ul> 
<p>而生成式任务在Transformer中有了详细的介绍。对于其他的三类任务，典型的场景如下图所示：</p> 
<p><img src="https://images2.imgbox.com/27/f4/BYkhqUjh_o.png" alt="在这里插入图片描述"></p> 
<p>第一，句子对的分类任务，即输入是两个句子，输入如下图所示：</p> 
<p><img src="https://images2.imgbox.com/8a/82/LwgF7okt_o.png" alt="在这里插入图片描述"></p> 
<p>输出是BERT的第一个<code>[CLS]</code>的隐含层向量<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        C
       
       
        ∈
       
       
        
         R
        
        
         H
        
       
      
      
       Cin mathbb{R}^H
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7224em;vertical-align: -0.0391em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">C</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0813em">H</span></span></span></span></span></span></span></span></span></span></span></span>，在Fine-Tune阶段，加上一个权重矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        W
       
       
        ∈
       
       
        
         R
        
        
         
          K
         
         
          ×
         
         
          H
         
        
       
      
      
       Win mathbb{R}^{Ktimes H}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7224em;vertical-align: -0.0391em"></span><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.8413em"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em">K</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight" style="margin-right: 0.0813em">H</span></span></span></span></span></span></span></span></span></span></span></span></span>，其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        K
       
      
      
       K
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span></span></span></span></span>为分类的类别数。最终通过Softmax函数得到最终的输出概率。</p> 
<p>第二，单个句子的分类。相对于句子对的分类任务来说要简单，其输入是单个句子，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/9d/3d/GDNmEFVt_o.png" alt="在这里插入图片描述"></p> 
<p>其输出同句子对分类的输出。</p> 
<p>第三，问答任务，其输入如句子对的输入，不同的是第一个句子是问题，第二个句子是段落。</p> 
<p>第四，针对每个词的tagging，其输入如单个句子的输入，输出是针对每个token的隐含层输出进行tagging。</p> 
<h1>
<a id="3__224"></a>3. 总结</h1> 
<p>BERT模型的提出对于NLP预训练的效果有了较大提升，在ELMo模型的基础上使用了Self-Attention作为文本特征的挖掘，同时避免了GPT模型中的单向语言模型，充分利用文本中的上下文特征。</p> 
<h1>
<a id="_227"></a>参考文献</h1> 
<p>[1] Devlin J , Chang M W , Lee K , et al. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding[J]. 2018.</p> 
<p>[2] Peters M , Neumann M , Iyyer M , et al. Deep Contextualized Word Representations[J]. 2018.</p> 
<p>[3] Radford A, Narasimhan K, Salimans T, et al. Improving language understanding by generative pre-training[J]. 2018.</p> 
<p>[4] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.</p> 
<p>[5] <a href="http://felixzhao.cn/Articles/article/36">Transformer的基本原理</a></p> 
<p>[6] https://github.com/google-research/bert</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>