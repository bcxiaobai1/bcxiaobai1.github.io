<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>PyTorch 之 神经网络 Mnist 分类任务 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch 之 神经网络 Mnist 分类任务</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#Mnist__4">一、Mnist 分类任务简介</a></li>
<li><a href="#Mnist__22">二、Mnist 数据集的读取</a></li>
<li><a href="#_Mnist__63">三、 Mnist 分类任务实现</a></li>
<li>
<ul>
<li><a href="#1__64">1. 标签和简单网络架构</a></li>
<li><a href="#2__73">2. 具体代码实现</a></li>
</ul>
  </li>
<li><a href="#_TensorDataset__DataLoader__237">四、使用 TensorDataset 和 DataLoader 简化</a></li>
</ul>
</div>
<p></p> 
<p>本文参加新星计划人工智能(Pytorch)赛道：<a href="https://bbs.csdn.net/topics/613989052">https://bbs.csdn.net/topics/613989052</a><br> <img src="https://images2.imgbox.com/7d/64/8cgi8ILl_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="Mnist__4"></a>一、Mnist 分类任务简介</h1> 
<ul>
<li>在上一篇博客当中，我们通过搭建 PyTorch 神经网络实现了气温预测，这本质上是一个回归任务。在本次博文当中，我们使用 PyTorch 做一个分类任务。</li>
<li>其实，分类任务和回归任务在本质上没有任何区别，只是说在结果上是不同的，损失函数是不同的，中间的网络架构却是大体一致的。</li>
<li>在本次的分类任务当中，我们使用的数据集是 Mnist 数据集，这个数据集大家都比较熟悉，可以在 <a href="http://yann.lecun.com/exdb/mnist/"> http://yann.lecun.com/exdb/mnist/</a> 中获取，主要包括四个文件：</li>
</ul> 
<table>
<thead><tr>
<th>文件名称</th>
<th>大小</th>
<th>内容</th>
</tr></thead>
<tbody>
<tr>
<td>train-images-idx3-ubyte.gz</td>
<td>9,681 kb</td>
<td>55000 张训练集，5000 张验证集</td>
</tr>
<tr>
<td>train-labels-idx1-ubyte.gz</td>
<td>29 kb</td>
<td>训练集图片对应的标签</td>
</tr>
<tr>
<td>t10k-images-idx3-ubyte.gz</td>
<td>1,611kb</td>
<td>10000 张测试集</td>
</tr>
<tr>
<td>t10k-labels-idx1-ubyte.gz</td>
<td>5 kb</td>
<td>测试集图片对应的标签</td>
</tr>
</tbody>
</table>
<ul>
<li>在上述在上述文件中，训练集 train 一共包含了 60000 张图像和标签，而测试集一共包含了 10000 张图像和标签。</li>
<li>idx3 表示 3 维，ubyte 表示是以字节的形式进行存储的，t10k 表示 10000 张测试图片（test10000）。</li>
<li>每张图片是一个 28*28 像素点的 0 ~ 9 的灰质手写数字图片，黑底白字，图像像素值为 0 ~ 255，越大该点越白。</li>
<li>本次分类任务主要包含如下的几个部分：</li>
<li>（1） 网络基本构建与训练方法，常用函数解析。</li>
<li>（2） torch.nn.functional 模块。</li>
<li>（3） nn.Module 模块。</li>
</ul> 
<h1>
<a id="Mnist__22"></a>二、Mnist 数据集的读取</h1> 
<ul><li>对于 Mnist 数据集，我们可以通过代码编写，就可以实现自动下载。</li></ul> 
<pre><code class="prism language-python"><span class="token operator">%</span>matplotlib inline
<span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path
<span class="token keyword">import</span> requests
​
DATA_PATH <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span>
PATH <span class="token operator">=</span> DATA_PATH <span class="token operator">/</span> <span class="token string">"mnist"</span>
​
PATH<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>parents<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
​
URL <span class="token operator">=</span> <span class="token string">"http://deeplearning.net/data/mnist/"</span>
FILENAME <span class="token operator">=</span> <span class="token string">"mnist.pkl.gz"</span>
</code></pre> 
<ul><li>对于我们上面定义的下载路径等等，会进行自动判断，如果该路径下没有 Minst 数据集的话，就会自动进行下载。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token punctuation">(</span>PATH <span class="token operator">/</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        content <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>URL <span class="token operator">+</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span>content
        <span class="token punctuation">(</span>PATH <span class="token operator">/</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"wb"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>content<span class="token punctuation">)</span>
</code></pre> 
<ul><li>由于下载出来的数据集是压缩包的状态，因此，我们还需要对其进行解压，具体的代码详见下面。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> pickle
<span class="token keyword">import</span> gzip
​
<span class="token keyword">with</span> gzip<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token punctuation">(</span>PATH <span class="token operator">/</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span>as_posix<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        <span class="token punctuation">(</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span><span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"latin-1"</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>在上述工作准备完成后，我们可以先查看一个数据，观察他的特征。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
​
pyplot<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"gray"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token comment">#(50000, 784)</span>
</code></pre> 
<ul><li>在此处，我们查看的训练集当中的第一个数据，大小重构为 （28，28，1），表示长是 28，宽是 28，颜色通道是 1（黑白图就只有一个颜色通道），颜色设置为灰色。在查看第一个数据的同时，我们也输出整个训练集的数据大小，其中，(50000, 784) 中的 50000 表示训练集一共有 50000 个数据样本，784 表示训练集中每个样本有 784 个像素点（可以理解成 784 个特征）。</li></ul> 
<p><img src="https://images2.imgbox.com/4e/0b/uEMexeRg_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="_Mnist__63"></a>三、 Mnist 分类任务实现</h1> 
<h2>
<a id="1__64"></a>1. 标签和简单网络架构</h2> 
<ul><li>在分类任务当中，标签的设计是有所不同的。</li></ul> 
<p><img src="https://images2.imgbox.com/a8/4d/RxJeoK4o_o.png" alt="在这里插入图片描述"></p> 
<ul>
<li>很多人认为预测出来的 9，具体指的是 0，1，2，3，4，5，6，7，8，9 当中的具体哪一个，但实际上并不是这样的，他也是一个 One-Hot 的编码，他预测的出来的不是一个具体的数值，而是十个概率，就是当前这个输入属于 0-9 这十个数字的概率是多少。</li>
<li>以上图为例，该输入属于 0 的概率就是 0，属于 1 的概率就是 12%，属于 9 的概率就是 87%，属于 9 的概率最高，因此，该输入的输出就是 9。</li>
</ul> 
<p><img src="https://images2.imgbox.com/c4/ee/UPRn6iip_o.png" alt="在这里插入图片描述"></p> 
<ul><li>对于这个网络架构，由于我们的每个数据样本都有 784 个像素点，中间进行特征提取，得到一定数量的特征，最终得到 10 个输出，通过 Softmax 层得到是个概率。</li></ul> 
<h2>
<a id="2__73"></a>2. 具体代码实现</h2> 
<ul>
<li>需要注意的是，我们需要先将数据转换成 tensor 才能参与后续建模训练。</li>
<li>这里的数据包括 x_train, y_train, x_valid, y_valid 四种，对于他们的含义，我们可以这样理解：</li>
<li>（1） x_train 包括所有自变量，这些变量将用于训练模型。</li>
<li>（2） y_train 是指因变量，需要此模型进行预测，其中包括针对自变量的类别标签，我们需要在训练/拟合模型时指定我们的因变量。</li>
<li>（3） x_valid 也就是 x_test，这些自变量将不会在训练阶段使用，并将用于进行预测，以测试模型的准确性。</li>
<li>（4） y_valid 也就是 y_test，此数据具有测试数据的类别标签，这些标签将用于测试实际类别和预测类别之间的准确性。</li>
</ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
​
x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_valid<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span>
    torch<span class="token punctuation">.</span>tensor<span class="token punctuation">,</span> <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
n<span class="token punctuation">,</span> c <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape
x_train<span class="token punctuation">,</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> y_train<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_train<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y_train<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_train<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#tensor([[0., 0., 0.,  ..., 0., 0., 0.],</span>
<span class="token comment">#        [0., 0., 0.,  ..., 0., 0., 0.],</span>
<span class="token comment">#        [0., 0., 0.,  ..., 0., 0., 0.],</span>
<span class="token comment">#        ...,</span>
<span class="token comment">#        [0., 0., 0.,  ..., 0., 0., 0.],</span>
<span class="token comment">#        [0., 0., 0.,  ..., 0., 0., 0.],</span>
<span class="token comment">#        [0., 0., 0.,  ..., 0., 0., 0.]]) tensor([5, 0, 4,  ..., 8, 4, 8])</span>
<span class="token comment">#torch.Size([50000, 784])</span>
<span class="token comment">#tensor(0) tensor(9)</span>
</code></pre> 
<ul>
<li>在模型训练的过程中，大家经常会看到 nn.Module 和 nn.functional。那什么时候使用 nn.Module，什么时候使用 nn.functional 呢？</li>
<li>一般情况下，如果模型有可学习的参数，最好用 nn.Module，其他情况 nn.functional 相对更简单一些。</li>
<li>我们先导入需要的模块包。</li>
</ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
​
loss_func <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy
​
<span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> xb<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weights<span class="token punctuation">)</span> <span class="token operator">+</span> bias
</code></pre> 
<ul><li>然后进行参数的设定。</li></ul> 
<pre><code class="prism language-python">bs <span class="token operator">=</span> <span class="token number">64</span>
xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>bs<span class="token punctuation">]</span>  <span class="token comment"># a mini-batch from x</span>
yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>bs<span class="token punctuation">]</span>
weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">,</span>  requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> 
bs <span class="token operator">=</span> <span class="token number">64</span>
bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
​
<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#tensor(10.7988, grad_fn=&lt;NllLossBackward&gt;)</span>
</code></pre> 
<ul>
<li>我们也创建一个 model 来更简化代码。</li>
<li>在这中间必须继承 nn.Module 且在其构造函数中需调用 nn.Module 的构造函数，无需写反向传播函数，nn.Module 能够利用 autograd 自动实现反向传播，Module 中的可学习参数可以通过 named_parameters() 或者 parameters() 返回迭代器。</li>
</ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
​
<span class="token keyword">class</span> <span class="token class-name">Mnist_NN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">)</span> <span class="token comment">#隐藏层1：784*128</span>
        self<span class="token punctuation">.</span>hidden2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span> <span class="token comment">#隐藏层2：128*256</span>
        self<span class="token punctuation">.</span>out  <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token comment">#输出层，256*10</span>
​
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hidden2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
        
net <span class="token operator">=</span> Mnist_NN<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
​<span class="token comment">#Mnist_NN(</span>
<span class="token comment">#  (hidden1): Linear(in_features=784, out_features=128, bias=True)</span>
<span class="token comment">#  (hidden2): Linear(in_features=128, out_features=256, bias=True)</span>
<span class="token comment">#  (out): Linear(in_features=256, out_features=10, bias=True)</span>
<span class="token comment">#)</span>
</code></pre> 
<ul><li>我们可以打印定义好名字里的权重和偏置项，首先打印名字，然后打印参数，最后打印参数的维度。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">for</span> name<span class="token punctuation">,</span> parameter <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span> parameter<span class="token punctuation">,</span>parameter<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment">#hidden1.weight Parameter containing:</span>
<span class="token comment">#tensor([[ 0.0018,  0.0218,  0.0036,  ..., -0.0286, -0.0166,  0.0089],</span>
<span class="token comment">#        [-0.0349,  0.0268,  0.0328,  ...,  0.0263,  0.0200, -0.0137],</span>
<span class="token comment">#        [ 0.0061,  0.0060, -0.0351,  ...,  0.0130, -0.0085,  0.0073],</span>
<span class="token comment">#        ...,</span>
<span class="token comment">#        [-0.0231,  0.0195, -0.0205,  ..., -0.0207, -0.0103, -0.0223],</span>
<span class="token comment">#        [-0.0299,  0.0305,  0.0098,  ...,  0.0184, -0.0247, -0.0207],</span>
<span class="token comment">#        [-0.0306, -0.0252, -0.0341,  ...,  0.0136, -0.0285,  0.0057]],</span>
<span class="token comment">#       requires_grad=True) torch.Size([128, 784])</span>
<span class="token comment">#hidden1.bias Parameter containing:</span>
<span class="token comment">#tensor([ 0.0072, -0.0269, -0.0320, -0.0162,  0.0102,  0.0189, -0.0118, -0.0063,</span>
<span class="token comment">#        -0.0277,  0.0349,  0.0267, -0.0035,  0.0127, -0.0152, -0.0070,  0.0228,</span>
<span class="token comment">#        -0.0029,  0.0049,  0.0072,  0.0002, -0.0356,  0.0097, -0.0003, -0.0223,</span>
<span class="token comment">#        -0.0028, -0.0120, -0.0060, -0.0063,  0.0237,  0.0142,  0.0044, -0.0005,</span>
<span class="token comment">#         0.0349, -0.0132,  0.0138, -0.0295, -0.0299,  0.0074,  0.0231,  0.0292,</span>
<span class="token comment">#        -0.0178,  0.0046,  0.0043, -0.0195,  0.0175, -0.0069,  0.0228,  0.0169,</span>
<span class="token comment">#         0.0339,  0.0245, -0.0326, -0.0260, -0.0029,  0.0028,  0.0322, -0.0209,</span>
<span class="token comment">#        -0.0287,  0.0195,  0.0188,  0.0261,  0.0148, -0.0195, -0.0094, -0.0294,</span>
<span class="token comment">#        -0.0209, -0.0142,  0.0131,  0.0273,  0.0017,  0.0219,  0.0187,  0.0161,</span>
<span class="token comment">#         0.0203,  0.0332,  0.0225,  0.0154,  0.0169, -0.0346, -0.0114,  0.0277,</span>
<span class="token comment">#         0.0292, -0.0164,  0.0001, -0.0299, -0.0076, -0.0128, -0.0076, -0.0080,</span>
<span class="token comment">#        -0.0209, -0.0194, -0.0143,  0.0292, -0.0316, -0.0188, -0.0052,  0.0013,</span>
<span class="token comment">#        -0.0247,  0.0352, -0.0253, -0.0306,  0.0035, -0.0253,  0.0167, -0.0260,</span>
<span class="token comment">#        -0.0179, -0.0342,  0.0033, -0.0287, -0.0272,  0.0238,  0.0323,  0.0108,</span>
<span class="token comment">#         0.0097,  0.0219,  0.0111,  0.0208, -0.0279,  0.0324, -0.0325, -0.0166,</span>
<span class="token comment">#        -0.0010, -0.0007,  0.0298,  0.0329,  0.0012, -0.0073, -0.0010,  0.0057],</span>
<span class="token comment">#       requires_grad=True) torch.Size([128])</span>
<span class="token comment">#hidden2.weight Parameter containing:</span>
<span class="token comment">#tensor([[-0.0383, -0.0649,  0.0665,  ..., -0.0312,  0.0394, -0.0801],</span>
<span class="token comment">#        [-0.0189, -0.0342,  0.0431,  ..., -0.0321,  0.0072,  0.0367],</span>
<span class="token comment">#        [ 0.0289,  0.0780,  0.0496,  ...,  0.0018, -0.0604, -0.0156],</span>
<span class="token comment">#        ...,</span>
<span class="token comment">#        [-0.0360,  0.0394, -0.0615,  ...,  0.0233, -0.0536, -0.0266],</span>
<span class="token comment">#        [ 0.0416,  0.0082, -0.0345,  ...,  0.0808, -0.0308, -0.0403],</span>
<span class="token comment">#        [-0.0477,  0.0136, -0.0408,  ...,  0.0180, -0.0316, -0.0782]],</span>
<span class="token comment">#       requires_grad=True) torch.Size([256, 128])</span>
<span class="token comment">#hidden2.bias Parameter containing:</span>
<span class="token comment">#tensor([-0.0694, -0.0363, -0.0178,  0.0206, -0.0875, -0.0876, -0.0369, -0.0386,</span>
<span class="token comment">#         0.0642, -0.0738, -0.0017, -0.0243, -0.0054,  0.0757, -0.0254,  0.0050,</span>
<span class="token comment">#         0.0519, -0.0695,  0.0318, -0.0042, -0.0189, -0.0263, -0.0627, -0.0691,</span>
<span class="token comment">#         0.0713, -0.0696, -0.0672,  0.0297,  0.0102,  0.0040,  0.0830,  0.0214,</span>
<span class="token comment">#         0.0714,  0.0327, -0.0582, -0.0354,  0.0621,  0.0475,  0.0490,  0.0331,</span>
<span class="token comment">#        -0.0111, -0.0469, -0.0695, -0.0062, -0.0432, -0.0132, -0.0856, -0.0219,</span>
<span class="token comment">#        -0.0185, -0.0517,  0.0017, -0.0788, -0.0403,  0.0039,  0.0544, -0.0496,</span>
<span class="token comment">#         0.0588, -0.0068,  0.0496,  0.0588, -0.0100,  0.0731,  0.0071, -0.0155,</span>
<span class="token comment">#        -0.0872, -0.0504,  0.0499,  0.0628, -0.0057,  0.0530, -0.0518, -0.0049,</span>
<span class="token comment">#         0.0767,  0.0743,  0.0748, -0.0438,  0.0235, -0.0809,  0.0140, -0.0374,</span>
<span class="token comment">#         0.0615, -0.0177,  0.0061, -0.0013, -0.0138, -0.0750, -0.0550,  0.0732,</span>
<span class="token comment">#         0.0050,  0.0778,  0.0415,  0.0487,  0.0522,  0.0867, -0.0255, -0.0264,</span>
<span class="token comment">#         0.0829,  0.0599,  0.0194,  0.0831, -0.0562,  0.0487, -0.0411,  0.0237,</span>
<span class="token comment">#         0.0347, -0.0194, -0.0560, -0.0562, -0.0076,  0.0459, -0.0477,  0.0345,</span>
<span class="token comment">#        -0.0575, -0.0005,  0.0174,  0.0855, -0.0257, -0.0279, -0.0348, -0.0114,</span>
<span class="token comment">#        -0.0823, -0.0075, -0.0524,  0.0331,  0.0387, -0.0575,  0.0068, -0.0590,</span>
<span class="token comment">#        -0.0101, -0.0880, -0.0375,  0.0033, -0.0172, -0.0641, -0.0797,  0.0407,</span>
<span class="token comment">#         0.0741, -0.0041, -0.0608,  0.0672, -0.0464, -0.0716, -0.0191, -0.0645,</span>
<span class="token comment">#         0.0397,  0.0013,  0.0063,  0.0370,  0.0475, -0.0535,  0.0721, -0.0431,</span>
<span class="token comment">#         0.0053, -0.0568, -0.0228, -0.0260, -0.0784, -0.0148,  0.0229, -0.0095,</span>
<span class="token comment">#        -0.0040,  0.0025,  0.0781,  0.0140, -0.0561,  0.0384, -0.0011, -0.0366,</span>
<span class="token comment">#         0.0345,  0.0015,  0.0294, -0.0734, -0.0852, -0.0015, -0.0747, -0.0100,</span>
<span class="token comment">#         0.0801, -0.0739,  0.0611,  0.0536,  0.0298, -0.0097,  0.0017, -0.0398,</span>
<span class="token comment">#         0.0076, -0.0759, -0.0293,  0.0344, -0.0463, -0.0270,  0.0447,  0.0814,</span>
<span class="token comment">#        -0.0193, -0.0559,  0.0160,  0.0216, -0.0346,  0.0316,  0.0881, -0.0652,</span>
<span class="token comment">#        -0.0169,  0.0117, -0.0107, -0.0754, -0.0231, -0.0291,  0.0210,  0.0427,</span>
<span class="token comment">#         0.0418,  0.0040,  0.0762,  0.0645, -0.0368, -0.0229, -0.0569, -0.0881,</span>
<span class="token comment">#        -0.0660,  0.0297,  0.0433, -0.0777,  0.0212, -0.0601,  0.0795, -0.0511,</span>
<span class="token comment">#        -0.0634,  0.0720,  0.0016,  0.0693, -0.0547, -0.0652, -0.0480,  0.0759,</span>
<span class="token comment">#         0.0194, -0.0328, -0.0211, -0.0025, -0.0055, -0.0157,  0.0817,  0.0030,</span>
<span class="token comment">#         0.0310, -0.0735,  0.0160, -0.0368,  0.0528, -0.0675, -0.0083, -0.0427,</span>
<span class="token comment">#        -0.0872,  0.0699,  0.0795, -0.0738, -0.0639,  0.0350,  0.0114,  0.0303],</span>
<span class="token comment">#       requires_grad=True) torch.Size([256])</span>
<span class="token comment">#out.weight Parameter containing:</span>
<span class="token comment">#tensor([[ 0.0232, -0.0571,  0.0439,  ..., -0.0417, -0.0237,  0.0183],</span>
<span class="token comment">#        [ 0.0210,  0.0607,  0.0277,  ..., -0.0015,  0.0571,  0.0502],</span>
<span class="token comment">#        [ 0.0297, -0.0393,  0.0616,  ...,  0.0131, -0.0163, -0.0239],</span>
<span class="token comment">#        ...,</span>
<span class="token comment">#        [ 0.0416,  0.0309, -0.0441,  ..., -0.0493,  0.0284, -0.0230],</span>
<span class="token comment">#        [ 0.0404, -0.0564,  0.0442,  ..., -0.0271, -0.0526, -0.0554],</span>
<span class="token comment">#        [-0.0404, -0.0049, -0.0256,  ..., -0.0262, -0.0130,  0.0057]],</span>
<span class="token comment">#       requires_grad=True) torch.Size([10, 256])</span>
<span class="token comment">#out.bias Parameter containing:</span>
<span class="token comment">#tensor([-0.0536,  0.0007,  0.0227, -0.0072, -0.0168, -0.0125, -0.0207, -0.0558,</span>
<span class="token comment">#         0.0579, -0.0439], requires_grad=True) torch.Size([10])</span>
</code></pre> 
<h1>
<a id="_TensorDataset__DataLoader__237"></a>四、使用 TensorDataset 和 DataLoader 简化</h1> 
<ul><li>自己构建数据集，使用 batch 取数据会略显麻烦，因此，我们可以使用 TensorDataset 和 DataLoader 这两个模块进行简化。</li></ul> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
​
train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
​
valid_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span>
valid_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>valid_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>
        DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        DataLoader<span class="token punctuation">(</span>valid_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>
</code></pre> 
<ul>
<li>一般在训练模型时加上 model.train()，这样会正常使用 Batch Normalization 和 Dropout。</li>
<li>测试的时候一般选择 model.eval()，这样就不会使用 Batch Normalization 和 Dropout。</li>
</ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
​
<span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>steps<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>steps<span class="token punctuation">)</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
            loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token punctuation">)</span>
​
        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            losses<span class="token punctuation">,</span> nums <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>
                <span class="token operator">*</span><span class="token punctuation">[</span>loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">]</span>
            <span class="token punctuation">)</span>
        val_loss <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>losses<span class="token punctuation">,</span> nums<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'当前step:'</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>step<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'验证集损失：'</span><span class="token operator">+</span><span class="token builtin">str</span><span class="token punctuation">(</span>val_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">from</span> torch <span class="token keyword">import</span> optim
<span class="token keyword">def</span> <span class="token function">get_model</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> Mnist_NN<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model<span class="token punctuation">,</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">loss_batch</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span>
​
    <span class="token keyword">if</span> opt <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
​
    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
</code></pre> 
<ul><li>我们也可以像上篇博文一样，使用三行代码进行解决。</li></ul> 
<pre><code class="prism language-python">train_dl<span class="token punctuation">,</span> valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span>
model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
fit<span class="token punctuation">(</span><span class="token number">25</span><span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>
<span class="token comment">#当前step:0 验证集损失：2.2796445930480957</span>
<span class="token comment">#当前step:1 验证集损失：2.2440698066711424</span>
<span class="token comment">#当前step:2 验证集损失：2.1889826164245605</span>
<span class="token comment">#当前step:3 验证集损失：2.0985311767578123</span>
<span class="token comment">#当前step:4 验证集损失：1.9517273582458496</span>
<span class="token comment">#当前step:5 验证集损失：1.7341805934906005</span>
<span class="token comment">#当前step:6 验证集损失：1.4719875366210937</span>
<span class="token comment">#当前step:7 验证集损失：1.2273896869659424</span>
<span class="token comment">#当前step:8 验证集损失：1.0362271406173706</span>
<span class="token comment">#当前step:9 验证集损失：0.8963696184158325</span>
<span class="token comment">#当前step:10 验证集损失：0.7927186088562012</span>
<span class="token comment">#当前step:11 验证集损失：0.7141492074012756</span>
<span class="token comment">#当前step:12 验证集损失：0.6529350900650024</span>
<span class="token comment">#当前step:13 验证集损失：0.60417300491333</span>
<span class="token comment">#当前step:14 验证集损失：0.5643046331882476</span>
<span class="token comment">#当前step:15 验证集损失：0.5317994566917419</span>
<span class="token comment">##当前step:16 验证集损失：0.5047958114624024</span>
<span class="token comment">#当前step:17 验证集损失：0.4813900615692139</span>
<span class="token comment">#当前step:18 验证集损失：0.4618900228500366</span>
<span class="token comment">#当前step:19 验证集损失：0.4443243554592133</span>
<span class="token comment">#当前step:20 验证集损失：0.4297310716629028</span>
<span class="token comment">#当前step:21 验证集损失：0.416976597738266</span>
<span class="token comment">#当前step:22 验证集损失：0.406348459148407</span>
<span class="token comment">#当前step:23 验证集损失：0.3963301926612854</span>
<span class="token comment">#当前step:24 验证集损失：0.38733808159828187​</span>
</code></pre>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>