<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>百亿级日访问量的应用如何做缓存架构设计？ - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">百亿级日访问量的应用如何做缓存架构设计？</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <h1>引言</h1> 
<p><strong>“ 微博日活跃用户 1.6 亿+，每日访问量达百亿级，面对庞大用户群的海量访问，良好的架构且不断改进的缓存体系具有非常重要的支撑作用。</strong></p> 
<p>本文由新浪微博技术专家陈波老师，分为如下四个部分跟大家详细讲解那些庞大的数据都是如何呈现的：</p> 
<ul>
<li>微博在运行过程中的数据挑战</li>
<li>Feed 平台系统架构</li>
<li>Cache 架构及演进</li>
<li>总结与展望</li>
</ul>
<h1> 微博在运行过程中的数据挑战</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/62/77/FCc72nWP_o.png"></p> 
<p></p> 
<h1> Feed 平台系统架构</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/be/b1/PRKJfEXo_o.png"></p> 
<p></p> 
<p>Feed 平台系统架构总共分为五层：</p> 
<ul>
<li>
<strong>最上面是端层，</strong>比如 Web 端、客户端、大家用的 iOS 或安卓的一些客户端，还有一些开放平台、第三方接入的一些接口。</li>
<li>
<strong>下一层是平台接入层，</strong>不同的池子，主要是为了把好的资源集中调配给重要的核心接口，这样遇到突发流量的时候，就有更好的弹性来服务，提高服务稳定性。</li>
<li>
<strong>再下面是平台服务层，</strong>主要是 Feed 算法、关系等等。</li>
<li>
<strong>接下来是中间层，</strong>通过各种中间介质提供一些服务。</li>
<li><strong>最下面一层就是存储层。</strong></li>
</ul>
<h1> Feed Timeline</h1> 
<p>大家日常刷微博的时候，比如在主站或客户端点一下刷新，最新获得了十到十五条微博，这是怎么构建出来的呢？</p> 
<p>刷新之后，首先会获得用户的关注关系。比如他有一千个关注，会把这一千个 ID 拿到，再根据这一千个 UID，拿到每个用户发表的一些微博。</p> 
<p>同时会获取这个用户的 Inbox，就是他收到的特殊的一些消息，比如分组的一些微博、群的微博、下面的关注关系、关注人的微博列表。</p> 
<p>拿到这一系列微博列表之后进行集合、排序，拿到所需要的那些 ID，再对这些 ID 去取每一条微博 ID 对应的微博内容。</p> 
<p>如果这些微博是转发过来的，它还有一个原微博，会进一步取原微博内容。通过原微博取用户信息，进一步根据用户的过滤词对这些微博进行过滤，过滤掉用户不想看到的微博。</p> 
<p>根据以上步骤留下的微博，会再进一步来看，用户对这些微博有没有收藏、点赞，做一些 Flag 设置，还会对这些微博各种计数，转发、评论、赞数进行组装，最后才把这十几条微博返回给用户的各种端。</p> 
<p>这样看来，用户一次请求得到的十几条记录，后端服务器大概要对几百甚至几千条数据进行实时组装，再返回给用户。</p> 
<p>整个过程对 Cache 体系强度依赖，所以 Cache 架构设计优劣会直接影响到微博体系表现的好坏。</p> 
<h1> Feed Cache 架构</h1> 
<p>接下来我们看一下 Cache 架构，它主要分为六层：</p> 
<ul>
<li>
<strong>第一层是 Inbox，</strong>主要是分组的一些微博，然后直接对群主的一些微博。Inbox 比较少，主要是推的方式。</li>
<li>
<strong>第二层是 Outbox，</strong>每个用户都会发常规的微博，都会到它的 Outbox 里面去。根据存的 ID 数量，实际上分成多个 Cache，普通的大概是 200 多条，如果长的大概是 2000 条。</li>
<li>
<strong>第三层是一些关系，</strong>它的关注、粉丝、用户。</li>
<li>
<strong>第四层是内容，</strong>每一条微博一些内容存在这里。</li>
<li>
<strong>第五层就是一些存在性判断，</strong>比如某条微博我有没有赞过。之前有一些明星就说我没有点赞这条微博怎么显示我点赞了，引发了一些新闻。而这种就是记录，实际上她有在某个时候点赞过但可能忘记了。</li>
<li>
<strong>最下面还有比较大的一层——计数，</strong>每条微博的评论、转发等计数，还有用户的关注数、粉丝数这些数据。</li>
</ul>
<h1> Cache 架构及演进</h1> 
<h1> 简单 KV 数据类型</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/cd/8a/Tz0BRMPV_o.png"></p> 
<p></p> 
<p>接下来我们着重讲一下微博的 Cache 架构演进过程。最开始微博上线时，我们是把它作为一个简单的 KV 数据类型来存储。</p> 
<p>我们主要采取哈希分片存储在 MC 池子里，上线几个月之后发现一些问题：有一些节点机器宕机或是其他原因，大量的请求会穿透 Cache 层达到 DB 上去，导致整个请求变慢，甚至 DB 僵死。</p> 
<p>于是我们很快进行了改造，增加了一个 HA 层，这样即便 Main 层出现某些节点宕机情况或者挂掉之后，这些请求会进一步穿透到 HA 层，不会穿透到 DB 层。</p> 
<p>这样可以保证在任何情况下，整个系统命中率不会降低，系统服务稳定性有了比较大的提升。</p> 
<p>对于这种做法，现在业界用得比较多，然后很多人说我直接用哈希，但这里面也有一些坑。</p> 
<p>比如我有一个节点，节点 3 宕机了，Main 把它给摘掉，节点 3 的一些 QA 分给其他几个节点，这个业务量还不是很大，穿透 DB，DB 还可以抗住。</p> 
<p>但如果这个节点 3 恢复了，它又加进来之后，节点 3 的访问就会回来，稍后节点 3 因为网络原因或者机器本身的原因，它又宕机了，一些节点 3 的请求又会分给其他节点。</p> 
<p>这个时候就会出现问题，之前分散给其他节点写回来的数据已经没有人更新了，如果它没有被剔除掉就会出现混插数据。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/38/87/6AavQ2D8_o.png"></p> 
<p></p> 
<p>实际上微博是一个广场型的业务，比如突发事件，某明星找个女朋友，瞬间流量就 30% 了。</p> 
<p>突发事件后，大量的请求会出现在某一些节点，会导致这些节点非常热，即便是 MC 也没办法满足这么大的请求量。这时 MC 就会变成瓶颈，导致整个系统变慢。</p> 
<p>基于这个原因，我们引入了 L1 层，还是一个 Main 关系池，每一个 L1 大概是 Main 层的 N 分之一，六分之一、八分之一、十分之一这样一个内存量，根据请求量我会增加 4 到 8 个 L1，这样所有请求来了之后首先会访问 L1。</p> 
<p>L1 命中的话就会直接访问，如果没有命中再来访问 Main-HA 层，这样在一些突发流量的时候，可以由 L1 来抗住大部分热的请求。</p> 
<p>对微博本身来说，新的数据就会越热，只要增加很少一部分内存就会抗住更大的量。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/17/8b/lJexTycj_o.png"></p> 
<p></p> 
<p><strong>简单总结一下：</strong>通过简单 KV 数据类型的存储，我们实际上是以 MC 为主的，层内 Hash 节点不漂移，Miss 穿透到下一层去读取。</p> 
<p>通过多组 L1 读取性能提升，能够抗住峰值、突发流量，而且成本会大大降低。</p> 
<p>对读写策略，采取多写，读的话采用逐层穿透，如果 Miss 的话就进行回写。对存在里面的数据，我们最初采用 Json/xml，2012 年之后就直接采用 Protocol Buffer 格式，对一些比较大的用 QuickL 进行压缩。</p> 
<h1> 集合类数据</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/28/c9/keFu9oQQ_o.png"></p> 
<p></p> 
<p>刚才讲到简单的 QA 数据，那对于复杂的集合类数据怎么来处理？</p> 
<p>比如我关注了 2000 人，新增 1 个人，就涉及到部分修改。有一种方式是把 2000 个 ID 全部拿下来进行修改，但这种对带宽、机器压力会很大。</p> 
<p>还有一些分页获取，我存了 2000 个，只需要取其中的第几页，比如第二页，也就是第十到第二十个，能不能不要全量把所有数据取回去。</p> 
<p>还有一些资源的联动计算，会计算到我关注的某些人里面 ABC 也关注了用户 D。这种涉及到部分数据的修改、获取，包括计算，对 MC 来说实际上是不太擅长的。</p> 
<p>各种关注关系都存在 Redis 里面取，通过 Hash 分布、储存，一主多从的方式来进行读写分离。现在 Redis 的内存大概有 30 个 T，每天都有 2-3 万亿的请求。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/c1/cc/r7YLXai4_o.png"></p> 
<p></p> 
<p>在使用 Redis 的过程中，实际上还是遇到其他一些问题。比如从关注关系，我关注了 2000 个 UID，有一种方式是全量存储。</p> 
<p>但微博有大量的用户，有些用户登录得比较少，有些用户特别活跃，这样全部放在内存里成本开销是比较大的。</p> 
<p>所以我们就把 Redis 使用改成 Cache，比如只存活跃的用户，如果你最近一段时间没有活跃，会把你从 Redis 里踢掉，再次有访问的时候再把你加进来。</p> 
<p>这时存在一个问题，因为 Redis 工作机制是单线程模式，如果它加某一个 UV，关注 2000 个用户，可能扩展到两万个 UID，两万个 UID 塞回去基本上 Redis 就卡住了，没办法提供其他服务。</p> 
<p>所以我们扩展一种新的数据结构，两万个 UID 直接开了端，写的时候直接依次把它写到 Redis 里面去，读写的整个效率就会非常高。</p> 
<p>它的实现是一个 long 型的开放数组，通过 Double Hash 进行寻址。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/b4/0d/WqORahBA_o.png"></p> 
<p></p> 
<p>我们对 Redis 进行了一些其他的扩展，大家可能也在网上看到过我们之前的一些分享，把数据放到公共变量里面。</p> 
<p>整个升级过程，我们测试 1G 的话加载要 10 分钟，10G 大概要 10 分钟以上，现在是毫秒级升级。</p> 
<p>对于 AOF，我们采用滚动的 AOF，每个 AOF 是带一个 ID 的，达到一定的量再滚动到下一个 AOF 里去。</p> 
<p>对 RDB 落地的时候，我们会记录构建这个 RDB 时，AOF 文件以及它所在的位置，通过新的 RDB、AOF 扩展模式，实现全增量复制。</p> 
<h1> 其他数据类型：计数</h1> 
<p>接下来还有一些其他的数据类型，比如一个计数，实际上计数在每个互联网公司都可能会遇到，对一些中小型的业务来说，实际上 MC 和 Redis 足够用的。</p> 
<p><strong>但在微博里计数出现了一些特点：</strong>单条 Key 有多条计数，比如一条微博，有转发数、评论数，还有点赞；一个用户有粉丝数、关注数等各种各样的数字。</p> 
<p>因为是计数，它的 Value size 是比较小的，根据它的各种业务场景，大概就是 2-8 个字节，一般 4 个字节为多。</p> 
<p>然后每日新增的微博大概十亿条记录，总记录就更可观了，然后一次请求，可能几百条计数要返回去。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/0d/d8/50Ju8b8C_o.png"></p> 
<p></p> 
<h1> 计数器 Counter Service</h1> 
<p>最初是可以采取 Memcached，但它有个问题，如果计数超过它内容容量时，会导致一些计数的剔除，宕机或重启后计数就没有了。</p> 
<p>另外可能有很多计数它为零，那这个时候怎么存，要不要存，存的话就占很多内存。</p> 
<p>微博每天上十亿的计数，光存 0 都要占大量的内存，如果不存又会导致穿透到 DB 里去，对服务的可溶性会存在影响。</p> 
<p>2010 年之后我们又采用 Redis 访问，随着数据量越来越大之后，发现 Redis 内存有效负荷还是比较低的，它一条 KV 大概需要至少 65 个字节。</p> 
<p>但实际上我们一个计数需要 8 个字节，然后 Value 大概 4 个字节，所以有效只有 12 个字节，还有四十多个字节都是被浪费掉的。</p> 
<p>这还只是单个 KV，如果在一条 Key 有多个计数的情况下，它就浪费得更多了。</p> 
<p>比如说四个计数，一个 Key 8 个字节，四个计数每个计数是 4 个字节，16 个字节大概需要 26 个字节就行了，但是用 Redis 存大概需要 200 多个字节。</p> 
<p>后来我们通过自己研发的 Counter Service，内存降至 Redis 的五分之一到十五分之一以下，而且进行冷热分离，热数据存在内存里，冷数据如果重新变热，就把它放到 LRU 里去。</p> 
<p>落地 RDB、AOF，实现全增量复制，通过这种方式，热数据单机可以存百亿级，冷数据可以存千亿级。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/2a/2d/TYuPiR8i_o.png"></p> 
<p></p> 
<p>整个存储架构大概是上图这样，上面是内存，下面是 SSD，在内存里是预先把它分成 N 个 Table，每个 Table 根据 ID 的指针序列，划出一定范围。</p> 
<p>任何一个 ID 过来先找到它所在的 Table，如果有直接对它增增减减，有新的计数过来，发现内存不够的时候，就会把一个小的 Table Dump（（内存信息）转储，转存 ） 到 SSD 里去，留着新的位置放在最上面供新的 ID 来使用。</p> 
<p>有些人疑问说，如果在某个范围内，我的 ID 本来设的计数是 4 个字节，但是微博特别热，超过了 4 个字节，变成很大的一个计数怎么处理？</p> 
<p>对于超过限制的，我们把它放在 Aux dict 进行存放，对于落在 SSD 里的 Table，我们有专门的 IndAux 进行访问，通过 RDB 方式进行复制。</p> 
<h1> 其他数据类型：存在性判断</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/63/a2/TsHswnVo_o.png"></p> 
<p></p> 
<p>除了计数，微博还有一些业务，一些存在性判断。比如一条微博展现的，有没有点赞、阅读、推荐，如果这个用户已经读过这个微博了，就不要再显示给他。</p> 
<p>这种有一个很大的特点，它检查是否存在，每条记录非常小，比如 Value 1 个 bit 就可以了，但总数据量巨大。</p> 
<p>比如微博每天新发表微博 1 亿左右，读的可能有上百亿、上千亿这种总的数据需要判断。</p> 
<p>怎么来存储是个很大的问题，而且这里面很多存在性就是 0。还是前面说的，0 要不要存？</p> 
<p>如果存了，每天就存上千亿的记录；如果不存，那大量的请求最终会穿透 Cache 层到 DB 层，任何 DB 都没办法抗住那么大的流量。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/96/85/o5N1CPeP_o.png"></p> 
<p></p> 
<p><strong>我们也进行了一些选型：</strong>首先直接考虑能不能用 Redis。单条 KV 65 个字节，一个 KV 可以 8 个字节的话，Value 只有 1 个 bit，这样算下来每日新增内存有效率是非常低的。</p> 
<p>第二种我们新开发的 Counter Service，单条 KV Value 1 个 bit，我就存 1 个 byt，总共 9 个 byt 就可以了。</p> 
<p>这样每日新增内存 900G，存的话可能就只能存最新若干天的，存个三天差不多快 3 个 T 了，压力也挺大，但比 Redis 已经好很多。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/15/a9/x1AzXxz5_o.png"></p> 
<p></p> 
<p>我们最终方案是自己开发 Phantom，先采用把共享内存分段分配，最终使用的内存只用 120G 就可以。</p> 
<p>算法很简单，对每个 Key 可以进行 N 次哈希，如果哈希的某一个位它是 1，那么进行 3 次哈希，三个数字把它设为 1。</p> 
<p>把 X2 也进行三次哈希，后面来判断 X1 是否存在的时候，从进行三次哈希来看，如果都为 1 就认为它是存在的；如果某一个哈希 X3，它的位算出来是 0，那就百分百肯定是不存在的。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/35/ce/uHwfuePD_o.png"></p> 
<p></p> 
<p>它的实现架构比较简单，把共享内存预先拆分到不同 Table 里，在里面进行开方式计算，然后读写，落地的话采用 AOF+RDB 的方式进行处理。</p> 
<p>整个过程因为放在共享内存里面，进程要升级重启数据也不会丢失。对外访问的时候，建 Redis 协议，它直接扩展新的协议就可以访问我们这个服务了。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/9d/e8/VCggvr0b_o.png"></p> 
<p></p> 
<p><strong>小结一下：</strong>到目前为止，我们关注了 Cache 集群内的高可用、扩展性、组件高性能，还有一个特别重要就是存储成本，还有一些我们没有关注到的，比如运维性如何，微博现在已经有几千差不多上万台服务器等。</p> 
<h1> 进一步优化</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/fd/53/XEJ9tqAl_o.png"></p> 
<p></p> 
<h1> 服务化</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/c5/b9/dD2kXpr9_o.png"></p> 
<p></p> 
<p>采取的方案首先就是对整个 Cache 进行服务化管理，对配置进行服务化管理，避免频繁重启，另外如果配置发生变更，直接用一个脚本修改一下。</p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/ee/64/Fq0mUC6m_o.png"></p> 
<p></p> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/7d/ce/32wqojIQ_o.png"></p> 
<p></p> 
<p>服务化还引入 Cluster Manager，实现对外部的管理，通过一个界面来进行管理，可以进行服务校验。</p> 
<p>服务治理方面，可以做到扩容、缩容，SLA 也可以得到很好的保障。另外，对于开发来说，现在就可以屏蔽 Cache 资源。</p> 
<h1> 总结与展望</h1> 
<p style="text-align:center"><img alt="百亿级日访问量的应用如何做缓存架构设计？" src="https://images2.imgbox.com/38/ba/l7Qv1Pie_o.png"></p> 
<p></p> 
<p>最后简单总结一下，对于微博 Cache 架构来说，我们从它的数据架构、性能、储存成本、服务化等不同方面进行了优化增强。</p> 
<p>如果有需要领取免费资料的小伙伴们，<a href="https://link.zhihu.com/?target=https%3A//docs.qq.com/doc/DWGVNWWxGbnF6QXhQ" title="可以点击此处领取资料哦！">可以点击此处领取资料哦！</a></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>