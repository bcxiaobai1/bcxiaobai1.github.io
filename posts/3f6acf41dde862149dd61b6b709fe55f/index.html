<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【深度学习】深度学习中经常会被问到的知识点总结（1） | GPT记录 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【深度学习】深度学习中经常会被问到的知识点总结（1） | GPT记录</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p> 
<div class="toc"> 
 <h3>文章目录</h3> 
 <ul>
<li><a href="#_7">前言</a></li>
<li><a href="#_12">一、优化器</a></li>
<li>
<ul>
<li><a href="#11_13">1.1、深度学习中有哪些常见的优化器，它们之间有什么区别呢</a></li>
<li><a href="#12Adam_33">1.2、Adam算法可以自动调整学习率，它的原理是？</a></li>
<li><a href="#13_36">1.3、这些优化器里面，哪种优化器是被用到最多的呢？</a></li>
<li><a href="#14SGDAdam_38">1.4、在使用SGD优化器进行网络训练，和使用Adam优化器进行网络训练相比，它们的损失函数形状有何不同？</a></li>
<li><a href="#15AdamSGD_40">1.5、所以Adam优化器在一定程度上优于SGD优化器，是这个意思吗</a></li>
<li><a href="#16pytorch_42">1.6、在pytorch框架下，定义优化器的代码是？</a></li>
<li><a href="#17_87">1.7、对于大模型和大数据集，通常需要使用较小的学习率；对于小模型和小数据集，通常可以使用较大的学习率。怎么理解这句话？</a></li>
<li><a href="#18pytorch_96">1.8、深度学习中，如何给不同的层设置不同的学习率。以pytorch为例</a></li>
<li><a href="#19pytorch_107">1.9、深度学习中，对损失函数进行优化的基本格式是？给出pytorch框架下的代码：</a></li>
<li><a href="#110__125">1.10、在神经网络每一次迭代前 为什么要将梯度清零呢</a></li>
<li><a href="#111pytorch__146">1.11、pytorch 中的学习率调度器在代码中如何实现的？</a></li>
<li><a href="#112pytorch__159">1.12、pytorch 中如何选择学习率调度器？</a></li>
</ul> 
  </li>
<li><a href="#_165">二、超参数调参</a></li>
<li>
<ul>
<li><a href="#21_166">2.1.深度学习中的超参数是什么意思？有哪些？</a></li>
<li><a href="#22__182">2.2 深度学习中的自动调参算法有哪些？</a></li>
<li><a href="#23__197">2.3 这些自动调参算法里，哪种算法用到最多？</a></li>
</ul> 
  </li>
<li><a href="#_204">三、损失函数</a></li>
<li>
<ul>
<li><a href="#31_205">3.1.深度学习中的常用的损失函数有哪些？</a></li>
<li><a href="#32L1L2_231">3.2.L1损失和L2损失的区别是？</a></li>
<li><a href="#33_244">3.3.感知损失和感知器损失的区别是？</a></li>
<li><a href="#34_260">3.4.余弦相似度是指？</a></li>
</ul> 
  </li>
<li><a href="#_267">四、防止过拟合的手段</a></li>
<li>
<ul>
<li><a href="#41_268">4.1.深度学习中防止过拟合的手段有哪些？</a></li>
<li><a href="#42_280">4.2.正则化方法的原理是？</a></li>
<li><a href="#43L1L2__285">4.3.L1正则化和L2正则化 哪一种用的多一些？</a></li>
<li><a href="#44L1pytorch_289">4.4.L1正则化的代码如何实现的？（pytorch）</a></li>
<li><a href="#45L2pytorch_304">4.5.L2正则化的代码如何实现的？（pytorch）</a></li>
<li><a href="#46dropout_309">4.6.dropout的原理是？</a></li>
<li><a href="#47dropoutpytorch_320">4.7.dropout的代码如何实现的？（pytorch）</a></li>
</ul> 
 </li>
</ul> 
</div> 
<p></p> 
<hr> 
<h1>
<a id="_7"></a>前言</h1> 
<p>这些问题的答案都来自<strong>chatgpt</strong>，可做一定参考</p> 
<h1>
<a id="_12"></a>一、优化器</h1> 
<h2>
<a id="11_13"></a>1.1、深度学习中有哪些常见的优化器，它们之间有什么区别呢</h2> 
<p>深度学习中常见的优化器有：</p> 
<p>梯度下降法（Gradient Descent）</p> 
<p>动量优化器（Momentum）</p> 
<p>Nesterov加速梯度（Nesterov Accelerated Gradient）</p> 
<p>自适应梯度算法（Adagrad）</p> 
<p>自适应矩估计算法（RMSprop）</p> 
<p>Adam优化器（Adam）</p> 
<p>AdaDelta算法（Adadelta）</p> 
<p>梯度裁剪（Gradient Clipping）<br> 这些优化器之间的区别主要在于它们的更新方式、参数的调整方式和收敛速度等方面。其中，梯度下降法是最基本的优化器，通过计算每个参数的梯度来更新参数；动量优化器则是在梯度下降法的基础上增加了动量项，来加速梯度更新；自适应梯度算法和自适应矩估计算法则是在每个参数的梯度更新时，根据历史梯度的平方和和平均值来自适应地调整学习率，从而提高模型的性能和收敛速度；而Adam优化器则是结合了动量优化器和自适应梯度算法的优点，同时考虑了梯度的一阶矩估计和二阶矩估计，具有更好的性能和更快的收敛速度；Adadelta算法则是在自适应梯度算法的基础上，引入了RMSprop算法中的平均平方梯度，从而更好地调整学习率和动态范围。梯度裁剪则是一种常用的技巧，用于避免梯度爆炸或梯度消失的问题，通过对梯度进行裁剪来限制其大小。</p> 
<h2>
<a id="12Adam_33"></a>1.2、Adam算法可以自动调整学习率，它的原理是？</h2> 
<p>Adam算法是一种自适应学习率的优化算法，可以自动调整每个参数的学习率。其原理是结合了动量优化器和自适应梯度算法的优点，同时考虑了梯度的一阶矩估计和二阶矩估计。<br> 具体来说，Adam算法在每次迭代时，计算每个参数的一阶矩估计（即梯度的均值）和二阶矩估计（即梯度的平方的均值），并根据这些估计值来更新每个参数的学习率。</p> 
<h2>
<a id="13_36"></a>1.3、这些优化器里面，哪种优化器是被用到最多的呢？</h2> 
<p>在深度学习中，Adam优化器是目前应用最广泛的优化器之一。它结合了动量优化器和自适应梯度算法的优点，同时考虑了梯度的一阶矩估计和二阶矩估计，具有更好的性能和更快的收敛速度。相比于其他优化器，Adam算法在很多任务中都取得了更好的结果，尤其是在处理大规模数据和深层网络时效果更为明显。同时，由于Adam算法的自适应性，它不需要手动调整学习率，使得深度学习的训练更加方便和高效。因此，Adam优化器被广泛应用于深度学习的各个领域，成为了深度学习中最常用的优化器之一。</p> 
<h2>
<a id="14SGDAdam_38"></a>1.4、在使用SGD优化器进行网络训练，和使用Adam优化器进行网络训练相比，它们的损失函数形状有何不同？</h2> 
<p>在使用SGD优化器进行网络训练和使用Adam优化器进行网络训练时，它们的损失函数形状可能会有所不同。具体来说，SGD优化器的更新方向只考虑了当前的梯度信息，因此在参数空间中可能会出现比较陡峭的梯度方向，导致更新过程中可能会出现震荡现象，更新速度可能会比较慢。而Adam优化器考虑了梯度的一阶矩估计和二阶矩估计，可以对梯度的变化进行更好的自适应调整，因此更新方向更加平滑，在参数空间中可能会出现更加平缓的梯度方向，更新速度也会更快。因此，在使用Adam优化器进行网络训练时，损失函数可能会更快地收敛到较小的值，收敛速度更快，而在使用SGD优化器进行网络训练时，损失函数可能会更加震荡，收敛速度可能会更慢。不过，是否出现这种情况还取决于具体的网络结构和参数初始化方式等因素。</p> 
<h2>
<a id="15AdamSGD_40"></a>1.5、所以Adam优化器在一定程度上优于SGD优化器，是这个意思吗</h2> 
<p>Adam优化器在一定程度上优于SGD优化器，但并不是说Adam优化器总是比SGD优化器更好。在某些情况下，SGD优化器也可以取得很好的结果。在实际应用中，选择哪种优化器取决于具体的任务和数据，需要根据实验结果来进行选择。一般来说，对于大规模数据和深层网络，Adam优化器的表现更好，而对于小规模数据和浅层网络，SGD优化器可能更适合。此外，SGD优化器也有一些改进的版本，如带动量的SGD（Momentum SGD）、Adagrad和RMSprop等，这些优化器也可以在一些特定的任务中取得比Adam优化器更好的结果。因此，在选择优化器时，需要综合考虑各种因素，包括数据集大小、网络深度、学习率等因素，以及进行实验验证，才能得出最优的选择。</p> 
<h2>
<a id="16pytorch_42"></a>1.6、在pytorch框架下，定义优化器的代码是？</h2> 
<p><strong>Adam：</strong></p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>params<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> betas<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token number">0.999</span><span class="token punctuation">)</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-08</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> amsgrad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
</code></pre> 
<p>其中，各个参数的含义如下：</p> 
<p>params：需要优化的参数，一般通过模型的 parameters() 方法获取；<br> 在训练时一般为<strong>model.parameters()</strong></p> 
<p>lr：学习率；</p> 
<p>betas：Adam算法的两个超参数β1和β2，分别控制一阶矩估计和二阶矩估计的权重，一般取默认值即可；</p> 
<p>eps：数值稳定性参数，一般取默认值即可；</p> 
<p>weight_decay：L2正则化系数；</p> 
<p>amsgrad：是否使用AMSGrad变种算法，一般取默认值False即可。<br> 在实际使用中，常常需要根据实际情况调整学习率和L2正则化系数等超参数，以获得更好的优化效果。<br> <strong>Adam优化器是一种自适应学习率的优化器，可以自动调整学习率。通常情况下不需要手动设置学习率，而是可以使用Adam的默认学习率0.001。</strong><br> <strong>SGD：</strong></p> 
<pre><code class="prism language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>
</code></pre> 
<p>torch.optim.SGD包含的参数如下：</p> 
<p>params (iterable) – 待优化参数的iterable或者是定义了参数组的dict。</p> 
<p>lr (float) – 学习率。</p> 
<p>momentum (float, 可选) – 动量因子（默认为0）</p> 
<p>dampening (float, 可选) – 动量的抑制因子（默认为0）</p> 
<p>weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认为0）</p> 
<p>nesterov (bool, 可选) – 使用Nesterov动量（默认为False）<br> 其中，params参数是需要优化的参数列表，可以通过model.parameters()得到。lr是学习率，用于控制参数更新的步长，通常设置在0.1到0.001之间。momentum是动量因子，用于控制参数更新的方向，通常设置在0.9左右。dampening是动量的抑制因子，通常取0。weight_decay是权重衰减因子，用于L2正则化，通常取小于0.1的值。nesterov是布尔值，表示是否使用Nesterov动量，默认为False。<br> 在使用torch.optim.SGD进行优化时，可以根据具体情况调整这些参数的值，以获得更好的训练效果。</p> 
<p>其他几种优化器就不展示了</p> 
<h2>
<a id="17_87"></a>1.7、对于大模型和大数据集，通常需要使用较小的学习率；对于小模型和小数据集，通常可以使用较大的学习率。怎么理解这句话？</h2> 
<p>对于大模型和大数据集，通常需要使用较小的学习率；对于小模型和小数据集，通常可以使用较大的学习率。<br> 原因如下：</p> 
<p>大模型和大数据集：如果使用过大的学习率，会导致模型参数在训练过程中波动较大，难以收敛，甚至出现不稳定的情况。此时，需要使用较小的学习率，以保证模型参数的稳定性和收敛性。</p> 
<p>小模型和小数据集：如果使用过小的学习率，会导致模型需要很长的时间才能收敛，训练效率低下。此时，可以使用较大的学习率，以提高训练效率和加快模型的收敛速度。<br> 需要注意的是，学习率的设置也需要考虑优化器的类型、损失函数的选择、训练的轮次等因素。一般来说，可以先使用较小的学习率进行初始训练，然后逐步调整学习率的大小，直到找到最佳的学习率设置，以获得更好的训练效果。</p> 
<h2>
<a id="18pytorch_96"></a>1.8、深度学习中，如何给不同的层设置不同的学习率。以pytorch为例</h2> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim
optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span><span class="token punctuation">[</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>conv1<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>conv2<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.01</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>fc1<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{<!-- --></span><span class="token string">'params'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>fc2<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">)</span>
</code></pre> 
<p>上述代码中，首先定义了一个SGD优化器，然后通过一个列表来设置不同层的学习率，其中第一层的学习率使用默认值0.001，第二层的学习率设置为0.01，第三层和第四层的学习率都设置为0.1。<br> 这样，在训练模型时，优化器就会根据不同层的学习率设置来更新模型参数，从而实现对模型不同部分的灵活控制。</p> 
<h2>
<a id="19pytorch_107"></a>1.9、深度学习中，对损失函数进行优化的基本格式是？给出pytorch框架下的代码：</h2> 
<pre><code class="prism language-python">optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span> labels<span class="token punctuation">)</span>
loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>optimizer.zero_grad()：将优化器的梯度清零，避免梯度累加的影响。</p> 
<p>outputs = net(inputs)：将输入数据inputs传入神经网络net进行前向计算，得到模型的输出outputs。</p> 
<p>loss = criterion(outputs, labels)：利用损失函数criterion计算模型输出outputs和标签数据labels之间的误差loss。</p> 
<p>loss.backward()：利用反向传播算法计算损失函数关于模型参数的梯度。</p> 
<p>optimizer.step()：利用优化器对模型参数进行更新，以最小化损失函数。<br> 综上，这段代码的作用是训练神经网络模型，其中optimizer.zero_grad()、loss.backward()和optimizer.step()是深度学习模型训练的基本步骤。</p> 
<h2>
<a id="110__125"></a>1.10、在神经网络每一次迭代前 为什么要将梯度清零呢</h2> 
<p>在神经网络中，每次反向传播计算梯度时，梯度值会被累加到之前的梯度上。因此，如果在下一次迭代时不将梯度清零，之前的梯度值会对当前的梯度计算产生影响，导致计算结果不正确。所以在每一次迭代前都需要将梯度清零，避免梯度的累加影响训练结果。</p> 
<p>举个例子：<br> 第一次打印梯度的结果如下：<br> tensor([[-0.0450, 0.0446, 0.0270, 0.0297, 0.0327, 0.0069, -0.0119, -0.0526,<br> -0.0047, -0.0317, -0.0219, 0.0352, -0.0028, 0.0132, 0.0395, 0.0111,<br> -0.0305, -0.0254, -0.0223, -0.0124],<br> [-0.0450, 0.0446, 0.0270, 0.0297, 0.0327, 0.0069, -0.0119, -0.0526,<br> -0.0047, -0.0317, -0.0219, 0.0352, -0.0028, 0.0132, 0.0395, 0.0111,<br> -0.0305, -0.0254, -0.0223, -0.0124]])</p> 
<p>第二次打印梯度的结果如下：<br> tensor([[-0.0900, 0.0892, 0.0540, 0.0594, 0.0654, 0.0138, -0.0239, -0.1051,<br> -0.0094, -0.0634, -0.0438, 0.0704, -0.0056, 0.0264, 0.0790, 0.0222,<br> -0.0609, -0.0508, -0.0447, -0.0248],<br> [-0.0900, 0.0892, 0.0540, 0.0594, 0.0654, 0.0138, -0.0239, -0.1051,<br> -0.0094, -0.0634, -0.0438, 0.0704, -0.0056, 0.0264, 0.0790, 0.0222,<br> -0.0609, -0.0508, -0.0447, -0.0248]])</p> 
<p>可以看到，第二次打印的梯度值是第一次打印的梯度值的两倍，因为在第二次反向传播时，第一次的梯度值没有被清零，导致两次梯度值相加。这会影响梯度的计算和模型的训练，从而导致学习效果不佳。因此，在每一次迭代前都需要将梯度清零，避免梯度的累加。</p> 
<h2>
<a id="111pytorch__146"></a>1.11、pytorch 中的学习率调度器在代码中如何实现的？</h2> 
<p>在PyTorch中，可以使用torch.optim.lr_scheduler模块中的学习率调度器来自动调整学习率。torch.optim.lr_scheduler模块提供了多种学习率调度器，包括StepLR、MultiStepLR、ExponentialLR、CosineAnnealingLR、ReduceLROnPlateau等。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler <span class="token keyword">import</span> StepLR
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
<span class="token comment"># 定义学习率调度器</span>
scheduler <span class="token operator">=</span> StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
<span class="token comment"># 训练模型</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 在每个epoch开始时更新学习率</span>
    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
</code></pre> 
<h2>
<a id="112pytorch__159"></a>1.12、pytorch 中如何选择学习率调度器？</h2> 
<p>如果你希望在训练的前期快速降低学习率以获得更好的收敛速度，可以使用StepLR或MultiStepLR学习率调度器。</p> 
<p>如果你希望在训练的后期逐渐降低学习率以获得更好的泛化能力，可以使用ExponentialLR或ReduceLROnPlateau学习率调度器。</p> 
<p>如果你希望在训练过程中模拟学习率的余弦退火过程以获得更好的泛化能力，可以使用CosineAnnealingLR学习率调度器。</p> 
<h1>
<a id="_165"></a>二、超参数调参</h1> 
<h2>
<a id="21_166"></a>2.1.深度学习中的超参数是什么意思？有哪些？</h2> 
<p>在深度学习中，超参数（Hyperparameters）是指那些需要手动设置的参数，比如学习率、正则化系数、批大小、迭代次数等，这些参数不能通过反向传播自动调整，需要手动设置。超参数的设置可能对模型的训练和性能产生重要影响，因此是深度学习中非常重要的一个环节。<br> 不同的模型和任务需要不同的超参数设置，而不同的数据集和数据分布也需要不同的超参数。超参数通常需要经过不断的尝试和调整，才能找到最优的超参数组合。超参数的调整通常通过以下方式进行：</p> 
<p>根据经验或文献设置初始的超参数值；</p> 
<p>在小规模数据集上进行模型训练和验证，调整超参数；</p> 
<p>在大规模数据集上进行模型训练和验证，进一步调整超参数；</p> 
<p>对多组超参数进行对比实验，并选择效果最好的超参数组合；</p> 
<p>对最终的超参数组合进行评估和验证，确保模型的泛化性能和稳定性。<br> 有时也可以使用自动调参算法，如贝叶斯优化、网格搜索、随机搜索等，来自动搜索最优的超参数组合，从而提高调参效率和准确性。</p> 
<h2>
<a id="22__182"></a>2.2 深度学习中的自动调参算法有哪些？</h2> 
<p>深度学习的自动调参算法是指通过计算机程序自动寻找最优超参数的算法。深度学习的模型通常具有大量的超参数，例如学习率、批次大小、正则化系数等，手动调整这些超参数的过程非常耗时且需要大量的实验经验。因此，自动调参算法成为了深度学习中非常重要的技术，常用的自动调参算法包括以下几种：</p> 
<p>网格搜索（Grid Search）：将超参数空间划分成网格，对每组超参数进行训练和验证，选择最优的超参数组合。网格搜索的优点是简单易懂，能够保证找到全局最优解，但是需要枚举所有可能的超参数组合，因此计算代价较高。</p> 
<p>随机搜索（Random Search）：在超参数空间中随机采样一组超参数，对其进行训练和验证，重复若干次，选择最优的超参数组合。随机搜索的优点是计算代价较低，但是可能无法找到全局最优解。</p> 
<p>贝叶斯优化（Bayesian Optimization）：通过高斯过程模型对超参数空间进行建模，利用贝叶斯公式计算后验概率分布，从而选择最有可能产生最优结果的超参数组合。贝叶斯优化的优点是能够高效地搜索高维、非凸、噪声较大的超参数空间，但是需要选择合适的先验分布和高斯过程参数。</p> 
<p>梯度优化（Gradient-based Optimization）：将超参数看作模型的参数，通过梯度下降等优化算法直接优化超参数，从而找到最优超参数组合。梯度优化的优点是计算代价较低，但是需要对超参数空间进行连续可导性和可行性的限制。<br> 除了以上四种自动调参算法，还有一些基于遗传算法、粒子群优化等进化算法的自动调参方法，以及基于神经网络的自动机器学习（AutoML）方法。这些方法各有优缺点，需要根据实际情况选择合适的算法来进行超参数调整。</p> 
<hr> 
<h2>
<a id="23__197"></a>2.3 这些自动调参算法里，哪种算法用到最多？</h2> 
<p>在学术界和业界，目前最流行的自动调参算法是基于贝叶斯优化的方法。贝叶斯优化是一种基于贝叶斯理论和高斯过程的优化算法，通过不断地利用历史信息进行先验分布估计和后验概率推断，来选择最有可能产生最优结果的超参数组合。相对于网格搜索和随机搜索，贝叶斯优化能够高效地搜索高维、非凸、噪声较大的超参数空间，并逐渐逼近全局最优解。因此，贝叶斯优化已经成为自动调参的主流算法之一，并且在许多深度学习任务中都获得了优异的性能。<br> 除了贝叶斯优化之外，随机搜索和网格搜索等算法也被广泛应用于自动调参。随机搜索是一种简单而有效的方法，它能够在有限的计算资源下快速地搜索超参数空间，同时还具有一定的随机性，能够避免陷入局部最优解。网格搜索则是一种基于均匀采样的方法，能够保证找到全局最优解，但是计算代价较高。<br> 总之，自动调参算法的选择应该根据实际问题和计算资源进行权衡，选择最适合的算法来进行超参数调整。</p> 
<h1>
<a id="_204"></a>三、损失函数</h1> 
<h2>
<a id="31_205"></a>3.1.深度学习中的常用的损失函数有哪些？</h2> 
<p>常用的损失函数如下：</p> 
<p>回归任务</p> 
<p>均方误差（Mean Squared Error，MSE）：常用于回归问题，是预测值与真实值之差的平方的平均值。</p> 
<p>均方根误差（Root Mean Squared Error，RMSE）：也是常用于回归问题的损失函数，是MSE的平方根。</p> 
<p>平均绝对误差（Mean Absolute Error，MAE）：是预测值与真实值之差的绝对值的平均值，对离群点具有较好的鲁棒性。</p> 
<p>Huber损失：是一种介于MSE和MAE之间的损失函数，对离群点的惩罚相对于MSE更小，相对于MAE更大，因此对于一些有噪声的数据集具有较好的鲁棒性。</p> 
<p>分类任务</p> 
<p>交叉熵损失（Cross-Entropy Loss）：常用于分类问题，是预测值和真实值之间的交叉熵损失，通常与Softmax函数结合使用。</p> 
<p>二元交叉熵损失（Binary Cross-Entropy Loss）：是交叉熵损失在二分类问题中的特例，仅适用于二分类问题。</p> 
<p>Hinge损失：常用于支持向量机（SVM）中，可以在二分类问题中直接优化分类准确率，对于分类错误的样本给予了一定的惩罚。</p> 
<p>Focal Loss：是一种专门用于解决类别不平衡问题的损失函数，一般用于多分类问题，旨在将算法对于难以分类的样本（如少数类别）的关注度提高。</p> 
<h2>
<a id="32L1L2_231"></a>3.2.L1损失和L2损失的区别是？</h2> 
<p>L1损失和L2损失是回归问题中两种常用的损失函数，它们的计算方式不同，因此会对模型的训练产生不同的影响。</p> 
<p>L1损失（也叫平均绝对误差，Mean Absolute Error，MAE）：L1损失是预测值与真实值之差的绝对值的平均值，它的公式为：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           L 
          
         
           1 
          
         
        
          = 
         
         
         
           1 
          
         
           n 
          
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           n 
          
         
         
         
           ∣ 
          
          
          
            y 
           
          
            i 
           
          
         
           − 
          
          
          
            y 
           
          
            ^ 
           
          
         
           i 
          
         
           ∣ 
          
         
        
       
         L_{1}=frac{1}{n} sum_{i=1}^{n}left|y_{i}-hat{y}{i}right| 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 2.9291em;vertical-align: -1.2777em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal" style="margin-right: 0.0359em">y</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em"><span class=""></span></span></span></span></span><span class="mord"><span class="mord mathnormal">i</span></span><span class="mclose delimcenter">∣</span></span></span></span></span></span></span><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         y 
        
       
         i 
        
       
      
        y{i} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.854em;vertical-align: -0.1944em"></span><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="mord"><span class="mord mathnormal">i</span></span></span></span></span></span>表示真实值，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
         
         
           y 
          
         
           ^ 
          
         
        
          i 
         
        
       
      
        hat{y}_{i} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8889em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal" style="margin-right: 0.0359em">y</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>表示预测值，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span>表示样本数量。L1损失对离群点具有较好的鲁棒性，因为它对于离群点的惩罚是线性的。</p> 
<p>L2损失（也叫均方误差，Mean Squared Error，MSE）：L2损失是预测值与真实值之差的平方的平均值，它的公式为：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           L 
          
         
           2 
          
         
        
          = 
         
         
         
           1 
          
         
           n 
          
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           n 
          
         
         
          
          
            ( 
           
           
           
             y 
            
           
             i 
            
           
          
            − 
           
           
            
            
              y 
             
            
              ^ 
             
            
           
             i 
            
           
          
            ) 
           
          
         
           2 
          
         
        
       
         L_{2}=frac{1}{n} sum_{i=1}^{n}left(y_{i}-hat{y}_{i}right)^{2} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 2.9291em;vertical-align: -1.2777em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal" style="margin-right: 0.0359em">y</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.954em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></span></span><br> L2损失对于较小的误差有很好的区分度，因为它对误差的惩罚是平方级别的。但是，L2损失对于离群点的惩罚是非常大的，因为它对离群点的误差惩罚是平方级别的。<br> 综上所述，L1损失对离群点较为鲁棒，而L2损失对小误差有较好的区分度，但对离群点的惩罚较大。在实际应用中，选择哪种损失函数应根据具体的问题和数据特点来决定。</p> 
<h2>
<a id="33_244"></a>3.3.感知损失和感知器损失的区别是？</h2> 
<p>感知器损失和感知损失是不同的概念。<br> 感知器损失（Perceptron Loss）是一种0-1损失函数，常用于支持向量机（SVM）中，可以在二分类问题中直接优化分类准确率。具体来说，对于一个样本<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ( 
        
        
        
          x 
         
        
          i 
         
        
       
         , 
        
        
        
          y 
         
        
          i 
         
        
       
         ) 
        
       
      
        (x_i,y_i) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>，感知器损失定义为：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           L 
          
          
          
            p 
           
          
            e 
           
          
            r 
           
          
            c 
           
          
            e 
           
          
            p 
           
          
            t 
           
          
            r 
           
          
            o 
           
          
            n 
           
          
         
        
          ( 
         
         
         
           x 
          
         
           i 
          
         
        
          , 
         
         
         
           y 
          
         
           i 
          
         
        
          ) 
         
        
          = 
         
         
         
           { 
          
          
           
            
             
              
              
                0 
               
              
                , 
               
              
             
            
            
             
              
               
               
                 y 
                
               
                 i 
                
               
              
                ( 
               
               
               
                 w 
                
               
                 T 
                
               
               
               
                 x 
                
               
                 i 
                
               
              
                + 
               
              
                b 
               
              
                ) 
               
              
                &gt; 
               
              
                0 
               
              
                  
               
              
                1 
               
              
                , 
               
              
             
            
            
             
              
              
                o 
               
              
                t 
               
              
                h 
               
              
                e 
               
              
                r 
               
              
                w 
               
              
                i 
               
              
                s 
               
              
                e 
               
              
             
            
           
          
         
        
       
         L_{perceptron}(x_i,y_i) = begin{cases} 0, &amp; y_i(w^Tx_i+b)&gt;0  1, &amp; otherwise end{cases} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0361em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">erce</span><span class="mord mathnormal mtight">pt</span><span class="mord mathnormal mtight">ro</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.8em;vertical-align: -0.65em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size2">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.97em"><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.47em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.97em"><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mord">0</span><span class="mspace"> </span><span class="mord">1</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.47em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.97em"><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord mathnormal">o</span><span class="mord mathnormal">t</span><span class="mord mathnormal">h</span><span class="mord mathnormal" style="margin-right: 0.0278em">er</span><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="mord mathnormal">i</span><span class="mord mathnormal">se</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.47em"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         w 
        
       
      
        w 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal" style="margin-right: 0.0269em">w</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         b 
        
       
      
        b 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>是模型参数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          y 
         
        
          i 
         
        
       
      
        y_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>表示样本的真实标签，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          w 
         
        
          T 
         
        
        
        
          x 
         
        
          i 
         
        
       
         + 
        
       
         b 
        
       
      
        w^Tx_i+b 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9913em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0269em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.6944em"></span><span class="mord mathnormal">b</span></span></span></span></span>是模型对样本<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          i 
         
        
       
      
        x_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的预测值。<br> 感知损失（Perceptual Loss）则是一种用于图像重建和图像风格迁移等任务中的损失函数，与感知器损失没有直接关系。感知损失是基于深度学习模型的特征提取能力，用于衡量重建结果与真实图像之间的差异。常用的感知损失包括基于VGG网络的内容损失和风格损失，其计算方式如下：</p> 
<p>首先，构建一个预训练的VGG网络（通常选择VGG16或者VGG19），用于提取图像的特征。</p> 
<p>给定一张真实图像和一张生成图像，通过VGG网络分别提取它们在某几个层的特征表示（通常选择高层特征，因为它们更加抽象和语义化），这些特征表示可以看做是两张图片的向量表示。</p> 
<p>用两张图像在这几个层的特征表示之间的欧氏距离作为它们的相似度，再取这些相似度的加权平均作为感知损失，即：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           L 
          
          
          
            p 
           
          
            e 
           
          
            r 
           
          
            c 
           
          
            e 
           
          
            p 
           
          
            t 
           
          
            u 
           
          
            a 
           
          
            l 
           
          
         
        
          = 
         
         
         
           1 
          
         
           n 
          
         
         
         
           ∑ 
          
          
          
            i 
           
          
            = 
           
          
            1 
           
          
         
           n 
          
         
         
          
          
            ∣ 
           
           
           
             ϕ 
            
           
             i 
            
           
          
            ( 
           
          
            x 
           
          
            ) 
           
          
            − 
           
           
           
             ϕ 
            
           
             i 
            
           
          
            ( 
           
           
           
             x 
            
           
             ^ 
            
           
          
            ) 
           
          
            ∣ 
           
          
         
           2 
          
         
        
       
         L_{perceptual} = frac{1}{n} sum_{i=1}^n left| phi_i(x) - phi_i(hat{x}) right|_2 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9694em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">erce</span><span class="mord mathnormal mtight">pt</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 2.9291em;vertical-align: -1.2777em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">n</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.2777em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="minner"><span class="mopen delimcenter">∣</span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal">x</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose delimcenter">∣</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         x 
        
       
      
        x 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>表示真实图像，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          ^ 
         
        
       
      
        hat{x} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal">x</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span></span></span></span></span></span></span></span>表示生成图像，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          ϕ 
         
        
          i 
         
        
       
         ( 
        
       
         ⋅ 
        
       
         ) 
        
       
      
        phi_i(cdot) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">⋅</span><span class="mclose">)</span></span></span></span></span>表示VGG网络在第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
      
        i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>层的特征提取函数，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         n 
        
       
      
        n 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">n</span></span></span></span></span>表示选取的层数。<br> 需要注意的是，这里的感知损失并不是单纯的L1或L2损失，而是通过VGG网络提取的特征表示计算得到的。感知损失可以用于<strong>图像重建和图像风格迁移</strong>等任务中，可以帮助生成图像更好地保留真实图像的语义信息。</p> 
<h2>
<a id="34_260"></a>3.4.余弦相似度是指？</h2> 
<p>在图像重建中，余弦相似度可以作为损失函数来衡量生成图像与目标图像之间的相似度。具体地，给定生成图像<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         G 
        
       
      
        G 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal">G</span></span></span></span></span>和目标图像<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         I 
        
       
      
        I 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.0785em">I</span></span></span></span></span>，它们对应的特征向量分别为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          G 
         
        
       
      
        x_G 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          I 
         
        
       
      
        x_I 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，则它们之间的余弦相似度损失可以定义为：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"> 
      
       
        
         
         
           L 
          
          
          
            c 
           
          
            o 
           
          
            s 
           
          
         
        
          ( 
         
        
          G 
         
        
          , 
         
        
          I 
         
        
          ) 
         
        
          = 
         
        
          1 
         
        
          − 
         
        
          cos 
         
        
          ⁡ 
         
        
          ( 
         
         
         
           x 
          
         
           G 
          
         
        
          , 
         
         
         
           x 
          
         
           I 
          
         
        
          ) 
         
        
          = 
         
        
          1 
         
        
          − 
         
         
          
           
           
             x 
            
           
             G 
            
           
          
            ⋅ 
           
           
           
             x 
            
           
             I 
            
           
          
          
           
           
             ∣ 
            
            
            
              x 
             
            
              G 
             
            
           
             ∣ 
            
           
           
           
             ∣ 
            
            
            
              x 
             
            
              I 
             
            
           
             ∣ 
            
           
          
         
        
       
         L_{cos}(G,I) = 1 - cos(x_G, x_I) = 1 - frac{x_G cdot x_I}{left|x_Gright|left|x_Iright|} 
        
       
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">cos</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">G</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0785em">I</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.7278em;vertical-align: -0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mop">cos</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.7278em;vertical-align: -0.0833em"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 2.0575em;vertical-align: -0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.1215em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="minner"><span class="mopen delimcenter">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">∣</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">∣</span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          G 
         
        
       
         ⋅ 
        
        
        
          x 
         
        
          I 
         
        
       
      
        x_G cdot x_I 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5945em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>表示<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          G 
         
        
       
      
        x_G 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          I 
         
        
       
      
        x_I 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的内积，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ∣ 
        
        
        
          x 
         
        
          G 
         
        
       
         ∣ 
        
       
      
        left|x_Gright| 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="minner"><span class="mopen delimcenter">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">∣</span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         ∣ 
        
        
        
          x 
         
        
          I 
         
        
       
         ∣ 
        
       
      
        left|x_Iright| 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="minner"><span class="mopen delimcenter">∣</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">∣</span></span></span></span></span></span>分别表示<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          G 
         
        
       
      
        x_G 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">G</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          x 
         
        
          I 
         
        
       
      
        x_I 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0785em">I</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的模长。<br> 在图像重建过程中，我们通常使用预训练的卷积神经网络（如VGG）提取图像的特征向量，然后计算生成图像与目标图像在这些特征向量上的余弦相似度损失。通过最小化余弦相似度损失，我们可以使生成图像尽可能地接近目标图像，从而实现图像重建的任务。</p> 
<h1>
<a id="_267"></a>四、防止过拟合的手段</h1> 
<h2>
<a id="41_268"></a>4.1.深度学习中防止过拟合的手段有哪些？</h2> 
<p>数据增强（Data Augmentation）：通过对训练集进行一些随机变换，如旋转、裁剪、平移、翻转等，生成新的训练样本，从而扩大训练集的规模，提高模型的泛化能力。</p> 
<p>正则化（Regularization）：在损失函数中添加正则化项，如L1正则化和L2正则化，对模型参数进行限制，防止过拟合。此外，还可以使用Dropout等正则化技术，随机屏蔽一些神经元，减少模型的复杂度。</p> 
<p>早停（Early Stopping）：在训练过程中，监测模型在验证集上的性能，当模型在验证集上的性能开始下降时，停止训练，避免模型过拟合。</p> 
<p>参数共享（Parameter Sharing）：对于一些具有相似特征的任务，可以使用参数共享的方法，让多个任务共享同一组参数，从而提高模型的泛化能力。</p> 
<p>增加训练数据：增加训练数据可以提高模型的泛化能力，因为更多的数据可以帮助模型更好地学习到数据的分布，降低过拟合的风险。</p> 
<p>模型简化（Simplification）：降低模型的复杂度，如减少神经元的个数、减少网络的层数等，可以减少模型的参数数量，降低过拟合的风险。</p> 
<h2>
<a id="42_280"></a>4.2.正则化方法的原理是？</h2> 
<p>正则化是一种常用的机器学习方法，用于减少模型的复杂度、防止过拟合。常见的正则化方法有L1正则化和L2正则化。<br> L1正则化的原理是在损失函数中添加L1范数正则化项，也称为Lasso正则化。L1正则化通过对模型参数的绝对值进行惩罚，使一部分参数变为0，从而实现特征选择的功能，即自动选择对目标变量有重要贡献的特征。因此，L1正则化可以用于特征选择、稀疏性建模等任务。<br> L2正则化的原理是在损失函数中添加L2范数正则化项，也称为Ridge正则化。L2正则化通过对模型参数的平方进行惩罚，使模型参数不会过大，从而避免过拟合。同时，L2正则化还可以提高模型的泛化能力，因为它可以将相似的特征对应的权重调整到相似的大小，减少模型的过度依赖某些特征的情况。<br> 总的来说，正则化方法的原理是在损失函数中添加正则化项，对模型参数进行限制，避免模型过于复杂，降低过拟合的风险。L1正则化可以进行特征选择，L2正则化可以提高模型的泛化能力。</p> 
<h2>
<a id="43L1L2__285"></a>4.3.L1正则化和L2正则化 哪一种用的多一些？</h2> 
<p>一般而言，L2正则化被使用得更为广泛，因为它可以有效地避免过拟合，提高模型的泛化能力。同时，L2正则化的计算比L1正则化更为简单，也更易于处理。因此，在大多数情况下，L2正则化是首选的正则化方法。<br> 然而，在某些特定的场景下，L1正则化也会被使用。例如，在特征选择或稀疏性建模的任务中，L1正则化可以将某些特征的权重调整为0，从而实现特征选择的功能。此外，在一些需要处理大规模稀疏数据的场景中，L1正则化也被广泛使用，因为它能够产生稀疏解，减少存储和计算的开销。<br> 因此，对于具体的问题，选择哪种正则化方法需要根据实际情况进行权衡和选择。</p> 
<h2>
<a id="44L1pytorch_289"></a>4.4.L1正则化的代码如何实现的？（pytorch）</h2> 
<pre><code class="prism language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
    output <span class="token operator">=</span> net<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>
    loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
    <span class="token comment"># 计算L1正则化项</span>
    l1_regularization <span class="token operator">=</span> <span class="token number">0</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        l1_regularization <span class="token operator">+=</span> torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>param<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    loss <span class="token operator">+=</span> <span class="token number">0.01</span> <span class="token operator">*</span> l1_regularization
    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>在损失函数中，我们通过循环计算所有模型参数的L1范数，并乘以0.01作为L1正则化项的权重，然后将这个正则化项加到损失函数中，一起进行反向传播和优化。</p> 
<h2>
<a id="45L2pytorch_304"></a>4.5.L2正则化的代码如何实现的？（pytorch）</h2> 
<pre><code class="prism language-python">optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
</code></pre> 
<p>在上述代码中，weight_decay参数即为L2正则化项的权重衰减系数，通过调整这个参数的大小，可以控制正则化的程度。</p> 
<h2>
<a id="46dropout_309"></a>4.6.dropout的原理是？</h2> 
<p>Dropout是一种用于神经网络正则化的技术。它的基本思想是在训练过程中，随机地让一部分神经元的输出值为0，从而减少神经网络的复杂度，避免过拟合，提高模型的泛化能力。具体来说，Dropout会在每次前向传播中，随机地以一定的概率p（一般取0.5）将某些神经元的输出值置为0，这样一来，这些神经元对于当前这个样本的信息就不再起作用，相当于被删除了。<br> Dropout的原理可以通过以下几个方面来解释：</p> 
<p>Dropout可以看作是对模型进行了集成学习。由于每次训练时都会随机地丢弃一些神经元，因此每次训练得到的模型都是不同的，相当于得到了多个不同的子模型。在测试时，我们需要对所有子模型的输出进行平均或加权平均，从而得到最终的输出结果。这样一来，Dropout可以有效地减少模型的方差，提高模型的泛化能力。</p> 
<p>Dropout可以防止过拟合。神经网络的过拟合往往是由于模型过于复杂，学习到了训练集中的噪声和细节，导致在测试集上表现不佳。Dropout通过随机丢弃部分神经元的输出值，相当于强制模型变得简单，从而有效地避免了过拟合的问题。</p> 
<p>Dropout可以提高模型的鲁棒性。在训练过程中，神经元的丢弃是随机的，因此模型对于输入的扰动具有一定的鲁棒性。这样一来，Dropout可以有效地增强模型对于噪声和异常输入的容忍能力，提高模型的鲁棒性。<br> 总之，Dropout是一种简单而有效的正则化技术，可以大大提高神经网络的泛化能力，避免过拟合的问题，同时还可以增强模型的鲁棒性。</p> 
<h2>
<a id="47dropoutpytorch_320"></a>4.7.dropout的代码如何实现的？（pytorch）</h2> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x
</code></pre> 
<p>在上述代码中，我们在模型中添加了一个nn.Dropout层，并将丢弃的概率设置为0.5。在模型的前向传播中，我们首先将输入x传入全连接层self.fc1，然后使用ReLU激活函数进行非线性变换，接着将变换后的结果传入nn.Dropout层进行随机丢弃，最后再传入全连接层self.fc2得到输出。在训练过程中，PyTorch会自动根据设定好的丢弃概率随机地丢弃一些神经元的输出，从而实现Dropout。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>