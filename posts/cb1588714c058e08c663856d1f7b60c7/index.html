<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>特征选取之单变量统计、基于模型选择、迭代选择SelectKBest特征选取 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">特征选取之单变量统计、基于模型选择、迭代选择SelectKBest特征选取</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p id="main-toc"><span style="color:#4da8ee"><strong>目录</strong></span></p> 
<p id="%E5%8D%95%E5%8F%98%E9%87%8F%E7%BB%9F%E8%AE%A1-toc" style="margin-left:40px"><a href="#%E5%8D%95%E5%8F%98%E9%87%8F%E7%BB%9F%E8%AE%A1">单变量统计</a></p> 
<p id="%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90-toc" style="margin-left:80px"><a href="#%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90">方差分析</a></p> 
<p id="%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-toc" style="margin-left:80px"><a href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">代码实现</a></p> 
<p id="articleContentId-toc" style="margin-left:80px"><a href="#articleContentId">SelectKBest特征选取</a></p> 
<p id="3-递归特征消除-recursive-feature-elimination-toc" style="margin-left:80px"><a href="#3-%E9%80%92%E5%BD%92%E7%89%B9%E5%BE%81%E6%B6%88%E9%99%A4-recursive-feature-elimination">递归特征消除(RFE)</a></p> 
<p id="%E7%9A%AE%E5%B0%94%E9%80%8A%E7%B3%BB%E6%95%B0%EF%BC%88%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%EF%BC%89-toc" style="margin-left:80px"><a href="#%E7%9A%AE%E5%B0%94%E9%80%8A%E7%B3%BB%E6%95%B0%EF%BC%88%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%EF%BC%89">皮尔逊系数（相关系数）</a></p> 
<p id="%E5%9F%BA%E4%BA%8E%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9-toc" style="margin-left:40px"><a href="#%E5%9F%BA%E4%BA%8E%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9">基于树模型的特征选择</a></p> 
<p id="%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%9A%84%E9%AB%98%E6%95%88%E6%A2%AF%E5%BA%A6%E6%A0%91-toc" style="margin-left:80px"><a href="#%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%9A%84%E9%AB%98%E6%95%88%E6%A2%AF%E5%BA%A6%E6%A0%91">轻量级的高效梯度树</a></p> 
<p id="%C2%A0%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97-toc" style="margin-left:80px"><a href="#%C2%A0%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"> 随机森林</a></p> 
<p id="%C2%A0%E8%BF%AD%E4%BB%A3%E9%80%89%E6%8B%A9-toc" style="margin-left:40px"><a href="#%C2%A0%E8%BF%AD%E4%BB%A3%E9%80%89%E6%8B%A9"> 迭代选择</a></p> 
<p id="%E6%AF%8F%E6%96%87%E4%B8%80%E8%AF%AD-toc" style="margin-left:80px"><a href="#%E6%AF%8F%E6%96%87%E4%B8%80%E8%AF%AD">每文一语</a></p> 
<hr id="hr-toc">
<h2 id="%E5%8D%95%E5%8F%98%E9%87%8F%E7%BB%9F%E8%AE%A1">单变量统计</h2> 
<h3 id="%E6%96%B9%E5%B7%AE%E5%88%86%E6%9E%90">方差分析</h3> 
<blockquote> 
 <p>在单变量统计中，我们计算每个特征和目标值之间的关系是否存在统计显著性，然后选择具有最高置信度的特征。对于分类问题，这也被称为方差分析（analysis of variance，ANOVA）。</p> 
</blockquote> 
<p><strong>这些测试的一个关键性质就是它们是单变量的（univariate），即它们只单独考虑每个特征。因此，如果一个特征只有在与另一个特征合并时才具有信息量，那么这个特征将被舍弃。</strong></p> 
<p><span style="color:#4da8ee"><strong>单变量测试的计算速度通常很快，并且不需要构建模型。另一方面，它们完全独立于你可能想要在特征选择之后应用的模型。</strong></span></p> 
<p><strong>注意：</strong>不需要构建模型，利用纯理论的数学思维去构建出来</p> 
<p>想要在 scikit-learn 中使用单变量特征选择，你需要选择一项测试——对分类问题通常是 <strong>f_classif（默认值）</strong>，对<strong>回归问题通常是 f_regression</strong>——然后基于测试中确定的 p 值来选择一种舍弃特征的方法。所有舍弃参数的方法都使用阈值来舍弃所有 p 值过大的特征（意味着它们不可能与目标值相关）。计算阈值的方法各有不同，最简单的是<span style="color:#fe2c24"> SelectKBest和 SelectPercentile</span>，前者<strong>选择固定数量的 k 个特征</strong>，后者<strong>选择固定百分比的特征</strong>。</p> 
<h3 id="%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">代码实现</h3> 
<pre><code class="language-python">from sklearn.feature_selection import SelectPercentile
select=SelectPercentile(percentile=50)    #选取50%的特征
select.fit(X_train,y_train)
X_train_selected=select.transform(X_train)
print(X_train.shape)
print(X_train_selected.shape)

</code></pre> 
<p><strong>案例分析</strong></p> 
<pre><code class="language-python">import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.feature_selection import SelectPercentile    #SelectPercentile选择百分比，SelectKBest选择个数
from sklearn.model_selection import train_test_split

#使用sklearn自带的数据集
cancer=load_breast_cancer()

#向数据中添加噪声特征，前30个特征来自原数据集，后50个是噪声
rng=np.random.RandomState(42)
#构造噪声特征
noise=rng.normal(size=(len(cancer.data),50))
X_w_noise=np.hstack([cancer.data,noise])

#分割数据集用于测试和训练
X_train,X_test,y_train,y_test=train_test_split(X_w_noise,cancer.target,random_state=0,test_size=.5)

#特征选取
select=SelectPercentile(percentile=50)    #选取50%的特征
select.fit(X_train,y_train)
X_train_selected=select.transform(X_train)
print(X_train.shape)
print(X_train_selected.shape)

</code></pre> 
<p style="text-align:center"><img alt="" height="69" src="https://images2.imgbox.com/49/15/luU0ai7x_o.png" width="154"></p> 
<p><strong>很明显的就发现特征变化了，而且是50%，还可以看看那些特征被选取</strong></p> 
<pre><code class="language-python">#查看哪些特征被选中
mask=select.get_support()
print(mask)
import matplotlib.pyplot as plt
plt.matshow(mask.reshape(1,-1),cmap='gray_r')
plt.xlabel('sample index')
plt.show()

</code></pre> 
<p>这一类的特征选取方法比较的直接，也就是说直接保留多少占比的特征数量，在某些时候的模型训练中，我们需要根据特征的权重进行自定义的筛选。</p> 
<h3 id="articleContentId">SelectKBest特征选取</h3> 
<p>这类选取方法和相关系数选取比较的相似，可以通过这些对象的得分，选取重要的数据列</p> 
<pre><code class="language-python">import pandas as pd 
import numpy as np
# 分类常用的
from sklearn.feature_selection import SelectKBest,f_classif,chi2,mutual_info_classif
# 分别是卡方检验，计算非负特征和类之间的卡方统计 chi
# 样本方差F值，f_classif
# 离散类别交互信息， mutual_info_classif
# 回归常用的
from sklearn.feature_selection import f_regression,mutual_info_regression</code></pre> 
<pre><code class="language-python">selector= SelectKBest(score_func= chi2,k=len(X.columns))
selector.fit(X,y)
score=-np.log10(selector.pvalues_)
# print(score)
score_index=np.argsort(score)[::-1]
for i in range(len(score)):
    print("%0.2f %s"%(score[score_index[i]],X.columns[score_index[i]]))</code></pre> 
<p><img alt="" height="504" src="https://images2.imgbox.com/ed/10/ubq1ZStQ_o.png" width="1200"></p> 
<p><strong>上述的数值大小代表着特征的重要性（下面有一个完整的代码实例）</strong></p> 
<pre><code class="language-python">from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.datasets import load_iris
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from  scipy.stats import chi2_contingency
import numpy as np

 
model = SelectKBest(chi2, k=8)#选择k个最佳特征
X_new = model.fit_transform(X, y)
#feature_data是特征数据，label_data是标签数据，该函数可以选择出k个特征 

print("model shape: ",X_new.shape)

scores = model.scores_
print('model scores:', scores)  # 得分越高，特征越重要

p_values = model.pvalues_
print('model p-values', p_values)  # p-values 越小，置信度越高，特征越重要

# 按重要性排序，选出最重要的 k 个
indices = np.argsort(scores)[::-1]
k_best_features = list(X.columns.values[indices[0:8]])

print('k best features are:n ',k_best_features)

print(k_best_features)</code></pre> 
<p>简而言之，使用SelectKBest选取，可以调整里面的参数值，最终确定最重要的几个特征</p> 
<h3 id="3-递归特征消除-recursive-feature-elimination">递归特征消除(RFE)</h3> 
<p>递归消除特征法<strong>使用一个基模型来进行多轮训练，每轮训练后，移除若干权值系数的特征，再基于新的特征集进行下一轮训练</strong>。</p> 
<p><strong>sklearn官方解释</strong>：对特征含有权重的预测模型(例如，线性模型对应参数coefficients)，RFE通过<strong>递归减少考察的特征集规模来选择特征</strong>。首先，预测模型在原始特征上训练，每个特征指定一个权重。之后，那些拥有最小绝对值权重的特征被踢出特征集。如此往复递归，直至剩余的特征数量达到所需的特征数量。</p> 
<p>RFE 通过交叉验证的方式执行RFE，以此来选择最佳数量的特征：对于一个数量为d的feature的集合，他的所有的子集的个数是2的d次方减1(包含空集)。指定一个外部的学习算法，比如SVM之类的。通过该算法计算所有子集的validation error。选择error最小的那个子集作为所挑选的特征。</p> 
<pre><code class="language-python">"""
使用RFE进行特征选择：RFE是常见的特征选择方法，也叫递归特征消除。它的工作原理是递归删除特征，
并在剩余的特征上构建模型。它使用模型准确率来判断哪些特征（或特征组合）对预测结果贡献较大。
"""
from sklearn import datasets
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
rfe = RFE(model, 3)
rfe = rfe.fit(X, y)
print(X.columns[rfe.support_])
print(rfe.support_)
print(rfe.ranking_)</code></pre> 
<p><img alt="" height="88" src="https://images2.imgbox.com/fe/55/Ch6U3a0U_o.png" width="1200"></p> 
<p>RFE的稳定性很大程度上取决于迭代时，底层用的哪种模型。比如RFE采用的是普通的回归（LR），没有经过正则化的回归是不稳定的，那么RFE就是不稳定的。假如采用的Lasso/Ridge，正则化的回归是稳定的，那么RFE就是稳定的。 </p> 
<h3 id="%E7%9A%AE%E5%B0%94%E9%80%8A%E7%B3%BB%E6%95%B0%EF%BC%88%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%EF%BC%89">皮尔逊系数（相关系数）</h3> 
<p>皮尔逊相关也称为积差相关（或者积矩相关）。我们假设有两个变量X,Y,那么两变量间的<a href="https://so.csdn.net/so/search?q=%E7%9A%AE%E5%B0%94%E9%80%8A%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0&amp;spm=1001.2101.3001.7020" title="皮尔逊相关系数">皮尔逊相关系数</a>计算如下:</p> 
<p><img alt="" height="572" src="https://images2.imgbox.com/2b/fd/XnKV0Wuh_o.png" width="1021"></p> 
<p> </p> 
<pre><code class="language-python"># 查看其他指标和income的相关性，并对其相关性系数排序
# 正相关与负相关，越接近1说明该变量值越大目标标签的值越容易分类为1
df.corr()[["n23"]].sort_values(by="n23",ascending=False)</code></pre> 
<pre><code class="language-python"># 可以根据相关性的值进行取值带入模型（特征选取与筛选）第一次选取
corr=df.corr()[["n23"]].sort_values(by="n23",ascending=False).iloc[1:10].index.values.astype("U")
corr_name=corr.tolist()
corr_name</code></pre> 
<p><img alt="" height="51" src="https://images2.imgbox.com/5d/eb/WMjKPmhU_o.png" width="1067"></p> 
<p> 利用该方法还可以绘制热力图:<strong>sns.clustermap(df.corr())</strong></p> 
<p> <img alt="" height="607" src="https://images2.imgbox.com/44/14/4mDY2QLB_o.png" width="738"></p> 
<p> <strong>Pearson相关系数的一个明显缺陷是，作为特征排序机制，他只对线性关系敏感。如果关系是非线性的，即便两个变量具有一一对应的关系，Pearson相关性也可能会接近0。</strong></p> 
<h2 id="%E5%9F%BA%E4%BA%8E%E6%A0%91%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9">基于树模型的特征选择</h2> 
<p><strong>一般基于模型所做出的特征选择，效果比单变量的要好，但是不管是所有的特征选取的方法都需要结合实际的经验，来辨别特征的有效性。</strong></p> 
<h3 id="%E8%BD%BB%E9%87%8F%E7%BA%A7%E7%9A%84%E9%AB%98%E6%95%88%E6%A2%AF%E5%BA%A6%E6%A0%91">轻量级的高效梯度树</h3> 
<pre><code class="language-python">from sklearn.model_selection import train_test_split,cross_val_score #拆分训练集和测试集
import lightgbm as lgbm #轻量级的高效梯度提升树
X_name=df.corr()[["n23"]].sort_values(by="n23",ascending=False).iloc[1:].index.values.astype("U")
X=df.loc[:,X_name.tolist()]
y=df.loc[:,['n23']]
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=1)
lgbm_reg = lgbm.LGBMRegressor(objective='regression',max_depth=6,num_leaves=25,learning_rate=0.005,n_estimators=1000,min_child_samples=80, subsample=0.8,colsample_bytree=1,reg_alpha=0,reg_lambda=0)
lgbm_reg.fit(X_train, y_train)
#选择最重要的20个特征，绘制他们的重要性排序图
lgbm.plot_importance(lgbm_reg, max_num_features=20)

##也可以不使用自带的plot_importance函数，手动获取特征重要性和特征名，然后绘图
feature_weight = lgbm_reg.feature_importances_
feature_name = lgbm_reg.feature_name_
feature_sort = pd.Series(data = feature_weight ,index = feature_name)
feature_sort = feature_sort.sort_values(ascending = False)
# plt.figure(figsize=(10,8))
# sns.barplot(feature_sort.values,feature_sort.index, orient='h')
lgbm_name=feature_sort.index[:15].tolist()
lgbm_name</code></pre> 
<p style="text-align:center"><img alt="" height="364" src="https://images2.imgbox.com/b9/5d/KEpzueJQ_o.png" width="587"></p> 
<h3 id="%C2%A0%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"> 随机森林</h3> 
<pre><code class="language-python">from sklearn import metrics
import warnings
warnings.filterwarnings("ignore")

from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

selected_feat_names=set()
for i in range(10):                           #这里我们进行十次循环取交集
    tmp = set()
    rfc = RandomForestClassifier(n_jobs=-1)
    rfc.fit(X, y)

    #print("training finished")

    importances = rfc.feature_importances_
    indices = np.argsort(importances)[::-1]   # 降序排列
    S={}
    for f in range(X.shape[1]):
        if  importances[indices[f]] &gt;=0.0001:
            tmp.add(X.columns[indices[f]])
            S[X.columns[indices[f]]]=importances[indices[f]]
            #print("%2d) %-*s %f" % (f + 1, 30, X.columns[indices[f]], importances[indices[f]]))
    selected_feat_names |= tmp
    imp_fea=pd.Series(S)
    
plt.figure(figsize=(10,8))

sns.barplot(imp_fea.values,imp_fea.index, orient='h')
print(imp_fea)
print(len(selected_feat_names), "features are selected")</code></pre> 
<p><img alt="" height="551" src="https://images2.imgbox.com/a1/67/3QLngAcQ_o.png" width="1200"></p> 
<p><img alt="" height="600" src="https://images2.imgbox.com/3e/52/fDiRXmaS_o.png" width="971"><strong>其实还有很多，比如在回归问题中，基于惩罚项的特征选取；L1、L2模型也是可以的</strong></p> 
<p><img alt="" height="459" src="https://images2.imgbox.com/8b/be/zqB8oaFM_o.png" width="1099"></p> 
<h2 id="%C2%A0%E8%BF%AD%E4%BB%A3%E9%80%89%E6%8B%A9"> 迭代选择</h2> 
<p>在迭代特征选择中，将会构建一系列模型，每个模型都使用不同数量的特征。有两种基本方法：开始时没有特征，然后逐个添加特征，直到满足某个终止条件；或者从所有特征开始，然后逐个删除特征，直到满足某个终止条件。由于构建了一系列模型，所以这些方法的计算成本要比前面讨论过的方法更高。</p> 
<p>其中一种特殊方法是递归特征消除（recursive feature elimination， RFE），<strong>在前面也介绍了</strong>，它从所有特征开始构建模型，并根据模型舍弃最不重要的特征，然后使用除被舍弃特征之外的所有特征来构建一个新模型，如此继续，直到仅剩下预设数量的特征。为了让这种方法能够运行，用于选择的模型需要提供某种确定特征重要性的方法，正如基于模型的选择所做的那样。<br>  </p> 
<h3 id="%E6%AF%8F%E6%96%87%E4%B8%80%E8%AF%AD">每文一语</h3> 
<blockquote> 
 <p><strong>坚持走好每一步，探寻机遇才能不慌不乱！加油！</strong></p> 
</blockquote> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>