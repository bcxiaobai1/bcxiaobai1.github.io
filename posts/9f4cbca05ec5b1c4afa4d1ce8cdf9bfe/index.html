<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>sklearn专题一：决策树 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">sklearn专题一：决策树</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%86%B3%E7%AD%96%E6%A0%91-toc" style="margin-left:0px"><a href="#%E5%86%B3%E7%AD%96%E6%A0%91" title="决策树">决策树</a></p> 
<p id="1%20%E6%A6%82%E8%BF%B0-toc" style="margin-left:40px"><a href="#1%20%E6%A6%82%E8%BF%B0" title="1 概述">1 概述</a></p> 
<p id="1.1%20%E5%86%B3%E7%AD%96%E6%A0%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84-toc" style="margin-left:80px"><a href="#1.1%20%E5%86%B3%E7%AD%96%E6%A0%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84" title="1.1 决策树是如何工作的">1.1 决策树是如何工作的</a></p> 
<p id="1.2%20sklearn%E4%B8%AD%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91-toc" style="margin-left:80px"><a href="#1.2%20sklearn%E4%B8%AD%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91" title="1.2 sklearn中的决策树">1.2 sklearn中的决策树</a></p> 
<p id="2%C2%A0%20DecisionTreeClassi%EF%AC%81er%E4%B8%8E%E7%BA%A2%E9%85%92%E6%95%B0%E6%8D%AE%E9%9B%86-toc" style="margin-left:40px"><a href="#2%C2%A0%20DecisionTreeClassi%EF%AC%81er%E4%B8%8E%E7%BA%A2%E9%85%92%E6%95%B0%E6%8D%AE%E9%9B%86" title="2  DecisionTreeClassiﬁer与红酒数据集">2  DecisionTreeClassiﬁer与红酒数据集</a></p> 
<p id="2.1%C2%A0%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0-toc" style="margin-left:80px"><a href="#2.1%C2%A0%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0" title="2.1 重要参数">2.1 重要参数</a></p> 
<p id="2.1.2%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%C2%A0random_state%20%26%C2%A0splitter-toc" style="margin-left:80px"><a href="#2.1.2%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%C2%A0random_state%20%26%C2%A0splitter" title="2.1.2​​​​​​​ random_state &amp; splitter">2.1.2 random_state &amp; splitter</a></p> 
<p id="2.1.3%C2%A0%E5%89%AA%E6%9E%9D%E5%8F%82%E6%95%B0-toc" style="margin-left:80px"><a href="#2.1.3%C2%A0%E5%89%AA%E6%9E%9D%E5%8F%82%E6%95%B0" title="2.1.3 剪枝参数">2.1.3 剪枝参数</a></p> 
<p id="2.1.4%C2%A0%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E7%9B%AE%E6%A0%87%E6%9D%83%E9%87%8D%E5%8F%82%E6%95%B0-toc" style="margin-left:80px"><a href="#2.1.4%C2%A0%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E7%9B%AE%E6%A0%87%E6%9D%83%E9%87%8D%E5%8F%82%E6%95%B0" title="2.1.4 目标权重参数">2.1.4 目标权重参数</a></p> 
<p id="%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B2.2%C2%A0%E9%87%8D%E8%A6%81%E5%B1%9E%E6%80%A7%E5%92%8C%E6%8E%A5%E5%8F%A3-toc" style="margin-left:80px"><a href="#%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B2.2%C2%A0%E9%87%8D%E8%A6%81%E5%B1%9E%E6%80%A7%E5%92%8C%E6%8E%A5%E5%8F%A3" title="2.2 重要属性和接口">2.2 重要属性和接口</a></p> 
<p id="3.%20DecisionTreeRegressor-toc" style="margin-left:40px"><a href="#3.%20DecisionTreeRegressor" title="3. DecisionTreeRegressor">3. DecisionTreeRegressor</a></p> 
<p id="3.1%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%EF%BC%8C%E5%B1%9E%E6%80%A7%E5%8F%8A%E6%8E%A5%E5%8F%A3-toc" style="margin-left:80px"><a href="#3.1%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%EF%BC%8C%E5%B1%9E%E6%80%A7%E5%8F%8A%E6%8E%A5%E5%8F%A3" title="3.1重要参数，属性及接口">3.1重要参数，属性及接口</a></p> 
<p id="%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B3.2%C2%A0%E5%AE%9E%E4%BE%8B%EF%BC%9A%E4%B8%80%E7%BB%B4%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BB%98%E5%88%B6%C2%A0-toc" style="margin-left:80px"><a href="#%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B3.2%C2%A0%E5%AE%9E%E4%BE%8B%EF%BC%9A%E4%B8%80%E7%BB%B4%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BB%98%E5%88%B6%C2%A0" title="3.2 实例：一维回归的图像绘制 ">3.2 实例：一维回归的图像绘制 </a></p> 
<p id="4%C2%A0%E5%AE%9E%E4%BE%8B%EF%BC%9A%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E5%B9%B8%E5%AD%98%E8%80%85%E7%9A%84%E9%A2%84%E6%B5%8B-toc" style="margin-left:40px"><a href="#4%C2%A0%E5%AE%9E%E4%BE%8B%EF%BC%9A%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E5%B9%B8%E5%AD%98%E8%80%85%E7%9A%84%E9%A2%84%E6%B5%8B" title="4 实例：泰坦尼克号幸存者的预测">4 实例：泰坦尼克号幸存者的预测</a></p> 
<p id="5.%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9-toc" style="margin-left:40px"><a href="#5.%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9" title="5.决策树的优缺点">5.决策树的优缺点</a></p> 
<hr id="hr-toc">
<h1 id="%E5%86%B3%E7%AD%96%E6%A0%91"><span style="color:#333333"><strong>决策树</strong></span></h1> 
<div> 
 <h2 id="1%20%E6%A6%82%E8%BF%B0">
<span style="color:#333333"><strong>1 </strong></span><span style="color:#333333"><strong>概述</strong></span>
</h2> 
 <h3 id="1.1%20%E5%86%B3%E7%AD%96%E6%A0%91%E6%98%AF%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%E7%9A%84">
<span style="color:#333333"><strong>1.1 </strong></span><span style="color:#333333"><strong>决策树是如何工作的 </strong></span>
</h3> 
 <div> 
  <div> 
   <div> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">决策树（</span><span style="color:#333333">Decision     Tree</span><span style="color:#333333">）是一种非参数的有监督学习方法，它能够从一系列有特征和标签的数据中总结出决策规则，并用树状图的结构来呈现这些规则，以解决分类和回归问题。决策树算法容易理解，适用各种数据，在解决各  </span><span style="color:#333333">种问题时都有良好表现，尤其是以树模型为核心的各种集成算法，在各个行业和领域都有广泛的应用。</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">我们来简单了解一下决策树是如何工作的。决策树算法的本质是一种图结构，我们只需要问一系列问题就可以对数 </span> <span style="color:#333333">据进行分类了。比如说，来看看下面这组数据集，这是一系列已知物种以及所属类别的数据：</span></p> 
    <p style="margin-left:0;text-align:left"><img alt="" height="272" src="https://images2.imgbox.com/e0/96/DTEBrtzU_o.png" width="648"></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">我们现在的目标是，将动物们分为哺乳类和非哺乳类。那根据已经收集到的数据，决策树算法为我们算出了下面的 </span> <span style="color:#333333">这棵决策树：</span></p> 
    <p style="text-align:center"><img alt="" height="257" src="https://images2.imgbox.com/3e/a4/Q5BbRlm5_o.png" width="328"></p> 
    <p> <span style="color:#333333">假如我们现在发现了一种新物种Python，它是冷血动物，体表带鳞片，并且不是胎生，我们就可以通过这棵决策 </span> <span style="color:#333333">树来判断它的所属类别。</span></p> 
   </div>   
   <div> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">可以看出，在这个决策过程中，我们一直在对记录的特征进行提问。最初的问题所在的地方叫做</span><strong><span style="color:#333333">根节点</span></strong><span style="color:#333333">，在得到结 </span> <span style="color:#333333">论前的每一个问题都是</span><strong><span style="color:#333333">中间节点</span></strong><span style="color:#333333">，而得到的每一个结论（动物的类别）都叫做</span><strong><span style="color:#333333">叶子节点</span></strong><span style="color:#333333">。</span></p> 
    <p style="margin-left:0;text-align:left"><strong><span style="color:#333333">关键概念：节点</span></strong></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">根节点：没有进边，有出边。包含最初的，针对特征的提问。</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">中间节点：既有进边也有出边，进边只有一条，出边可以有很多条。都是针对特征的提问。 </span><span style="color:#333333">叶子节点：有进边，没有出边，</span><strong><span style="color:#333333">每个叶子节点都是一个类别标签</span></strong><span style="color:#333333">。</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">*</span><span style="color:#333333">子节点和父节点：在两个相连的节点中，更接近根节点的是父节点，另一个是子节点。</span></p> 
    <p style="margin-left:0;text-align:left"><strong><strong><span style="color:#333333"><strong>决策树算法的核心是要解决两个问题：</strong></span></strong></strong></p> 
    <ol>
<li style="text-align:left"><strong><span style="color:#333333">如何从数据表中找出最佳节点和最佳分枝？</span></strong></li>
<li style="text-align:left"><strong><span style="color:#333333">如何让决策树停止生长，防止过拟合？</span></strong></li>
</ol>
    <p style="margin-left:0;text-align:justify"><span style="color:#333333">几乎所有决策树有关的模型调整方法，都围绕这两个问题展开。这两个问题背后的原理十分复杂，我们会在讲解模 </span> <span style="color:#333333">型参数和属性的时候为大家简单解释涉及到的部分。在这门课中，我会尽量避免让大家太过深入到决策树复杂的原 </span> <span style="color:#333333">理和数学公式中（尽管决策树的原理相比其他高级的算法来说是非常简单了），</span><span style="color:#333333">这门课会专注于实践和应用。如果 </span> <span style="color:#333333">大家希望理解更深入的细节，建议大家在听这门课之前还是先去阅读和学习一下决策树的原理。</span></p> 
    <p style="margin-left:0;text-align:left"></p> 
    <h3 id="1.2%20sklearn%E4%B8%AD%E7%9A%84%E5%86%B3%E7%AD%96%E6%A0%91">1.2<strong> <strong><span style="color:#333333"><strong>sklearn</strong></span></strong><strong><span style="color:#333333"><strong>中的决策树</strong></span></strong></strong>
</h3> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">模块</span><span style="color:#333333">sklearn.tree</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">sklearn</span><span style="color:#333333">中决策树的类都在</span><span style="color:#333333">”tree“</span><span style="color:#333333">这个模块之下。这个模块总共包含五个类：</span></p> 
    <table border="1" cellspacing="0" style="margin-left:11.9pt"><tbody>
<tr>
<td style="background-color:#f8f8f8;width:207.85pt"> <p><strong><span style="color:#333333">tree.DecisionTreeClassiﬁer</span></strong></p> </td>
<td style="background-color:#f8f8f8;width:281.4pt"> <p><strong><span style="color:#333333">分类树</span></strong></p> </td>
</tr>
<tr>
<td style="vertical-align:top;width:207.85pt"> <p><strong><span style="color:#333333">tree.DecisionTreeRegressor</span></strong></p> </td>
<td style="vertical-align:top;width:281.4pt"> <p><strong><span style="color:#333333">回归树</span></strong></p> </td>
</tr>
<tr>
<td style="background-color:#f8f8f8;width:207.85pt"> <p><strong><span style="color:#333333">tree.export_graphviz</span></strong></p> </td>
<td style="background-color:#f8f8f8;width:281.4pt"> <p><strong><span style="color:#333333">将生成的决策树导出为</span></strong><strong><span style="color:#333333">DOT</span></strong><strong><span style="color:#333333">格式，画图专用</span></strong></p> </td>
</tr>
<tr>
<td style="vertical-align:top;width:207.85pt"> <p><strong><span style="color:#333333">tree.ExtraTreeClassiﬁer</span></strong></p> </td>
<td style="vertical-align:top;width:281.4pt"> <p><strong><span style="color:#333333">高随机版本的分类树</span></strong></p> </td>
</tr>
<tr>
<td style="background-color:#f8f8f8;width:207.85pt"> <p><strong><span style="color:#333333">tree.ExtraTreeRegressor</span></strong></p> </td>
<td style="background-color:#f8f8f8;width:281.4pt"> <p><strong><span style="color:#333333">高随机版本的回归树</span></strong></p> </td>
</tr>
</tbody></table>
    <p style="margin-left:0;text-align:left"><span style="color:#333333">我们会主要讲解分类树和回归树，并用图像呈现给大家。</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">sklearn</span><span style="color:#333333">的基本建模流程</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">在那之前，我们先来了解一下</span><span style="color:#333333">sklearn</span><span style="color:#333333">建模的基本流程。</span></p> 
   </div> 
   <p style="text-align:center"><img alt="" height="144" src="https://images2.imgbox.com/af/a4/rEFzgpZy_o.png" width="445"></p>   
   <div> 
    <p style="margin-left:0;text-align:left"></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">在这个流程下，分类树对应的代码是：</span></p> 
    <pre><code class="language-python">from sklearn import tree #导入需要的模块
clf = tree.DecisionTreeClassifier()     #实例化
clf = clf.fit(X_train,y_train) #用训练集数据训练模型
result = clf.score(X_test,y_test) #导入测试集，从接口中调用需要的信息</code></pre> 
    <h2 id="2%C2%A0%20DecisionTreeClassi%EF%AC%81er%E4%B8%8E%E7%BA%A2%E9%85%92%E6%95%B0%E6%8D%AE%E9%9B%86" style="text-align:left"><strong>2  <strong><span style="color:#333333"><strong>DecisionTreeClassiﬁer</strong></span></strong><strong><span style="color:#333333"><strong>与红酒数据集</strong></span></strong></strong></h2> 
    <p style="margin-left:0;text-align:left"></p> 
    <div>
     <span style="color:#333333"><em>class </em></span>
     <span style="color:#333333">sklearn.tree.DecisionTreeClassifier </span>
     <span style="color:#333333">(</span>
     <span style="color:#333333"><em>criterion=’gini’</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>splitter=’best’</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>max_depth=None</em></span>
     <span style="color:#333333">, </span>
    </div> 
    <div>
     <span style="color:#333333"><em>min_samples_split=2</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>min_samples_leaf=1</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>min_weight_fraction_leaf=0.0</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>max_features=None</em></span>
     <span style="color:#333333">, </span>
    </div> 
    <div>
     <span style="color:#333333"><em>random_state=None</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>max_leaf_nodes=None</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>min_impurity_decrease=0.0</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>min_impurity_split=None</em>, <em>class_weight=None</em></span>
     <span style="color:#333333">, </span>
     <span style="color:#333333"><em>presort=False</em></span>
     <span style="color:#333333">) </span>
    </div> 
    <p style="margin-left:0;text-align:left"></p> 
    <h3 id="2.1%C2%A0%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0" style="margin-left:0px;text-align:justify">2.1 <strong><strong><span style="color:#333333"><strong>重要参数</strong></span></strong></strong><strong> </strong>
</h3> 
    <p style="margin-left:0;text-align:justify"><strong><strong><span style="color:#333333"><strong>criterion</strong></span></strong></strong></p> 
    <p style="margin-left:0;text-align:justify"><span style="color:#333333">为了要将表格转化为一棵树，决策树需要找出最佳节点和最佳的分枝方法，对分类树来说，衡量这个</span><span style="color:#333333">“</span><span style="color:#333333">最佳</span><span style="color:#333333">”</span><span style="color:#333333">的指标   </span><span style="color:#333333">叫做</span><span style="color:#333333">“</span><span style="color:#333333">不纯度</span><span style="color:#333333">”</span><span style="color:#333333">。通常来说，不纯度越低，决策树对训练集的拟合越好。现在使用的决策树算法在分枝方法上的核心   </span><span style="color:#333333">大多是围绕在对某个不纯度相关指标的最优化上。</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">不纯度基于节点来计算，树中的每个节点都会有一个不纯度，并且子节点的不纯度一定是低于父节点的，也就是  </span><span style="color:#333333">说，在同一棵决策树上，叶子节点的不纯度一定是最低的。</span></p> 
    <p style="margin-left:0;text-align:left"><span style="color:#333333">Criterion</span><span style="color:#333333">这个参数正是用来决定不纯度的计算方法的。</span><span style="color:#333333">sklearn</span><span style="color:#333333">提供了两种选择： </span><span style="color:#333333">1</span><span style="color:#333333">）输入</span><span style="color:#333333">”entropy“</span><span style="color:#333333">，使用</span><strong><span style="color:#333333">信息熵</span></strong><span style="color:#333333">（</span><span style="color:#333333">Entropy</span><span style="color:#333333">）</span></p> 
    <ol>
<li style="text-align:justify">
<span style="color:#333333">输入</span><span style="color:#333333">”gini“</span><span style="color:#333333">，使用</span><strong><span style="color:#333333">基尼系数</span></strong><span style="color:#333333">（</span><span style="color:#333333">Gini</span> <span style="color:#333333">Impurity</span><span style="color:#333333">）</span>
</li>
<li style="text-align:justify">
<span style="color:#333333">输入</span><span style="color:#333333">”gini“</span><span style="color:#333333">，使用</span><span style="color:#333333"><strong>基尼系数</strong></span><span style="color:#333333">（</span><span style="color:#333333">Gini Impurity</span><span style="color:#333333">） </span>
</li>
</ol>
    <p style="text-align:center"><img alt="" height="114" src="https://images2.imgbox.com/29/db/oy5hsUtQ_o.png" width="248"></p> 
   </div> 
   <p style="margin-left:0;text-align:justify"><span style="color:#333333">其中</span><span style="color:#333333">t</span><span style="color:#333333">代表给定的节点，i代表标签的任意分类，</span><img alt="" height="15" src="https://images2.imgbox.com/44/16/qqt7mJyF_o.png" width="33"><span style="color:#333333">代表标签分类i在节点</span><span style="color:#333333">t</span><span style="color:#333333">上所占的比例。注意，当使用信息熵</span><span style="color:#333333">时，</span><span style="color:#333333">sklearn</span><span style="color:#333333">实际计算的是基于信息熵的信息增益</span><span style="color:#333333">(Information</span> <span style="color:#333333">Gain)</span><span style="color:#333333">，即父节点的信息熵和子节点的信息熵之差。</span> </p> 
   <p style="margin-left:0;text-align:justify"><span style="color:#333333">比起基尼系数，信息熵对不纯度更加敏感，对不纯度的惩罚最强。但是</span><strong><span style="color:#333333">在实际使用中，信息熵和基尼系数的效果基 </span></strong><strong> </strong><strong><span style="color:#333333">本相同。</span></strong><span style="color:#333333">信息熵的计算比基尼系数缓慢一些，因为基尼系数的计算不涉及对数。另外，因为信息熵对不纯度更加敏 </span> <span style="color:#333333">感，所以信息熵作为指标时，决策树的生长会更加</span><span style="color:#333333">“</span><span style="color:#333333">精细</span><span style="color:#333333">”</span><span style="color:#333333">，因此对于高维数据或者噪音很多的数据，信息熵很容易   </span><span style="color:#333333">过拟合，基尼系数在这种情况下效果往往比较好。当模型拟合程度不足的时候，即当模型在训练集和测试集上都表 </span> <span style="color:#333333">现不太好的时候，使用信息熵。当然，这些不是绝对的。</span></p> 
   <table border="1" cellpadding="1" cellspacing="1" style="width:590px"><tbody>
<tr>
<td style="background-color:#f8f8f8;width:143px"> <p><strong><span style="color:#333333">参数</span></strong></p> </td>
<td style="background-color:#f8f8f8;width:444px"> <p><strong><span style="color:#333333">criterion</span></strong></p> </td>
</tr>
<tr>
<td style="vertical-align:top;width:143px"> <p><span style="color:#333333">如何影响模型</span><span style="color:#333333">?</span></p> </td>
<td style="vertical-align:top;width:444px"> <p><span style="color:#333333">确定不纯度的计算方法，帮忙找出最佳节点和最佳分枝，不纯度越低，决策树对训练集 </span><span style="color:#333333">的拟合越好</span></p> </td>
</tr>
<tr>
<td style="background-color:#f8f8f8;width:143px"> <p><span style="color:#333333">可能的输入有哪</span><span style="color:#333333">些？</span></p> </td>
<td style="background-color:#f8f8f8;width:444px"> <p><span style="color:#333333">不填默认基尼系数，填写</span><span style="color:#333333">gini</span><span style="color:#333333">使用基尼系数，填写</span><span style="color:#333333">entropy</span><span style="color:#333333">使用信息增益</span></p> </td>
</tr>
<tr>
<td style="vertical-align:top;width:143px"> <p style="margin-left:0pt"></p> <p><span style="color:#333333">怎样选取参数？</span></p> </td>
<td style="vertical-align:top;width:444px"> <p><span style="color:#333333">通常就使用基尼系数</span></p> <p><span style="color:#333333">数据维度很大，噪音很大时使用基尼系数</span></p> <p><span style="color:#333333">维度低，数据比较清晰的时候，信息熵和基尼系数没区别</span><span style="color:#333333">当决策树的拟合程度不够的时候，使用信息熵</span></p> <p><span style="color:#333333">两个都试试，不好就换另外一个</span></p> </td>
</tr>
</tbody></table>
  </div> 
  <p><span style="color:#333333">到这里，决策树的基本流程其实可以简单概括如下：</span> </p> 
  <div>
   <img alt="" height="74" src="https://images2.imgbox.com/7f/0b/ApBP204w_o.png" width="640">
  </div> 
  <p></p> 
  <p><span style="color:#333333">直到没有更多的特征可用，或整体的不纯度指标已经最优，决策树就会停止生长。</span></p> 
  <p> <strong><span style="color:#333333">建立一棵树</span></strong></p> 
  <p><strong>1.<span style="color:#333333">导入需要的算法库和模块</span></strong></p> 
  <pre><code class="language-python"># 导入需要的算法库和模块
from sklearn import tree
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
import graphviz</code></pre> 
  <p><strong>2.<span style="color:#333333">探索数据</span></strong></p> 
  <pre><code class="language-python"># 探索数据，也就是查看数据长什么样子
wine = load_wine()
print(wine.data.shape)   #(178, 13)
print(wine.target)</code></pre> 
  <p>输出</p> 
  <pre><code class="language-python">(178, 13)
[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]</code></pre> 
  <pre><code class="language-python"># 查看表长什么样子
import pandas as pd
table=pd.concat([pd.DataFrame(wine.data),pd.DataFrame(wine.target)],axis=1)
print(table)</code></pre> 
  <p><img alt="" height="406" src="https://images2.imgbox.com/ae/06/L8RMbSUn_o.png" width="833"></p> 
  <pre><code class="language-python">#查看表头的名字
print(wine.feature_names)
#查看表的分类
print(wine.target_names)</code></pre> 
  <pre><code class="language-python">['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']
['class_0' 'class_1' 'class_2']</code></pre> 
  <p>3.​​​​​​​<strong><span style="color:#333333">分训练集和测试集</span></strong></p> 
  <pre><code class="language-python">Xtrain, Xtest, Ytrain, Ytest = train_test_split(wine.data,wine.target,test_size=0.3)
print(Xtrain.shape)  #(124, 13)
print(Xtest.shape)   #(54, 13)</code></pre> 
  <p>4.​​​​​​​<strong><span style="color:#333333">建立模型</span></strong></p> 
  <pre><code class="language-python">clf = tree.DecisionTreeClassifier(criterion="entropy")
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest) #返回预测的准确度
print(score)</code></pre> 
  <p style="margin-left:.0001pt;text-align:justify"><strong><span style="color:#333333">5.画出一棵树吧</span></strong></p> 
  <pre><code class="language-python">feature_name = ['酒精','苹果酸','灰','灰的碱性','镁','总酚','类黄酮','非黄烷类酚类','花青素','颜色强度','色调','od280/od315稀释葡萄酒','脯氨酸']
import graphviz
dot_data = tree.export_graphviz(clf
                                ,out_file = None
                                ,feature_names = feature_name
                                ,class_names=["琴酒","雪莉","贝尔摩德"]
                                ,filled=True  #颜色填充
                                ,rounded=True   #框框的圆角
                               ) 
graph = graphviz.Source(dot_data.replace("helvetica", "MicrosoftYaHei"))
graph.view()</code></pre> 
  <p><img alt="" height="821" src="https://images2.imgbox.com/c2/f1/XQvov9k6_o.png" width="1059"></p> 
  <p><strong>​​​​​​​6.<span style="color:#333333">探索决策树</span></strong></p> 
  <pre><code class="language-python">#特征重要性

clf.feature_importances_</code></pre> 
  <pre><code class="language-python">array([0.02089151, 0.01416599, 0.        , 0.03389895, 0.        ,
       0.        , 0.44769643, 0.        , 0.        , 0.17234392,
       0.        , 0.        , 0.3110032 ])</code></pre> 
  <pre><code class="language-python">[*zip(feature_name,clf.feature_importances_)]</code></pre> 
  <pre><code class="language-python">[('酒精', 0.020891507777503787),
 ('苹果酸', 0.014165989652962183),
 ('灰', 0.0),
 ('灰的碱性', 0.033898949274323104),
 ('镁', 0.0),
 ('总酚', 0.0),
 ('类黄酮', 0.4476964329287443),
 ('非黄烷类酚类', 0.0),
 ('花青素', 0.0),
 ('颜色强度', 0.17234391840479193),
 ('色调', 0.0),
 ('od280/od315稀释葡萄酒', 0.0),
 ('脯氨酸', 0.31100320196167475)]</code></pre> 
  <p style="margin-left:0;text-align:left"><span style="color:#333333">我们已经在只了解一个参数的情况下，建立了一棵完整的决策树。但是回到步骤</span><span style="color:#333333">4</span><span style="color:#333333">建立模型，</span><span style="color:#333333">score</span><span style="color:#333333">会在某个值附近   </span><span style="color:#333333">波动，引起步骤</span><span style="color:#333333">5</span><span style="color:#333333">中画出来的每一棵树都不一样。它为什么会不稳定呢？如果使用其他数据集，它还会不稳定吗？</span></p> 
  <p style="margin-left:0;text-align:left"><span style="color:#333333">我们之前提到过，无论决策树模型如何进化，在分枝上的本质都还是追求某个不纯度相关的指标的优化，而正如我 </span> <span style="color:#333333">们提到的，不纯度是基于节点来计算的，也就是说，决策树在建树时，是靠优化节点来追求一棵优化的树，但最优 </span> <span style="color:#333333">的节点能够保证最优的树吗？集成算法被用来解决这个问题：</span><span style="color:#333333">sklearn</span><span style="color:#333333">表示，既然一棵树不能保证最优，那就建更    </span><span style="color:#333333">多的不同的树，然后从中取最好的。怎样从一组数据集中建不同的树？在每次分枝时，不从使用全部特征，而是随 </span> <span style="color:#333333">机选取一部分特征，从中选取不纯度相关指标最优的作为分枝用的节点。这样，每次生成的树也就不同了。</span></p> 
  <pre><code>clf = tree.DecisionTreeClassifier(criterion="entropy"
                                    ,random_state=30
                                    )
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest)
score</code></pre> 
  <h3 id="2.1.2%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%C2%A0random_state%20%26%C2%A0splitter"><strong>2.1.2​​​​​​​​​​​​​​ <span style="color:#333333">random_state &amp;</span> <span style="color:#333333">splitter</span></strong></h3> 
  <pre><code class="language-python">clf = tree.DecisionTreeClassifier(criterion="entropy"
                                    ,random_state=30
                                    ,splitter="random" 
                                    #splitter也是用来控制决策树中的随机选项，默认best
                                    )
clf = clf.fit(Xtrain, Ytrain)
score = clf.score(Xtest, Ytest)
score

import graphviz
dot_data = tree.export_graphviz(clf
                                ,out_file = None
                                ,feature_names= feature_name
                                ,class_names=["琴酒","雪莉","贝尔摩德"]
                                ,filled=True
                                ,rounded=True
                                )
graph = graphviz.Source(dot_data)
graph</code></pre> 
  <h3 id="2.1.3%C2%A0%E5%89%AA%E6%9E%9D%E5%8F%82%E6%95%B0"><strong>2.1.3 <span style="color:#333333">剪枝参数</span></strong></h3> 
  <p><span style="color:#333333">在不加限制的情况下，一棵决策树会生长到衡量不纯度的指标最优，或者没有更多的特征可用为止。这样的决策树 </span> <span style="color:#333333">往往会过拟合，这就是说，</span><strong><span style="color:#333333">它会在训练集上表现很好，在测试集上却表现糟糕。</span></strong><span style="color:#333333">我们收集的样本数据不可能和整体 </span> <span style="color:#333333">的状况完全一致，因此当一棵决策树对训练数据有了过于优秀的解释性，它找出的规则必然包含了训练样本中的噪 </span> <span style="color:#333333">声，并使它对未知数据的拟合程度不足。</span></p> 
  <p style="text-align:center"><img alt="" height="85" src="https://images2.imgbox.com/55/cc/owrk7y2X_o.png" width="426"></p> 
  <p style="margin-left:0;text-align:left"><span style="color:#333333">为了让决策树有更好的泛化性，我们要对决策树进行剪枝。</span><strong><span style="color:#333333">剪枝策略对决策树的影响巨大，正确的剪枝策略是优化  </span></strong><strong><span style="color:#333333">决策树算法的核心。</span></strong><span style="color:#333333">sklearn</span><span style="color:#333333">为我们提供了不同的剪枝策略：</span></p> 
  <ul><li><strong><strong><span style="color:#333333"><strong>max_depth</strong></span></strong></strong></li></ul>
  <p style="margin-left:0;text-align:left"><span style="color:#333333">限制树的最大深度，超过设定深度的树枝全部剪掉</span></p> 
  <p style="margin-left:0;text-align:justify"><span style="color:#333333">这是用得最广泛的剪枝参数，在高维度低样本量时非常有效。决策树多生长一层，对样本量的需求会增加一倍，所  以限制树深度能够有效地限制过拟合。在集成算法中也非常实用。实际使用时，建议从</span><span style="color:#333333">=3</span><span style="color:#333333">开始尝试，看看拟合的效 </span> <span style="color:#333333">果再决定是否增加设定深度。</span></p> 
  <ul><li><strong><strong><span style="color:#333333"><strong>min_samples_leaf &amp; min_samples_split</strong></span></strong></strong></li></ul>
  <p style="margin-left:0;text-align:left"><span style="color:#333333">min_samples_leaf</span><span style="color:#333333">限定，一个节点在分枝后的每个子节点都必须包含至少</span><span style="color:#333333">min_samples_leaf</span><span style="color:#333333">个训练样本，否则分    </span> <span style="color:#333333">枝就不会发生，或者，分枝会朝着满足每个子节点都包含</span><span style="color:#333333">min_samples_leaf</span><span style="color:#333333">个样本的方向去发生</span></p> 
  <p style="margin-left:0;text-align:justify"><span style="color:#333333">一般搭配</span><span style="color:#333333">max_depth</span><span style="color:#333333">使用，在回归树中有神奇的效果，可以让模型变得更加平滑。这个参数的数量设置得太小会引    </span><span style="color:#333333">起过拟合，设置得太大就会阻止模型学习数据。一般来说，建议从</span><span style="color:#333333">=5</span><span style="color:#333333">开始使用。如果叶节点中含有的样本量变化很  大，建议输入浮点数作为样本量的百分比来使用。同时，这个参数可以保证每个叶子的最小尺寸，可以在回归问题  </span><span style="color:#333333">中避免低方差，过拟合的叶子节点出现。对于类别不多的分类问题，</span><span style="color:#333333">=1</span><span style="color:#333333">通常就是最佳选择。</span></p> 
  <p style="margin-left:0;text-align:left"><span style="color:#333333">min_samples_split</span><span style="color:#333333">限定，一个节点必须要包含至少</span><span style="color:#333333">min_samples_split</span><span style="color:#333333">个训练样本，这个节点才允许被分枝，否则    </span> <span style="color:#333333">分枝就不会发生。</span></p> 
  <pre><code class="language-python">clf = tree.DecisionTreeClassifier(criterion="entropy"
                                    ,random_state=30
                                    ,splitter="random"
                                    ,max_depth=3
                                    ,min_samples_leaf=10
                                    ,min_samples_split=10
                                )

clf = clf.fit(Xtrain, Ytrain)

dot_data = tree.export_graphviz(clf
                                ,out_file = None
                                ,feature_names= feature_name
                                ,class_names=["琴酒","雪莉","贝尔摩德"]
                                ,filled=True
                                ,rounded=True
                                )
graph = graphviz.Source(dot_data.replace("helvetica", "MicrosoftYaHei"))
graph.view()</code></pre> 
  <p><img alt="" height="459" src="https://images2.imgbox.com/dc/32/D9YlX5fJ_o.png" width="946"></p> 
  <ul><li><strong><strong><span style="color:#333333"><strong>max_features &amp; min_impurity_decrease</strong></span></strong></strong></li></ul>
  <p style="margin-left:0;text-align:left"><span style="color:#333333">一般</span><span style="color:#333333">max_depth</span><span style="color:#333333">使用，用作树的</span><span style="color:#333333">”</span><span style="color:#333333">精修</span><span style="color:#333333">“</span></p> 
  <p style="margin-left:0;text-align:left"><span style="color:#333333">max_features</span><span style="color:#333333">限制分枝时考虑的特征个数，超过限制个数的特征都会被舍弃。和</span><span style="color:#333333">max_depth</span><span style="color:#333333">异曲同工， </span><span style="color:#333333">max_features</span><span style="color:#333333">是用来限制高维度数据的过拟合的剪枝参数，但其方法比较暴力，是直接限制可以使用的特征数量     </span><span style="color:#333333">而强行使决策树停下的参数，在不知道决策树中的各个特征的重要性的情况下，强行设定这个参数可能会导致模型  </span><span style="color:#333333">学习不足。如果希望通过降维的方式防止过拟合，建议使用</span><span style="color:#333333">PCA</span><span style="color:#333333">，</span><span style="color:#333333">ICA</span><span style="color:#333333">或者特征选择模块中的降维算法。</span></p> 
  <p style="margin-left:0;text-align:left"><span style="color:#333333">min_impurity_decrease</span><span style="color:#333333">限制信息增益的大小，信息增益小于设定数值的分枝不会发生。这是在</span><span style="color:#333333">0.19</span><span style="color:#333333">版本中更新的    </span><span style="color:#333333">功能，在</span><span style="color:#333333">0.19</span><span style="color:#333333">版本之前时使用</span><span style="color:#333333">min_impurity_split</span><span style="color:#333333">。</span></p> 
  <p style="margin-left:0;text-align:left"></p> 
  <ul><li><strong><strong><span style="color:#333333"><strong>确认最优的剪枝参数</strong></span></strong></strong></li></ul>
  <p style="margin-left:0;text-align:justify"><span style="color:#333333">那具体怎么来确定每个参数填写什么值呢？这时候，我们就要使用确定超参数的曲线来进行判断了，继续使用我们  已经训练好的决策树模型</span><span style="color:#333333">clf</span><span style="color:#333333">。超参数的学习曲线，是一条以超参数的取值为横坐标，模型的度量指标为纵坐标的曲   </span><span style="color:#333333">线，它是用来衡量不同超参数取值下模型的表现的线。在我们建好的决策树里，我们的模型度量指标就是</span><span style="color:#333333">score</span><span style="color:#333333">。</span></p> 
  <p style="margin-left:0;text-align:justify"></p> 
  <pre><code class="language-python">import matplotlib.pyplot as plt
%matplotlib inline
test = []
for i in range(10):
    clf = tree.DecisionTreeClassifier(max_depth=i+1
                                      ,criterion="entropy"
                                      ,random_state=30
                                      ,splitter="random"
                                      )
    clf = clf.fit(Xtrain, Ytrain)
    score = clf.score(Xtest, Ytest)
    test.append(score)
plt.plot(range(1,11),test,color="red",label="max_depth")
plt.legend()
plt.show()</code></pre> 
  <p> <img alt="" height="279" src="https://images2.imgbox.com/9f/41/686hgpNU_o.png" width="425"></p> 
 </div> 
</div> 
<h3 id="2.1.4%C2%A0%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E7%9B%AE%E6%A0%87%E6%9D%83%E9%87%8D%E5%8F%82%E6%95%B0">2.1.4 ​​​​​​​<strong><strong><span style="color:#333333"><strong>目标权重参数</strong></span></strong></strong>
</h3> 
<p></p> 
<p><strong><strong><span style="color:#333333"><strong>class_weight &amp; min_weight_fraction_leaf</strong></span></strong></strong></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">完成样本标签平衡的参数。样本不平衡是指在一组数据集中，标签的一类天生占有很大的比例。比如说，在银行要 </span> <span style="color:#333333">判断</span><span style="color:#333333">“</span><span style="color:#333333">一个办了信用卡的人是否会违约</span><span style="color:#333333">”</span><span style="color:#333333">，就是是</span><span style="color:#333333">vs</span><span style="color:#333333">否（</span><span style="color:#333333">1%</span><span style="color:#333333">：</span><span style="color:#333333">99%</span><span style="color:#333333">）的比例。这种分类状况下，即便模型什么也不     做，全把结果预测成</span><span style="color:#333333">“</span><span style="color:#333333">否</span><span style="color:#333333">”</span><span style="color:#333333">，正确率也能有</span><span style="color:#333333">99%</span><span style="color:#333333">。因此我们要使用</span><span style="color:#333333">class_weight</span><span style="color:#333333">参数对样本标签进行一定的均衡，给    少量的标签更多的权重，让模型更偏向少数类，向捕获少数类的方向建模。该参数默认</span><span style="color:#333333">None</span><span style="color:#333333">，此模式表示自动给    </span><span style="color:#333333">与数据集中的所有标签相同的权重。</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">有了权重之后，样本量就不再是单纯地记录数目，而是受输入的权重影响了，因此这时候剪枝，就需要搭配</span><span style="color:#333333">min_ </span><span style="color:#333333">weight_fraction_leaf</span><span style="color:#333333">这个基于权重的剪枝参数来使用。另请注意，基于权重的剪枝参数（例如</span><span style="color:#333333">min_weight_ </span><span style="color:#333333">fraction_leaf</span><span style="color:#333333">）</span><span style="color:#333333">将比不知道样本权重的标准（比如</span><span style="color:#333333">min_samples_leaf</span><span style="color:#333333">）</span><span style="color:#333333">更少偏向主导类。如果样本是加权的，则使    </span> <span style="color:#333333">用基于权重的预修剪标准来更容易优化树结构，这确保叶节点至少包含样本权重的总和的一小部分。</span></p> 
<p style="margin-left:0;text-align:left"></p> 
<h3 id="%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B2.2%C2%A0%E9%87%8D%E8%A6%81%E5%B1%9E%E6%80%A7%E5%92%8C%E6%8E%A5%E5%8F%A3">​​​​​​​2.2 <strong><strong><span style="color:#333333"><strong>重要属性和接口</strong></span></strong></strong>
</h3> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">属性是在模型训练之后，能够调用查看的模型的各种性质。对决策树来说，最重要的是</span><span style="color:#333333">feature_importances_</span><span style="color:#333333">，能     </span><span style="color:#333333">够查看各个特征对模型的重要性。</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">sklearn</span><span style="color:#333333">中许多算法的接口都是相似的，比如说我们之前已经用到的</span><span style="color:#333333">ﬁt</span><span style="color:#333333">和</span><span style="color:#333333">score</span><span style="color:#333333">，几乎对每个算法都可以使用。除了    </span><span style="color:#333333">这两个接口之外，决策树最常用的接口还有</span><span style="color:#333333">apply</span><span style="color:#333333">和</span><span style="color:#333333">predict</span><span style="color:#333333">。</span><span style="color:#333333">apply</span><span style="color:#333333">中输入测试集返回每个测试样本所在的叶子节</span><span style="color:#333333">点的索引，</span><span style="color:#333333">predict</span><span style="color:#333333">输入测试集返回每个测试样本的标签。返回的内容一目了然并且非常容易，大家感兴趣可以自己    </span><span style="color:#333333">下去试试看。</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">在这里不得不提的是，</span><strong><span style="color:#333333">所有接口中要求输入</span></strong><strong><span style="color:#333333">X_train</span></strong><strong><span style="color:#333333">和</span></strong><strong><span style="color:#333333">X_test</span></strong><strong><span style="color:#333333">的部分，输入的特征矩阵必须至少是一个二维矩阵。</span></strong><strong><span style="color:#333333">sklearn</span></strong><strong><span style="color:#333333">不接受任何一维矩阵作为特征矩阵被输入。</span></strong><span style="color:#333333">如果你的数据的确只有一个特征，那必须用</span><span style="color:#333333">reshape(-1,1)</span><span style="color:#333333">来给    </span><span style="color:#333333">矩阵增维；如果你的数据只有一个特征和一个样本，使用</span><span style="color:#333333">reshape(1,-1)</span><span style="color:#333333">来给你的数据增维。</span></p> 
<pre><code class="language-python">#apply返回每个测试样本所在的叶子节点的索引
clf.apply(Xtest)

#predict返回每个测试样本的分类/回归结果
clf.predict(Xtest)</code></pre> 
<p><img alt="" height="243" src="https://images2.imgbox.com/b5/30/xTwEp66C_o.png" width="740"></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">至此，我们已经学完了分类树</span><span style="color:#333333">DecisionTreeClassiﬁer</span><span style="color:#333333">和用决策树绘图（</span><span style="color:#333333">export_graphviz</span><span style="color:#333333">）</span><span style="color:#333333">的所有基础。我们讲解      </span><span style="color:#333333">了决策树的基本流程，分类树的八个参数，一个属性，四个接口，以及绘图所用的代码。</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333"><strong>八个参数</strong>：</span><span style="color:#333333">Criterion</span><span style="color:#333333">，两个随机性相关的参数（</span><span style="color:#333333">random_state</span><span style="color:#333333">，</span><span style="color:#333333">splitter</span><span style="color:#333333">），五个剪枝参数（</span><span style="color:#333333">max_depth, min_samples_split</span><span style="color:#333333">，</span><span style="color:#333333">min_samples_leaf</span><span style="color:#333333">，</span><span style="color:#333333">max_feature</span><span style="color:#333333">，</span><span style="color:#333333">min_impurity_decrease</span><span style="color:#333333">）</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333"><strong>一个属性</strong>：</span><span style="color:#333333">feature_importances_</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333"><strong>四个接口</strong>：</span><span style="color:#333333">ﬁt</span><span style="color:#333333">，</span><span style="color:#333333">score</span><span style="color:#333333">，</span><span style="color:#333333">apply</span><span style="color:#333333">，</span><span style="color:#333333">predict</span></p> 
<h2 id="3.%20DecisionTreeRegressor"><strong><span style="color:#333333"><strong>3. DecisionTreeRegressor</strong></span></strong></h2> 
<p style="margin-left:0;text-align:left"><em><span style="color:#333333">class</span></em><em> </em><span style="color:#333333">(</span><em><span style="color:#333333">criterion=’mse’</span></em><span style="color:#333333">, </span><em><span style="color:#333333">splitter=’best’</span></em><span style="color:#333333">,</span> <em><span style="color:#333333">max_depth=None</span></em><span style="color:#333333">,</span></p> 
<p style="margin-left:0;text-align:left"><em><span style="color:#333333">min_samples_split=2</span></em><span style="color:#333333">, </span><em><span style="color:#333333">min_samples_leaf=1</span></em><span style="color:#333333">, </span><em><span style="color:#333333">min_weight_fraction_leaf=0.0</span></em><span style="color:#333333">, </span><em><span style="color:#333333">max_features=None</span></em><span style="color:#333333">, </span><em><span style="color:#333333">random_state=None</span></em><span style="color:#333333">, </span><em><span style="color:#333333">max_leaf_nodes=None</span></em><span style="color:#333333">, </span><em><span style="color:#333333">min_impurity_decrease=0.0</span></em><span style="color:#333333">, </span><em><span style="color:#333333">min_impurity_split=None</span></em><span style="color:#333333">, </span><em><span style="color:#333333">presort=False</span></em><span style="color:#333333">)</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">几乎所有参数，属性及接口都和分类树一模一样。需要注意的是，在回归树种，没有标签分布是否均衡的问题，因</span><span style="color:#333333">此没有</span><span style="color:#333333">class_weight</span><span style="color:#333333">这样的参数。</span></p> 
<h3 id="3.1%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%EF%BC%8C%E5%B1%9E%E6%80%A7%E5%8F%8A%E6%8E%A5%E5%8F%A3" style="margin-left:0px;text-align:left"><span style="color:#333333">3.1重要参数，属性及接口</span></h3> 
<p><strong><strong><span style="color:#333333"><strong>criterion</strong></span></strong></strong></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">回归树衡量分枝质量的指标，支持的标准有三种：</span></p> 
<ol>
<li style="text-align:left">
<span style="color:#333333">输入</span><span style="color:#333333">"mse"</span><span style="color:#333333">使用均方误差</span><span style="color:#333333">mean</span>  <span style="color:#333333">squared</span>  <span style="color:#333333">error(MSE)</span><span style="color:#333333">，父节点和叶子节点之间的均方误差的差额将被用来作为</span><span style="color:#333333">特征选择的标准，这种方法通过使用叶子节点的均值来最小化</span><span style="color:#333333">L2</span><span style="color:#333333">损失</span>
</li>
<li style="text-align:left">
<span style="color:#333333">输入</span><span style="color:#333333">“friedman_mse”</span><span style="color:#333333">使用费尔德曼均方误差，这种指标使用弗里德曼针对潜在分枝中的问题改进后的均方误差 </span><span style="color:#333333">3</span><span style="color:#333333">）输入</span><span style="color:#333333">"mae"</span><span style="color:#333333">使用绝对平均误差</span><span style="color:#333333">MAE</span><span style="color:#333333">（</span><span style="color:#333333">mean</span> <span style="color:#333333">absolute</span> <span style="color:#333333">error</span><span style="color:#333333">），这种指标使用叶节点的中值来最小化</span><span style="color:#333333">L1</span><span style="color:#333333">损失</span>
</li>
</ol>
<p style="margin-left:0;text-align:left"><span style="color:#333333">属性中最重要的依然是</span><span style="color:#333333">feature_importances_</span><span style="color:#333333">，接口依然是</span><span style="color:#333333">apply, ﬁt, predict, score</span><span style="color:#333333">最核心。</span></p> 
<p style="text-align:center"><img alt="" height="67" src="https://images2.imgbox.com/d3/a4/iSr5b8ST_o.png" width="213"></p> 
<p style="margin-left:0;text-align:justify"><span style="color:#333333">其中</span><span style="color:#333333">N</span><span style="color:#333333">是样本数量，</span><span style="color:#333333">i</span><span style="color:#333333">是每一个数据样本，</span><span style="color:#333333">ﬁ</span><span style="color:#333333">是模型回归出的数值，</span><span style="color:#333333">yi</span><span style="color:#333333">是样本点</span><span style="color:#333333">i</span><span style="color:#333333">实际的数值标签。所以</span><span style="color:#333333">MSE</span><span style="color:#333333">的本质，   其实是样本真实数据与回归结果的差异。</span><strong><span style="color:#333333">在回归树中，</span></strong><strong><span style="color:#333333">MSE</span></strong><strong><span style="color:#333333">不只是我们的分枝质量衡量指标，也是我们最常用的衡  </span></strong><strong><span style="color:#333333">量回归树回归质量的指标</span></strong><span style="color:#333333">，当我们在使用交叉验证，或者其他方式获取回归树的结果时，我们往往选择均方误差作  为我们的评估（在分类树中这个指标是</span><span style="color:#333333">score</span><span style="color:#333333">代表的预测准确率）。在回归中，我们追求的是，</span><span style="color:#333333">MSE</span><span style="color:#333333">越小越好。</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">然而，</span><strong><span style="color:#333333">回归树的接口</span></strong><strong><span style="color:#333333">score</span></strong><strong><span style="color:#333333">返回的是</span></strong><strong><span style="color:#333333">R</span></strong><strong><span style="color:#333333">平方，并不是</span></strong><strong><span style="color:#333333">MSE</span></strong><span style="color:#333333">。</span><span style="color:#333333">R</span><span style="color:#333333">平方被定义如下：</span></p> 
<p style="text-align:center"><img alt="" height="99" src="https://images2.imgbox.com/a1/d7/xJ1kMr5b_o.png" width="313"></p> 
<p></p> 
<p style="margin-left:0;text-align:justify"><span style="color:#333333">其中</span><span style="color:#333333">u</span><span style="color:#333333">是残差平方和（</span><span style="color:#333333">MSE</span> <span style="color:#333333">*</span>  <span style="color:#333333">N</span><span style="color:#333333">），</span><span style="color:#333333">v</span><span style="color:#333333">是总平方和，</span><span style="color:#333333">N</span><span style="color:#333333">是样本数量，</span><span style="color:#333333">i</span><span style="color:#333333">是每一个数据样本，</span><span style="color:#333333">ﬁ</span><span style="color:#333333">是模型回归出的数值，</span><span style="color:#333333">yi </span><span style="color:#333333">是样本点</span><span style="color:#333333">i</span><span style="color:#333333">实际的数值标签。</span><span style="color:#333333">y</span><span style="color:#333333">帽是真实数值标签的平均数。</span><span style="color:#333333">R</span><span style="color:#333333">平方可以为正为负（</span><span style="color:#333333">如果模型的残差平方和远远大于  </span><span style="color:#333333">模型的总平方和，模型非常糟糕，</span><span style="color:#333333">R</span><span style="color:#333333">平方就会为负），而均方误差永远为正。</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">值得一提的是，</span><strong><span style="color:#333333">虽然均方误差永远为正，但是</span></strong><strong><span style="color:#333333">sklearn</span></strong><strong><span style="color:#333333">当中使用均方误差作为评判标准时，却是计算</span></strong><strong><span style="color:#333333">”</span></strong><strong><span style="color:#333333">负均方误      差</span></strong><strong><span style="color:#333333">“</span></strong><strong><span style="color:#333333">（</span></strong><strong><span style="color:#333333">neg_mean_squared_error</span></strong><strong><span style="color:#333333">）</span></strong><span style="color:#333333">。这是因为</span><span style="color:#333333">sklearn</span><span style="color:#333333">在计算模型评估指标的时候，会考虑指标本身的性质，均</span></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">方误差本身是一种误差，所以被</span><span style="color:#333333">sklearn</span><span style="color:#333333">划分为模型的一种损失</span><span style="color:#333333">(loss)</span><span style="color:#333333">，因此在</span><span style="color:#333333">sklearn</span><span style="color:#333333">当中，都以负数表示。真正的    </span><span style="color:#333333">均方误差</span><span style="color:#333333">MSE</span><span style="color:#333333">的数值，其实就是</span><span style="color:#333333">neg_mean_squared_error</span><span style="color:#333333">去掉负号的数字。</span></p> 
<p><strong><strong><span style="color:#333333"><strong>简单看看回归树是怎样工作的</strong></span></strong></strong></p> 
<pre><code class="language-python">from sklearn.datasets import load_boston
from sklearn.model_selection import cross_val_score 
from sklearn.tree import DecisionTreeRegressor</code></pre> 
<pre><code class="language-python">boston = load_boston()
regressor = DecisionTreeRegressor(random_state=0)
cross_val_score(regressor, boston.data, boston.target, cv=10,
    scoring = "neg_mean_squared_error")

#交叉验证cross_val_score的用法</code></pre> 
<blockquote> 
 <ul>
<li>regressor：模型</li>
<li>boston.data：完整数据集</li>
<li>boston.target：完整标签</li>
<li>cv=10：交叉次数</li>
<li>scoring = "neg_mean_squared_error"  ：得分，指定的是负均方误差</li>
</ul>
</blockquote> 
<pre><code class="language-python">array([-16.41568627, -10.61843137, -18.30176471, -55.36803922,
       -16.01470588, -43.57745098, -12.2148    , -95.2186    ,
       -57.764     , -37.9534    ])</code></pre> 
<p style="margin-left:.0001pt;text-align:justify"><span style="color:#333333">交叉验证是用来观察模型的稳定性的一种方法，我们将数据划分为</span><span style="color:#333333">n</span><span style="color:#333333">份，依次使用其中一份作为测试集，其他</span><span style="color:#333333">n-1</span><span style="color:#333333">份 </span> <span style="color:#333333">作为训练集，多次计算模型的精确性来评估模型的平均准确程度。训练集和测试集的划分会干扰模型的结果，因此  </span><span style="color:#333333">用交叉验证</span><span style="color:#333333">n</span><span style="color:#333333">次的结果求出的平均值，是对模型效果的一个更好的度量。</span></p> 
<p style="margin-left:.0001pt;text-align:justify"><img alt="" height="312" src="https://images2.imgbox.com/88/1d/bpKHCw1O_o.png" width="636"></p> 
<h3 id="%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B%E2%80%8B3.2%C2%A0%E5%AE%9E%E4%BE%8B%EF%BC%9A%E4%B8%80%E7%BB%B4%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9B%BE%E5%83%8F%E7%BB%98%E5%88%B6%C2%A0">​​​​​​​3.2 <strong><strong><span style="color:#333333"><strong>实例：一维回归的图像绘制</strong></span></strong></strong> </h3> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">接下来我们到二维平面上来观察决策树是怎样拟合一条曲线的。我们用回归树来拟合正弦曲线，并添加一些噪声来 </span> <span style="color:#333333">观察回归树的表现。</span></p> 
<p style="margin-left:0;text-align:left"><strong>1.导入需要的库</strong></p> 
<pre><code class="language-python">import numpy as np
from sklearn.tree import DecisionTreeRegressor 
import matplotlib.pyplot as plt</code></pre> 
<p><strong>2.创建一条含有噪声的正弦曲线</strong></p> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">在这一步，我们的基本思路是，先创建一组随机的，分布在</span><span style="color:#333333">0~5</span><span style="color:#333333">上的横坐标轴的取值</span><span style="color:#333333">(x)</span><span style="color:#333333">，然后将这一组值放到</span><span style="color:#333333">sin</span><span style="color:#333333">函 </span> <span style="color:#333333">数中去生成纵坐标的值</span><span style="color:#333333">(y)</span><span style="color:#333333">，接着再到</span><span style="color:#333333">y</span><span style="color:#333333">上去添加噪声。全程我们会使用</span><span style="color:#333333">numpy</span><span style="color:#333333">库来为我们生成这个正弦曲线。</span></p> 
<pre><code class="language-python">rng = np.random.RandomState(1)
X = np.sort(5 * rng.rand(80,1), axis=0) 
y = np.sin(X).ravel()
y[::5] += 3 * (0.5 - rng.rand(16))
#np.random.rand(数组结构)，生成随机数组的函数

#了解降维函数ravel()的用法
np.random.random((2,1)) 
np.random.random((2,1)).ravel() 
np.random.random((2,1)).ravel().shape</code></pre> 
<p style="margin-left:0;text-align:left"><strong>3.实例化&amp;训练模型</strong></p> 
<pre><code class="language-python">regr_1 = DecisionTreeRegressor(max_depth=2) 
regr_2 = DecisionTreeRegressor(max_depth=5) 
regr_1.fit(X, y)
regr_2.fit(X, y)</code></pre> 
<p style="margin-left:0;text-align:left"><strong>4.<span style="color:#333333">测试集导入模型，预测结果</span></strong></p> 
<pre><code class="language-python">X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] 
y_1 = regr_1.predict(X_test)
y_2 = regr_2.predict(X_test)</code></pre> 
<pre><code class="language-python">plt.figure()
plt.scatter(X, y, s=20, edgecolor="black",c="darkorange", label="data") 
plt.plot(X_test, y_1, color="cornflowerblue",label="max_depth=2", linewidth=2) 
plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2) 
plt.xlabel("data")
plt.ylabel("target") 
plt.title("Decision Tree Regression") 
plt.legend()
plt.show()</code></pre> 
<p style="text-align:center"><img alt="" height="299" src="https://images2.imgbox.com/75/1b/w7AwUyW2_o.png" width="485"></p> 
<p></p> 
<p style="margin-left:0;text-align:justify"><span style="color:#333333">可见，回归树学习了近似正弦曲线的局部线性回归。我们可以看到，如果树的最大深度（由</span><span style="color:#333333">max_depth</span><span style="color:#333333">参数控制</span><span style="color:#333333">）    </span><span style="color:#333333">设置得太高，则决策树学习得太精细，它从训练数据中学了很多细节，包括噪声得呈现，从而使模型偏离真实的正  </span><span style="color:#333333">弦曲线，形成过拟合。</span></p> 
<h2 id="4%C2%A0%E5%AE%9E%E4%BE%8B%EF%BC%9A%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E5%B9%B8%E5%AD%98%E8%80%85%E7%9A%84%E9%A2%84%E6%B5%8B" style="margin-left:0px;text-align:justify">4 <span style="color:#333333">实例：泰坦尼克号幸存者的预测</span>
</h2> 
<p style="margin-left:0;text-align:left"><span style="color:#333333">泰坦尼克号的沉没是世界上最严重的海难事故之一，今天我们通过分类树模型来预测一下哪些人可能成为幸存者。 </span> <span style="color:#333333">数据集来源</span><a href="https://www.kaggle.com/c/titanic" title="Titanic - Machine Learning from Disaster | Kaggle">Titanic - Machine Learning from Disaster | Kaggle</a></p> 
<p style="margin-left:0;text-align:left"><strong>1.导入所需要的的库</strong></p> 
<pre><code>#1.导入所需要的的库
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split 
from sklearn.model_selection import GridSearchCV 
from sklearn.model_selection import cross_val_score 
import matplotlib.pyplot as plt</code></pre> 
<p><strong>2.导入数据集，探索数据</strong></p> 
<pre><code class="language-python">#2.导入数据集，探索数据
data=pd.read_csv(r"E:数据分析师学习jupyter notebook泰坦尼克号数据.csv",engine='python',index_col=0)
data.head()</code></pre> 
<p><img alt="" height="540" src="https://images2.imgbox.com/ea/2a/7hc32ybt_o.png" width="1068"></p> 
<pre><code class="language-python">data.info()</code></pre> 
<p> ​​​​​​​<img alt="" height="305" src="https://images2.imgbox.com/6a/21/GAmmjRKq_o.png" width="1067"></p> 
<p><strong>3.对数据集进行预处理</strong></p> 
<p><span style="color:#0d0016">3.1删除缺失值过多的列，和观察判断来说和预测的y没有关系的列 </span></p> 
<pre><code class="language-python">#删除缺失值过多的列，和观察判断来说和预测的y没有关系的列
data.drop(["Cabin","Name","Ticket"],inplace=True,axis=1)
data.head()</code></pre> 
<p><img alt="" height="276" src="https://images2.imgbox.com/31/22/nrEGSkua_o.png" width="784"></p> 
<p><span style="color:#0d0016">3.2处理缺失值，对缺失值较多的列进行填补，有一些特征只确实一两个值，可以采取直接删除记录的方法 </span></p> 
<pre><code class="language-python">#处理缺失值，对缺失值较多的列进行填补，有一些特征只确实一两个值，可以采取直接删除记录的方法
data["Age"]=data["Age"].fillna(data["Age"].mean())
data.info()</code></pre> 
<p><img alt="" height="248" src="https://images2.imgbox.com/e3/69/hWYU0nfU_o.png" width="699"></p> 
<pre><code class="language-python">#对少量空值的行直接删掉
data=data.dropna() 
data.info()</code></pre> 
<p><img alt="" height="256" src="https://images2.imgbox.com/e7/08/xORMXcln_o.png" width="737"></p> 
<p><strong>3.2将分类变量转换成数值型变量</strong></p> 
<p>3.2.1将二分类变量转换成数字型变量</p> 
<p style="margin-left:0"><span style="color:#0d0016">astype能够将一个pandas对象转换为某种类型，和apply(int(x))不同，astype可以将文本类转换为数字，用这    个方式可以很便捷地将二分类特征转换为0~1</span></p> 
<pre><code class="language-python">data["Sex"]=(data["Sex"]=="male").astype("int")
data.head()</code></pre> 
<p><img alt="" height="266" src="https://images2.imgbox.com/8f/07/uJ5l2GIc_o.png" width="941"></p> 
<p>3.2.2将三分类变量转换成数值型变量</p> 
<pre><code class="language-python">labels=data["Embarked"].unique().tolist()
data["Embarked"]=data["Embarked"].apply(lambda x:labels.index(x))
data.head(10)</code></pre> 
<p><img alt="" height="463" src="https://images2.imgbox.com/cc/53/UNb25m9M_o.png" width="815"></p> 
<p><strong>4.提取标签和特征矩阵，分测试集和训练集</strong></p> 
<pre><code class="language-python">x=data.iloc[:,data.columns!="Survived"]
y=data.iloc[:,data.columns=="Survived"]
from sklearn.model_selection import train_test_split
Xtrain,Xtest,Ytrain,Ytest=train_test_split(x,y,test_size=0.3)
#修正测试集和训练集的索引
for i in [Xtrain,Xtest,Ytrain,Ytest]:
    i.index=range(i.shape[0])
#查看分好的训练集和测试集
xtrain.info()</code></pre> 
<p><img alt="" height="551" src="https://images2.imgbox.com/7a/00/Zy1CTJv3_o.png" width="649"></p> 
<p><strong>5.导入模型，策略跑一下查看结果</strong></p> 
<pre><code class="language-python">clf = DecisionTreeClassifier(random_state=1) 
clf = clf.fit(Xtrain, Ytrain)
score1 = clf.score(Xtest, Ytest)

score1</code></pre> 
<p>结果：0.7640449438202247</p> 
<p>用交叉验证：</p> 
<pre><code class="language-python">score = cross_val_score(clf,x,y,cv=10).mean()

score</code></pre> 
<p>结果：0.7739402451481103</p> 
<p>6.在不同max_depth下观察模型的拟合状况</p> 
<pre><code class="language-python">tr=[]
te=[]
for i in range(10):
    clf=DecisionTreeClassifier(random_state=25
                               ,max_depth=i+1
                               ,criterion="entropy"
                              )
    clf=clf.fit(Xtrain, Ytrain)
    score_tr = clf.score(Xtest, Ytest)
    score_te = cross_val_score(clf,x,y,cv=10).mean()
    tr.append(score_tr)
    te.append(score_te)
print(max(te))
plt.plot(range(1,11),tr,color="red",label="train")
plt.plot(range(1,11),te,color="blue",label="test")
plt.xticks(range(1,11)) #指定横坐标
plt.legend()
plt.show()</code></pre> 
<p>     <img alt="" height="290" src="https://images2.imgbox.com/52/7f/M8pQJt3u_o.png" width="437">   </p> 
<p><strong>7.用网格搜索调整参数</strong></p> 
<pre><code class="language-python">import numpy as np
gini_thresholds = np.linspace(0,0.5,20)


parameters = {'splitter':('best','random')
                        ,'criterion':("gini","entropy")
                        ,"max_depth":[*range(1,10)]
                        ,'min_samples_leaf':[*range(1,50,5)]
                        ,'min_impurity_decrease':[*np.linspace(0,0.5,20)]
            }


clf = DecisionTreeClassifier(random_state=25) 
GS = GridSearchCV(clf, parameters, cv=10) 
GS.fit(Xtrain,Ytrain)

GS.best_params_


GS.best_score</code></pre> 
<h2 id="5.%E5%86%B3%E7%AD%96%E6%A0%91%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9">5.决策树的优缺点</h2> 
<p style="margin-left:0"><strong><span style="color:#333333">决策树优点</span></strong></p> 
<ol>
<li><span style="color:#333333">易于理解和解释，因为树木可以画出来被看见</span></li>
<li>
<span style="color:#333333">需要很少的数据准备。其他很多算法通常都需要数据规范化，需要创建虚拟变量并删除空值等。但请注意，</span><span style="color:#333333">sklearn</span><span style="color:#333333">中的决策树模块不支持对缺失值的处理。</span>
</li>
<li><span style="color:#333333">使用树的成本（比如说，在预测数据的时候）是用于训练树的数据点的数量的对数，相比于其他算法，这是  一个很低的成本。</span></li>
<li>
<span style="color:#333333">能够同时处理数字和分类数据，既可以做回归又可以做分类。其他技术通常专门用于分析仅具有一种变量类  </span><span style="color:#333333">型的数据集。</span>
</li>
<li><span style="color:#333333">能够处理多输出问题，即含有多个标签的问题，注意与一个标签中含有多种标签分类的问题区别开</span></li>
<li>
<span style="color:#333333">是一个白盒模型，结果很容易能够被解释。如果在模型中可以观察到给定的情况，则可以通过布尔逻辑轻松  </span><span style="color:#333333">解释条件。相反，在黑盒模型中（例如，在人工神经网络中），结果可能更难以解释。</span>
</li>
<li><span style="color:#333333">可以使用统计测试验证模型，这让我们可以考虑模型的可靠性。</span></li>
<li><span style="color:#333333">即使其假设在某种程度上违反了生成数据的真实模型，也能够表现良好。</span></li>
</ol>
<p style="margin-left:0"></p> 
<p><span style="color:#333333">决策树的缺点</span></p> 
<ol>
<li style="text-align:justify">
<span style="color:#333333">决策树学习者可能创建过于复杂的树，这些树不能很好地推广数据。</span><span style="color:#333333">这称为过度拟合。修剪，设置叶节点所  需的最小样本数或设置树的最大深度等机制是避免此问题所必需的，而这些参数的整合和调整对初学者来说  </span><span style="color:#333333">会比较晦涩</span>
</li>
<li style="text-align:justify"><span style="color:#333333">决策树可能不稳定，数据中微小的变化可能导致生成完全不同的树，这个问题需要通过集成算法来解决。</span></li>
<li style="text-align:justify"><span style="color:#333333">决策树的学习是基于贪婪算法，它靠优化局部最优（每个节点的最优）来试图达到整体的最优，但这种做法  不能保证返回全局最优决策树。这个问题也可以由集成算法来解决，在随机森林中，特征和样本会在分枝过  程中被随机采样。</span></li>
<li style="text-align:justify">
<span style="color:#333333">有些概念很难学习，因为决策树不容易表达它们，例如</span><span style="color:#333333">XOR</span><span style="color:#333333">，奇偶校验或多路复用器问题。</span>
</li>
<li style="text-align:justify">
<span style="color:#333333">如果标签中的某些类占主导地位，决策树学习者会创建偏向主导类的树。因此，建议在拟合决策树之前平衡  </span><span style="color:#333333">数据集。</span>
</li>
</ol>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>