<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>opencv训练自己的模型，实现特定物体的识别 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">opencv训练自己的模型，实现特定物体的识别</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="1_0"></a>1.说明</h1> 
<ul><li>opencv安装包中有训练好的分类器文件，可以实现人脸的识别。当然，我们也可以训练自己的分类器文件，实现对特定物体的识别。本文章就详细介绍下如何训练自己的分类器文件。</li></ul> 
<h2>
<a id="2_2"></a>2.效果</h2> 
<ul><li>我训练的是检测苹果的的分类器文件，可以实现对苹果的识别。<br> <img src="https://images2.imgbox.com/f6/98/cfnnJrYu_o.png" alt="在这里插入图片描述">
</li></ul> 
<h2>
<a id="3_5"></a>3.准备</h2> 
<h3>
<a id="31__6"></a>3.1 程序准备</h3> 
<ul>
<li>训练自己的分类器文件，需要用到两个程序 : <em>opencv_createsamples.exe</em>和<em>opencv_traincascade.exe</em>
</li>
<li>opencv最新的安装包中没有这两个程序，我们可以下载 3.4.14这个版本的安装包进行安装。 
  <ul><li>
<a href="https://sourceforge.net/projects/opencvlibrary/files/3.4.14/">opencv安装包</a> : opencv-3.4.14-vc14_vc15.exe</li></ul> </li>
<li>安装完成后，在这个目录下就会有这两个程序文件 opencvbuildx64vc15bin</li>
</ul> 
<h3>
<a id="32__11"></a>3.2 样本数据准备</h3> 
<ul>
<li> <p><strong>正样本数据</strong> : 也就是我们需要检测的物体图片，可以自己用手机拍摄下你要检测的物体的图片，多拍摄一些不同角度的图片。</p> </li>
<li> <p>我的正样本数据在这个目录下 imagepositiveimg，大概有50多张图片<br> <img src="https://images2.imgbox.com/8a/50/EYut14c8_o.png" alt="在这里插入图片描述"></p> </li>
<li> <p>然后在imagepositive目录下新建一个info.dat文件，在其中记录正样本图片信息<br> <img src="https://images2.imgbox.com/2c/0e/3SDu2EB1_o.png" alt="请添加图片描述"></p> </li>
<li> <p>参数介绍</p> 
  <ul>
<li>
<strong>img/1.jpg</strong> : 文件路径和文件名</li>
<li>
<strong>1</strong>：表示图片中有几个目标物体，一般一个就行了</li>
<li>
<strong>0，0</strong>：目标物体起始坐标</li>
<li>
<strong>1280，1706</strong>：目标物体大小</li>
</ul> </li>
<li> <p><strong>负样本数据</strong>：不包含我们要检测物体的图片，可以拍摄一些风景之类的图片，尽量多一些。</p> </li>
<li> <p>我的负样本数据在这个目录下 imagenegitiveimg<br> <img src="https://images2.imgbox.com/96/1d/PfSF1Q9H_o.png" alt="在这里插入图片描述"></p> </li>
<li> <p>然后在imagenegitive目录下新建一个bg.txt文件，在其中记录负样本图片信息</p> </li>
<li> <p>负样本图片信息我们只需记录路径和文件名就行了，但是这里要注意，<strong>路径名要写绝对路径</strong>，后面会说为什么。<br> <img src="https://images2.imgbox.com/3a/bc/hIoXkKcL_o.png" alt="在这里插入图片描述"></p> </li>
</ul> 
<h3>
<a id="33_VEC_31"></a>3.3 正样本VEC文件创建</h3> 
<ul>
<li>训练样本之前先要生成vec文件，要用到<strong>opencv_createsamples.exe</strong>程序</li>
<li>
<strong>opencv_createsamples.exe</strong>部分参数介绍</li>
</ul> 
<pre><code>  [-info &lt;collection_file_name&gt;]  # 记录样本数据的文件(就是我们刚才创建的info.data文件)
  [-img &lt;image_file_name&gt;]    
  [-vec &lt;vec_file_name&gt;]   # 输出文件，内含用于训练的正样本。 
  [-bg &lt;background_file_name&gt;]  # 背景图像的描述文件
  [-num &lt;number_of_samples = 1000&gt;]   #样本数量（默认为1000）
  [-bgcolor &lt;background_color = 0&gt;]    #指定背景颜色
  [-w &lt;sample_width = 24&gt;]#输出样本的宽度（以像素为单位）
  [-h &lt;sample_height = 24&gt;]#输出样本的高度（以像素为单位）
</code></pre> 
<p><a href="https://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/user_guide/ug_traincascade.html">参考</a></p> 
<ul><li>在安装包的这个目录下opencvbuildx64vc15bin可以找到opencv_createsamples.exe程序，我们生成下vec文件</li></ul> 
<pre><code>D:opencv3.4.12opencvbuildx64vc15binopencv_createsamples.exe -info C:UserslngDesktopimagepositiveinfo.dat -vec C:UserslngDesktopimagesample.vec -num 58 -bgcolor 0 -bgthresh 0 -w 24 -h 24
</code></pre> 
<ul><li>在image目录下就生成了vec文件<br> <img src="https://images2.imgbox.com/0b/54/rTbd0wzR_o.png" alt="在这里插入图片描述">
</li></ul> 
<h1>
<a id="4_51"></a>4.样本数据训练</h1> 
<ul>
<li>完成上面的准备工作，就可以开始训练样本。训练样本需要用到<strong>opencv_traincascaded.exe</strong>程序</li>
<li>
<strong>opencv_traincascaded.exe</strong>程序部分参数介绍</li>
</ul> 
<pre><code> -data &lt;cascade_dir_name&gt;     #目录名，如不存在训练程序会创建它，用于存放训练好的分类器
 -vec &lt;vec_file_name&gt;              #包含正样本的vec文件名
 -bg &lt;background_file_name&gt;   #背景描述文件
 [-numPos &lt;number_of_positive_samples = 2000&gt;]   #每级分类器训练时所用的正样本数目
 [-numNeg &lt;number_of_negative_samples = 1000&gt;]   #每级分类器训练时所用的负样本数目
 [-numStages &lt;number_of_stages = 20&gt;]   #训练的分类器的级数
--cascadeParams--
 [-featureType &lt;{HAAR(default), LBP, HOG}&gt;]  # 特征的类型： HAAR - 类Haar特征； LBP - 局部纹理模式特征
 [-w &lt;sampleWidth = 24&gt;] #训练样本的尺寸（单位为像素）
 [-h &lt;sampleHeight = 24&gt;] #训练样本的尺寸（单位为像素）
--boostParams--
 [-minHitRate &lt;min_hit_rate&gt; = 0.995&gt;] #分类器的每一级希望得到的最小检测率
 [-maxFalseAlarmRate &lt;max_false_alarm_rate = 0.5&gt;] #分类器的每一级希望得到的最大误检率
</code></pre> 
<p><a href="https://www.opencv.org.cn/opencvdoc/2.3.2/html/doc/user_guide/ug_traincascade.html">参考</a></p> 
<ul>
<li>在安装包的这个目录下opencvbuildx64vc15bin可以找到opencv_traincascade.exe程序，开始训练样本</li>
<li>
<strong>这里注意下</strong> 
  <ul>
<li>指定-bg参数时，文件名前不能加路径，所以需要把刚才在imagenegitive下创建的bg.txt文件拷贝到opencv_traincascade.exe程序所在目录下，所以要在bg.txt写负样本图片的绝对路径。</li>
<li>指定numPos参数时，因为每个阶段训练时有些正样本可能会被识别为负样本，故每个训练阶段后都会消耗一定的正样本。因此，此处使用的正样本数量绝对不能等于或超过positive文件夹下的正样本个数，一般留有一定的余量</li>
<li>指定-numNeg参数时，可以多于negitive目录下的负样本数量</li>
</ul> </li>
</ul> 
<pre><code>D:opencv3.4.12opencvbuildx64vc15binopencv_traincascade.exe -data C:UserslngDesktopimage -vec C:UserslngDesktopimagesample.vec -bg bg.txt -numPos 50 -numNeg 500 -numStages 12 -feattureType HAAR -w 24 -h 24 -minHitRate 0.995 -maxFalseAlarmRate 0.5
</code></pre> 
<ul><li>执行结果</li></ul> 
<pre><code>PARAMETERS:
cascadeDirName: C:UserslngDesktopimage
vecFileName: C:UserslngDesktopimagesample.vec
bgFileName: bg.txt
numPos: 50
numNeg: 500
numStages: 12
precalcValBufSize[Mb] : 1024
precalcIdxBufSize[Mb] : 1024
acceptanceRatioBreakValue : -1
stageType: BOOST
featureType: HAAR
sampleWidth: 24
sampleHeight: 24
boostType: GAB
minHitRate: 0.995
maxFalseAlarmRate: 0.5
weightTrimRate: 0.95
maxDepth: 1
maxWeakCount: 100
mode: BASIC
Number of unique features given windowSize [24,24] : 162336

===== TRAINING 0-stage =====
&lt;BEGIN
POS count : consumed   50 : 50
NEG count : acceptanceRatio    500 : 1
Precalculation time: 0.581
+----+---------+---------+
|  N |    HR   |    FA   |
+----+---------+---------+
|   1|        1|        1|
+----+---------+---------+
|   2|        1|     0.05|
+----+---------+---------+
END&gt;
Training until now has taken 0 days 0 hours 0 minutes 1 seconds.

===== TRAINING 1-stage =====
&lt;BEGIN
POS count : consumed   50 : 50
NEG count : acceptanceRatio    500 : 0.084832
Precalculation time: 0.576
+----+---------+---------+
|  N |    HR   |    FA   |
+----+---------+---------+
|   1|        1|        1|
+----+---------+---------+
|   2|        1|    0.146|
+----+---------+---------+
END&gt;
Training until now has taken 0 days 0 hours 0 minutes 3 seconds.

===== TRAINING 2-stage =====
&lt;BEGIN
POS count : consumed   50 : 50
NEG count : acceptanceRatio    500 : 0.0149993
Precalculation time: 0.592
+----+---------+---------+
|  N |    HR   |    FA   |
+----+---------+---------+
|   1|        1|        1|
+----+---------+---------+
|   2|        1|    0.186|
+----+---------+---------+
END&gt;
Training until now has taken 0 days 0 hours 0 minutes 5 seconds.

===== TRAINING 3-stage =====
&lt;BEGIN
POS count : consumed   50 : 50
NEG count : acceptanceRatio    500 : 0.00288033
Precalculation time: 0.652
+----+---------+---------+
|  N |    HR   |    FA   |
+----+---------+---------+
|   1|        1|        1|
+----+---------+---------+
|   2|        1|    0.298|
+----+---------+---------+
END&gt;
Training until now has taken 0 days 0 hours 0 minutes 7 seconds.

===== TRAINING 4-stage =====
&lt;BEGIN
POS count : consumed   50 : 50
NEG count : acceptanceRatio    500 : 0.000768845
Precalculation time: 0.615
+----+---------+---------+
|  N |    HR   |    FA   |
+----+---------+---------+
|   1|        1|        1|
+----+---------+---------+
|   2|        1|        1|
+----+---------+---------+
|   3|        1|    0.366|
+----+---------+---------+
END&gt;
Training until now has taken 0 days 0 hours 0 minutes 11 seconds.

===== TRAINING 5-stage =====
&lt;BEGIN
POS count : consumed   50 : 50
NEG count : acceptanceRatio    500 : 0.000375057
Precalculation time: 0.61
+----+---------+---------+
|  N |    HR   |    FA   |
+----+---------+---------+
|   1|        1|        1|
+----+---------+---------+
|   2|        1|        1|
+----+---------+---------+
|   3|        1|    0.366|
+----+---------+---------+
END&gt;
Training until now has taken 0 days 0 hours 0 minutes 15 seconds.

===== TRAINING 6-stage =====
&lt;BEGIN
POS count : consumed   50 : 50
NEG count : acceptanceRatio    2 : 0.00016276
Required leaf false alarm rate achieved. Branch training t
</code></pre> 
<ul>
<li>训练完成后，在img目录下就会生成以下文件。<br> <img src="https://images2.imgbox.com/c3/aa/btS5a9DX_o.png" alt="在这里插入图片描述">
</li>
<li>cascade.xml就是我们需要的分类器文件，其他都是过程文件。</li>
</ul> 
<h1>
<a id="5_207"></a>5.测试代码</h1> 
<ul><li>main.cpp</li></ul> 
<pre><code>#include &lt;iostream&gt;
#include &lt;opencv2/opencv.hpp&gt;

char* face_cascade_name = "C:\Users\lng\Desktop\image\cascade.xml";

void faceRecongize(cv::CascadeClassifier faceCascade, cv::Mat frame);

int main(){
    cv::VideoCapture *videoCap = new cv::VideoCapture;

	cv::CascadeClassifier faceCascade;

    // 加载苹果分类器文件
	if (!faceCascade.load(face_cascade_name)) {
		std::cout &lt;&lt; "load face_cascade_name failed. " &lt;&lt; std::endl;
		return -1;
	}

    // 打开摄像机
	videoCap-&gt;open(0);


	if (!videoCap-&gt;isOpened()) {
		videoCap-&gt;release();
		std::cout &lt;&lt; "open camera failed"&lt;&lt; std::endl;
        return -1;
	}

	std::cout &lt;&lt; "open camera success"&lt;&lt; std::endl;

    while(1){
		cv::Mat frame;
		//读取视频帧
		videoCap-&gt;read(frame);

		if (frame.empty()) {
			videoCap-&gt;release();
			return -1;
		}

        //进行苹果识别
		faceRecongize(faceCascade, frame);

        //窗口进行展示
        imshow("face", frame);

        //等待回车键按下退出程序
		if (cv::waitKey(30) == 13) {
			cv::destroyAllWindows();
			return 0;
		}
    }

    system("pause");
    return 0;
}

void faceRecongize(cv::CascadeClassifier faceCascade, cv::CascadeClassifier eyesCascade, cv::CascadeClassifier mouthCascade, cv::Mat frame) {
	std::vector&lt;cv::Rect&gt; faces;

    // 检测苹果
	faceCascade.detectMultiScale(frame, faces, 1.1, 2, 0 | cv::CASCADE_SCALE_IMAGE, cv::Size(30, 30));
	for (int i = 0; i &lt; faces.size(); i++) {
		
        // 用椭圆画出苹果部分
        cv::Point center(faces[i].x + faces[i].width / 2, faces[i].y + faces[i].height / 2);
		ellipse(frame, center, cv::Size(faces[i].width / 2, faces[i].height / 2), 0, 0, 360, cv::Scalar(255, 0, 255), 4, 8, 0);
		
		cv::Mat faceROI = frame(faces[i]);
		std::vector&lt;cv::Rect&gt; eyes;
		

            
        // 苹果上方区域写字进行标识
		cv::Point centerText(faces[i].x + faces[i].width / 2 - 40, faces[i].y - 20);
		cv::putText(frame, "apple", centerText, cv::FONT_HERSHEY_SIMPLEX, 1, cv::Scalar(0, 0, 255), 2);
		

	}
}

</code></pre> 
<ul><li>CMakeLists</li></ul> 
<pre><code>cmake_minimum_required (VERSION 3.5)
project (faceRecongize2015)

MESSAGE(STATUS "PROJECT_SOURCE_DIR " ${PROJECT_SOURCE_DIR})
SET(SRC_LISTS ${PROJECT_SOURCE_DIR}/src/main.cpp)

set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -std=c++11")

#set(CMAKE_AUTOMOC ON)
#set(CMAKE_AUTOUIC ON)
#set(CMAKE_AUTORCC ON)

# 配置头文件目录
include_directories(${PROJECT_SOURCE_DIR}/src)
include_directories("D:\opencv3.4.12\opencv\build\include")
include_directories("D:\opencv3.4.12\opencv\build\include\opencv2")

# 设置不显示命令框
if(MSVC)
	#set(CMAKE_EXE_LINKER_FLAGS "${CMAKE_EXE_LINKER_FLAGS} /SUBSYSTEM:WINDOWS /ENTRY:mainCRTStartup")
endif()

# 添加库文件
set(PRO_OPENCV_LIB "D:\opencv3.4.12\opencv\build\x64\vc15\lib\opencv_world3412.lib" "D:\opencv3.4.12\opencv\build\x64\vc15\lib\opencv_world3412d.lib")

IF(WIN32)
    # 生成可执行程序
	ADD_EXECUTABLE(faceRecongize2015 ${SRC_LISTS})
	# 链接库文件
    TARGET_LINK_LIBRARIES(faceRecongize2015 ${PRO_OPENCV_LIB})
ENDIF()
</code></pre> 
<h1>
<a id="6_328"></a>6.编译说明</h1> 
<ul>
<li>我的opencv 3.4.12的安装路径是 D:opencv3.4.12</li>
<li>目录结构</li>
</ul> 
<pre><code>- src
  - mian.cpp
- build_x64
- CMakeLists
</code></pre> 
<ul><li>编译命令, 在build_x64目录下执行</li></ul> 
<pre><code>cmake -G "Visual Studio 14 2015 Win64" ..
cmake --build ./ --config Release
</code></pre> 
<ul><li>编译完成后拷贝D:opencv3.4.12opencvbuildx64vc15bin目录下的opencv_world3412.dll和opencv_world3412d.dll到可执行程序目录下。</li></ul> 
<h1>
<a id="_343"></a>备注</h1> 
<ul><li>经过测试，自己训练的样本，准确度还是比较差的。可能是正样本数据太少，且图片背景占据位置较多。要提高准确度，首先增加正样本图片数量，还要就是尽量让你的目标物体占满整个图片，不要留有太多的背景。而且也要有尽可能多的负样本数据。</li></ul>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>