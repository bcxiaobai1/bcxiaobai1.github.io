<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>YOLO系列梳理（九）初尝新鲜出炉的YOLOv6 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">YOLO系列梳理（九）初尝新鲜出炉的YOLOv6</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <blockquote>
 <strong>前言</strong> 近日，美团视觉智能部开源了YOLOv6的框架。YOLOv4、YOLOv5更多是注重于数据增强，而对网络结构的改动则比较少。和YOLOv4、YOLOv5不同，YOLOv6对网络结构的改动还是蛮大的。
</blockquote> 
<p><strong>欢迎关注公众号<a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495585%26idx%3D2%26sn%3D5df76c5d0956cc998e9bbb21b86ca460%26chksm%3Dc19450bff6e3d9a9d72330f85cf37ffb446cbc70f3a83c492ad8214c82005605c196a875b148%26token%3D1526765687%26lang%3Dzh_CN%23rd" title="CV技术指南">CV技术指南</a>，专注于计算机视觉的技术总结、最新技术跟踪、经典论文解读、CV招聘信息。</strong></p> 
<p>从YOLOv6的报告来看，它的性能也是再创新高。YOLOv6-s在COCO上精度达到了43.1%AP，在T4上推理速度也高达520FPS！此外，YOLOv6不仅仅关注于学术界所重点关注的AP、FPS指标，对于工业界也是十分友好。在部署方面它提供了TensorRT、NCNN、OPENVINO等平台的部署。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/27/f8/ZEcLsnxu_o.png"></p> 
<h3><strong>YOLOv6的关键技术</strong></h3> 
<p>YOLOv6的Backbone不再使用Cspdarknet，而是转为比Rep更高效的EfficientRep；它的Neck也是基于Rep和PAN搭建了Rep-PAN；而Head则和YOLOX一样，进行了解耦，并且加入了更为高效的结构。</p> 
<p>值得一提的是，YOLOv6也是沿用anchor-free的方式，抛弃了以前基于anchor的方法。</p> 
<p>除了模型的结构之外，它的数据增强和YOLOv5的保持一致；而标签分配上则是和YOLOX一样，采用了simOTA；并且引入了新的边框回归损失：SIOU。这样看来，YOLOv6可谓是结合了两者的优良点。</p> 
<h3><strong>Backbone</strong></h3> 
<p>我们知道，YOLOv5和YOLOX都是采用多分支的残差结构CSPNet，但是这种结构对于硬件来说并不是很友好。所以为了更加适应GPU设备，在backbone上就引入了ReVGG的结构，并且基于硬件又进行了改良，提出了效率更高的EfficientRep。</p> 
<p>RepVGG为每一个3×3的卷积添加平行了一个1x1的卷积分支和恒等映射的分支。这种结构就构成了构成一个RepVGG Block。和ResNet不同的是，RepVGG是每一层都添加这种结构，而ResNet是每隔两层或者三层才添加。RepVGG介绍称，通过融合而成的3x3卷积结构，对计算密集型的硬件设备很友好。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ae/fc/zdqRC9ha_o.png"></p> 
<p>RegVGG是一种简单又强力的CNN结构，在训练时使用了性能高的多分支模型，而在推理时使用了速度快、省内存的单路模型，也是更具备速度和精度的均衡。</p> 
<p>EfficientRep将在backbone中stride=2的卷积层换成了stride=2的RepConv层。并且也将CSP-Block修改为了RepBlock。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f3/0f/L8CTt9cl_o.png"></p> 
<h3><strong>Neck</strong></h3> 
<p>同样为了降低在硬件上的延时，在Neck上的特征融合结构中也引入了Rep结构。在Neck中使用的是Rep-PAN。</p> 
<p>Rep-PAN是基于PAN和RepBlock两者的结合。主要就是替换PAN中的CSP-Block为RepBlock。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/b0/d0/aEZ4ecTa_o.png"></p> 
<h3><strong>Head</strong></h3> 
<p>和YOLOX一样，YOLOv6也对检测头进行了解耦，分开了边框回归与类别分类的过程。将边框回归和类别分类耦合在一起时会影响性能，因为这样不仅加慢收敛的速度，也会提高检测头的复杂程度。</p> 
<p>因为在YOLOX的解耦头中，新增了两个额外的3x3卷积，这样也会一定程度增加运算的复杂度。YOLOv6基于Hybrid Channels的策略重新设计出了一个更高效的解耦头结构。在不怎么改变精度的情况下降低延时，这样也达到了速度与精度的权衡。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f4/61/QWNDCZFQ_o.png"></p> 
<h3><strong>Anchor-free</strong></h3> 
<p>传统的目标检测都是基于anchor的，如今，anchor-free的使用越来越广。因为使用anchor会存在以下几个问题：</p> 
<p>1.需要在训练之前进行聚类分析以确定最佳anchor集合，但是这些anchor集合存在数据相关性，泛化性能较差。</p> 
<p>2. anchor机制提升了检测头的复杂度。</p> 
<h3><strong>SimOTA</strong></h3> 
<p>SimOTA主要用于正样本的匹配，即为不同目标设定不同的正样本数量。我们知道检测模型推理的过程中会出现正负样本不均衡的现象，所以就需要进行正负样本的平均。</p> 
<p>传统的anchor-free版本仅仅对每个目标赋予一个正样本，而忽视了其他高质量预测。因此这里为每个中心点赋予了多个正样本。并且为每个GT分配不同数量的anchor。所用的策略是参考FCOS，赋予中心3×3区域为正样本。落入其中区域的都为正样本。</p> 
<p>SimOTA实际就是为各个GT框进行anchor的分配。分配策略可以理解为，根据不同anchor的模糊程度赋予不同的权重，从而得到分配的数量。因为在OTA中，将模糊的anchor分配给任何GT或背景都会对其他GT的梯度造成不利影响。所以，对模糊anchor样本的分配策略是特殊的。为此，需要进行更优的分配策略：除了考虑分类信息，还考虑了位置信息。也就是为图像中的所有GT对象找到全局的高置信度进行分配。</p> 
<p>但是这里又引入了一个问题：最优运输问题优化会带来25%的额外训练耗时。所以在YOLOX中，又简化为动态的top-k策略进行分配，也就是得到一个近似解的策略：SimOTA。YOLOv6也沿用这个标签分配的方法。</p> 
<h3><strong>SIOU</strong></h3> 
<p>SIOU是2022年5月提出的。常用的IOU损失有IOU、GIOU、DIOU等，它们引入了框与框之间的重叠程度、中心点距离等等因素来进行匹配。而SIOU 损失函数是引入了所需回归之间的向量角度。这样就重新定义了距离损失，有效降低了回归的自由度，从而加速网络的收敛，进一步提升回归的准确性。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/79/f7/DJlHF49N_o.png"></p> 
<h3><strong>实验结果</strong></h3> 
<p>以下是多种修改后的消融实验结果，可以看到，经过不断的改进，模型的mAP和FPS都获得了明显的增益。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/5f/32/OH7eoi97_o.png"></p> 
<p>以下是YOLOv6和目前比较主流的YOLO系列算法比较的结果，包括速度、参数量等指标：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/41/15/4ZWcMkXX_o.png"></p> 
<h3><strong>使用细节</strong></h3> 
<p>从github上的readme来看，训练的方式也比较常规，总体来说，和YOLOv5、YOLOX的挺类似，使用的还是大家比较熟悉的方式。</p> 
<h3><strong>训练前的准备</strong></h3> 
<p>首先需要根据requirements.txt安装，里面也没有什么太特别的库。就是torch、torchvision、opencv-python、onnx这一些常规库。</p> 
<p>然后就需要准备数据集了。依旧是每张图片对应一个标签，每一行代表一个物体。标签的格式如下：</p> 
<pre><code># class_id center_x center_y bbox_width bbox_height
0 0.300926 0.617063 0.601852 0.765873
1 0.575 0.319531 0.4 0.55156
</code></pre> 
<p>注意类别数目需要从0开始，并且宽、中心点需要进行归一化，也就是除以图片的宽高。</p> 
<p>接下里就可以组织你的训练集和评估集，它们的文件夹组成如下：</p> 
<pre><code># image directory
path/to/data/images/train/im0.jpg
path/to/data/images/val/im1.jpg
path/to/data/images/test/im2.jpg

# label directory
path/to/data/labels/train/im0.txt
path/to/data/labels/val/im1.txt
path/to/data/labels/test/im2.txt</code></pre> 
<p>这里也有YAML文件和config文件，可以高效地进行配置。这样一来，你就只需要在这些文件里修改就行了，而不需要改动代码，也不需要在整个项目中，东挑一个文件进行改动，西选一个文件进行修改。</p> 
<pre><code># 这是dataset.yaml
train: path/to/data/images/train # train images 
val: path/to/data/images/val # val images 
test: path/to/data/images/test # test images (optional)
# Classes
nc: 20 # number of classes 
names: ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'] # class name

## 这是模型配置文件
model = dict(
    type='YOLOv6s',
    pretrained='./weights/yolov6s.pt', # download pretrain model from YOLOv6 github if use pretrained model
    depth_multiple = 0.33,
    width_multiple = 0.50,
    ...
)
solver=dict(
    optim='SGD',
    lr_scheduler='Cosine',
    ...
)

data_aug = dict(
    hsv_h=0.015,
    hsv_s=0.7,
    hsv_v=0.4,
    ...
)

</code></pre> 
<h3><strong>训练</strong></h3> 
<p>这是训练的命令：</p> 
<pre><code>python tools/train.py --batch 256 --conf configs/yolov6s.py --data data/coco.yaml --device 0 configs/yolov6n.py
</code></pre> 
<p>以上是单卡训练的命令，如果你想使用多卡训练，可以用以下的命令：</p> 
<pre><code>python tools/train.py --batch 256 --conf configs/yolov6s.py --data data/coco.yaml --device 0 configs/yolov6n.pypython -m torch.distributed.launch --nproc_per_node 8 tools/train.py --batch 256 --conf configs/yolov6s.py --data data/coco.yaml --device 0,1,2,3,4,5,6,7 configs/yolov6n.pypython -m torch.distributed.launch --nproc_per_node 8 tools/train.py --batch 256 --conf configs/yolov6s.py --data data/coco.yaml --device 0,1,2,3,4,5,6,7 configs/yolov6n.py
</code></pre> 
<h3><strong>评估</strong></h3> 
<p>训练完后，用于评估的命令：</p> 
<pre><code>python tools/eval.py --data data/coco.yaml --batch 32 --weights yolov6s.pt --task val yolov6n.pt
</code></pre> 
<h3><strong>测试</strong></h3> 
<p>进行测试推理，可以使用以下命令：</p> 
<pre><code>python tools/infer.py --weights yolov6s.pt --source [img.jpg / imgdir] yolov6n.pt</code></pre> 
<p>如果你需要进行部署，就需要进行export转换，以转为onnx为例：</p> 
<pre><code>python deploy/ONNX/export_onnx.py --weights output_dir/name/weights/best_ckpt.pt --device 0
</code></pre> 
<h3><strong>文末</strong></h3> 
<p>以上，就是对于YOLOv6的初步品尝。可以看到，YOLOv6使用了不同的改良策略后，性能的提升都是立竿见影的。从在COCO数据集上的实验结果来看，YOLOv6 在目标检测算法中可谓是一马当先了。</p> 
<p>并且如今的开源项目，对于工业上的部署也越来越支持了，完全考虑到了如今工程师们的刚需。虽然提升性能这一项"做好"工作很重要，但是工业部署上的“做到”更加重要。这也是我们在工作中需要考虑的一点：有时完成比完美更加重要，做好的前提是做到。只有先做到，才有后面的做好。</p> 
<p><strong>参考链接</strong></p> 
<pre><code>https://arxiv.org/pdf/2101.03697.pdf 
https://arxiv.org/pdf/2107.08430.pdf 
https://arxiv.org/ftp/arxiv/papers/2205/2205.12740.pdf 
https://arxiv.org/ftp/arxiv/papers/2205/2205.12740.pdf 
https://tech.meituan.com/2022/06/23/yolov6-a-fast-and-accurate-target-detection-framework-is-opening-source.html 
https://github.com/meituan/YOLOv6</code></pre> 
<p></p> 
<p>CV技术指南创建了一个计算机视觉技术交流群和免费版的知识星球，目前星球内人数已经700+，主题数量达到200+。</p> 
<p>知识星球内将会每天发布一些作业，用于引导大家去学一些东西，大家可根据作业来持续打卡学习。</p> 
<p>CV技术群内每天都会发最近几天出来的顶会论文，大家可以选择感兴趣的论文去阅读，持续follow最新技术，若是看完后写个解读给我们投稿，还可以收到稿费。 另外，技术群内和本人朋友圈内也将发布各个期刊、会议的征稿通知，若有需要的请扫描加好友，并及时关注。</p> 
<p>加群加星球方式：关注公众号CV技术指南，获取编辑微信，邀请加入。</p> 
<p></p> 
<h2> 公众号其它文章</h2> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494483%26idx%3D1%26sn%3D7069ade230575cfcb1c1f8c8e8763ecb%26chksm%3Dc194544df6e3dd5bea7a98723b764c7db8591e292a775c4c465f715acc260d5d1fc53aa6487c%26payreadticket%3DHIjWViK3B_NTMPuq4Zzm_wEIyKtyvPbtyDFiQTwJsqOLFvAW28Qv38O0pcR_VdMzz15Xsb0%23rd" title="计算机视觉入门路线">计算机视觉入门路线</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247496011%26idx%3D2%26sn%3D6d48b3e084a73f521a6e372809cfa1e0%26chksm%3Dc1944e55f6e3c7433a6a7f8a3354125c5c8a2ea46020fa30d5fdfbf7c52cf148d627a94d59fa%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="计算机视觉中的论文常见单词总结">计算机视觉中的论文常见单词总结</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495965%26idx%3D1%26sn%3Dd111ac563865f101b39bad6f949e333e%26chksm%3Dc1944e03f6e3c71599b359defe93b955c3b0e2f8147f24cb197a84d05a442362e47b9275a6c7%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="YOLO系列梳理（四）关于YOLO的部署">YOLO系列梳理（四）关于YOLO的部署</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494638%26idx%3D1%26sn%3Dac84cfb2e3d2e346aa766ff5c5185609%26chksm%3Dc19454f0f6e3dde67258380c28b9882b7453a37f0d657bdbf85e407e0e8f4d2650d7384f037e%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="YOLO系列梳理（三）YOLOv5">YOLO系列梳理（三）YOLOv5</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494393%26idx%3D1%26sn%3Dcdbad1b535816a06213cac31e7d8e4db%26chksm%3Dc19455e7f6e3dcf19d9eb19ed8aa22ddc23d2c5553ebfe5ff46f82b2534894316363975a603a%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="YOLO系列梳理（二）YOLOv4">YOLO系列梳理（二）YOLOv4</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494324%26idx%3D1%26sn%3D8e8eeb92ede34988d7b7c46837c3d308%26chksm%3Dc19455aaf6e3dcbc91020d6c559a7539e3ece7b0fa13104b486c48cf9662ef2ce2188cfebb91%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="YOLO系列梳理（一）YOLOv1-YOLOv3">YOLO系列梳理（一）YOLOv1-YOLOv3</a></p> 
<p><a href="https://mp.weixin.qq.com/s?__biz=MzkyMDE2OTA3Mw==&amp;mid=2247496412&amp;idx=1&amp;sn=00dc09dcd3fcec56d6333295b5c72576&amp;chksm=c1944dc2f6e3c4d438424797bebfc6a8f9611748ee835ad03cd7a3fbc06243a632a663b6ec4e&amp;token=1421245552&amp;lang=zh_CN#rd" title="CVPR2022 | 可精简域适应">CVPR2022 | 可精简域适应</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247496273%26idx%3D2%26sn%3D55c822931d8ab2ddc7e1d1d51e7ac71b%26chksm%3Dc1944d4ff6e3c4590d05e9fb6f46b4d7fc5f2eaf539d0a45387025c2805961b0313de048ddae%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="CVPR2022 | 基于自我中心数据的OCR评估">CVPR2022 | 基于自我中心数据的OCR评估</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495946%26idx%3D1%26sn%3D85ba25eee4db64dbd88b95fa2ccc7e36%26chksm%3Dc1944e14f6e3c702818f5ef87cb7a981969d9fa90e63850668318a11f6ae87d8a5bcd8f37ac8%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="CVPR 2022 | 使用对比正则化方法应对噪声标签">CVPR 2022 | 使用对比正则化方法应对噪声标签</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247496143%26idx%3D1%26sn%3D878fe644077193937ad07ceb87b0809f%26chksm%3Dc1944ed1f6e3c7c7dab4e312981a68cca45c43165852c3763a3e95538a703331d97a057ea862%26token%3D1421245552%26lang%3Dzh_CN%23rd" title="CVPR2022 | 弱监督多标签分类中的损失问题">CVPR2022 | 弱监督多标签分类中的损失问题</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495819%26idx%3D1%26sn%3Dc5fd1d9fc523964b4cd557f8351ea0a0%26chksm%3Dc1944f95f6e3c6839a94da66493635489352ed4f49a1eeb33ab4dc2624be6eac379ab136e3d9%26token%3D1812044639%26lang%3Dzh_CN%23rd" title="CVPR2022 | iFS-RCNN：一种增量小样本实例分割器">CVPR2022 | iFS-RCNN：一种增量小样本实例分割器</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495738%26idx%3D1%26sn%3De480949d62c3183da82c08bc9b67293b%26chksm%3Dc1944f24f6e3c6327f1768aa8035807d9b5fe47a07ec272edffe331455aefb1f8ae0b60e3ef2%26token%3D1812044639%26lang%3Dzh_CN%23rd" title="CVPR2022 | A ConvNet for the 2020s &amp; 如何设计神经网络总结">CVPR2022 | A ConvNet for the 2020s &amp; 如何设计神经网络总结</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495700%26idx%3D1%26sn%3D61afff6b3b5a545a433de5d2cd95f52b%26chksm%3Dc1944f0af6e3c61c8a6a22bbe4e3e4f5604d46225723bafc0b3592ef6910334ceb766b9aa2af%26token%3D1812044639%26lang%3Dzh_CN%23rd" title="CVPR2022 | PanopticDepth：深度感知全景分割的统一框架">CVPR2022 | PanopticDepth：深度感知全景分割的统一框架</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495644%26idx%3D1%26sn%3D23d4728e50f911c0eab24aa2073fd4e5%26chksm%3Dc19450c2f6e3d9d4c85eef2243bfd1896529fc8d7de377879b7147623902753a0f325a743e6f%26token%3D1526765687%26lang%3Dzh_CN%23rd" title="CVPR2022 | 重新审视池化：你的感受野不是最理想的">CVPR2022 | 重新审视池化：你的感受野不是最理想的</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495622%26idx%3D1%26sn%3D1e57c7ccb630186219e6471b28a159ba%26chksm%3Dc19450d8f6e3d9ceee6acab0867a7decf5ea600e7f9029b2920df3c96051524ee289970d0ba3%26token%3D1526765687%26lang%3Dzh_CN%23rd" title="CVPR2022 | 未知目标检测模块STUD：学习视频中的未知目标">CVPR2022 | 未知目标检测模块STUD：学习视频中的未知目标</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495554%26idx%3D1%26sn%3D74c2069f4db40459f536c9748bcdf190%26chksm%3Dc194509cf6e3d98a957012a496e85701ceef49efe2383917dd6358ae56a0c7d53abd8dfbadf5%26token%3D1526765687%26lang%3Dzh_CN%23rd" title="CVPR2022 | 基于排名的siamese视觉跟踪">CVPR2022 | 基于排名的siamese视觉跟踪</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495585%26idx%3D1%26sn%3Db29b4fbdea981e6fb0d65b763cf95879%26chksm%3Dc19450bff6e3d9a96de93fb028d29671b2bcbee9c0ddeb20f18bb8b43d0c3d39dcc3e5ab5318%26token%3D1526765687%26lang%3Dzh_CN%23rd" title="从零搭建Pytorch模型教程（六）编写训练过程和推理过程">从零搭建Pytorch模型教程（六）编写训练过程和推理过程</a></p> 
<p><a href="http://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495346%26idx%3D1%26sn%3D7c6d04843956b9c8364f0e2dbae173b0%26chksm%3Dc19451acf6e3d8bae14df712fdf3f34afa77c1be531f608fd4187d09c06db7b44773f0784c32%26token%3D1448674844%26lang%3Dzh_CN%23rd" title="从零搭建Pytorch模型教程（五）编写训练过程--一些基本的配置">从零搭建Pytorch模型教程（五）编写训练过程--一些基本的配置</a></p> 
<p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247495136%26idx%3D1%26sn%3Db7dfcb870ab03617978d790f3fe7bb60%26chksm%3Dc19452fef6e3dbe83268c1e2185985c039c2ec724b367e933fbee52dc203fd8e3b26fccec645%26scene%3D21%23wechat_redirect" title="从零搭建Pytorch模型教程（四）编写训练过程--参数解析">从零搭建Pytorch模型教程（四）编写训练过程--参数解析</a></p> 
<p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494373%26idx%3D1%26sn%3D98d5967bcf889aa86cc126c3e6eff5b6%26chksm%3Dc19455fbf6e3dced4ccdb561aa06453d6df1b18adb8ee9179ba9c62798bac63839f917413ea7%26scene%3D21%23wechat_redirect" title="从零搭建Pytorch模型教程（三）搭建Transformer网络">从零搭建Pytorch模型教程（三）搭建Transformer网络</a></p> 
<p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247494150%26idx%3D1%26sn%3Dda191e151efb8db5fef1aab64e9bec7d%26chksm%3Dc1945518f6e3dc0e19e5c83f205ae3d24b15c867b9f1038018b18bf7dae597d375f15c13a348%26scene%3D21%23wechat_redirect" title="从零搭建Pytorch模型教程（二）搭建网络">从零搭建Pytorch模型教程（二）搭建网络</a></p> 
<p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493728%26idx%3D1%26sn%3D3a30e67a71f2c18df697680c6004336b%26chksm%3Dc194577ef6e3de68e47294d8121c4f43c8170b114b5490cc129f212e8baf6aa379365a7a6fd9%26scene%3D21%23wechat_redirect" title="从零搭建Pytorch模型教程（一）数据读取">从零搭建Pytorch模型教程（一）数据读取</a></p> 
<p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzkyMDE2OTA3Mw%3D%3D%26mid%3D2247493117%26idx%3D1%26sn%3Dfc82e1477d082db07ce74040cfadcb43%26chksm%3Dc1945ae3f6e3d3f578b7590e9dcca4615a4b560a55735f98cb3eb3d0995210fb905d494028f2%26scene%3D21%23wechat_redirect" title="关于快速学习一项新技术或新领域的一些个人思维习惯与思想总结">关于快速学习一项新技术或新领域的一些个人思维习惯与思想总结</a></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>