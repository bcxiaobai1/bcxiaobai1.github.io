<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>TOOD: Task-aligned One-stage Object Detection 原理与代码解析 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TOOD: Task-aligned One-stage Object Detection 原理与代码解析</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <p><strong>paper：<a href="https://arxiv.org/abs/2108.07755v3" title="TOOD: Task-aligned One-stage Object Detection">TOOD: Task-aligned One-stage Object Detection</a></strong></p> 
<p><strong>code：<a href="https://github.com/fcjian/TOOD" title="https://github.com/fcjian/TOOD">https://github.com/fcjian/TOOD</a> </strong></p> 
<h1>存在的问题 </h1> 
<p>目标检测包括分类和定位两个子任务，分类任务学习的特征主要关注物体的关键或显著区域，而定位任务是为了精确定位整个对象主要关注物体的边界。由于分类和定位学习机制的不同，两个任务学习的特征的空间分布可能会不同，当使用两个独立的分支进行预测时，会导致一定程度的<strong>misalignment</strong>。如下图所示，Result栏第一行是ATSS预测dining table的结果，其中红色和绿色的patch分别是置信度最高和IoU最大的anchor，对应的预测bounding box分别是红色和绿色的box。可以看出，分类得分最高的anchor预测的bounding box和披萨的IoU更大，而和目标IoU最大的anchor分类得分却很低，反映出两个任务的misalignment。而第二行是TOOD的预测结果，可以看出分类得分最高的anchor同时也是IoU最大的。</p> 
<p style="text-align:center"><img alt="" height="544" src="https://images2.imgbox.com/f6/d9/JMF2wa0C_o.png" width="485"></p> 
<h1>本文的创新点</h1> 
<p>为了解决上述存在的问题，本文提出了一种任务对齐的单阶段目标检测模型TOOD（Task-aligned One-stage Object Detection），通过设计一种新的head和alignment-oriented学习方法，来更准确地对齐两个任务。具体如下</p> 
<p><strong>Task-aligned head.</strong> 与传统的单阶段目标检测模型中，分类和定位分别使用两个并行分支的结构不同，本文设计了一个任务对齐的head，以增强两个任务之间的交互，从而使它们的预测保持一致。T-head包括计算task-interactive特征，并通过新提出的Task-Aligned Predictor（TAP）进行预测，然后根据task alignment learning提供的信息对两个预测的空间分布进行对齐。</p> 
<p><strong>Task alignment learning.</strong> 为了进一步克服misalignment问题，作者提出了Task Alignment Learning（TAP）来拉近两个任务最优的anchor。这是通过设计了一个新的样本分配方法和任务对齐的损失函数来实现的，前者通过计算每个anchor的任务对齐程度来分配标签，后者在训练过程中逐步统一最适合两个任务的anchor。因此在推理阶段，一个分类得分最高的anchor同时也是定位精度最高的。</p> 
<h1>方法介绍</h1> 
<p>本文通过新设计的T-head和TAL来对齐两个子任务，如下图所示，两者可以协同工作以改进两个任务的对齐方式。具体来说，T-head首先对FPN特征的分类和定位进行预测，然后TAL计算一个对齐metric，这个指标用来衡量两个任务的对齐程度，最后T-head利用在反向传播过程中从TAL计算得到的对齐指标自动调整分类概率和定位预测。</p> 
<p style="text-align:center"><img alt="" height="584" src="https://images2.imgbox.com/d8/9c/9BF6ntRA_o.png" width="484"></p> 
<h2>Task-aligned Head</h2> 
<h2 style="text-align:center"><img alt="" src="https://images2.imgbox.com/e0/73/N8adcbol_o.png"></h2> 
<p></p> 
<p>如上图所示，(a)是常见的one-stage detector的head结构，(b)是本文提出的T-head，它包括一个简单的特征提取器和两个TAP。为了增强分类和定位之间的interaction，本文使用特征提取器从多个卷积层中学习task-interactive特征，如(b)中间蓝色部分所示。(X^{fpn} in mathbb{R}^Htimes Wtimes C) 是FPN特征，特征提取器使用 (N) 个连续的卷积层和激活函数来计算task-interactive特征，如下所示</p> 
<p style="text-align:center"><img alt="" height="69" src="https://images2.imgbox.com/7f/58/8N8n93Co_o.png" width="532"></p> 
<p>其中 (conv_{k}) 是第 (k) 个卷积层，(delta) 是 (relu) 函数。这样就用head中的单个分支从FPN特征中提取出了丰富的多尺度特征，接着这些交互特征被送进两个TAP进行分类和定位的对齐。</p> 
<p><strong>Task-aligned Predictor（TAP）</strong></p> 
<p>由于单一分支的设计，任务交互特征不可避免的会在两个不同任务之间引入一定程度的特征冲突，这是因为分类和定位的目标不同，因此专注于不同类型的特征，比如不同的层次和感受野。因此本文提出一种<strong>层注意力机制</strong>，通过在层级动态计算不同任务特定的特征从而进行<strong>任务分解</strong>。如图3(c)所示，对于分类和定位，分别计算其task-specific特征：</p> 
<p style="text-align:center"><img alt="" height="36" src="https://images2.imgbox.com/fe/fa/x6gXlhSp_o.png" width="525"></p> 
<p>其中 (omega_{k}) 是学习到的layer attention权重 (omega in mathbb{R}^{N}) 的第 (k) 个元素。(omega) 是根据cross-layer任务交互特征计算得到的，并且能够捕获层之间的交互关系：</p> 
<p style="text-align:center"><img alt="" height="34" src="https://images2.imgbox.com/b7/06/WCyFyxRK_o.png" width="530"></p> 
<p>其中 (fc_{1}) 和 (fc_{2}) 是两个全连接层。(sigma) 是sigmoid激活函数，(x^{inter}) 是对 (X^{inter}) 应用average pooling得到的，而 (X^{inter}) 是对 (X^{inter}_{k}) 进行concatenate得到的。</p> 
<p>最后，分类和定位的预测结果根据每个任务的 (X^{task}) 按下式计算得到：</p> 
<p style="text-align:center"><img alt="" height="34" src="https://images2.imgbox.com/62/cb/LDiukQe1_o.png" width="530"></p> 
<p>其中 (X^{task}) 是对 (X^{task}_{k}) 进行concatenate得到的，(conv1) 是用于降维的1x1卷积，(Z^{task}) 经过 (sigmoid) 转换为分类得分 (Pin mathbb{R}^{Htimes Wtimes 80})，经过 (distance{small -}to{small -}bbox) 转换得到定位预测 (Bin mathbb{R}^{Htimes Wtimes 4})。</p> 
<p><strong>Prediction alignment.</strong></p> 
<p>在预测阶段，通过调整两个预测的空间分布进一步对齐两个任务。和之前使用centerness分支或IoU分支的模型不同，它们只能根据分类特征或定位特征对分类得分进行调整，而本文在用任务交互特征对齐两个预测时同时考虑到了两个任务。值得注意的是作者在两个任务上分别执行对齐方法，如上图(c)所示，作者使用了一个空间概率图 (Min mathbb{R}^{Htimes Wtimes 1}) 来调整分类预测</p> 
<p style="text-align:center"><img alt="" height="30" src="https://images2.imgbox.com/ef/d1/hXbDPbFC_o.png" width="528"></p> 
<p>其中 (M) 是从根据任务交互特征计算得到的，因此在每个空间位置都可以学习到两个任务的一致性。</p> 
<p>同时为了对定位预测进行对齐，还从任务交互特征中学习了空间偏移图 (Oin mathbb{R}^{Htimes Wtimes 8})，具体来说学习到的空间偏移量是最对齐的anchor能够识别周围的最佳边界预测：</p> 
<p style="text-align:center"><img alt="" height="29" src="https://images2.imgbox.com/41/29/AEH9ZVj1_o.png" width="544"></p> 
<p>其中索引 ((i,j,c)) 表示一个tensor中通道 (c) 中 ((i,j)) 位置。式(6)是通过双线性插值实现的，每个通道的偏移量都是独立学习的，这意味着每个对象的每个边界都有其自己学习到的偏移量，这就使得四个边界的预测更加准确，因为每个边界都可以从其附近最精确的anchor进行学习。</p> 
<p>两个对齐特征图 (M) 和 (O) 的计算方式如下</p> 
<p style="text-align:center"><img alt="" height="78" src="https://images2.imgbox.com/2f/87/4n72Lb2H_o.png" width="510"></p> 
<p>其中 (conv1) 和 (conv3) 是两个用于降维的1x1卷积，(M) 和 (O) 的学习是通过接下来要介绍的Task Alignment Learning（TAL）进行的。</p> 
<h2>Task Alignment Learning</h2> 
<p>TAL和之前的方法有两点不同，首先，从任务对齐的角度看，它基于一个单独设计的度量指标动态的挑选高质量的anchor。其次，它同时考虑到了anchor的分配和权重。具体包括一个样本分配策略和一个专门为调整这两个任务而设计的新的损失函数。</p> 
<h3>Task-aligned Sample Assignment</h3> 
<p>为了应对NMS，一个训练示例的样本分配应该满足以下的准则：（1）一个well-aligned的anchor的分类和定位预测都应该很精确。（2）一个misaligned的anchor预测的分类得分应该低这样在NMS中才能被抑制。本文设计了一个anchor对齐度量指标，用来衡量anchor的任务对齐程度，并集成到样本分配和损失函数中，以动态的细化每个anchor的预测。</p> 
<p><strong>Anchor alignment metric.</strong></p> 
<p>分类得分以及预测的bounding box和gt之间的IoU分别表明了两个任务的预测质量，因此作者将两者结合到一起设计了一个新的对齐衡量指标，如下</p> 
<p style="text-align:center"><img alt="" height="30" src="https://images2.imgbox.com/81/0d/QjMjiFB2_o.png" width="515"></p> 
<p>其中 (s) 和 (u) 分别表示分类得分和IoU，(alpha) 和 (beta) 是权重系数用来控制两个任务对任务对齐衡量指标的影响大小。</p> 
<p><strong>Training sample alignment.</strong></p> 
<p>作者将任务对齐指标引入样本分配汇总，具体来说，对每个目标，选择 (t) 值最大的 (m) 个anchor作为正样本，其余的作为负样本。</p> 
<h3>Task-aligned Loss</h3> 
<p><strong>Classification objective.</strong></p> 
<p>作者用 (t) 值替换正样本的二分类标签值1，但是作者发现当 (alpha) 和 (beta) 增大时，(t) 变得非常小从而导致网络无法收敛，因此采用了一个normalized的 (t) 即 (hat{t})，(hat{t}) 的最大值等于和每个对象IoU的最大值。替换后的交叉熵loss如下</p> 
<p style="text-align:center"><img alt="" height="70" src="https://images2.imgbox.com/d2/1b/2zZcWeaC_o.png" width="480"></p> 
<p>为了缓解样本不平衡问题，作者使用了focal loss，将其中对应的标签替换后，最终的分类loss如下</p> 
<p style="text-align:center"><img alt="" height="70" src="https://images2.imgbox.com/6f/48/gcQWYuNq_o.png" width="552"></p> 
<p>其中 (j) 表示 (N_{neg}) 负样本中的第 (j) 个anchor，(gamma) 是权重参数。</p> 
<p><strong>Localization objective.</strong></p> 
<p>和分类一样，回归损失里也加入了 (hat{t}) 来进行re-weight，改进后的GIoU loss如下</p> 
<p style="text-align:center"><img alt="" height="78" src="https://images2.imgbox.com/b6/d8/rbb1FKz8_o.png" width="552"></p> 
<p>其中 (b) 和 (bar{b}) 分别表示预测的bounding box和对应的gt box。</p> 
<h1>代码解析</h1> 
<p>假设batch_size=2，模型的输入shape=(2, 3, 300, 300)，backbone=ResNet-50，neck=FPN，FPN的输出为[(2,256,38,38),(2,256,19,19),(2,256,10,10),(2,256,5,5),(2,256,3,3)]，然后遍历FPN每个level的输出， 分别计算分类和定位的预测结果。接下来的tood head部分实现都在tood_head.py中，这里以第一层(2, 256, 38, 38)为例，首先按式(1)提取task-interactive特征，代码如下，其中self.inter_convs就是6个连续的conv3x3-GN-ReLU，最终得到 (X_{k}^{inter})。</p> 
<pre><code class="language-python"># extract task interactive features
inter_feats = []
for inter_conv in self.inter_convs:
    x = inter_conv(x)
    inter_feats.append(x)  
# [(2,256,38,38),(2,256,38,38),(2,256,38,38),(2,256,38,38),(2,256,38,38),(2,256,38,38)]</code></pre> 
<p>接着将 (X_{k}^{inter}) 拼接成 (X^{inter})，再经过平均池化得到 (x^{inter})，然后分别对分类和定位任务进行分解。对应式(2)-式(4)</p> 
<pre><code class="language-python">feat = torch.cat(inter_feats, 1)  # X^{inter}_{k} -&gt; X^{inter}, (2,1536,38,38),1536=256x6
# task decomposition
avg_feat = F.adaptive_avg_pool2d(feat, (1, 1))  # x^{inter}, (2,1536,1,1)
cls_feat = self.cls_decomp(feat, avg_feat)  # (2,256,38,38)
reg_feat = self.reg_decomp(feat, avg_feat)  # (2,256,38,38)</code></pre> 
<p>任务分解包括layer attention和降维，注意任务交互特征是6层卷积的输出，每层通道数为256，因此拼接后通道数为256x6=1536，但层注意力是每层共享一个权重，因此权重的shape为(2, 6, 1, 1)。</p> 
<p>mmdet中的实现是先将注意力的权重矩阵和降维 (conv1) 的权重矩阵相乘，然后再对任务交互特征进行计算。在paddledetection中这两步是分开的<a href="https://github.com/PaddlePaddle/PaddleDetection/blob/ba2aad26e6bc1e5c2dad76ca96692a0d63eccfac/ppdet/modeling/heads/tood_head.py#L51" title="tood_head.py">tood_head.py</a>，这里仿照paddle的实现将两部分开得到tmp_feat如下面代码中注释部分，最终结果是一致的，但注意torch.allclose中要设置atol=1e2，将结果打印出来发现小数后第四位就会有差异，这可能是框架底层实现的原因。</p> 
<p><strong>另外mmdet和paddle的实现中都没有式(4)中的 </strong>(conv2)。</p> 
<pre><code class="language-python">class TaskDecomposition(nn.Module):
    """Task decomposition module in task-aligned predictor of TOOD.

    Args:
        feat_channels (int): Number of feature channels in TOOD head.
        stacked_convs (int): Number of conv layers in TOOD head.
        la_down_rate (int): Downsample rate of layer attention.
        conv_cfg (dict): Config dict for convolution layer.
        norm_cfg (dict): Config dict for normalization layer.
    """

    def __init__(self,
                 feat_channels,
                 stacked_convs,
                 la_down_rate=8,  # 48
                 conv_cfg=None,
                 norm_cfg=None):
        super(TaskDecomposition, self).__init__()
        self.feat_channels = feat_channels  # 256
        self.stacked_convs = stacked_convs  # 6
        self.in_channels = self.feat_channels * self.stacked_convs  # 256x6=1536
        self.norm_cfg = norm_cfg
        self.layer_attention = nn.Sequential(
            nn.Conv2d(self.in_channels, self.in_channels // la_down_rate, 1),  # 1536//48=32
            nn.ReLU(inplace=True),
            nn.Conv2d(
                self.in_channels // la_down_rate,  # 32
                self.stacked_convs,  # 6
                1,
                padding=0), nn.Sigmoid())

        self.reduction_conv = ConvModule(
            self.in_channels,
            self.feat_channels,
            1,
            stride=1,
            padding=0,
            conv_cfg=conv_cfg,
            norm_cfg=norm_cfg,
            bias=norm_cfg is None)

    def init_weights(self):
        for m in self.layer_attention.modules():
            if isinstance(m, nn.Conv2d):
                normal_init(m, std=0.001)
        normal_init(self.reduction_conv.conv, std=0.01)

    def forward(self, feat, avg_feat=None):  # (2,1536,38,38),(2,1536,1,1)
        b, c, h, w = feat.shape
        if avg_feat is None:
            avg_feat = F.adaptive_avg_pool2d(feat, (1, 1))
        weight = self.layer_attention(avg_feat)  # (2,6,1,1)


        # tmp_weight = weight.unsqueeze(-1)  # (2,6,1,1,1)
        # tmp_feat = feat.reshape(2, 6, 256, 38, 38)
        # tmp_feat = tmp_feat * tmp_weight  # (2,6,256,38,38)
        # # tmp_feat = tmp_feat.reshape(2, 1536, 38, 38)
        # tmp_feat = tmp_feat.flatten(1, 2)  # (2,1536,38,38)
        # tmp_feat = self.reduction_conv(tmp_feat)  # (2,256,38,38)


        # here we first compute the product between layer attention weight and
        # conv weight, and then compute the convolution between new conv weight
        # and feature map, in order to save memory and FLOPs.
        conv_weight = weight.reshape(
            b, 1, self.stacked_convs,
            1) * self.reduction_conv.conv.weight.reshape(
                1, self.feat_channels, self.stacked_convs, self.feat_channels)
        # (2,6,1,1)-&gt;(2,1,6,1) * (256,1536,1,1)-&gt;(1,256,6,256) = (2,256,6,256)
        conv_weight = conv_weight.reshape(b, self.feat_channels,
                                          self.in_channels)  # (2,256,1536)
        feat = feat.reshape(b, self.in_channels, h * w)  # (2,1536,1444)
        feat = torch.bmm(conv_weight, feat).reshape(b, self.feat_channels, h,
                                                    w)  # (2,256,1444)-&gt;(2,256,38,38)
        if self.norm_cfg is not None:
            feat = self.reduction_conv.norm(feat)
        feat = self.reduction_conv.activate(feat)  # ReLU(inplace=True), (2,256,38,38)
        # 好像少了式(4)中的conv2


        # t = torch.allclose(tmp_feat, feat, atol=1e-2)
        # print(tmp_feat[0][0][0])
        # print(feat[0][0][0])
        # print(t)
        # exit()

        return feat</code></pre> 
<p>接下来计算分类预测和分类的空间概率图，其中sigmoid_geometric_mean是将入参分别取sigmoid、相乘、开根，对应式（5）</p> 
<pre><code class="language-python"># cls prediction and alignment
cls_logits = self.tood_cls(cls_feat)  # 没有sigmoid的P, (2,20,38,38)
cls_prob = self.cls_prob_module(feat)  # 没有sigmoid的M, (2,1,38,38)
cls_score = sigmoid_geometric_mean(cls_logits, cls_prob)  # P^{align}, (2,20,38,38)</code></pre> 
<p>接着计算定位预测和定位空间偏移图，其中deform_sampling是根据学习到的空间偏移图对定位预测进行调整，其中具体实现用的可分离卷积，和双线性插值是一样的。</p> 
<pre><code class="language-python"># reg prediction and alignment
if self.anchor_type == 'anchor_free':
    reg_dist = scale(self.tood_reg(reg_feat).exp()).float()  # (2,4,38,38), 为什么要exp
    reg_dist = reg_dist.permute(0, 2, 3, 1).reshape(-1, 4)  # (2,4,38,38)-&gt;(2,38,38,4)-&gt;(2888,4)
    reg_bbox = distance2bbox(
        self.anchor_center(anchor) / stride[0],
        reg_dist).reshape(b, h, w, 4).permute(0, 3, 1,
                                              2)  # (2888,4)-&gt;(2,38,38,4)-&gt;(2,4,38,38)
reg_offset = self.reg_offset_module(feat)  # O, (2,8,38,38)
bbox_pred = self.deform_sampling(reg_bbox.contiguous(),
                                 reg_offset.contiguous())  # (2,4,38,38)
</code></pre> 
<p>在对分类和定位预测结果进行调整后得到模型的最终预测，接下来就是正负样本分配和计算损失。在task_aligned_assigner.py中针对每个对象计算了 (t) 值并挑选 (m) 个 (t) 值最大的anchor作为正样本，其余作为负样本，代码如下，其中<strong>m=13</strong></p> 
<pre><code class="language-python"># select top-k bboxes as candidates for each gt
alignment_metrics = bbox_scores ** alpha * overlaps ** beta  # t
topk = min(self.topk, alignment_metrics.size(0))  # 13</code></pre> 
<p>在分配好正负样本后，就是计算损失函数，回到tood_head.py中，在函数_get_target_single()中计算每个anchor对应的分类标签和回归target，其中对 (t) 值normalized得到 (hat{t})，代码如下</p> 
<pre><code class="language-python">pos_norm_alignment_metrics = pos_alignment_metrics / (
        pos_alignment_metrics.max() + 10e-8) * pos_ious.max()  # normalized t</code></pre> 
<p><strong>注意</strong>，在mmdet的实现中，前4个epoch，标签分配用的是ATSS，topk=9，损失函数用的是Focal loss。之后标签分配改为本文提出的TaskAligned标签分配方法，topk=13，损失函数改为Qualiy Focal loss。定位损失全程是GIoU loss。</p> 
<p>另外一些参数的选择，提取任务交互的连续卷积层个数 (N=6)，anchor对齐度量指标 (t) 中 (alpha=1,beta=6)，标签分配取 (t) 值最大的 (m=13) 个anchor作为正样本。</p> 
<h1>实验结果</h1> 
<p>下面是一些本文提出的T-head+TAL的测试图，以及和ATSS的对比，可以看出T-head+TAL可以很好的对齐两个预测，最终分类得分最高的预测同时也是IoU最大的。</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/a2/5e/IBMT5GKL_o.png"></p> 
<p>下面是和一些当前主流的单阶段检测模型以及标签分配方法的对比，TOOD取得了SOTA的结果。</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/54/6a/kXErsnhZ_o.png"></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>