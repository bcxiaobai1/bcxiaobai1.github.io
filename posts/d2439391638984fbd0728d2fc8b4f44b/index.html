<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>目标检测：R-CNN论文翻译 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">目标检测：R-CNN论文翻译</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <h1>摘要：</h1> 
<p>在本文中，我们提出一个检测算法，R-CNN，在VOC2012中，map到达53.3%。我们主要结合了两个关键的想法，（1）将高容量的卷积神经网络用于自底向上的区域候选框，为了定位及分割对象。（2）当标记训练数据稀疏时，选进行监督与训练，然后进行特定区域的精细调优。最终效果很好。</p> 
<h1>1. Introduction</h1> 
<p>以前目标检测大多基于SIFT和HOG获得图像局部特征。1990年CNN被大量使用，随着SVM支持向量机的兴起cnn过时，又因在ILSVRC竞赛中获得高的精确度而兴起。</p> 
<p>在Imagenet竞赛中RCNN获得很好的成绩，在ILSVRC2012讨论会上，提出了一个问题，即模型的泛化能力如何？（CNN在ImageNet上的分类结果在多大程度上推广到PASCALVOC挑战上的对象检测结果?）</p> 
<p>回答上述问题：通过在图像分类和目标检测之间的差距来回答此问题。这篇文章第一次展示cnn在PASCALVOC上的检测效果比HOG特征有显著的提升。为了实现这一的结果，我们关注下面的问题：(a)使用深度网络定位对象、(b)训练使用少量的注释检测数据的大容量模型。</p> 
<p>目标检测需要在图像中识别对象的具体位置，（1）我们把此类定位问题看做回归问题regression problem。（2）建立滑动窗口检测器sliding window detector，适用于受约束的对象类别。以前为了保持高分辨率图像，通常CNN只有两个卷积层和池化层，现在提出具有5个卷积层，在输入图像中有非常大的接受域(195 × 195像素)和步长(32×32像素)，这使得在滑动窗口的精确定位成为一个开放的技术挑战。</p> 
<p>解决问题（a):在本文章提出“使用区域识别”方法来解决CNN定位问题，这种方法在目标检测及语义分割中成功地使用。在测试时，首先，输入图像生成大约2000个独立的候选框；其次cnn从每个候选框中提取一个固定长度的特征向量；然后，使用SVM对每个候选框进行分类。输出的候选框大小一致（我们使用一种简单的技术(仿射图像扭曲)从每个区域建议中计算固定大小的CNN输入，而不考虑区域的形状）流程：输入图像-&gt;生成2k个候选区-&gt;CNN提取特征-&gt;使用SVM分类</p> 
<p>使用候选框的实验结果：在200级的ILSVRC2013数据及上，RCNN vs OverFeat比较，mAP之比 31.4% vs 24.3%，显然rcnn优于overFeat.</p> 
<p>解决问题（b）：问题是有标记的数据少，不足以训练大型的CNN。传统方法：使用无监督的预训练，然后是有监督的微调。本文的方法:在大型数据集（ILSVRC）上进行有监督的预训练，然后再小型数据(PASCAL)上进行特定领域的微调,这是在数据稀缺时学习高容量cnn的有效方法。实验结果表明，这种方法可以提高模型的检测精度。</p> 
<p>模型相当高效。唯一的类特定的计算是一个相当小的矩阵向量乘积和贪婪的非最大抑制。这计算属性来源于所有类别共享的特征，而且这些特征比以前使用的区域特征低两个数量级。</p> 
<p>失效模式：了解方法的失效模式对于改进RCNN是很重要的，因此我们使用Hoiem等人提供的检测分析工具，分析直接表明，一个简单的边界盒回归方法显著减少了错误定位，这是主要的错误模式。</p> 
<p>在语义分割任务上，模型RCNN在PASCALVOC分割任务上也取得了具有竞争力的结果，在VOC2011测试集上mAP=47.9%</p> 
<h1>2. Object detection with R-CNN</h1> 
<p>检测系统包含三个模块：（1）模块：生成独立的候选框，这些框定义检测器可用的候选检测集；（2）模块：一个大型卷积神经网络，从每个区域提取固定长度的特征向量；（3）模块：一组类特定的线性支持向量机；本节主要工作：介绍每个模块的设计决策，描述他们的测试使用时间，详细介绍如何学习其参数，并展示PASCAL VOC 2010-12和ILSVRC2013上的检测结果。</p> 
<h2>2.1. Module design</h2> 
<h3>Region proposals候选区域：</h3> 
<p>包括：objectness 对象性[1], selective search选择性搜索 [39], category-independent object proposals 类别无关的对象建议[14], constrained parametric min-cuts (CPMC)约束参数最小切割 [5], multi-scale combinatorial grouping多尺度组合分组 [3], and Cires 5can [6]。虽然R-CNN对特定区域提议方法不可知，但我们使用选择性搜索来实现与先前检测工作的比较。</p> 
<h3>Feature extraction特征提取：</h3> 
<p>使用Krizhevsky等人描述的CNN实现从每个候选框中使用Caffe提取4096维特征向量。通过5个卷积层和2个全连通层正向传播均值减去的227 × 227 RGB图像计算特征。</p> 
<p>对图形进行扭曲，输入像素大小固定：为了计算候选框的特征，我们必须首先将该区域的图像数据转换为与CNN兼容的形式(其体系结构要求输入固定的227 × 227像素大小)。在任意形状区域的许多可能转换中，我们选择最简单的一种。无论候选区域的大小或纵横比如何，我们都将其周围一个紧密边界框中的所有像素扭曲到所需的大小。在扭曲之前，我们扩大了紧密的边界框，这样在扭曲的尺寸下，原始框周围刚好有p个扭曲的图像上下文像素(我们使用p = 16)。</p> 
<h2>2.2. Test-time detection测试时间检测</h2> 
<p>在测试时，我们在测试图像上运行“fast mode”的选择性搜索，提取2K个候选框。为了计算特征，对每个候选框进行扭曲，并通过Cnn向前传播。然后，对于每个类，使用为该类训练的支持向量机对每个提取的特征向量进行评分。计算出所有候选框的评分，应用非极大抑制，如果一个区域与一个高于学习阈值的评分选择区域有交叉过并(IoU)重叠，则拒绝该区域。（意思指保留IOU值大的候选框）</p> 
<h3>Run-time analysis 运行时间分析：</h3> 
<p>影响检测时间的属性有两个，（1）第一个CNN参数共享，（2)第二个利用CNN计算出的特征向量是低维的。</p> 
<p>(1)这种共享的结果是，花费在计算候选框和特征上的时间(GPU上13秒/张图像或CPU上53秒/张图像)被摊到所有类上。唯一与类相关的计算是特征与支持向量机权重和非最大抑制之间的点积。在实践中，一个图像的所有点积都被批处理成一个矩阵与矩阵乘积。特征矩阵通常是2000×4096，支持向量机权重矩阵是4096×N，其中N是类的数量。</p> 
<p>（2）对于UVA系统，由于其高维特征，将会慢两个数量级，并且需要134GB的内存来存储100k线性预测器，相比之下，我们的低维特征只需要1.5GB。将R-CNN与Dean等人最近在使用DPMs和哈希[8]进行可伸缩检测方面的工作进行对比也很有趣。他们报告说，VOC 2007的mAP值约为16%，每张图像运行时间为5分钟，当引入10k分散类时。使用我们的方法，10k检测器可以在CPU上大约一分钟内运行，因为没有进行近似，mAP将保持在59%(章节3.2)。</p> 
<h2>2.3. Training</h2> 
<h3>Supervised pre-training</h3> 
<p>有监督的预训练：我们在一个大型辅助数据集(ILSVRC2012分类)上只使用图像级注释(该数据无法使用边界框标签)对CNN进行有区别的预训练。使用开源Caffe CNN库进行预训练。简而言之，我们的CNN几乎与Krizhevsky等人的性能相匹配，在ILSVRC2012分类验证集上获得了高2.2个百分点的top-1错误率。这种差异是由于训练过程的简化造成的。</p> 
<h3>Domain-specific fine-tuning 特定领域的微调</h3> 
<p>微调学习率及IOU的阈值：为了使我们的CNN适应新的任务(检测)和新的域(扭曲建议窗口)，我们只使用扭曲区域建议继续训练CNN参数的随机梯度下降(SGD)。除了用一个随机初始化的(N + 1)方式分类层(其中N是对象类的数量，加上1是背景)替换CNN的特定于imagenet1000方式分类层之外，CNN的体系结构没有改变。VOC， N = 20, ILSVRC2013 N = 200。我们对所有候选框的IOU值与真实框重叠部分进行分类，IOU&gt;=0.5框是积极的，其余为负面的。以学习率为0.001开始进行SGD迭代，初试预训练的学习率为0.1），这是允许微调，同时不破坏初始化。在每次SGD迭代中，我们统一采样32个正窗口(所有类)和96个背景窗口，以构建一个大小为128的小批处理。我们偏向于正窗口的采样，因为它们与背景相比极其罕见。</p> 
<h3>Object category classifiers.对象类别分类器</h3> 
<p>IoU重叠阈值对mAP的影响：例如，考虑训练一个二进制分类器来检测汽车。一个紧紧包围汽车的图像区域应该是一个积极的例子，与汽车无关的背景区域应该是一个负面的例子。不太清楚的是，如何给部分与汽车重叠的区域贴上标签。我们通过IoU重叠阈值来解决这个问题，低于这个阈值的区域被定义为负值。重叠阈值0.3是通过对{0,0.1，…0.5}进行网格搜索选定的。我们发现仔细选择这个阈值很重要。将其设置为0.5，如[39]，mAP减少5点。类似地，将其设置为0将mAP减少4点。正例被简单地定义为每个类的基本实际边界框。</p> 
<p>一旦特征提取和训练标签应用，我们优化每个类一个线性支持向量机。由于训练数据太大，无法装入内存，我们采用标准难负挖掘方法[17,37]（把错误再送回模型训练，直到效果不再提升为止）难负挖掘收敛很快，实际上mAP只在所有图像上经过一次后就停止增加。</p> 
<h2>2.4. Results on PASCAL VOC 2010-12</h2> 
<p>根据PASCAL VOC最佳实验，我们验证了VOC 2007数据集上的所有设计决策和超参数(章节3.2)。对于VOC 2010-12数据集的最终结果，我们对VOC 2012训练上的CNN进行了微调，并优化了VOC 2012训练上的检测支持向量机。对于两种主要算法变体(包含和不包含边界框回归)，我们只向评估服务器提交了一次测试结果。</p> 
<p>表1显示了VOC 2010的完整结果。我们将我们的方法与四个强基线进行比较，其中包括SegDPM，它将DPM检测器与语义分割系统[4]的输出结合起来，并使用额外的检测器间上下文和图像分类器重新记录。最相关的比较是Uijlings等人的UVA系统，因为我们的系统使用相同的区域提议算法。为了对区域进行分类，他们的方法建立了一个四层空间金字塔，并将其填充密集采样SIFT，Extended Opponent SIFT和RGB-SIFT描述符，每个向量用4000字的码本量化。用直方图交集核支持向量机进行分类。与他们的多特征、非线性核支持向量机方法相比，我们实现了mAP的很大改进，mAP从35.1%提高到53.7%，同时速度也快得多(第2.2节)。我们的方法在VOC 2011/12测试中达到了类似的性能(53.3% mAP)。</p> 
<h2>2.5. Results on ILSVRC2013 detection</h2> 
<p>我们使用与PASCAL VOC相同的系统超参数，在200类ILSVRC2013检测数据集上运行R-CNN。我们遵循向ILSVRC2013评估服务器提交测试结果的相同协议，只有两次，一次有边界框回归，一次没有边界框回归。</p> 
<p>图3比较了R-CNN与ILSVRC 2013年比赛的参赛作品以及比赛后OverFeat的结果[34]。R-CNN的mAP值为31.4%，明显领先于OverFeat的24.3%，位居第二。为了了解AP在各个类的分布情况，还给出了框图，并在论文末尾的表8中列出了各个类的AP。大多数参赛作品(OverFeat、NEC-MU、UvAEuvision、Toronto A和UIUC-IFP)使用了卷积神经网络，这表明在如何将cnn应用于目标检测方面存在显著的细微差别，导致结果差异很大。</p> 
<h1>3. Visualization, ablation, and modes of error可视化，消融和错误模式</h1> 
<h2>3.1. Visualizing learned features 可视化学习到的特征</h2> 
<p>第一层过滤器可以直接可视化，很容易理解[25]。它们捕捉有方向的边缘和对立色。理解后面的层更具挑战性。Zeiler和Fergus在[42]中提出了一种具有视觉吸引力的反卷积方法。我们提出了一个简单的(和补充的)非参数方法，直接显示网络学习的内容。</p> 
<p>其想法是在网络中选出一个特定的单元(特征)，并将其作为一个独立的对象检测器来使用。也就是说，我们计算一个大集合的保留区域提议(大约1000万)上的单位激活，从最高到最低的激活排序提议，执行非最大抑制，然后显示得分最高的区域。我们的方法让所选的单元“自己说话”，显示它发射的确切输入。我们避免平均，以看到不同的视觉模式和获得洞察的不变性计算的单位。</p> 
<p>我们将来自层pool5的单元可视化，这是网络的第五个也是最后一个卷积层的maxpooled输出。pool5特征图为6 × 6 × 256 = 9216维。忽略边界效应，每个pool5单元在原始的227×227像素输入中有一个195×195像素的接收域。中央的pool5单元几乎可以看到全局视图，而靠近边缘的单元则有一个较小的剪切支架。</p> 
<p>图4中的每一行都显示了来自我们在VOC 2007培训活动中微调的CNN的pool5单元的前16个激活。256个功能独特的单元中的6个是可视化的(附录D包括更多)。选择这些单元是为了展示网络学习的代表性样本。在第二行中，我们看到一个向狗的脸和点数组发射的单元。第三行对应的单元是一个红色斑点探测器。也有针对人脸和更抽象的模式(如文本和带有窗口的三角形结构)的探测器。该网络似乎学习了一种将少量类调优特征与形状、纹理、颜色和材料属性的分布式表示相结合的表示。随后的全连接层fc6有能力对这些丰富特征的大量组成进行建模。</p> 
<h2>3.2. Ablation studies消融研究</h2> 
<h3>不进行微调时每一层的性能，</h3> 
<p>为了检测哪一层对性能的影响较大，我们对CNN的最后三层进行分析，在数据集VOC2007上。pool5在上一节已经介绍了，下面主要对后两层进行总结。</p> 
<p>fc6全连接到pool5 ，为了计算特征，将4096*9216权重矩阵乘以pool5特征映射(重新塑造为9216维向量)，然后添加一个偏置向量。这个中间向量是按分量整流的半波（x&lt;-max(0,x))</p> 
<p>fc7层是网络的最后一层。它是通过将fc6计算出的特征乘以一个4096 × 4096的权重矩阵来实现的，并类似地添加一个偏差向量并应用半波整流。</p> 
<p>我们首先查看在没有微调的CNN在PASCAL数据集上的结果，即所有CNN参数仅在ILSVRC 2012上预训练。逐层分析性能(表2行1-3)显示fc7的特性比fc6的特性泛化得更差。这意味着，在不降低mAP的情况下，CNN的29%(约1680万)参数可以被删除。更令人惊讶的是，除去fc7和fc6会产生相当好的结果，即使pool5特征只使用CNN的6%的参数来计算。CNN的大部分表现能力来自它的卷积层，而不是更大的紧密连接层。这一发现表明，仅使用CNN的卷积层计算任意大小图像的密集特征图(HOG)具有潜在的实用价值。这种表示方式将支持在pool5特性之上使用滑动窗口检测器(包括DPM)进行实验。（既然除去fc7和fc6会产生相当好的结果，为什么还加入呢？）</p> 
<h3>进行微调时每一层的性能</h3> 
<p>经过微调CNN参数后，在数据及VOC2007上的训练。改进后是显著(表2行4-6):微调将mAP提高了8.0个百分点，达到54.2%。fc6和fc7的微调效果要比pool5大得多，这表明从ImageNet中学到的pool5特性是通用的，大多数改进都是通过在它们之上学习特定领域的非线性分类器获得的。</p> 
<h3>与最近的特征学习方法的比较。</h3> 
<p>在PASCAL VOC检测中使用的特征学习方法相对较少。我们来看两种建立在可变形零件模型上的最新方法。作为参考，我们还包括了标准的HOG基于DPM的结果。</p> 
<p>第一种DPM特征学习方法：DPM ST用“sketch token”概率的直方图来扩充HOG特征，直观地说，草图标记是通过图像补丁中心的等高线的紧密分布。草图令牌概率在每个像素上由随机森林计算，该森林经过训练，将35 × 35像素补丁划分为150个草图令牌或背景中的一个。（DPM是一个目标检测算法）</p> 
<p>第二种DPM HSC方法：第二种方法DPM HSC用稀疏编码直方图( HSC)代替HOG。为了计算一个HSC，稀疏代码激活在每个像素处使用1007 × 7像素(灰度)原子的学习字典求解。产生的激活以三种方式进行整流(全波和两个半波)，空间池化，单元l2归一化，然后能量转换。</p> 
<p>比较map:所有R-CNN变体的性能都明显优于三个DPM基线(表2行8-10)，包括两个使用特性学习的基线。与只使用HOG特性的最新版本DPM相比，我们的mAP提高了20多个百分点:54.2% vs. 33.7%——相对提高了61%。HOG和草图令牌的组合比HOG单独产生2.5个mAP点，而HSC比HOG提高了4个mAP点(当与它们的私有DPM基线内部比较时——两者都使用DPM的非公共实现，其性能低于开源版本[20])。这些方法的map分别达到29.1%和34.3%。</p> 
<h2>3.3. Network architectures</h2> 
<p>本文的大部分结果使用了Krizhevsky等人[25]的网络架构。然而，我们发现架构的选择对R-CNN检测性能有很大影响。在表3中，我们展示了使用Simonyan和Zisserman[43]最近提出的16层深度网络对VOC 2007测试的结果。在最近的ILSVRC 2014分类挑战中，该网络是表现最好的网络之一。该网络是由13层3 × 3卷积核组成的均匀结构，其中5层最大池化层穿插其中，顶部有3层全连接层。我们把这个网络称为”O-Net”OxfordNet，把TorontoNet称为“T-Net”。</p> 
<p>网络骨架是VGG16：为了在R-CNN中使用O-Net，我们从Caffe model zoo下载了公开的VGG16模型的预训练网络权值。然后我们使用与T-Net相同的协议对网络进行了优化。唯一的区别是根据需要使用更小的小批量(24个示例)，以适应GPU内存。表3结果显示，O-Net的RCNN明显优于TNet的R-CNN, mAP从58.5%提高到66.0%。然而，在计算时间方面有相当大的缺点，O-Net的前向通过时间大约是T-Net的7倍。</p> 
<h2>3.4. Detection error analysis检测误差分析</h2> 
<p>使用检测分析工具检测模型：我们应用了来自Hoiem等人[23]的优秀检测分析工具，以揭示我们方法的错误模式，理解微调如何改变它们，并查看我们的错误类型与DPM的比较。分析工具的完整总结超出了本文的范围，我们鼓励读者咨询[23]以了解更详细的细节(如“规范化AP”)。由于分析最好在相关的图的上下文中进行，我们在图5和图6的标题中进行讨论。</p> 
<h2>3.5. Bounding-box regression 回归后的边界框</h2> 
<p>提出线性回归模型：在误差分析的基础上，实现了一种减少定位误差的简单方法。受到DPM[17]中使用的边界盒回归的启发，我们训练一个线性回归模型，以预测一个新的检测窗口，并给出一个选择性搜索区域建议的pool5特征。附录c给出了详细信息。表1、表2和图5的结果显示，这种简单的方法修复了大量的错误本地化检测，将mAP提高了3到4个点。</p> 
<h2>3.6. Qualitative results 定性结论</h2> 
<p>ILSVRC2013定性检测结果见文末图8和图9。每个图像从val2集合中随机采样，所有检测器的所有检测精度大于0.5显示，这些都是没有策划的，并给出了运行中的检测器的真实印象。更多的定性结果显示在图10和图11中，但这些都是经过策划的。我们选择每张图片是因为它包含有趣的、令人惊讶的或有趣的结果。在这里，也显示了精度大于0.5的所有检测。</p> 
<h1>4. The ILSVRC2013 detection dataset</h1> 
<p>PASCAL VOC，需要选择如何使用它。由于这些决策并非微不足道，因此我们将在本节中讨论它们。</p> 
<h2>4.1. Dataset overview 数据的介绍</h2> 
<p>ILSVRC2013有三个部分组成：train(395,918)、val(20,121)和test(40,152)。Val和test分割是从相同的图像分布绘制的。这些图像像场景一样，在复杂性(物体的数量，杂乱的数量，姿势的变化等)上与PASCAL VOC图像相似。val和test分割进行了详尽的注释，这意味着在每个图像中，所有200个类的所有实例都用边界框标记。相比之下，训练集是从ILSVRC2013分类图像分布中绘制的。这些图像有更多可变的复杂性，倾向于一个中心物体的图像。与val和test不同的是，训练图像(由于它们的数量很大)没有完全注释。在任何给定的训练图像中，来自200个类的实例可能会被标记，也可能不会被标记。除了这些图像集，每个类都有一组额外的负图像。手动检查负映像，以验证它们不包含其关联类的任何实例。在这项工作中没有使用负图像集。更多关于ILSVRC如何被收集和注释的信息可以在[11,36]中找到。</p> 
<p>这些划分为训练R-CNN提供了多种选择。训练图像不能用于难负例挖掘，因为注释不是穷尽的。负面的例子从何而来?此外，训练图像具有不同于val和test的统计量。训练图像是否应该被使用?如果是，使用到什么程度?虽然我们没有彻底评估大量的选择，但我们根据以前的经验提出了似乎是最明显的路径。</p> 
<p>我们的总体策略是严重依赖val集，并使用一些训练图像作为正例的辅助来源。为了同时使用val进行训练和验证，我们将它分为大小大致相同的“val1”和“val2”集。由于有些类在val中只有很少的示例(最小的只有31个，一半的示例少于110)，因此产生一个近似类平衡的分区是很重要的。为了做到这一点，生成了大量的候选分割和最大相对最小的一个阶级失衡被选中。每个候选分割都是通过使用val图像的类数作为特征聚类生成的，然后进行随机局部搜索，以提高分割的平衡。这里使用的特殊分割的最大相对不平衡约为11%，中位数相对不平衡约为4%。val1/val2拆分和用于生成它们的代码将公开，以便其他研究人员比较本报告中使用的val拆分方法。</p> 
<h2>4.2. Region proposals候选框</h2> 
<p>我们采用了在PASCAL上用于检测的相同候选框方法。选择性搜索[39]以“快速模式”在val1、val2和test中的每个图像上运行(但不在训练图像上运行)。为了处理选择性搜索不是尺度不变的事实，因此产生的区域数量取决于图像分辨率，需要进行一个小的修改。ILSVRC图像的大小从非常小到几百万像素不等，因此我们在运行选择性搜索之前将每张图像调整为固定宽度(500像素)。在val上，选择性搜索的结果是每张图像平均2403个候选框，对所有真实框边界框的召回率为91.6% (0.5 IoU阈值)。这一召回率明显低于PASCAL，后者约为98%，表明在区域提案阶段有显著的改进空间。</p> 
<h2>4.3. Training data</h2> 
<p>对于训练数据，我们形成了一组图像和方框，其中包括val1中的所有选择性搜索和真实框，以及train中的每个类中最多N个真实方框(如果一个类的train中真实方框少于N个，则我们将它们全部取走)。我们称这个图像和盒子的数据集为val1+trainN。在消融研究中，我们显示了N∈{0,500,1000}时val2上的mAP(第4.5节)。</p> 
<p>在R-CNN中有三个步骤需要训练数据:(1)CNN微调，(2)检测器支持向量机训练，(3)边界框回归器训练。CNN微调在val1+trainN上运行50k SGD迭代，使用与PASCAL完全相同的设置。使用Caffe对一台NVIDIA特斯拉K20进行微调花了13个小时。对于支持向量机训练，使用val1+trainN中的所有真实框作为各自类的正例。对val1中随机选择的5000张图像子集执行难负例挖掘。最初的实验表明，与5000个图像子集(大约一半)相比，从val1中难负挖掘只会导致mAP下降0.5个百分点，同时减少一半的SVM训练时间。在训练中没有消极的例子，因为注释不是详尽的。多余的经过验证的消极图像组没有被使用。在val1上训练边界框回归。</p> 
<h2>4.4. Validation and evaluation 验证和评估</h2> 
<p>在向评估服务器提交结果之前，我们使用上面描述的训练数据验证了数据使用选择以及微调和边界框回归对val2集的影响。所有的系统超参数(例如SVM C超参数、用于区域扭曲的填充、NMS阈值、边界框回归超参数)都固定在PASCAL所用的相同值上。毫无疑问，其中一些超参数选择对于ILSVRC来说略显次优，然而，这项工作的目标是在不进行大量数据集调优的情况下，在ILSVRC上生成一个初步的R-CNN结果。在val2上选择了最佳选项后，我们向ILSVRC2013评估服务器提交了恰好两个结果文件。第一次提交不使用界限盒回归，第二次提交使用界限盒回归。对于这些提交，我们扩展了支持向量机和边界盒回归训练集，分别使用val+train1k和val。我们使用在val1+train1k上进行微调的CNN，以避免重新运行微调和特征计算。</p> 
<h2>4.5. Ablation study 消融实验</h2> 
<p>表4显示了不同数量的训练数据、微调和边界盒回归的影响的消融研究。第一个观察结果是，val2上的mAP与test上的mAP非常接近。这让我们相信，val2上的mAP是测试集性能的一个很好的指示器。第一个结果是20.9%，这是R-CNN使用在ILSVRC2012分类数据集上预训练的CNN(没有进行精细调优)并获得val1中少量训练数据的访问权(回想一下，val1中的一半类有15到55个示例)获得的结果。将训练集扩展到val1+训练N可以将性能提高到24.1%，在N = 500和N = 1000之间基本没有差别。使用val1中的示例对CNN进行微调，得到了适度的改进，达到26.5%，但由于正训练示例数量较少，可能存在显著的过拟合。将微调集扩展到val1+train1k，这将从列车集的每个类中添加多达1000个正示例，这将有显著帮助，将mAP提高到29.7%。有界盒回归将结果提高到31.0%，这是一个较小的相对增益在PASCAL中观察到的。</p> 
<h2>4.6. Relationship to OverFeat</h2> 
<p>R-CNN和OverFeat之间有一个有趣的关系:OverFeat可以被(粗略地)视为R-CNN的一个特例。如果要替换一个候选框使用规则正方形区域的多尺度金字塔，并将每个类的边界框回归器更改为单个边界框回归器，那么系统将非常相似(对训练方式中一些潜在的显著差异取模:CNN检测微调、使用支持向量机等)。值得注意的是，OverFeat在速度上比R-CNN有明显的优势:从[34]引用的每张图片2秒的数据来看，它大约快9倍。这种速度来自于OverFeat的滑动窗口(即候选框)在图像级别上没有扭曲，因此计算可以很容易地在重叠窗口之间共享。共享是通过在任意大小的输入上以卷积方式运行整个网络来实现的。加快R-CNN的速度应该可以通过多种方式实现，这仍然是未来的工作。</p> 
<h1>5. Semantic segmentation 语义分割</h1> 
<p>区域分类是语义分割的标准技术，允许我们轻松地将R-CNN应用于PASCAL VOC分割挑战。为了便于与当前领先的语义分割系统(称为O2P，表示“二阶池”)[4]进行直接比较，我们在他们的开源框架中工作。O2P使用CPMC为每张图像生成150个区域建议，然后使用支持向量回归(SVR)预测每个类的每个区域的质量。他们方法的高性能是由于CPMC区域的质量和多种特征类型(SIFT和LBP的丰富变体)的强大二阶池化。我们还注意到Farabet等人[16]最近在使用CNN作为多尺度每像素分类器的几个密集场景标记数据集(不包括PASCAL)上展示了良好的结果。</p> 
<p>我们遵循[2,4]并扩展PASCAL分割训练集，以包括Hariharan等人[22]提供的额外注释。设计决策和超参数在VOC 2011验证集上进行交叉验证。最终的测试结果只评估了一次。</p> 
<h3>CNN features for segmentation.</h3> 
<p>CNN feature用于分割。我们评估了三种计算CPMC区域特征的策略，所有这些策略都是从将该区域周围的矩形窗口扭曲为227 × 227开始的。第一个策略忽视了区域的形状和并直接在弯曲的窗口上计算CNN特征，就像我们在检测时所做的一样。然而，这些特征忽略了区域的非矩形形状。两个区域可能有非常相似的边界框，但重叠很少。因此，第二种策略(fg)只计算区域前景掩码上的CNN特征。我们用均值输入代替背景，使背景区域在均值相减后为零。第三种策略(full+fg)只是将full和fg功能连接起来;我们的实验验证了它们的互补性。</p> 
<h3>Results on VOC 2011.</h3> 
<p>表5显示了我们对VOC 2011验证集与O2P验证集的结果总结。(请参阅附录E以了解每个类别的完整结果。)在每个特征计算策略中，fc6层总是优于fc7层，下面的讨论参考fc6特征。fg策略的表现略优于full策略，表明遮蔽区域形状提供了更强的信号，与我们的直觉相匹配。然而，full+fg的平均准确率为47.9%，我们的最佳结果是4.2%的差距(也略优于O2P)，这表明，即使在fg特征的情况下，full特征提供的上下文也具有很高的信息量。值得注意的是，训练20个SVRs使用我们的full+fg功能在单个核上需要一个小时，而在O2P功能上需要10多个小时。</p> 
<p>在表6中，我们展示了VOC 2011测试集的结果，将我们性能最好的方法fc6 (full+fg)与两个强基线进行比较。我们的方法在21个类别中的11个类别中获得了最高的分割精度，并且在类别之间的平均分割精度最高，达到了47.9%(但可能与O2P结果在任何合理的误差范围内相一致)。通过微调，仍有可能获得更好的性能。</p> 
<h1>6. Conclusion</h1> 
<p>近年来，目标检测性能一直停滞不前。表现最好的系统是将多个低级图像特征与来自物体检测器和场景分类器的高级背景相结合的复杂集成。本文提出了一种简单且可扩展的对象检测算法，该算法相对于PASCAL VOC 2012上的最佳结果有30%的改进。</p> 
<p>我们通过两个想法实现了这一性能。第一个是将大容量卷积神经网络应用于自底向上的候选框，以定位和分割对象。第二种是在标记训练数据稀缺的情况下训练大型cnn。我们表明，对数据丰富的辅助任务(图像分类)进行预先训练(带有监督)，然后对数据稀缺的目标任务(检测)微调网络是非常有效的。我们推测，“有监督的训练前/特定领域的微调”方法对于各种数据稀缺的视觉问题将是非常有效的。</p> 
<p>最后，我们指出，通过结合使用来自计算机视觉和深度学习的经典工具(自底向上候选框和卷积神经网络)，我们获得了这些结果，这是非常重要的。</p> 
<p>现在模型：R-CNN</p> 
<p>以前的模型：SIFT、HOG、CNN、OverFeat</p> 
<p>模型骨架：VGG16</p> 
<p>模型结构：1、5个卷积层和2个全连接层</p> 
<p>               2、输入图像-&gt;生成2k个候选区-&gt;CNN提取特征-&gt;使用SVM分类</p> 
<p>改进之处: 1、提出“使用区域识别”方法来解决CNN定位问题</p> 
<p>2、在大型数据集（ILSVRC）上进行有监督的预训练，然后再小型数据(PASCAL)上进行特定领域的微调</p> 
<p>3、CNN参数共享，利用CNN计算出的特征向量是低维的。</p> 
<p>模型优点：物体识别准确度较高</p> 
<p>模型缺点：1.训练时一个多阶段过程；2.训练需要大量的时间和空间；3.检测速度慢</p> 
<p>模型性能：在VOC2012中，mAP到达53.3%</p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>