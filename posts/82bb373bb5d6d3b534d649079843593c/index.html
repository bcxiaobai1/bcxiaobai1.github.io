<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>2023年AI十大展望：GPT-4领衔大模型变革，谷歌拉响警报，训练数据告急 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">2023年AI十大展望：GPT-4领衔大模型变革，谷歌拉响警报，训练数据告急</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center"><img alt="ec3904c0efb946d18409e5f6b0aefd85.jpeg" src="https://images2.imgbox.com/84/10/g8K4n0xi_o.jpg"></p> 
 <p style="text-align:justify">新年伊始，大模型的话题热度不减。2022年11月底，<a href=""><strong>ChatGPT</strong></a>展现的惊人能力将大模型研究和应用热度推向高潮，人们激烈讨论着这个高级“物种”的推出意味着什么，比如是否会颠覆搜索引擎市场格局。踏入2023年，这不禁让我们对GPT-4的发布充满遐想，它会比ChatGPT更上一层楼吗？会有哪些不一样的惊喜？<br><br> 岁末年初之际，科技圈的年度盘点不胜枚举，相关技术预测倒是不多。本文作者Rob Toews发布了2023年AI发展的十大预测，整体来看，大部分预测都离不开“大模型”这个关键词，具体分析也有其道理。当然，其中的文生图、人形机器人等领域的发展也举足轻重。2023，让我们拭目以待。</p> 
 <p><br><strong>作者｜Rob Toews<br> 翻译｜杨婷、徐佳渝</strong></p> 
 <h2></h2> 
 <p style="text-align:left"><strong>1</strong><strong>.</strong><strong> </strong><strong>重磅事件：GPT-4将在年初发布</strong></p> 
 <p>GPT-4是OpenAI的新一代生成语言模型，它有着强大的功能，最近到处流传着它的消息。</p> 
 <p>GPT-4预计将在2023年年初发布，相较于GPT-3和3.5，GPT-4的性能有着跳跃式的提升。尽管最近有关ChatGPT的讨论正在火热朝天地进行，但相比GPT-4，这只是前奏而已，让我们拭目以待！</p> 
 <p>GPT-4 会是什么样子的呢？与人们的直觉不同，我们预测它不会比其前身GPT-3大太多。在今年早些时候发表的一篇有影响力的研究论文（https://arxiv.org/pdf/2203.15556.pdf）中，DeepMind研究人员称现在的大型语言模型实际上比本来应该有的大小还要大。为了能在给定的有限计算预算中获得最佳模型性能，现在的模型应该用更少的参数在更大的数据集上进行训练。也就是说，<strong>训练数据比模型大小重要。</strong></p> 
 <p>当今大多数主要语言模型都是在约3000亿个token的数据语料库上训练的，比如说OpenAI的GPT-3（1750 亿个参数）、AI21 Labs的Jurassic（1780 亿个参数）和微软/英伟达的Megatron-Turing（5700 亿个参数）。</p> 
 <p>我们预测，<strong>GPT-4的数据集要比上面提到的大一个数量级，也就是说它可能在10万亿个token的数据集上进行训练。同时它的参数将比Megatron-Turing的要少。</strong></p> 
 <p>据说，GPT-4有可能是多模态的，除文本生成之外，它还可以生成图片、视频以及其他数据类型的输入。这意味着GPT-4能够像DALL-E一样根据输入的文本提示词（prompt）生成图像，或者是可以输入视频然后通过文本的形式回答问题。</p> 
 <p>多模态GPT-4的发布可能是一个重磅消息。<strong>但是它更可能和以前的GPT模型一样是纯文本模型，它在语言任务上的表现将重新定义SOTA。</strong>具体来说GPT-4会是什么样的呢？那就是它在记忆（保留和参考前期对话信息的能力）和摘要（提取和精简大规模文本的能力）这两个语言领域的性能会有跨越式提升。</p> 
 <h2></h2> 
 <p><strong>2. </strong><strong>训练大型语言模型将逐渐开始耗尽数据</strong></p> 
 <p>数据是新时代的石油这种说法早就已经是陈词滥调了，但这样说还不足以表明数据的重要性：因为石油和数据都是有限的，都有消耗殆尽的一天，在AI领域，语言模型对数据的需求量最大，数据耗尽的压力也更大。</p> 
 <p>正如前面提到的，DeepMind的Chinchilla work等研究已经表明，构建大型语言模型（LLM）最有效的方式不是把它们做得更大，而是在更多的数据上对其进行训练。</p> 
 <p>但是世界上有多少语言数据呢？更准确地说有多少语言数据达到了可以用来训练语言模型的要求呢？<strong>实际上，现在网络上大多数的文本数据并没有达到要求，不能用来训练大型语言模型。</strong></p> 
 <p style="text-align:left">对于这个问题，我们很难给出确切的答案，但是根据一个研究小组（<em>https://arxiv.org/pdf/2211.04325.pdf</em>）给出的数据，全球高质量文本数据的总存量在4.6万亿到17.2万亿个token之间。这包括了世界上所有的书籍、科学论文、新闻文章、维基百科、公开代码以及网络上经过筛选的达标数据，例如网页、博客和社交媒体。最近的另一项研究数据（<em>https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications</em>）显示，数据总数大约为3.2万亿个token。</p> 
 <p>DeepMind的Chinchilla模型是在1.4万亿个token上训练的。也就是说，在这个数量级内，我们很有可能耗尽世界上所有有用的语言训练数据。这可能成为人工智能语言模型领域持续进步的一大障碍。许多前沿AI研究人员和企业家私下里都对此忧心忡忡。</p> 
 <p>随着研究人员开始寻求解决数据短缺这一迫在眉睫的问题，预计2023年对这方面的关注度会增加。针对这一问题，<strong>合成数据是一种可能的解决方案</strong>，尽管该如何操作这一方法还未可知。还有另一种可能的方法，那就是系统性地转录会议上的讲话，毕竟口头交流代表着还有大量未捕获的文本数据。</p> 
 <p>作为世界领先的LLM研究机构，人们十分好奇OpenAI在其即将发布的GPT-4研究中会如何应对这一挑战，同时，大家也期待着可以获得一些启发。</p> 
 <h2></h2> 
 <p><strong>3. </strong><strong>一些消费者开始将完全自动驾驶作为日常出行模式</strong></p> 
 <h2></h2> 
 <p>在多年预热炒作却一再失信之后，最近自动驾驶汽车领域出现了一些少有人注意的新变化：真正的无人驾驶汽车时代已经悄悄来临。</p> 
 <p>现在在旧金山，你可以下载Cruise应用程序（这个程序和Uber或Lyft的APP差不多），通过这个APP你可以叫到真正的没有司机辅助的无人驾驶汽车带你在街道上穿行。</p> 
 <p>目前，这些无人驾驶汽车仅在夜间服务，服务时间是晚上22:00点到早上5:30，但是Cruise已经准备好了要在旧金山提供全天候服务，这个计划预计将在几周后实行。另一边Cruise的竞争对手Waymo紧跟其后，也开始在旧金山投放无人驾驶汽车。</p> 
 <p>2023年，人们将快速习惯robotaxi服务，作为一种方便、可行的交通服务方式，人们一开始会感到新颖、奇怪，然后会快速习惯robotaxi的存在，直到司空见惯。街道上无人驾驶汽车的数量和使用它们的人数将会激增。简而言之，<strong>无人驾驶汽车即将进入商业化和规模化阶段。</strong></p> 
 <p>无人驾驶汽车的推广将以城市为单位，逐个进行。在旧金山的基础上，来年可能会新增至少两个面向公众投放无人驾驶汽车的美国城市。比较理想的候选城市有凤凰城、奥斯汀、拉斯维加斯和迈阿密。</p> 
 <h2></h2> 
 <p><strong>4.</strong><strong> </strong><strong>AI绘画工具Midjourney将筹集风险投资资金</strong></p> 
 <h2></h2> 
 <p>OpenAI的DALL-E，Stability AI（和其他贡献者）的Stable Diffusion和Midjourney是目前最著名、最有影响力的三个文生图的AI平台。</p> 
 <p>2019年，OpenAI从微软处获得了10亿美元的投资，并且目前正在商讨筹集更多资金。几个月前，Stability AI获得了1亿美元的投资，而且已经开始寻求更多投资。</p> 
 <p>相比之下，Midjourney没有任何外部投资。不过，Midjourney的用户和发展速度增长极快，目前为止，它拥有近600万用户和相当可观的收入。但是其网站显示Midjourney仍然是一个“小型自筹资金”组织，只有11名全职团队成员。</p> 
 <p>大卫·霍尔茨（David Holz）是Midjourney的创始人兼首席执行官，他曾是Leap Motion的联合创始人兼首席技术官，Leap Motion曾是一家飞速发展的虚拟现实创业公司，这家公司在2010年代筹集了近1亿美元的风险投资，之后公司发展状况迅速恶化，最终被收购。据称这段失败的经历让霍尔茨拒绝接受外部资金。到目前为止，Midjourney已经拒绝了很多投资者。</p> 
 <p>然而，面对公司的迅猛发展、激烈的竞争以及大量的市场机会，我们推测霍尔茨将在2023年开始为Midjourney筹集投资。否则，Midjourney将快速在这场由其引发的生成式人工智能淘金热中落伍。</p> 
 <h2></h2> 
 <h2></h2> 
 <p><strong>5. </strong><strong>谷歌作为主流搜索引擎的地位将面临挑战</strong></p> 
 <p>搜索引擎是现代互联网体验的核心，是我们浏览和访问数字信息的主要方式。现在的大型语言模型可以进行复杂级别的读和写，这在几年前几乎是不可思议的，这会对我们的搜索方式产生深远影响。</p> 
 <p>在ChatGPT出现之后，重新定义搜索的对话式搜索（conversational search）引起了人们的广泛注意。对话式搜索让我们可以与AI智能体进行动态对话以找到要查找的内容，不用再像传统的搜索引擎一样先输入要查询的内容，然后返回一长串链接，比如现在谷歌搜索的做法。</p> 
 <p>对话式搜索具有广阔的发展前景，但是它还有一个亟待解决的大问题，<strong>那就是搜索结果不够准确</strong>，只有解决了这个问题，对话搜索才能发展起来。目前对话式LLM的准确性还不够，因为它们有时会给出与事实不符的信息。</p> 
 <p>最近OpenAI首席执行官Sam Altman称，“现在在重要的事情上，我们还不能依赖ChatGPT”。大多数用户也不会接受不能百分百保证结果准确的搜索引擎，即使它的准确率可以达到95%甚至99%。2023年，研究人员面临的主要挑战之一就是，以一种可扩展且稳健的方式来解决这个问题。</p> 
 <p>一批发展势头良好的初创公司正在对谷歌的搜索引擎发起挑战，它们希望通过LLM和对话界面重塑消费者的搜索方式，比如You.com，Character.AI，Metaphor 和Perfucity等公司。</p> 
 <p>但是，<strong>LLM不仅仅只是会改变消费者的互联网搜索方式，它还会改变其他搜索类型。</strong></p> 
 <p>比如企业搜索（组织搜索和检索私有数据的方式）同样也处于新黄金时代的风口浪尖。由于大规模矢量化的出现，LLM首次实现了真正的语义搜索：它能够根据基本概念和上下文而不是简单的关键字来索引和访问信息。这将使企业搜索更加强大和高效。像Hebbia和Glean这样的初创公司正在使用大型语言模型引领改变企业搜索的潮流。</p> 
 <p>下一代搜索引擎将不仅限于文本。人<strong>工智能的最新进展为多模态搜索提供了新的可能性：即跨数据模式查询和检索信息的能力。</strong></p> 
 <p>因为视频占据了互联网数据总量的80%左右，所以视频搜索代表了最大的发展机会。想象一下，如果我们可以轻松且准确地搜索视频中的某个片段、某个人、某个概念或者某个动作，这将是什么样的局面？Twelve Labs是一家初创公司，它构建了一个多模态AI平台，以实现精细化的视频搜索和理解。</p> 
 <p>自谷歌在互联网时代崛起以来，搜索领域就几乎没怎么发生过变化。2023年，有了大语言模型，该领域将发生巨变。</p> 
 <h2></h2> 
 <h2></h2> 
 <p><strong>6. </strong><strong>开发人形机器人将吸引大量注意力、人才以及资金，2023年将有几个新的人形机器人项目启动</strong></p> 
 <p>人形机器人可能是好莱坞电影对AI进行夸张化的极端代表，比如说电影 《机械姬》和《我，机器人》。人形机器人发展迅速，并逐渐成为现实。</p> 
 <p>为什么要打造人形机器人呢？原因很简单，因为我们现实世界的大部分架构都是为了人类而打造，如果我们想利用机器人在工厂、购物中心、办公室和学校这样的场所自动完成复杂活动，最有效的方法就是让机器人拥有和人类一样的外形。这样，机器人就可以应用到多种场景中，且无需适应周围环境。</p> 
 <p>今年9月，特斯拉在人工智能日推出了擎天柱（Optimus）机器人，这大大推动了人形机器人领域的发展。埃隆·马斯克表示，擎天柱最终会比汽车业务更有价值。然而，擎天柱机器人要想完全成熟，还任重而道远 。但是，当特斯拉将所有资源都投入到优化擎天柱这项任务时，能够取得的进展是无法估量的。</p> 
 <p>同样地，许多具有发展前景的初创公司也推动着人形机器人领域的发展，包括Agility robotics、Halodi robotics、Sanctuary AI以及Collaborative robotics。</p> 
 <p>随着人形机器人产业竞争愈演愈烈，预计2023年会有更多的公司加入到这场角逐中，包括初创企业和一些知名公司（如丰田、三星、通用汽车、松下）。这类似于2016年的自动驾驶汽车，当越来越多的人开始意识到汽车行业拥有巨大的市场机会时，次年将会有大量人才和资本涌入该领域。</p> 
 <h2></h2> 
 <p><strong>7. </strong><strong>MLOps的新版本：LLMOps将登台亮相</strong></p> 
 <p>当某种新的技术平台出现时，相应的需求和机会也会随之出现，比如说用以支持新平台的工具和基础设施。风险投资家们通常把这些辅助工具视为“镐头和铲子”（以迎接即将到来的淘金热）。</p> 
 <p>近年来，初创企业界最热门的当属广为人知的MLOps机器学习工具。初创公司开始一蜂窝地研究MLOps，并以令人咋舌的估值筹集到大量资金：Weights &amp; Biases（获得2亿美元融资，平台估值达10亿美元）、Tecton（获得1.6亿美元融资）、Snorkel（获得1.38亿美元融资，平台估值达10亿美元）、OctoML（获得1.33亿美元融资，平台估值达8.5亿美元）等等。</p> 
 <p>如今，我们目睹了新一代人工智能技术平台——大型语言模型（LLMs）的问世。与预训练语言学习模型（pre-LLM）相比，大型语言模型具有独特的工作流程、技能组件和发展潜能，代表了人工智能的新范式。通过API或者开源，人们可以很容易获得大量预训练模型，这完全改变了人工智能产品。因此，注定会出现一套新的工具和基础设施。</p> 
 <p>我们预测LLMOps将成为新的流行趋势，<strong>它代表着新一代人工智能的镐头和铲子。</strong>以新一代LLMOps产品为例，包括基础模型微调工具、无代码LLM部署、GPU访问与优化、提示词实验、提示词链以及数据合成与数据增强。</p> 
 <p><strong>8. </strong><strong>基于或引用AlphaFold的研究项目数量将会激增</strong></p> 
 <p>2020年底，DeepMind公司首次推出了AlphaFold平台，破解了生命的一大谜团：即蛋白质的折叠问题。AlphaFold能够仅从蛋白质的一维氨基酸序中准确地预测出蛋白质的三维形态。这是一个里程碑式的成就，解决了困扰研究人员几十年的问题（AlphaFold代表了人工智能历史上最重要的成就）。</p> 
 <p>因为蛋白质是地球上所有生物进行重要活动的内在基础，对蛋白质的结构和功能了解得越透彻，就越能为生物学和人类健康提供新的可能。不论是从开发救生疗法（life-saving therapeutics），到改善农业，还是从对抗疾病到研究生命起源，蛋白质存在于生活中的方方面面。</p> 
 <p>DeepMind于2021年7月开源了AIphaFold，并推出了一个数据库，它包含350,000种三维蛋白质结构（作为参考，在推出AlphaFold之前，人类已知的蛋白质结构大约有180,000种）。此外，几个月前，DeepMind公布了另一个包含2亿种蛋白质结构的数据库——这几乎覆盖了所有科学上已知的蛋白质。</p> 
 <p>DeepMind最新版本发布短短几个月后，就有来自190多个国家的50多万名研究人员使用AlphaFold平台，用它查看了200万种不同的蛋白质结构。但这仅仅只是开始。AlphaFold的巨大突破所带来的影响需要好几年才能逐渐展现出全貌。</p> 
 <p>到2023年，预计基于AlphaFold的研究数量将会激增。<strong>研究人员将利用这一庞大的新型基础生物学知识宝库，将其应用于新型疫苗、新型塑料研发等多个跨学科领域，进而改变世界。</strong></p> 
 <p><strong>9.</strong><strong> </strong><strong>DeepMind、Google Brain和OpenAI将致力于为机器人构建基础模型</strong></p> 
 <p>去年，斯坦福大学的一个研究团队提出了“基础模型”（foundation model）一词，它是指基于大量数据训练的大规模人工智能模型。该模型的构建并不是为了执行特定任务，而是为了能有效执行各种不同活动的任务。</p> 
 <p>基础模型一直是人工智能最新发展的关键驱动力。如今，基础模型非常强大。但无论是GPT-3这样的文本生成模型，还是Stable Diffusion这样的文本转图像模型，又或是Adept这样的计算机操作（computer actions）模型，均只能运用于数字领域。</p> 
 <p>AI系统在真实世界的应用随处可见，例如自动驾驶汽车、仓库机器人、无人机、人形机器人等等，但到目前为止，它们大多还未受到基础模型新范式的影响。</p> 
 <p>这种情况将在2023年发生变化。预计用于机器人的基础模型这一早期开创性工作，将由世界领先的人工智能研究机构DeepMind、Google Brain和OpenAI完成（尽管OpenAI去年退出了机器人研究）。</p> 
 <p>构建用于机器人的基础模型意味着什么？换句话说，构建物理世界的基础模型意味着什么呢？<strong>从高层次来看，这样的模型可以用不同传感器模式（如相机、雷达、激光雷达）的大量数据进行训练，以产生对物理和现实世界物体的普遍理解能力</strong>：比如这些不同的物体是如何移动的、它们之间如何相互作用、它们有多重、多脆弱、多柔软、多灵活以及当你触碰、投掷或者扔它们的时候会发生什么。这种“真实世界的基础模型”可以针对特定的硬件平台和特定的下游任务进行微调。</p> 
 <h2></h2> 
 <p><strong>10. </strong><strong>美国将投资数十亿美元建设本国芯片制造设施</strong></p> 
 <p>人工智能和人类智能（human intelligence）一样同时依赖于硬件和软件设施。先进半导体对推动现代人工智能而言至关重要。到目前为止，影响最大、应用最为广泛的是英伟达的GPU；像AMD、因特尔以及一些研究人工智能芯片的新兴企业也试图跻身芯片市场。</p> 
 <p>几乎所有的人工智能芯片都是由美国设计，并在台湾完成制造。并且全球最先进的芯片几乎都是由台积电（TSMC）这家公司生产的，包括英伟达的GPU。</p> 
 <p>由于中美地缘政治的紧张局势，为了降低人工智能硬件瓶颈的不确定性，降低对台湾的依赖，2023年，美国政府将加大激励措施并对在美建设先进芯片制造设施的工厂给予补贴。而今年夏天通过的《芯片与科学法》（The CHIPS and Science Act）则为此提供了立法动力和预算资源。</p> 
 <p>这一进程已经开始。两周前，台积电宣布将投资400亿美元在亚利桑那州建立两家新的芯片制造厂（美国总统拜登亲自访问了亚利桑那州的工厂选址，对其称赞不绝），更重要的是，新台积电工厂预计将于2026年开始运行，生产3纳米芯片，该芯片将成为当今世界最先进的半导体。</p> 
 <p>随着美国开始在国内寻找生产基地以化解关键人工智能硬件所面临的风险，预计2023年将看到更多这样的承诺。</p> 
 <p style="text-align:left">（注：本文作者是Radical Ventures的合伙人，Radical Ventures是Hebbia、Twelve Labs和You.com的投资者。本文经授权后由OneFlow编译发布，若需转载请先联系获得授权。原文：https://www.forbes.com/sites/robtoews/2022/12/20/10-ai-predictions-for-2023/?sh=51a9e1ddfab7）<br>  </p> 
 <p style="text-align:left">其他人都在看</p> 
 <ul>
<li> <p><a href="">OneFlow源码解析：静态图与运行时</a></p> </li>
<li> <p><a href="">ChatGPT的一小步，NLP范式转变的一大步</a></p> </li>
<li> <p><a href="">李白：你的模型权重很不错，可惜被我没收了</a></p> </li>
<li> <p><a href="">OpenAI掌门Sam Altman：AI下一个发展阶段</a></p> </li>
<li> <p><a href="">32篇年度最佳AI论文;Python编译器Codon开源</a></p> </li>
<li> <p><a href="">比快更快，开源Stable Diffusion刷新作图速度</a></p> </li>
<li> <p><a href="">OneEmbedding:单卡</a><a href="">训练TB级推荐模型不是梦</a></p> </li>
</ul>
 <h2>欢迎Star、试用OneFlow最新版本：<a class="has-card" href="https://github.com/Oneflow-Inc/oneflow/" title="GitHub - Oneflow-Inc/oneflow: OneFlow is a deep learning framework designed to be user-friendly, scalable and efficient."><span class="link-card-box"><span class="link-title">GitHub - Oneflow-Inc/oneflow: OneFlow is a deep learning framework designed to be user-friendly, scalable and efficient.</span><span class="link-desc">OneFlow is a deep learning framework designed to be user-friendly, scalable and efficient. - GitHub - Oneflow-Inc/oneflow: OneFlow is a deep learning framework designed to be user-friendly, scalable and efficient.</span><span class="link-link"><img alt="" class="link-link-icon" src="https://images2.imgbox.com/24/e1/iGI0zQMD_o.png">https://github.com/Oneflow-Inc/oneflow/</span></span></a>
</h2> 
</div>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>