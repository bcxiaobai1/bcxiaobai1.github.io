<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>ChatGPT背后的技术原理：领略Transformer架构的魅力 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">ChatGPT背后的技术原理：领略Transformer架构的魅力</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <blockquote> 
 <p>?惊艳了时代的ChatGPT，背后到底有怎样的技术支持？本文将深入剖析ChatGPT背后的技术原理，从Transformer架构、自注意力机制到位置编码等方面，带领读者一探究竟?！</p> 
</blockquote> 
<h2>
<a id="font_face_colororange_size3ChatGPTTransformerfont_1"></a><font face="微软雅黑" color="orange" size="3">ChatGPT与Transformer架构</font>
</h2> 
<p>?ChatGPT，这个最近让大家炸裂的人工智能语言模型，背后的秘密武器就是Transformer架构。这种神奇的架构突破了传统的循环神经网络（RNN）和长短时记忆网络（LSTM）的局限性，能够在大规模数据集上进行高效训练。</p> 
<p>Transformer架构的核心在于自注意力机制与位置编码，以及层与子层的重要地位。让我们一起揭开这神秘的面纱吧！</p> 
<h3>
<a id="font_faceLiSu_colororange_size4font_6"></a><font face="LiSu" color="orange" size="4">自注意力机制：解锁语言的力量</font>
</h3> 
<p>自注意力机制（Self-Attention Mechanism）是Transformer架构中的核心组成部分之一，用于捕捉输入序列中元素之间的关系。自注意力机制能够自动地计算每个元素与其他元素之间的相似度，并给予不同权重，从而使得模型能够关注与当前元素最相关的信息。自注意力机制相对于传统的注意力机制，不需要外部上下文输入，因此也被称为“自注意力”。</p> 
<p>在Transformer架构中，自注意力机制由三个向量组成：查询向量（Query）、键向量（Key）和值向量（Value）。对于输入序列中的每个元素，我们将其转换为向量表示，并将其分别作为查询向量、键向量和值向量的输入。具体而言，对于输入序列中的第<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
      
        i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>个元素，我们可以表示为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          X 
         
        
          i 
         
        
       
      
        X_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，并将其通过三个线性变换映射为查询向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          i 
         
        
       
      
        Q_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>、键向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          K 
         
        
          i 
         
        
       
      
        K_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0715em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>和值向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          V 
         
        
          i 
         
        
       
      
        V_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.2222em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，如下所示：</p> 
<p><img src="https://images2.imgbox.com/a3/2a/1I79Qb8c_o.png" alt="在这里插入图片描述"><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          Q 
         
        
       
      
        W_Q 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9694em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">Q</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span></span></span></span>、<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          K 
         
        
       
      
        W_K 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em">K</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          W 
         
        
          V 
         
        
       
      
        W_V 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em">V</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>表示三个线性变换的权重矩阵。查询向量、键向量和值向量的维度可以不同，但它们的长度必须相等。</p> 
<p>接下来，我们需要计算查询向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          Q 
         
        
          i 
         
        
       
      
        Q_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>与键向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          K 
         
        
          j 
         
        
       
      
        K_j 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9694em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0715em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span></span></span></span>之间的相似度。在Transformer中，采用点积（Dot Product）计算相似度，具体公式如下：<br> <img src="https://images2.imgbox.com/ca/b4/QtyQjL84_o.png" alt="在这里插入图片描述"><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
        
          k 
         
        
       
      
        d_k 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>表示键向量的维度，这里采用了标准化的点积，以避免在相似度计算中出现梯度爆炸或梯度消失的问题。</p> 
<p>我们可以将查询向量与所有键向量计算相似度，得到一个关于元素<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
      
        i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>的注意力分布向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          A 
         
        
          i 
         
        
       
      
        A_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，如下所示：<br> <img src="https://images2.imgbox.com/f4/5d/ubjTJeLs_o.png" alt="在这里插入图片描述"><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         softmax 
        
       
      
        text{softmax} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em"></span><span class="mord text"><span class="mord">softmax</span></span></span></span></span></span>函数用于将分数转换为概率分布，表示元素<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
      
        i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>需要关注哪些元素。注意力分布向量的维度与输入序列中元素的数量相同。</p> 
<p>最后，我们将注意力分布向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          A 
         
        
          i 
         
        
       
      
        A_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">A</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>与所有值向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          V 
         
        
          j 
         
        
       
      
        V_j 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9694em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.2222em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span></span></span></span>进行加权求和，得到自注意力机制的输出向量<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          O 
         
        
          i 
         
        
       
      
        O_i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0278em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，如下所示：</p> 
<p><img src="https://images2.imgbox.com/61/7d/ImQj4c5m_o.png" alt="在这里插入图片描述"><br> 通过自注意力机制，我们可以捕捉输入序列中元素之间的相互关系，并将其编码为每个元素的向量表示。自注意力机制的作用类似于卷积神经网络中的卷积操作，但它不仅仅捕捉局部特征，还能够关注序列中不同位置之间的关系，因此具有更强的表达能力。</p> 
<p>需要注意的是，自注意力机制的计算复杂度随着输入序列长度的增加而呈现<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         O 
        
       
         ( 
        
        
        
          n 
         
        
          2 
         
        
       
         ) 
        
       
      
        O(n^2) 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0641em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>的增长趋势，这在处理长序列时会导致计算资源的瓶颈。为了解决这个问题，Transformer架构引入了多头注意力机制，将输入序列分为多个子序列，对每个子序列分别计算自注意力，并将多个注意力输出向量拼接起来，从而提高了模型的效率和性能。</p> 
<p>总之，自注意力机制是Transformer架构的核心组成部分之一，通过计算元素之间的相似度，帮助模型捕捉输入序列中元素之间的关系，从而实现高效而准确的自然语言处理任务。</p> 
<h3>
<a id="font_faceLiSu_colororange_size4font_30"></a><font face="LiSu" color="orange" size="4">位置编码：赋予序列位置信息</font>
</h3> 
<p>位置编码（Positional Encoding）是Transformer架构中的一个重要组成部分，用于在输入序列中添加位置信息，帮助模型理解元素之间的位置关系。在自注意力机制中，每个输入元素都生成了一个查询向量（Query）、一个键向量（Key）和一个值向量（Value），它们之间的相关性将由注意力机制决定。但是，自注意力机制并不考虑元素在输入序列中的位置信息，因此需要引入位置编码来解决这个问题。</p> 
<p>位置编码采用正弦和余弦函数进行编码，具体公式如下：<br> <img src="https://images2.imgbox.com/fe/ad/EYVLDiIu_o.png" alt="在这里插入图片描述"><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         p 
        
       
         o 
        
       
         s 
        
       
      
        pos 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.1944em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span></span></span></span></span>表示输入序列中元素的位置，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
      
        i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>表示位置编码的维度，<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         d 
        
       
         _ 
        
        
        
          m 
         
        
          o 
         
        
          d 
         
        
          e 
         
        
          l 
         
        
       
      
        d_{model} 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0044em;vertical-align: -0.31em"></span><span class="mord mathnormal">d</span><span class="mord" style="margin-right: 0.0278em">_</span><span class="mord"><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right: 0.0197em">l</span></span></span></span></span></span>表示模型的维度。位置编码的目的是让模型能够学习到输入序列中元素之间的位置信息，进而识别出元素之间的相对位置关系。正弦函数和余弦函数的选择是为了让不同维度的位置编码之间更为独立，增强位置信息的可学习性。</p> 
<p>例如，假设输入序列为"hello world"，我们可以将每个字符转换为词向量，并在其上添加位置编码。假设模型的维度<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
        
        
          d 
         
         
         
           m 
          
         
           o 
          
         
           d 
          
         
           e 
          
         
           l 
          
         
        
       
         = 
        
       
         512 
        
       
      
        d_{model}=512 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">512</span></span></span></span></span>，则每个位置编码的维度<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         i 
        
       
      
        i 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>的取值范围为<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         [ 
        
       
         0 
        
       
         , 
        
       
         256 
        
       
         ] 
        
       
      
        [0, 256] 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord">256</span><span class="mclose">]</span></span></span></span></span>。在这种情况下，位置编码可以表示为：<br> <img src="https://images2.imgbox.com/0d/19/HHjqzeDN_o.png" alt="在这里插入图片描述"><br> 可以看到，通过位置编码，我们为输入序列添加了位置信息，让模型能够理解元素之间的相对位置关系，从而提高模型的自然语言处理能力。</p> 
<h3>
<a id="font_faceLiSu_colororange_size4font_42"></a><font face="LiSu" color="orange" size="4">层与子层：构建高级语义表示</font>
</h3> 
<h3>
<a id="font_faceLiSu_colorgrey_size31font_43"></a><font face="LiSu" color="grey" size="3">1）层与子层的作用</font>
</h3> 
<p>在Transformer架构中，层与子层主要用于提取输入序列的高级语义表示。通过堆叠多个编码器层和解码器层，模型能够学习到更杂的语义特征和关系。这有助于改善模型的自然语言处理能力。每个编码器层和解码器层都由多个子层组成，这些子层共同协作来提取和处理各种特征。</p> 
<h3>
<a id="font_faceLiSu_colorgrey_size32Transformerfont_46"></a><font face="LiSu" color="grey" size="3">2）层与子层在Transformer架构中的关系</font>
</h3> 
<p>在Transformer架构中，编码器（Encoder）和解码器（Decoder）是由若干个相同的层堆叠而成。这些层包含不同的子层，它们分别负责处理不同的任务。</p> 
<p>编码器层（Encoder Layer）由以下两个子层组成：</p> 
<pre><code class="prism language-shell">a. 多头自注意力子层（Multi-head Attention）：负责处理输入序列中各个元素之间的关系，捕捉长距离依赖关系。
b. 前馈神经网络子层（Feed-Forward Neural Network）：用于提取局部特征，对注意力子层输出的结果进行进一步处理。
</code></pre> 
<p>解码器层（Decoder Layer）由以下三个子层组成：</p> 
<pre><code class="prism language-shell">a. 多头自注意力子层（Multi-head Attention）：同编码器层中的自注意力子层，处理解码器输入序列中各个元素之间的关系。
b. 编码器-解码器注意力子层（Encoder-Decoder Attention）：用于关联编码器的输出与解码器的输入，使模型能够理解源语言和目标语言之间的映射关系。
c. 前馈神经网络子层（Feed-Forward Neural Network）：与编码器层中的前馈神经网络子层相同，对注意力子层的输出结果进行进一步处理。
</code></pre> 
<p>此外，每个子层随残差连接（Residual Connection）和层归一化（Layer Normalization）。残差连接是一种跳跃式连接，将子层的输入与输出相加，从而保留了输入的原始信息。这有助于缓解梯度消失问题，使模型能够进行深层训练。而层归一化则负责对子层输出的各个维度进行标准化，降低模型的内部协变量偏移（Internal Covariate Shift），提高模型的训练稳定性和收敛速度。</p> 
<p>总之，在Transformer架构中，层与子层发挥着关键作用，它们共同帮助模型提取输入序列的高级语义表示。通过多个编码器层和解码器层的堆叠，以及自注意力机制和位置编码的应用，Transformer架构能够在自然语言处理任务中取得优异表现。虽然层与子层不是架构的核心，但它们在整个架构中具有重要地位，并与自注意力机制和位置编码相辅相成。</p> 
<h2>
<a id="font_face_colororange_size4font_70"></a><font face="微软雅黑" color="orange" size="4">总结?</font>
</h2> 
<blockquote> 
 <p>ChatGPT作为一个革命性的人工智能语言模型，正是因为Transformer架构、自注意力机制与位置编码等技术的支持，才能够在众多领域中大放异彩。我们有理由相信，随着技术的不断进步，ChatGPT将在未来持续引领自然语言处理领域的发展，创造出更多的奇迹。</p> 
</blockquote> 
<p>感谢您的阅读，希望这篇文章能帮助您深入了解ChatGPT背后的技术原理。欢迎关注，我会持续为您带来更多有趣的文章！</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>