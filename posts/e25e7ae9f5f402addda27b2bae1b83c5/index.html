<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>WebRTC 一对一语音通话中音频端到端分段延迟分析 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">WebRTC 一对一语音通话中音频端到端分段延迟分析</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p>WebRTC 一对一语音通话中的音频端到端延迟指从一个音频信号被发送端采集，到同一个信号被接收端播放出来这整个过程的时间。音频端到端延迟由多个阶段组成。音频端到端处理的冲采样、混音、回声和降噪等操作会使音频数据在数值上变得面目全非，变得难以比较。真正的音频端到端延迟一般使用专业的声卡设备配上专门的音频处理软件来测，这种测试在线上环境中是难以实现的。音频端到端分段延迟常常也能在很大程度上反应音频端到端延迟，分段延迟的分析甚至可以帮我找到造成延迟的瓶颈所在。</p> 
<h2>
<a id="WebRTC__3"></a>WebRTC 一对一语音通话中音频处理的主要过程</h2> 
<p>WebRTC 一对一语音通话中音频的处理主要经过了如下的过程：</p> 
<ol>
<li>音频采集；</li>
<li>音频数据处理，也称为音频的前处理，主要包括回声消除、降噪和自动增益控制等；</li>
<li>音频编码；</li>
<li>编码音频包的发送；</li>
<li>编码音频包在网络中的传输；</li>
<li>从网络中接收音频包；</li>
<li>音频包在缓冲区中等待被解码处理；</li>
<li>音频解码及播放。</li>
</ol> 
<p>这里分析这其中每个阶段的延迟。</p> 
<h2>
<a id="_18"></a>音频采集延迟</h2> 
<p>当启动设备的音频采集时，开发者可能可以对系统的音频采集数据缓冲区大小做一些配置，也可能系统有一些自动的配置。当采集数据缓冲区满时，应用层代码可以通过系统的回调，或者主动调用获取音频采集数据的接口获取采集的音频数据。</p> 
<p>在 WebRTC 中，通过 <code>webrtc::AudioDeviceModule</code> 与音频设备交互，这包括与音频采集设备的交互。WebRTC 为不同的操作系统平台，甚至是同一操作系统平台的不同音频接口提供不同的 <code>webrtc::AudioDeviceModule</code> 的具体实现。<code>webrtc::AudioDeviceModule</code> 屏蔽不同操作系统平台及各种音频接口获取采集的音频数据的方式的差异，用一种统一的方式，也就是回调接口 <code>webrtc::AudioTransport</code>，将采集的音频数据送出去。<code>webrtc::AudioDeviceModule</code> 每次通过 <code>webrtc::AudioTransport</code> 送出去 10 ms 的音频采集数据，比如，如果音频采集设备的采样率是 48 kHz，则每次送出每通道 480 个采样点的音频数据，进一步如果音频采集设备的通道数是 1，则每次共送出 480 个采样点的数据，而如果音频采集设备的通道数是 2，则每次共送出 2 * 480 = 960 个采样点的数据。</p> 
<p>以 Mac 平台为例，<code>webrtc::AudioDeviceModule</code> 将数据送到 <code>webrtc::AudioTransport</code> 的调用过程大体如下：</p> 
<pre><code>  * frame #0: webrtc::AudioTransportImpl::RecordedDataIsAvailable(audio_data=0x0000000106f450d0, number_of_frames=480, bytes_per_sample=2, number_of_channels=1, sample_rate=48000, audio_delay_milliseconds=2147981, (null)=0, (null)=0, key_pressed=false, (null)=0x000070000d2366ac) at audio_transport_impl.cc:118:3
    frame #1: webrtc::AudioDeviceBuffer::DeliverRecordedData() at audio_device_buffer.cc:271:38
    frame #2: webrtc::AudioDeviceMac::CaptureWorkerThread() at audio_device_mac.cc:2495:22
    frame #3: webrtc::AudioDeviceMac::StartRecording()::$_1::operator()() const at audio_device_mac.cc:1314:16
</code></pre> 
<p>音频采集在 WebRTC 一对一语音通话中造成延迟，主要在于音频采集数据缓冲区大小的设置，和 10 ms 数据帧格式的对齐。不难理解，采集数据缓冲区越大，填满缓冲区所需的数据量就越多，所需时间越长，整体消耗的系统 CPU 资源更少，但音频采集延迟更大；而采集数据缓冲区越小，则填满缓冲区所需数据量越少，所需时间越短，整体消耗的系统 CPU 资源更多，但采集延迟更小。采集数据缓冲区大小决定通过系统接口获取音频采集数据的最小间隔。</p> 
<p><code>webrtc::AudioDeviceModule</code> 每次通过 <code>webrtc::AudioTransport</code> 送出 10 ms 的音频采集数据，因而小于 10 ms 的获取系统音频采集数据的周期没有太大意义。不同操作系统平台及不同的音频 API 接口，甚至运行相同操作系统相同音频 API 接口的不同具体设备，其获取系统音频采集数据的周期都会有一些差异。比如 Windows、Linux 和 Mac 一般都支持 10 ms 的获取系统音频采集数据周期，就像上面看到的 Mac 的那样，iOS 支持每次通过系统接口获取 512 或 1024 的每通道采样数的音频数据，Android 平台支持基于 AudioTrack/AudioRecord Java 接口、OpenSL ES 接口和 AAudio 接口实现的多种类型的 <code>webrtc::AudioDeviceModule</code>，不同的 Android 系统版本、不同的 <code>webrtc::AudioDeviceModule</code> 类型和不同的具体 Android 设备，获取系统音频采集数据的周期从 10 ms 到接近 200 ms 不等。</p> 
<p>音频采集在 WebRTC 一对一语音通话中造成延迟，还可能出于 10 ms 音频数据帧的对齐。这主要发生在 iOS 操作系统平台。iOS 一般支持采样率 48kHz、32kHz 和 16kHz 等，其每个音频采集数据获取周期吐 512 或 1024 的每通道采样数难以和这些采样率下 10 ms 音频数据的整数倍的数据对齐，如采样率为 48kHz，10 ms 的数据为每通道 480 个样本数，512 的每通道采样数在吐出一个 10 ms 的音频数据帧之后，会多出 32 个采样数，这会造成额外的对齐延迟。</p> 
<p>WebRTC 中 <code>webrtc::AudioDeviceModule</code> 的内部实现如下：</p> 
<p><img src="https://images2.imgbox.com/f0/cc/OBnVhyU6_o.jpg" alt="1661417037586.jpg"></p> 
<p><code>webrtc::AudioDeviceGeneric</code> 的各个操作系统平台和音频 API 接口的子类，实现通过各个操作系统平台和音频 API 提供的接口，与音频设备交互。当获得的系统音频采集数据不是 10 ms 的数据时，如 iOS 和部分 Android 系统的一些实现，需要先把采集的数据放进一个缓冲区中，缓冲区的数据达到或超过 10 ms 的数据时，送 10 ms 的数据出去，直到缓冲区中的数据不足 10 ms，剩余的数据则等待下次有数据到达时，和下次到达的数据一起拼成 10 ms 的音频数据帧再送出去。这里的缓冲区在 WebRTC 中是 <code>webrtc::FineAudioBuffer</code>。</p> 
<p>WebRTC 的 <code>webrtc::AudioDeviceModule</code> 中，先把采集数据通过 <code>webrtc::FineAudioBuffer::DeliverRecordedData()</code> 送进 <code>webrtc::FineAudioBuffer</code>，<code>webrtc::FineAudioBuffer</code> 将送进来的音频采集数据切成 10 ms 数据的音频帧，并通过 <code>webrtc::AudioDeviceBuffer::SetRecordedBuffer()</code> 和 <code>webrtc::AudioDeviceBuffer::DeliverRecordedData()</code> 送给 <code>webrtc::AudioDeviceBuffer</code>，<code>webrtc::AudioDeviceBuffer</code> 将 10 ms 数据的音频帧，借助于 <code>webrtc::AudioTransport::RecordedDataIsAvailable()</code> 通过 <code>webrtc::AudioTransport</code> 送出去。</p> 
<p>对于音频采集延迟，两次获得系统音频采集数据的时间间隔是比较重要的指标。获得的系统音频采集数据不是 10 ms 的数据时，可以将 <code>webrtc::FineAudioBuffer::DeliverRecordedData()</code> 的两次调用间隔视作获得系统音频采集数据的间隔；平台支持获得 10 ms 的系统音频采集数据时，可以将 <code>webrtc::AudioDeviceBuffer::DeliverRecordedData()</code> 的两次调用间隔视作获得系统音频采集数据的间隔。</p> 
<p>笔者在 <code>webrtc::AudioDeviceBuffer</code> 中加了一点点代码，来测 Windows 平台的两次获得系统音频采集数据的间隔，相关代码改动如下图：</p> 
<p><img src="https://images2.imgbox.com/d2/b2/Y8SwN6I8_o.png" alt="Audio device delay"></p> 
<p>笔者用 Debug 版的二进制文件，在一台 Windows 10 笔记本电脑上跑简单的 WebRTC 一对一语音通话测试，跑了大概 10 分钟，总共获得 58491 条数据。对 WebRTC 一对一语音通话中音频端到端延迟影响比较大的，不仅仅是各个阶段的平均耗时，各个阶段耗时的异常值常常也会有比较大的影响。因而，这里统计音频采集延迟的数值分布，这些数据的数值分布统计结果如下表（延迟数据单位为 ms）：</p> 
<table>
<thead><tr>
<th>Delay (ms)</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>0</td>
<td>53</td>
<td>0.000906</td>
</tr>
<tr>
<td>1</td>
<td>10</td>
<td>0.000171</td>
</tr>
<tr>
<td>2</td>
<td>5</td>
<td>0.000085</td>
</tr>
<tr>
<td>3</td>
<td>9</td>
<td>0.000154</td>
</tr>
<tr>
<td>5</td>
<td>1587</td>
<td>0.027132</td>
</tr>
<tr>
<td>6</td>
<td>11015</td>
<td>0.188320</td>
</tr>
<tr>
<td>7</td>
<td>2593</td>
<td>0.044332</td>
</tr>
<tr>
<td>8</td>
<td>778</td>
<td>0.013301</td>
</tr>
<tr>
<td>9</td>
<td>219</td>
<td>0.003744</td>
</tr>
<tr>
<td>10</td>
<td>4677</td>
<td>0.079961</td>
</tr>
<tr>
<td>11</td>
<td>16504</td>
<td>0.282163</td>
</tr>
<tr>
<td>12</td>
<td>18447</td>
<td>0.315382</td>
</tr>
<tr>
<td>13</td>
<td>2143</td>
<td>0.036638</td>
</tr>
<tr>
<td>14</td>
<td>308</td>
<td>0.005266</td>
</tr>
<tr>
<td>15</td>
<td>40</td>
<td>0.000684</td>
</tr>
<tr>
<td>16</td>
<td>19</td>
<td>0.000325</td>
</tr>
<tr>
<td>17</td>
<td>9</td>
<td>0.000154</td>
</tr>
<tr>
<td>18</td>
<td>6</td>
<td>0.000103</td>
</tr>
<tr>
<td>19</td>
<td>8</td>
<td>0.000137</td>
</tr>
<tr>
<td>20</td>
<td>16</td>
<td>0.000274</td>
</tr>
<tr>
<td>21</td>
<td>10</td>
<td>0.000171</td>
</tr>
<tr>
<td>22</td>
<td>9</td>
<td>0.000154</td>
</tr>
<tr>
<td>23</td>
<td>10</td>
<td>0.000171</td>
</tr>
<tr>
<td>24</td>
<td>9</td>
<td>0.000154</td>
</tr>
<tr>
<td>25</td>
<td>3</td>
<td>0.000051</td>
</tr>
<tr>
<td>27</td>
<td>1</td>
<td>0.000017</td>
</tr>
<tr>
<td>28</td>
<td>1</td>
<td>0.000017</td>
</tr>
<tr>
<td>29</td>
<td>2</td>
<td>0.000034</td>
</tr>
</tbody>
</table>
<p>从表中可以看到，在大多数情况下，两次获得系统音频采集数据的间隔都在 10 ms 左右，更准确地说，是集中在 5 ～ 14 ms 之间，但偶尔会有一些比较大的波动，这个间隔最大的时候可以达到 30 ms。</p> 
<h2>
<a id="_87"></a>音频数据信号处理延迟</h2> 
<p>在 WebRTC 中，音频采集数据被送去编码的调用过程如下：</p> 
<pre><code>  * frame #0: webrtc::voe::(anonymous namespace)::ChannelSend::ProcessAndEncodeAudio(this=0x0000000107b1d810, audio_frame=&lt;unavailable&gt;) at channel_send.cc:809:3
    frame #1: webrtc::internal::AudioSendStream::SendAudioData(this=0x000000010f854200, audio_frame=nullptr) at audio_send_stream.cc:403:18
    frame #2: webrtc::AudioTransportImpl::SendProcessedData(this=0x0000000109024f10, audio_frame=nullptr) at audio_transport_impl.cc:190:30
    frame #3: webrtc::AudioTransportImpl::RecordedDataIsAvailable(this=0x0000000109024f10, audio_data=0x00000001079206b0, number_of_frames=480, bytes_per_sample=2, number_of_channels=1, sample_rate=48000, audio_delay_milliseconds=61, (null)=0, (null)=0, key_pressed=false, (null)=0x000070000db196ac) at audio_transport_impl.cc:171:5
    frame #4: webrtc::AudioDeviceBuffer::DeliverRecordedData(this=0x000000010790c678) at audio_device_buffer.cc:271:38
    frame #5: webrtc::AudioDeviceMac::CaptureWorkerThread(this=0x000000010800d200) at audio_device_mac.cc:2495:22
    frame #6: webrtc::AudioDeviceMac::StartRecording(this=0x0000000107c0a008)::$_1::operator()() const at audio_device_mac.cc:1314:16
</code></pre> 
<p>采集的音频数据在被送去编码之前，还需要进行音频信号处理。在 <code>webrtc::AudioTransport</code> 实现的 <code>RecordedDataIsAvailable()</code> 函数中，把采集的音频数据送进 <code>webrtc::AudioSendStream</code> 编码之前，会先进行音频数据的信号处理，这也称为音频前处理。音频数据的信号处理主要包括回声消除（AEC）、降噪（ANS）和自动增益控制（AGC）等。</p> 
<p><code>AudioTransportImpl::RecordedDataIsAvailable()</code> 函数实现如下：</p> 
<pre><code>// Not used in Chromium. Process captured audio and distribute to all sending
// streams, and try to do this at the lowest possible sample rate.
int32_t AudioTransportImpl::RecordedDataIsAvailable(
    const void* audio_data,
    const size_t number_of_frames,
    const size_t bytes_per_sample,
    const size_t number_of_channels,
    const uint32_t sample_rate,
    const uint32_t audio_delay_milliseconds,
    const int32_t /*clock_drift*/,
    const uint32_t /*volume*/,
    const bool key_pressed,
    uint32_t&amp; /*new_mic_volume*/) {  // NOLINT: to avoid changing APIs
  RTC_DCHECK(audio_data);
  RTC_DCHECK_GE(number_of_channels, 1);
  RTC_DCHECK_LE(number_of_channels, 2);
  RTC_DCHECK_EQ(2 * number_of_channels, bytes_per_sample);
  RTC_DCHECK_GE(sample_rate, AudioProcessing::NativeRate::kSampleRate8kHz);
  // 100 = 1 second / data duration (10 ms).
  RTC_DCHECK_EQ(number_of_frames * 100, sample_rate);
  RTC_DCHECK_LE(bytes_per_sample * number_of_frames * number_of_channels,
                AudioFrame::kMaxDataSizeBytes);

  int send_sample_rate_hz = 0;
  size_t send_num_channels = 0;
  bool swap_stereo_channels = false;
  {
    MutexLock lock(&amp;capture_lock_);
    send_sample_rate_hz = send_sample_rate_hz_;
    send_num_channels = send_num_channels_;
    swap_stereo_channels = swap_stereo_channels_;
  }

  std::unique_ptr&lt;AudioFrame&gt; audio_frame(new AudioFrame());
  InitializeCaptureFrame(sample_rate, send_sample_rate_hz, number_of_channels,
                         send_num_channels, audio_frame.get());
  voe::RemixAndResample(static_cast&lt;const int16_t*&gt;(audio_data),
                        number_of_frames, number_of_channels, sample_rate,
                        &amp;capture_resampler_, audio_frame.get());
  ProcessCaptureFrame(audio_delay_milliseconds, key_pressed,
                      swap_stereo_channels, audio_processing_,
                      audio_frame.get());

  // Typing detection (utilizes the APM/VAD decision). We let the VAD determine
  // if we're using this feature or not.
  // TODO(solenberg): GetConfig() takes a lock. Work around that.
  bool typing_detected = false;
  if (audio_processing_ &amp;&amp;
      audio_processing_-&gt;GetConfig().voice_detection.enabled) {
    if (audio_frame-&gt;vad_activity_ != AudioFrame::kVadUnknown) {
      bool vad_active = audio_frame-&gt;vad_activity_ == AudioFrame::kVadActive;
      typing_detected = typing_detection_.Process(key_pressed, vad_active);
    }
  }

  // Copy frame and push to each sending stream. The copy is required since an
  // encoding task will be posted internally to each stream.
  {
    MutexLock lock(&amp;capture_lock_);
    typing_noise_detected_ = typing_detected;
  }

  RTC_DCHECK_GT(audio_frame-&gt;samples_per_channel_, 0);
  if (async_audio_processing_)
    async_audio_processing_-&gt;Process(std::move(audio_frame));
  else
    SendProcessedData(std::move(audio_frame));

  return 0;
}

void AudioTransportImpl::SendProcessedData(
    std::unique_ptr&lt;AudioFrame&gt; audio_frame) {
  RTC_DCHECK_GT(audio_frame-&gt;samples_per_channel_, 0);
  MutexLock lock(&amp;capture_lock_);
  if (audio_senders_.empty())
    return;

  auto it = audio_senders_.begin();
  while (++it != audio_senders_.end()) {
    auto audio_frame_copy = std::make_unique&lt;AudioFrame&gt;();
    audio_frame_copy-&gt;CopyFrom(*audio_frame);
    (*it)-&gt;SendAudioData(std::move(audio_frame_copy));
  }
  // Send the original frame to the first stream w/o copying.
  (*audio_senders_.begin())-&gt;SendAudioData(std::move(audio_frame));
}
</code></pre> 
<p>这里的音频数据信号处理为软件音频数据信号处理，如软件 AEC 等，即这些信号处理主要由集成的软件算法和库实现。这里的这些软件音频数据信号处理，常常是音频数据流水线上的 CPU 和内存资源消耗大户。音频数据信号处理造成的延迟，主要来源于这些信号处理繁重的 CPU 数据运算。然而许多设备和系统，本身为了支持电话/通话等场景，在系统集成了回声消除等音频数据信号处理。对于硬件音频数据信号处理，在实现上，设备和系统可能是在内部集成了软件音频数据信号处理的库，也可能设备本身带有专门的数字信号处理芯片。设备和系统支持的回声消除、降噪等音频数据信号处理，称为硬件 AEC 和硬件 ANS。Windows、Android 和 iOS 等系统都具有硬件 AEC 等功能。</p> 
<p>设备和系统的硬件音频数字信号处理的优点是，比较高效，资源消耗较少，硬件 AEC 消除回声比较彻底，比如多个进程同时在播放声音，麦克风录制的声音经过硬件 AEC 可以消除掉这所有的回声，而对于软件 AEC，则只有当前进程播放的音频数据才有可能被消除等。硬件音频数字信号处理的缺点则是，难以调试分析和优化更新，对多场景的适应性一般，比如硬件 AEC 大多针对语音通话做优化，但在处理录制的音乐时，这种回声消除处理会对音质造成不太好的影响。相对的软件音频数据信号处理的优劣势也比较明显。</p> 
<p>**音频数据信号处理延迟与操作系统、具体的设备及 WebRTC 的配置有关。当操作系统及设备支持并开启了硬件的回声消除、硬件的降噪等硬件音频数据信号处理，并关闭了软件的音频数据信号处理时，可以认为音频数据信号处理造成的延迟为 0。而开启了软件的音频数据信号处理，音频数据信号处理延迟则受设备的运算能力影响较大，但一般不会超过 5 ms。**笔者做测试的 Windows 10 笔记本电脑支持硬件音频数据信号处理，且配置了开启硬件音频数据信号处理，这里忽略音频数据信号处理的延迟。</p> 
<h2>
<a id="_199"></a>音频编码延迟</h2> 
<p>在 WebRTC 中，采集到的音频数据由 <code>webrtc::AudioDeviceModule</code> 的采集线程一直送到 <code>webrtc::voe::(anonymous namespace)::ChannelSend::ProcessAndEncodeAudio()</code>，这个函数将音频数据转到一个专门的编码任务队列，异步地进行音频编码：</p> 
<pre><code>void ChannelSend::ProcessAndEncodeAudio(
    std::unique_ptr&lt;AudioFrame&gt; audio_frame) {
  RTC_DCHECK_RUNS_SERIALIZED(&amp;audio_thread_race_checker_);
  RTC_DCHECK_GT(audio_frame-&gt;samples_per_channel_, 0);
  RTC_DCHECK_LE(audio_frame-&gt;num_channels_, 8);

  // Profile time between when the audio frame is added to the task queue and
  // when the task is actually executed.
  audio_frame-&gt;UpdateProfileTimeStamp();
  encoder_queue_.PostTask(
      [this, audio_frame = std::move(audio_frame)]() mutable {
        RTC_DCHECK_RUN_ON(&amp;encoder_queue_);
        if (!encoder_queue_is_active_) {
          if (fixing_timestamp_stall_) {
            _timeStamp +=
                static_cast&lt;uint32_t&gt;(audio_frame-&gt;samples_per_channel_);
          }
          return;
        }
        // Measure time between when the audio frame is added to the task queue
        // and when the task is actually executed. Goal is to keep track of
        // unwanted extra latency added by the task queue.
        RTC_HISTOGRAM_COUNTS_10000("WebRTC.Audio.EncodingTaskQueueLatencyMs",
                                   audio_frame-&gt;ElapsedProfileTimeMs());

        bool is_muted = InputMute();
        AudioFrameOperations::Mute(audio_frame.get(), previous_frame_muted_,
                                   is_muted);

        if (_includeAudioLevelIndication) {
          size_t length =
              audio_frame-&gt;samples_per_channel_ * audio_frame-&gt;num_channels_;
          RTC_CHECK_LE(length, AudioFrame::kMaxDataSizeBytes);
          if (is_muted &amp;&amp; previous_frame_muted_) {
            rms_level_.AnalyzeMuted(length);
          } else {
            rms_level_.Analyze(
                rtc::ArrayView&lt;const int16_t&gt;(audio_frame-&gt;data(), length));
          }
        }
        previous_frame_muted_ = is_muted;

        // Add 10ms of raw (PCM) audio data to the encoder @ 32kHz.

        // The ACM resamples internally.
        audio_frame-&gt;timestamp_ = _timeStamp;
        // This call will trigger AudioPacketizationCallback::SendData if
        // encoding is done and payload is ready for packetization and
        // transmission. Otherwise, it will return without invoking the
        // callback.
        if (audio_coding_-&gt;Add10MsData(*audio_frame) &lt; 0) {
          RTC_DLOG(LS_ERROR) &lt;&lt; "ACM::Add10MsData() failed.";
          return;
        }

        _timeStamp += static_cast&lt;uint32_t&gt;(audio_frame-&gt;samples_per_channel_);
      });
}
</code></pre> 
<p>编码之后的音频帧，被打上 timestamp，被打包为音频 RTP 包，并被送进 pacing 模块等待发送，这个调用过程如下：</p> 
<pre><code>  * frame #0: webrtc::TaskQueuePacedSender::EnqueuePackets(this=0x000000010f823a00, packets=size=1) at task_queue_paced_sender.cc:130:3
    frame #1: webrtc::voe::(anonymous namespace)::RtpPacketSenderProxy::EnqueuePackets(this=0x000000010785e9e0, packets=size=0) at channel_send.cc:267:24
    frame #2: webrtc::RTPSender::SendToNetwork(this=0x000000010f83bc50, packet=nullptr) at rtp_sender.cc:491:18
    frame #3: webrtc::RTPSenderAudio::SendAudio(this=0x0000000107860b30, frame_type=kAudioFrameSpeech, payload_type='?', rtp_timestamp=2577682277, payload_data="ox", payload_size=69, absolute_capture_timestamp_ms=-1) at
rtp_sender_audio.cc:316:35
    frame #4: webrtc::voe::(anonymous namespace)::ChannelSend::SendRtpAudio(this=0x000000010785e6e0, frameType=kAudioFrameSpeech, payloadType='?', rtp_timestamp=0, payload=ArrayView&lt;const unsigned char, -4711L&gt; @ 0x00007000020f1b08, absolute_capture_timestamp_ms=-1) at channel_send.cc:439:27
    frame #5: webrtc::voe::(anonymous namespace)::ChannelSend::SendData(this=0x000000010785e6e0, frameType=kAudioFrameSpeech, payloadType='?', rtp_timestamp=0, payloadData="ox", payloadSize=69, absolute_capture_timestamp_ms=-1) at channel_send.cc:367:10
    frame #6:  webrtc::voe::(anonymous namespace)::ChannelSend::SendData(webrtc::AudioFrameType, unsigned char, unsigned int, unsigned char const*, unsigned long, long long) at channel_send.cc:0
    frame #7: webrtc::(anonymous namespace)::AudioCodingModuleImpl::Encode(this=0x000000010f83d200, input_data=0x000000010f83d208, absolute_capture_timestamp_ms=optional&lt;long long&gt; @ 0x00007000020f2290)::AudioCodingModuleImpl::InputData const&amp;, absl::optional&lt;long long&gt;) at audio_coding_module.cc:304:32
    frame #8: webrtc::(anonymous namespace)::AudioCodingModuleImpl::Add10MsData(this=0x000000010f83d200, audio_frame=0x000000011580da00) at audio_coding_module.cc:341:16
    frame #9: webrtc::voe::(anonymous namespace)::ChannelSend::ProcessAndEncodeAudio(this=0x0000000107e0b3c8)::$_7::operator()() at channel_send.cc:857:28
</code></pre> 
<p>音频编码延迟主要包括音频数据编码任务被丢进编码任务队列等待执行的时间，和音频编码操作的耗时。音频编码延迟可以通过统计音频数据编码任务对象的生命周期获得，我们可以对 <code>webrtc/audio/channel_send.cc</code> 做类似下面这样的修改来实现：</p> 
<p><img src="https://images2.imgbox.com/61/b0/gAIoSKoz_o.png" alt="Audio encoding delay"></p> 
<p>对于音频编码，还需要注意的一个点是，<code>webrtc::AudioDeviceModule</code> 每次给 <code>webrtc::AudioTransport</code> 送 10 ms 的音频数据，但 WebRTC 默认的音频编码器 OPUS 的默认配置是，每次编码 20 ms 的数据，也就是每两帧的采集音频帧编码一帧的编码帧，<code>webrtc::voe::(anonymous namespace)::ChannelSend::ProcessAndEncodeAudio()</code> 中统计的编码延迟，有一半的统计记录，由于创建的音频数据编码任务只是把采集的音频数据放进音频编码器的缓冲区，而没有真正地执行编码操作，是没什么太大意义的。</p> 
<p>笔者用 debug 版的二进制文件，在一台 Windows 10 笔记本电脑上，执行与上面测音频采集延迟时相同的测试用例，测 Windows 平台的音频编码延迟。测试用例跑了大概 10 分钟，总共获得了 58494 条数据，统计数据时将耗时超过 1 ms 或真正地执行了编码操作的记录统计进来，这些记录总共有 30764 条。我们同样主要关注音频编码延迟的数值分布，而不是简单的平均值。数据统计结果如下表（延迟数据单位为 us）：</p> 
<table>
<thead><tr>
<th>Delay (us)</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>0</td>
<td>7722</td>
<td>0.251008</td>
</tr>
<tr>
<td>1000</td>
<td>22421</td>
<td>0.728806</td>
</tr>
<tr>
<td>2000</td>
<td>604</td>
<td>0.019633</td>
</tr>
<tr>
<td>3000</td>
<td>14</td>
<td>0.000455</td>
</tr>
<tr>
<td>4000</td>
<td>3</td>
<td>0.000098</td>
</tr>
</tbody>
</table>
<p>从表中可以看到，98% 以上的情况中，音频编码延迟小于 2 ms，极少数情况会达到 4 ms，没有超过 5 ms 的记录。</p> 
<h2>
<a id="_296"></a>编码音频包的发送延迟</h2> 
<p>前面我们看到，编码之后的音频包被送进 pacing 模块来做平滑发送。Pacing 模块中有两个 <code>webrtc::RtpPacketPacer</code>/<code>webrtc::RtpPacketSender</code> 的实现，分别为 <code>webrtc::TaskQueuePacedSender</code> 和 <code>webrtc::PacedSender</code>，默认为 <code>webrtc::TaskQueuePacedSender</code>。</p> 
<p>以使用 <code>webrtc::TaskQueuePacedSender</code> 控制平滑发送为例，来看编码音频包的发送过程。音频编码模块通过 <code>TaskQueuePacedSender::EnqueuePackets()</code> 将编码音频包送进 <code>webrtc::TaskQueuePacedSender</code>，<code>webrtc::TaskQueuePacedSender</code> 立即在名为 <strong>TaskQueuePacedSender</strong> 的任务队列中起一个异步任务，将编码音频包送进 <code>webrtc::PacingController</code> 的包队列中，<code>TaskQueuePacedSender::EnqueuePackets()</code> 的实现如下：</p> 
<pre><code>void TaskQueuePacedSender::EnqueuePackets(
    std::vector&lt;std::unique_ptr&lt;RtpPacketToSend&gt;&gt; packets) {
#if RTC_TRACE_EVENTS_ENABLED
  TRACE_EVENT0(TRACE_DISABLED_BY_DEFAULT("webrtc"),
               "TaskQueuePacedSender::EnqueuePackets");
  for (auto&amp; packet : packets) {
    TRACE_EVENT2(TRACE_DISABLED_BY_DEFAULT("webrtc"),
                 "TaskQueuePacedSender::EnqueuePackets::Loop",
                 "sequence_number", packet-&gt;SequenceNumber(), "rtp_timestamp",
                 packet-&gt;Timestamp());
  }
#endif

  task_queue_.PostTask([this, packets_ = std::move(packets)]() mutable {
    RTC_DCHECK_RUN_ON(&amp;task_queue_);
    for (auto&amp; packet : packets_) {
      packet_size_.Apply(1, packet-&gt;size());
      RTC_DCHECK_GE(packet-&gt;capture_time_ms(), 0);
      pacing_controller_.EnqueuePacket(std::move(packet));
    }
    MaybeProcessPackets(Timestamp::MinusInfinity());
  });
}
</code></pre> 
<p><code>webrtc::TaskQueuePacedSender</code> 将编码音频包送进 <code>webrtc::PacingController</code> 的包队列的调用过程如下：</p> 
<pre><code>  * frame #0: webrtc::PacingController::EnqueuePacketInternal(this=0x000000010f058e28, packet=webrtc::RtpPacketToSend @ 0x0000000106f24070, priority=1) at pacing_controller.cc:291:3
    frame #1: webrtc::PacingController::EnqueuePacket(this=0x000000010f058e28, packet=nullptr) at pacing_controller.cc:242:3
    frame #2: webrtc::TaskQueuePacedSender::EnqueuePackets(this=0x0000000107817d28)::$_9::operator()() at task_queue_paced_sender.cc:145:26
</code></pre> 
<p>随后，在 <strong>TaskQueuePacedSender</strong> 任务队列中处理编码音频包并发送出去，这个调用过程如下：</p> 
<pre><code>  * frame #0: cricket::MediaChannel::SendRtp(this=0x00000001078162b0, data="x90xbfB", len=101, options=0x000070000db31220) at media_channel.cc:170:8
    frame #1: cricket::WebRtcVoiceMediaChannel::SendRtp(this=0x00000001078162b0, data="x90xbfB", len=101, options=0x000070000db31220) at webrtc_voice_engine.cc:2571:17
    frame #2: webrtc::RtpSenderEgress::SendPacketToNetwork(this=0x000000010e82bd78, packet=0x0000000106f24070, options=0x000070000db31220, pacing_info=0x000070000db32270) at rtp_sender_egress.cc:553:30
    frame #3: webrtc::RtpSenderEgress::SendPacket(this=0x000000010e82bd78, packet=0x0000000106f24070, pacing_info=0x000070000db32270) at rtp_sender_egress.cc:273:29
    frame #4: webrtc::ModuleRtpRtcpImpl2::TrySendPacket(this=0x000000010e82b400, packet=0x0000000106f24070, pacing_info=0x000070000db32270) at rtp_rtcp_impl2.cc:376:30
    frame #5: webrtc::PacketRouter::SendPacket(this=0x000000010f0586a0, packet=webrtc::RtpPacketToSend @ 0x0000000106f24070, cluster_info=0x000070000db32270) at packet_router.cc:160:20
    frame #6: webrtc::PacingController::ProcessPackets(this=0x000000010f058e28) at pacing_controller.cc:585:21
    frame #7: webrtc::TaskQueuePacedSender::MaybeProcessPackets(this=0x000000010f058e00, scheduled_process_time=Timestamp @ 0x000070000db32738) at task_queue_paced_sender.cc:234:24
    frame #8: webrtc::TaskQueuePacedSender::MaybeProcessPackets(this=0x0000000107d04098)::$_14::operator()() const at task_queue_paced_sender.cc:275:39
</code></pre> 
<p>在 <code>cricket::MediaChannel</code> 中，编码音频包被转到网络线程 <strong>pc_network_thread</strong> 并发送出去：</p> 
<pre><code>void MediaChannel::SendRtp(const uint8_t* data,
                           size_t len,
                           const webrtc::PacketOptions&amp; options) {
  auto send =
      [this, packet_id = options.packet_id,
       included_in_feedback = options.included_in_feedback,
       included_in_allocation = options.included_in_allocation,
       packet = rtc::CopyOnWriteBuffer(data, len, kMaxRtpPacketLen)]() mutable {
        rtc::PacketOptions rtc_options;
        rtc_options.packet_id = packet_id;
        if (DscpEnabled()) {
          rtc_options.dscp = PreferredDscp();
        }
        rtc_options.info_signaled_after_sent.included_in_feedback =
            included_in_feedback;
        rtc_options.info_signaled_after_sent.included_in_allocation =
            included_in_allocation;
        SendPacket(&amp;packet, rtc_options);
      };

  // TODO(bugs.webrtc.org/11993): ModuleRtpRtcpImpl2 and related classes (e.g.
  // RTCPSender) aren't aware of the network thread and may trigger calls to
  // this function from different threads. Update those classes to keep
  // network traffic on the network thread.
  if (network_thread_-&gt;IsCurrent()) {
    send();
  } else {
    network_thread_-&gt;PostTask(ToQueuedTask(network_safety_, std::move(send)));
  }
}

void MediaChannel::SendRtcp(const uint8_t* data, size_t len) {
  auto send = [this, packet = rtc::CopyOnWriteBuffer(
                         data, len, kMaxRtpPacketLen)]() mutable {
    rtc::PacketOptions rtc_options;
    if (DscpEnabled()) {
      rtc_options.dscp = PreferredDscp();
    }
    SendRtcp(&amp;packet, rtc_options);
  };

  if (network_thread_-&gt;IsCurrent()) {
    send();
  } else {
    network_thread_-&gt;PostTask(ToQueuedTask(network_safety_, std::move(send)));
  }
}
</code></pre> 
<p>最终在网络线程 <strong>pc_network_thread</strong> 中，编码音频包被发送到网络：</p> 
<pre><code>  * frame #0: rtc::PhysicalSocket::DoSendTo(this=0x000000010781adc0, socket=7, buf="x90xbfB", len=111, flags=0, dest_addr=0x000070000dbb2bf8, addrlen=16) at physical_socket_server.cc:510:19
    frame #1: rtc::PhysicalSocket::SendTo(this=0x000000010781adc0, buffer=0x000000011200a000, length=111, addr=0x000000011480c9c8) at physical_socket_server.cc:375:7
    frame #2: rtc::AsyncUDPSocket::SendTo(this=0x000000010781b6c0, pv=0x000000011200a000, cb=111, addr=0x000000011480c9c8, options=0x000070000dbb33a8) at async_udp_socket.cc:84:22
    frame #3: cricket::UDPPort::SendTo(this=0x000000010e819600, data=0x000000011200a000, size=111, addr=0x000000011480c9c8, options=0x000070000dbb37a8, payload=true) at stun_port.cc:281:23
    frame #4: cricket::ProxyConnection::Send(this=0x000000011480c800, data=0x000000011200a000, size=111, options=0x000070000dbb37a8) at connection.cc:1371:14
    frame #5: cricket::P2PTransportChannel::SendPacket(this=0x000000010e815600, data="x90xbfB", len=111, options=0x000070000dbb44e8, flags=0) at p2p_transport_channel.cc:1616:36
    frame #6: cricket::DtlsTransport::SendPacket(this=0x0000000107814880, data="x90xbfB", size=111, options=0x000070000dbb44e8, flags=1) at dtls_transport.cc:417:32
    frame #7: webrtc::RtpTransport::SendPacket(this=0x000000010e817c00, rtcp=false, packet=0x0000000107b08608, options=0x000070000dbb44e8, flags=1) at rtp_transport.cc:147:24
    frame #8: webrtc::SrtpTransport::SendRtpPacket(this=0x000000010e817c00, packet=0x0000000107b08608, options=0x000070000dbb56d0, flags=1) at srtp_transport.cc:173:10
    frame #9: cricket::BaseChannel::SendPacket(this=0x00000001078168c0, rtcp=false, packet=0x0000000107b08608, options=0x000070000dbb56d0) at channel.cc:437:33
    frame #10: cricket::BaseChannel::SendPacket(this=0x00000001078168c0, packet=0x0000000107b08608, options=0x000070000dbb56d0) at channel.cc:318:10
    frame #11: cricket::MediaChannel::DoSendPacket(this=0x00000001078162b0, packet=0x0000000107b08608, rtcp=false, options=0x000070000dbb56d0) at media_channel.cc:163:40
    frame #12: cricket::MediaChannel::SendPacket(this=0x00000001078162b0, packet=0x0000000107b08608, options=0x000070000dbb56d0) at media_channel.cc:71:10
    frame #13: cricket::MediaChannel::SendRtp(this=0x0000000107b085f8)::$_2::operator()() at media_channel.cc:184:9
</code></pre> 
<p>在逻辑上，编码音频包的发送过程可以看作包含 5 个阶段，这 5 个阶段由两个线程接力完成：</p> 
<ol>
<li>编码音频包被送进 <code>webrtc::TaskQueuePacedSender</code>，<code>webrtc::TaskQueuePacedSender</code> 在 <strong>TaskQueuePacedSender</strong> 任务队列中起任务将编码音频包放进 <code>webrtc::PacingController</code> 的包队列中；</li>
<li>编码音频包在 <code>webrtc::PacingController</code> 的包队列中待一段时间等着被处理；</li>
<li>
<code>webrtc::PacingController</code> 的包队列中的编码音频包在 <strong>TaskQueuePacedSender</strong> 任务队列中被处理，并被转到网络线程 <strong>pc_network_thread</strong> 中等待被发送；</li>
<li>编码音频包在网络线程 <strong>pc_network_thread</strong> 的任务队列中待一段时间；</li>
<li>编码音频包被网络线程 <strong>pc_network_thread</strong> 发送到网络。</li>
</ol> 
<p>编码音频包的发送延迟可以认为是编码音频包进入 <code>webrtc::TaskQueuePacedSender</code> 到 <code>cricket::MediaChannel</code> 向网络线程 <strong>pc_network_thread</strong> 中抛的编码音频包发送任务执行结束的时长。这个时长可以通过打多个点来计算：在 <code>TaskQueuePacedSender::EnqueuePackets()</code> 打点记录编码音频包的 timestamp 和时间戳，这个时候编码音频包还没有有效的 sequence number；在 <code>RtpSenderEgress::SendPacket()</code> 打点记录编码音频包的 timestamp 和 sequence number，以建立编码音频包的 timestamp 和 sequence number 之间的关联；在 <code>MediaChannel::SendRtp()</code> 中，编码音频包发送任务执行结束后，打点记录编码音频包的 sequence number 和时间戳。</p> 
<p>由于在 <code>TaskQueuePacedSender::EnqueuePackets()</code> 中，编码音频包还没有有效的 sequence number 而只能访问编码音频包的 timestamp，在 <code>MediaChannel::SendRtp()</code> 中，访问编码音频包的 sequence number 比较方便，故需要在 <code>RtpSenderEgress::SendPacket()</code> 打点建立编码音频包的 timestamp 和 sequence number 之间的关联。</p> 
<p>统计编码音频包发送延迟相关的具体改动如下：</p> 
<p><img src="https://images2.imgbox.com/ac/6b/Wv6bjS1X_o.jpg" alt="Audio send delay"></p> 
<p>笔者用 debug 版的二进制文件，在笔者的一台 Mac 笔记本电脑上跑简单的 WebRTC 一对一语音通话用例，运行大概 10 分钟，总共获得 24483 条数据。同样，我们主要关注编码音频包发送延迟的数值分布，对这些数据的统计结果如下表（延迟数据单位为 ms）：</p> 
<table>
<thead><tr>
<th>Delay</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>0</td>
<td>15035</td>
<td>0.604228</td>
</tr>
<tr>
<td>1</td>
<td>9485</td>
<td>0.381184</td>
</tr>
<tr>
<td>2</td>
<td>44</td>
<td>0.001768</td>
</tr>
<tr>
<td>3</td>
<td>50</td>
<td>0.002009</td>
</tr>
<tr>
<td>4</td>
<td>56</td>
<td>0.002251</td>
</tr>
<tr>
<td>5</td>
<td>27</td>
<td>0.001085</td>
</tr>
<tr>
<td>6</td>
<td>23</td>
<td>0.000924</td>
</tr>
<tr>
<td>7</td>
<td>24</td>
<td>0.000965</td>
</tr>
<tr>
<td>8</td>
<td>25</td>
<td>0.001005</td>
</tr>
<tr>
<td>9</td>
<td>16</td>
<td>0.000643</td>
</tr>
<tr>
<td>10</td>
<td>10</td>
<td>0.000402</td>
</tr>
<tr>
<td>11</td>
<td>5</td>
<td>0.000201</td>
</tr>
<tr>
<td>12</td>
<td>4</td>
<td>0.000161</td>
</tr>
<tr>
<td>13</td>
<td>11</td>
<td>0.000442</td>
</tr>
<tr>
<td>14</td>
<td>4</td>
<td>0.000161</td>
</tr>
<tr>
<td>15</td>
<td>12</td>
<td>0.000482</td>
</tr>
<tr>
<td>16</td>
<td>17</td>
<td>0.000683</td>
</tr>
<tr>
<td>17</td>
<td>14</td>
<td>0.000563</td>
</tr>
<tr>
<td>18</td>
<td>15</td>
<td>0.000603</td>
</tr>
<tr>
<td>19</td>
<td>1</td>
<td>0.000040</td>
</tr>
<tr>
<td>20</td>
<td>3</td>
<td>0.000121</td>
</tr>
<tr>
<td>23</td>
<td>1</td>
<td>0.000040</td>
</tr>
<tr>
<td>41</td>
<td>1</td>
<td>0.000040</td>
</tr>
</tbody>
</table>
<p>在表中可以看到，98% 以上的情况中，编码音频包发送延迟小于 2 ms，但这一延迟也会有一些波动，这一延迟的最大值甚至可以达到 41 ms。</p> 
<h2>
<a id="_462"></a>编码音频包网络传输延迟</h2> 
<p>由于互联网环境的错综复杂，编码音频包网络传输延迟常常是音频端到端延迟比较重要的组成部分。这部分延迟，可以认为是在 <code>cricket::MediaChannel</code> 中，创建的网络线程 <strong>pc_network_thread</strong> 上的编码音频包发送任务执行结束，到接收端从网络上收到编码音频包之间的时间。</p> 
<p>如果发送端和接收端运行于不同的机器，则两台机器的时钟难以保持绝对同步，这会给网络传输延迟的统计造成一些障碍。即使两台机器通过相同的时钟源来校准，基于编码音频包的发送 NTP 时间和接收 NTP 时间来统计网络传输延迟，也会由于校准的精度问题，而使统计的网络传输延迟存在一定的可见的误差。</p> 
<p>统计网络传输延迟，最好的方法还是在同一台机器上既运行发送端，也运行接收端，从而基于同一个时钟来计算。</p> 
<p>编码音频包在整个发送、传输和接收过程中，timestamp 保持不变，因而可以通过编码音频包的 timestamp 来关联发送的包和接收的包。</p> 
<p>接收端编码音频包的接收过程主要为，在网络线程中，从网络上接收编码音频包，随后在 worker thread 中把编码音频包放进 NetEq 的 PacketBuffer 里。</p> 
<p>网络线程 <strong>pc_network_thread</strong> 中，从网络接收编码音频包并向 NetEQ 传递的调用过程为：</p> 
<pre><code>* thread #6, name = 'pc_network_thread', stop reason = breakpoint 2.1
  * frame #0: cricket::WebRtcVoiceMediaChannel::OnPacketReceived(this=0x0000000107d07040, packet=&lt;unavailable&gt;, packet_time_us=6039635487554) at webrtc_voice_engine.cc:2216:3
    frame #1: cricket::BaseChannel::OnRtpPacket(this=0x0000000107d060b0, parsed_packet=0x000070000361acd0) at channel.cc:467:19
    frame #2: webrtc::RtpDemuxer::OnRtpPacket(this=0x000000010882cb70, packet=0x000070000361acd0) at rtp_demuxer.cc:249:11
    frame #3: webrtc::RtpTransport::DemuxPacket(this=0x000000010882ca00, packet=CopyOnWriteBuffer @ 0x000070000361b2e0, packet_time_us=6039635487554) at rtp_transport.cc:194:21
    frame #4: webrtc::SrtpTransport::OnRtpPacketReceived(this=0x000000010882ca00, packet=CopyOnWriteBuffer @ 0x000070000361b820, packet_time_us=6039635487554) at srtp_transport.cc:226:3
    frame #5: webrtc::RtpTransport::OnReadPacket(this=0x000000010882ca00, transport=0x0000000107839470, data="x90?", len=147, packet_time_us=0x000070000361d078, flags=1) at rtp_transport.cc:268:5
    frame #10: cricket::DtlsTransport::OnReadPacket(this=0x0000000107839470, transport=0x0000000108826c00, data="x90?", size=147, packet_time_us=0x000070000361d078, flags=0) at dtls_transport.cc:627:9
    frame #15: cricket::P2PTransportChannel::OnReadPacket(this=0x0000000108826c00, connection=0x0000000114008800, data="x90?", len=147, packet_time_us=6039635487554) at p2p_transport_channel.cc:2215:5
    frame #20: cricket::Connection::OnReadPacket(this=0x0000000114008800, data="x90?", size=147, packet_time_us=6039635487554) at connection.cc:465:5
    frame #21: cricket::UDPPort::OnReadPacket(this=0x000000010f06f800, socket=0x0000000107b4cfb0, data="x90?", size=147, remote_addr=0x000070000361e480, packet_time_us=0x000070000361dd30) at stun_port.cc:389:11
    frame #22: cricket::UDPPort::HandleIncomingPacket(this=0x000000010f06f800, socket=0x0000000107b4cfb0, data="x90?", size=147, remote_addr=0x000070000361e480, packet_time_us=6039635487554) at stun_port.cc:330:3
    frame #23: cricket::AllocationSequence::OnReadPacket(this=0x0000000107b4cbe0, socket=0x0000000107b4cfb0, data="x90?", size=147, remote_addr=0x000070000361e480, packet_time_us=0x000070000361e2d8) at basic_port_allocator.cc:1639:18
    frame #28: rtc::AsyncUDPSocket::OnReadEvent(this=0x0000000107b4cfb0, socket=0x0000000107b4cd70) at async_udp_socket.cc:132:3
    frame #33: rtc::SocketDispatcher::OnEvent(this=0x0000000107b4cd70, ff=1, err=0) at physical_socket_server.cc:842:5
    frame #34: rtc::ProcessEvents(dispatcher=0x0000000107b4cd70, readable=true, writable=false, error_event=false, check_error=true) at physical_socket_server.cc:1249:17
    frame #35: rtc::PhysicalSocketServer::WaitSelect(this=0x0000000106f08570, cmsWait=32, process_io=true) at physical_socket_server.cc:1357:9
    frame #36: rtc::PhysicalSocketServer::Wait(this=0x0000000106f08570, cmsWait=32, process_io=true) at physical_socket_server.cc:1183:10
</code></pre> 
<p><code>WebRtcVoiceMediaChannel::OnPacketReceived()</code> 将接收到的编码音频包转到 worker thread，并送进 NetEQ：</p> 
<pre><code>void WebRtcVoiceMediaChannel::OnPacketReceived(rtc::CopyOnWriteBuffer packet,
                                               int64_t packet_time_us) {
  RTC_DCHECK_RUN_ON(&amp;network_thread_checker_);
  // TODO(bugs.webrtc.org/11993): This code is very similar to what
  // WebRtcVideoChannel::OnPacketReceived does. For maintainability and
  // consistency it would be good to move the interaction with call_-&gt;Receiver()
  // to a common implementation and provide a callback on the worker thread
  // for the exception case (DELIVERY_UNKNOWN_SSRC) and how retry is attempted.
  worker_thread_-&gt;PostTask(ToQueuedTask(task_safety_, [this, packet,
                                                       packet_time_us] {
    RTC_DCHECK_RUN_ON(worker_thread_);

    webrtc::PacketReceiver::DeliveryStatus delivery_result =
        call_-&gt;Receiver()-&gt;DeliverPacket(webrtc::MediaType::AUDIO, packet,
                                         packet_time_us);

    if (delivery_result != webrtc::PacketReceiver::DELIVERY_UNKNOWN_SSRC) {
      return;
    }

    // Create an unsignaled receive stream for this previously not received
    // ssrc. If there already is N unsignaled receive streams, delete the
    // oldest. See: https://bugs.chromium.org/p/webrtc/issues/detail?id=5208
    uint32_t ssrc = ParseRtpSsrc(packet);
    RTC_DCHECK(!absl::c_linear_search(unsignaled_recv_ssrcs_, ssrc));

    // Add new stream.
    StreamParams sp = unsignaled_stream_params_;
    sp.ssrcs.push_back(ssrc);
    RTC_LOG(LS_INFO) &lt;&lt; "Creating unsignaled receive stream for SSRC=" &lt;&lt; ssrc;
    if (!AddRecvStream(sp)) {
      RTC_LOG(LS_WARNING) &lt;&lt; "Could not create unsignaled receive stream.";
      return;
    }
    unsignaled_recv_ssrcs_.push_back(ssrc);
    RTC_HISTOGRAM_COUNTS_LINEAR("WebRTC.Audio.NumOfUnsignaledStreams",
                                unsignaled_recv_ssrcs_.size(), 1, 100, 101);

    // Remove oldest unsignaled stream, if we have too many.
    if (unsignaled_recv_ssrcs_.size() &gt; kMaxUnsignaledRecvStreams) {
      uint32_t remove_ssrc = unsignaled_recv_ssrcs_.front();
      RTC_DLOG(LS_INFO) &lt;&lt; "Removing unsignaled receive stream with SSRC="
                        &lt;&lt; remove_ssrc;
      RemoveRecvStream(remove_ssrc);
    }
    RTC_DCHECK_GE(kMaxUnsignaledRecvStreams, unsignaled_recv_ssrcs_.size());

    SetOutputVolume(ssrc, default_recv_volume_);
    SetBaseMinimumPlayoutDelayMs(ssrc, default_recv_base_minimum_delay_ms_);

    // The default sink can only be attached to one stream at a time, so we hook
    // it up to the *latest* unsignaled stream we've seen, in order to support
    // the case where the SSRC of one unsignaled stream changes.
    if (default_sink_) {
      for (uint32_t drop_ssrc : unsignaled_recv_ssrcs_) {
        auto it = recv_streams_.find(drop_ssrc);
        it-&gt;second-&gt;SetRawAudioSink(nullptr);
      }
      std::unique_ptr&lt;webrtc::AudioSinkInterface&gt; proxy_sink(
          new ProxySink(default_sink_.get()));
      SetRawAudioSink(ssrc, std::move(proxy_sink));
    }

    delivery_result = call_-&gt;Receiver()-&gt;DeliverPacket(webrtc::MediaType::AUDIO,
                                                       packet, packet_time_us);
    RTC_DCHECK_NE(webrtc::PacketReceiver::DELIVERY_UNKNOWN_SSRC,
                  delivery_result);
  }));
}
</code></pre> 
<p><code>WebRtcVoiceMediaChannel::OnPacketReceived()</code> 将接收到的编码音频包在 worker thread 上送进 NetEQ 的调用过程如下：</p> 
<pre><code>* thread #5, name = 'Thread 0x0x10780af20', stop reason = breakpoint 1.1
  * frame #0: webrtc::PacketBuffer::InsertPacket(this=0x0000000107b49080, packet=0x0000000106f9cea0, stats=0x0000000107b48de0, last_decoded_length=480, sample_rate=16000, target_level_ms=80, decoder_database=0x0000000107b49030) at packet_buffer.cc:140:7
    frame #1: webrtc::PacketBuffer::InsertPacketList(this=0x0000000107b49080, packet_list=0x0000700003598948 size=1, decoder_database=0x0000000107b49030, current_rtp_payload_type=0x0000000107b498ad, current_cng_rtp_payload_type=0x0000000107b498af, stats=0x0000000107b48de0, last_decoded_length=480, sample_rate=16000, target_level_ms=80) at packet_buffer.cc:243:9
    frame #2: webrtc::NetEqImpl::InsertPacketInternal(this=0x0000000107b49750, rtp_header=0x0000700003599ea0, payload=ArrayView&lt;const unsigned char, -4711L&gt; @ 0x0000700003598af0) at neteq_impl.cc:690:35
    frame #3: webrtc::NetEqImpl::InsertPacket(this=0x0000000107b49750, rtp_header=0x0000700003599ea0, payload=ArrayView&lt;const unsigned char, -4711L&gt; @ 0x0000700003598d48) at neteq_impl.cc:170:7
    frame #4: webrtc::acm2::AcmReceiver::InsertPacket(this=0x000000010f067210, rtp_header=0x0000700003599ea0, incoming_payload=ArrayView&lt;const unsigned char, -4711L&gt; @ 0x00007000035993f8) at acm_receiver.cc:136:15
    frame #5: webrtc::voe::(anonymous namespace)::ChannelReceive::OnReceivedPayloadData(this=0x000000010f067000, payload=ArrayView&lt;const unsigned char, -4711L&gt; @ 0x0000700003599758, rtpHeader=0x0000700003599ea0) at channel_receive.cc:340:21
    frame #6: webrtc::voe::(anonymous namespace)::ChannelReceive::ReceivePacket(this=0x000000010f067000, packet="x90xbf", packet_length=99, header=0x0000700003599ea0) at channel_receive.cc:719:5
    frame #7: webrtc::voe::(anonymous namespace)::ChannelReceive::OnRtpPacket(this=0x000000010f067000, packet=0x000070000359a7a0) at channel_receive.cc:669:3
    frame #8: webrtc::RtpDemuxer::OnRtpPacket(this=0x00000001070122b0, packet=0x000070000359a7a0) at rtp_demuxer.cc:249:11
    frame #9: webrtc::RtpStreamReceiverController::OnRtpPacket(this=0x0000000107012238, packet=0x000070000359a7a0) at rtp_stream_receiver_controller.cc:52:19
    frame #10: webrtc::internal::Call::DeliverRtp(this=0x0000000107012000, media_type=AUDIO, packet=CopyOnWriteBuffer @ 0x000070000359aa18, packet_time_us=6039178656533) at call.cc:1606:36
    frame #11: webrtc::internal::Call::DeliverPacket(this=0x0000000107012000, media_type=AUDIO, packet=CopyOnWriteBuffer @ 0x000070000359b4a8, packet_time_us=6039178656533) at call.cc:1637:10
    frame #12: cricket::WebRtcVoiceMediaChannel::OnPacketReceived(this=0x0000000106f9cc48)::$_3::operator()() const at webrtc_voice_engine.cc:2227:28
</code></pre> 
<p>编码音频包网络传输延迟通过编码音频包的发送完成时间和接收开始时间来统计。为了能将发送的编码音频包和接收的编码音频包关联起来，需要获得编码音频包的 timestamp，timestamp 在编码音频包经过解密和解析之后访问起来比较方便。由上面的编码音频包接收过程可以看到，<code>cricket::BaseChannel::OnRtpPacket()</code> 无疑是做这种统计最合适的点。</p> 
<p>统计编码音频包的发送时间点的方法如上面那样，这里在 <code>cricket::BaseChannel::OnRtpPacket()</code> 加上统计编码音频包的接收时间的逻辑。整体的改动如下：</p> 
<p><img src="https://images2.imgbox.com/20/bc/okbCrMHn_o.jpg" alt="Audio network transport delay "></p> 
<p>笔者用 debug 版的二进制文件，在一台 Windows 10 笔记本电脑上跑简单的 WebRTC 一对一语音通话用例，发送端和接收端跑在同一台机器上，跑了大概 10 分钟，总共获得 27111 条数据。同样主要关注编码音频包网络传输延迟的数值分布，对这些数据的统计结果如下表（延迟数据单位为 ms）：</p> 
<table>
<thead><tr>
<th>Delay</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>(0-5]</td>
<td>132</td>
<td>0.004869</td>
</tr>
<tr>
<td>(5-10]</td>
<td>5408</td>
<td>0.199476</td>
</tr>
<tr>
<td>(10-15]</td>
<td>7362</td>
<td>0.271550</td>
</tr>
<tr>
<td>(15-20]</td>
<td>4644</td>
<td>0.171296</td>
</tr>
<tr>
<td>(20-35]</td>
<td>6650</td>
<td>0.245288</td>
</tr>
<tr>
<td>(35-60]</td>
<td>2476</td>
<td>0.091328</td>
</tr>
<tr>
<td>(60-100]</td>
<td>382</td>
<td>0.014090</td>
</tr>
<tr>
<td>(100-…]</td>
<td>57</td>
<td>0.002102</td>
</tr>
</tbody>
</table>
<p>在笔者的网络环境中，绝大部分的编码音频包网络端到端传输延迟在 5 ～ 60 ms。</p> 
<p><code>WebRtcVoiceMediaChannel::OnPacketReceived()</code> 将接收到的编码音频包转到 worker thread 上，这是让控制线程干了一些数据流的活。<em><strong>接收到的编码音频包的转线程及将编码音频包送进 NetEQ 的过程，一般来说对于音频端到端延迟不会造成可见的影响，因而这个过程的时间忽略不计。</strong></em></p> 
<h2>
<a id="_610"></a>解码等待延迟</h2> 
<p>解码过程是由 <code>webrtc::AudioDeviceModule</code> 内的播放线程驱动的。<code>webrtc::AudioDeviceModule</code> 内会起一个播放线程，它定时地通过回调从 NetEQ 拿解码后的音频数据。送进 NetEQ 的编码音频包不会立即被拿去解码，而是要等播放线程的回调请求时，才会真正的解码。</p> 
<p>在编码音频包被插入 NetEQ 的包队列，到编码音频包被拿来解码，通常需要经过一段时间。WebRTC 在 NetEQ 中会统计编码音频包从接收到解码经过的等待时间，并会计算编码音频包的平均等待时间。我们主要关注的还是这个等待时间的分布，及不同延迟值范围的分布和比例，而不仅仅是平均等待时间。我们可以在 NetEQ 中把各个编码音频包的等待解码时间吐出来。相关改动如下：</p> 
<p><img src="https://images2.imgbox.com/0e/cd/sPFM6d47_o.jpg" alt="Audio packet waiting time"></p> 
<p>笔者用 debug 版的二进制文件，在笔者的一台 Windows 10 笔记本电脑上跑简单的 WebRTC 一对一语音通话用例，跑了大概 10 分钟，在接收端总共获得 27123 条数据，各个编码音频包等待解码时间分布如下表（延迟数据单位为 ms）：</p> 
<table>
<thead><tr>
<th>Delay</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>10</td>
<td>214</td>
<td>0.007890</td>
</tr>
<tr>
<td>20</td>
<td>260</td>
<td>0.009586</td>
</tr>
<tr>
<td>30</td>
<td>816</td>
<td>0.030085</td>
</tr>
<tr>
<td>40</td>
<td>1796</td>
<td>0.066217</td>
</tr>
<tr>
<td>50</td>
<td>4149</td>
<td>0.152970</td>
</tr>
<tr>
<td>60</td>
<td>8162</td>
<td>0.300925</td>
</tr>
<tr>
<td>70</td>
<td>7775</td>
<td>0.286657</td>
</tr>
<tr>
<td>80</td>
<td>2409</td>
<td>0.088818</td>
</tr>
<tr>
<td>90</td>
<td>767</td>
<td>0.028279</td>
</tr>
<tr>
<td>100</td>
<td>298</td>
<td>0.010987</td>
</tr>
<tr>
<td>110</td>
<td>186</td>
<td>0.006858</td>
</tr>
<tr>
<td>120</td>
<td>63</td>
<td>0.002323</td>
</tr>
<tr>
<td>130</td>
<td>73</td>
<td>0.002691</td>
</tr>
<tr>
<td>140</td>
<td>32</td>
<td>0.001180</td>
</tr>
<tr>
<td>150</td>
<td>22</td>
<td>0.000811</td>
</tr>
<tr>
<td>160</td>
<td>22</td>
<td>0.000811</td>
</tr>
<tr>
<td>170</td>
<td>17</td>
<td>0.000627</td>
</tr>
<tr>
<td>180</td>
<td>7</td>
<td>0.000258</td>
</tr>
<tr>
<td>190</td>
<td>3</td>
<td>0.000111</td>
</tr>
<tr>
<td>200</td>
<td>4</td>
<td>0.000147</td>
</tr>
<tr>
<td>210</td>
<td>8</td>
<td>0.000295</td>
</tr>
<tr>
<td>220</td>
<td>12</td>
<td>0.000442</td>
</tr>
<tr>
<td>230</td>
<td>4</td>
<td>0.000147</td>
</tr>
<tr>
<td>240</td>
<td>7</td>
<td>0.000258</td>
</tr>
<tr>
<td>250</td>
<td>5</td>
<td>0.000184</td>
</tr>
<tr>
<td>260</td>
<td>2</td>
<td>0.000074</td>
</tr>
<tr>
<td>270</td>
<td>3</td>
<td>0.000111</td>
</tr>
<tr>
<td>280</td>
<td>4</td>
<td>0.000147</td>
</tr>
<tr>
<td>290</td>
<td>2</td>
<td>0.000074</td>
</tr>
<tr>
<td>310</td>
<td>1</td>
<td>0.000037</td>
</tr>
</tbody>
</table>
<p>编码音频包等待解码时间大多在 20 ～ 100 ms，但也会有一些比较大的值，最大的甚至达到了 310 ms。</p> 
<h2>
<a id="_655"></a>音频解码及播放延迟</h2> 
<p>播放线程的回调调上来时，会完成一系列操作：</p> 
<ol>
<li>对所有的远端音频流做解码、PLC；</li>
<li>根据各个远端音频流的采样率计算获得一个音频采样率；</li>
<li>将所有的远端音频流重采样到计算获得的音频采样率；</li>
<li>统一各个远端音频流的通道数，选择音强最强的几个音频流，做混音；</li>
<li>将混音之后的音频数据重采样到音频设备支持的采样率和通道数；</li>
<li>将重采样之后的数据送进设备播放出来。</li>
</ol> 
<p>上面的整个过程可以认为是在 <code>AudioTransportImpl::NeedMorePlayData()</code> 中串起来的，其中的第 1 ~ 4 步可以认为是藉由 AudioMixer 串起来的：</p> 
<pre><code>// Mix all received streams, feed the result to the AudioProcessing module, then
// resample the result to the requested output rate.
int32_t AudioTransportImpl::NeedMorePlayData(const size_t nSamples,
                                             const size_t nBytesPerSample,
                                             const size_t nChannels,
                                             const uint32_t samplesPerSec,
                                             void* audioSamples,
                                             size_t&amp; nSamplesOut,
                                             int64_t* elapsed_time_ms,
                                             int64_t* ntp_time_ms) {
  RTC_DCHECK_EQ(sizeof(int16_t) * nChannels, nBytesPerSample);
  RTC_DCHECK_GE(nChannels, 1);
  RTC_DCHECK_LE(nChannels, 2);
  RTC_DCHECK_GE(
      samplesPerSec,
      static_cast&lt;uint32_t&gt;(AudioProcessing::NativeRate::kSampleRate8kHz));

  // 100 = 1 second / data duration (10 ms).
  RTC_DCHECK_EQ(nSamples * 100, samplesPerSec);
  RTC_DCHECK_LE(nBytesPerSample * nSamples * nChannels,
                AudioFrame::kMaxDataSizeBytes);

  mixer_-&gt;Mix(nChannels, &amp;mixed_frame_);
  *elapsed_time_ms = mixed_frame_.elapsed_time_ms_;
  *ntp_time_ms = mixed_frame_.ntp_time_ms_;

  if (audio_processing_) {
    const auto error =
        ProcessReverseAudioFrame(audio_processing_, &amp;mixed_frame_);
    RTC_DCHECK_EQ(error, AudioProcessing::kNoError);
  }

  nSamplesOut = Resample(mixed_frame_, samplesPerSec, &amp;render_resampler_,
                         static_cast&lt;int16_t*&gt;(audioSamples));
  RTC_DCHECK_EQ(nSamplesOut, nChannels * nSamples);
  return 0;
}
</code></pre> 
<p>音频播放延迟的来源，与我们前面在 <strong>音频采集延迟</strong> 中见到的类似，主要来源于两次播放数据请求的回调之间的时间间隔。音频播放延迟的统计方法也与音频采集延迟的统计方法类似，前面看到的音频采集延迟统计的相关改动中也包含了统计音频播放延迟相关的逻辑。</p> 
<p>这里我们增加统计 AudioMixer 中的各种操作的耗时，和混音之后的音频数据重采样的耗时的逻辑。相关的改动如：</p> 
<p><img src="https://images2.imgbox.com/68/af/w6Q2OkoY_o.png" alt="Decoding and resampling delay"></p> 
<p>笔者用 debug 版的二进制文件，在笔者的一台 Windows 10 笔记本电脑上跑简单的 WebRTC 一对一语音通话用例，跑了大概 10 分钟，在接收端总共获得 52956 条两次播放回调间时间间隔的数据，这些时间间隔的分布如下表（延迟数据单位为 ms）：</p> 
<table>
<thead><tr>
<th>Delay</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>0</td>
<td>5</td>
<td>0.000094</td>
</tr>
<tr>
<td>1</td>
<td>52</td>
<td>0.000982</td>
</tr>
<tr>
<td>2</td>
<td>43</td>
<td>0.000812</td>
</tr>
<tr>
<td>3</td>
<td>108</td>
<td>0.002039</td>
</tr>
<tr>
<td>4</td>
<td>182</td>
<td>0.003437</td>
</tr>
<tr>
<td>5</td>
<td>275</td>
<td>0.005193</td>
</tr>
<tr>
<td>6</td>
<td>291</td>
<td>0.005495</td>
</tr>
<tr>
<td>7</td>
<td>324</td>
<td>0.006118</td>
</tr>
<tr>
<td>8</td>
<td>1562</td>
<td>0.029496</td>
</tr>
<tr>
<td>9</td>
<td>12318</td>
<td>0.232608</td>
</tr>
<tr>
<td>10</td>
<td>22554</td>
<td>0.425901</td>
</tr>
<tr>
<td>11</td>
<td>12362</td>
<td>0.233439</td>
</tr>
<tr>
<td>12</td>
<td>1569</td>
<td>0.029628</td>
</tr>
<tr>
<td>13</td>
<td>365</td>
<td>0.006893</td>
</tr>
<tr>
<td>14</td>
<td>321</td>
<td>0.006062</td>
</tr>
<tr>
<td>15</td>
<td>277</td>
<td>0.005231</td>
</tr>
<tr>
<td>16</td>
<td>149</td>
<td>0.002814</td>
</tr>
<tr>
<td>17</td>
<td>89</td>
<td>0.001681</td>
</tr>
<tr>
<td>18</td>
<td>55</td>
<td>0.001039</td>
</tr>
<tr>
<td>19</td>
<td>35</td>
<td>0.000661</td>
</tr>
<tr>
<td>20</td>
<td>12</td>
<td>0.000227</td>
</tr>
<tr>
<td>21</td>
<td>3</td>
<td>0.000057</td>
</tr>
<tr>
<td>22</td>
<td>2</td>
<td>0.000038</td>
</tr>
<tr>
<td>23</td>
<td>1</td>
<td>0.000019</td>
</tr>
<tr>
<td>29</td>
<td>1</td>
<td>0.000019</td>
</tr>
<tr>
<td>35</td>
<td>1</td>
<td>0.000019</td>
</tr>
</tbody>
</table>
<p>从表中可以看到，绝大部分的时间间隔在 8 ~ 12 ms 之间。偶尔会有一些超过的，最高的在 35 ms。</p> 
<p>相同的测试获得 52957 条 AudioMixer 的一系列操作的耗时的数据，我们主要关注这些数据的数值分布，这些数据统计结果如下表（延迟数据单位为 us）：</p> 
<table>
<thead><tr>
<th>Delay</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>0</td>
<td>37538</td>
<td>0.708839</td>
</tr>
<tr>
<td>1000</td>
<td>14400</td>
<td>0.271919</td>
</tr>
<tr>
<td>2000</td>
<td>601</td>
<td>0.011349</td>
</tr>
<tr>
<td>3000</td>
<td>127</td>
<td>0.002398</td>
</tr>
<tr>
<td>4000</td>
<td>75</td>
<td>0.001416</td>
</tr>
<tr>
<td>5000</td>
<td>76</td>
<td>0.001435</td>
</tr>
<tr>
<td>6000</td>
<td>49</td>
<td>0.000925</td>
</tr>
<tr>
<td>7000</td>
<td>47</td>
<td>0.000888</td>
</tr>
<tr>
<td>8000</td>
<td>21</td>
<td>0.000397</td>
</tr>
<tr>
<td>9000</td>
<td>18</td>
<td>0.000340</td>
</tr>
<tr>
<td>10000</td>
<td>3</td>
<td>0.000057</td>
</tr>
<tr>
<td>11000</td>
<td>2</td>
<td>0.000038</td>
</tr>
</tbody>
</table>
<p>从表中可以看到，超过 98% 的耗时都在 2000 us，也就是 2 ms 以内。（受限于系统时间接口的精度，延迟时间的精度只能达到 ms 级）。</p> 
<p>然后是混音之后的音频数据的重采样耗时。相同测试中获得 52957 条数据记录，统计结果如下表（延迟数据单位为 us）：</p> 
<table>
<thead><tr>
<th>Delay</th>
<th>Item Count</th>
<th>The percentage</th>
</tr></thead>
<tbody>
<tr>
<td>0</td>
<td>52889</td>
<td>0.998716</td>
</tr>
<tr>
<td>1000</td>
<td>68</td>
<td>0.001284</td>
</tr>
</tbody>
</table>
<p>尽管重采样属于语音通话中音频端到端数据处理过程中比较消耗资源的操作，但耗时基本上都没有超过 1 ms。</p> 
<p>从上面获得的 WebRTC 一对一语音通话中音频端到端分段延迟的数据可以看到，对于端到端延迟影响比较大的过程包括：音频采集，编码音频包在网络中的传输，音频包在缓冲区中等待被解码处理，音频播放等。但其它过程，如编码音频包的发送，偶尔会出现一些比较大的异常延迟值。音频编码成为音频端到端延迟瓶颈的可能性在于，编码音频帧时长的配置，音频编码操作本身耗时有限。</p> 
<p>尽管笔者这里的分析都基于 debug 版的二进制文件进行，这可能会造成一些额外的资源消耗，但统计数据的数量级上是不会有什么差别的。</p> 
<p>WebRTC 一对一语音通话中音频端到端分段延迟大体如上。</p> 
<p>这里的分析的代码基于 WebRTC M98 版的代码进行（<strong><a href="https://github.com/hanpfei/OpenRTCClient">OpenRTCClient</a></strong>），文中所用的数据分析脚本，可以参考 <a href="https://github.com/hanpfei/OpenRTCClient/blob/m98_4758/build_system/e2e_delay_analysis.py">e2e_delay_analysis.py</a></p> 
<p>Done。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>