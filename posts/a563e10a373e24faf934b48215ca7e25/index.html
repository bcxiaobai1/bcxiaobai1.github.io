<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>绝了！分割mask生成动漫人脸！爆肝数周，从零搭建 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">绝了！分割mask生成动漫人脸！爆肝数周，从零搭建</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <pre class="has"><code class="language-go">En点击下方“AI算法与图像处理”，一起进步！

重磅干货，第一时间送达</code></pre>
 <p style="text-align:left">大家好，我是 阿潘～</p>
 <p style="text-align:left">很多小伙伴期待已久的实战项目来了，今天分享一个国外论坛medium大佬的文章，从 0 做项目的整个过程，具有很大的参考价值，大家感兴趣的可以试着参考这个思路去实现，比起直接跑别人现有的完整，一定能更有收获和成就感。</p>
 <p style="text-align:left"><strong>如果文章对你有帮助，记得“在看+点赞+分享”！</strong><br></p>
 <p style="text-align:left">主要流程包括：</p>
 <p style="text-align:left">1、确定目标（分割mask ---&gt; 动漫人脸）</p>
 <p style="text-align:left">2、确定技术路线（语义分割 + 语义合成）</p>
 <p style="text-align:left">3、实现（数据集标注 + 模型调优 + 界面编写）<br></p>
 <blockquote>
  <p>PS：原作者并没有开源数据集和代码， 不过给了所有参考资料的源码和数据集链接！复现应该没有问题</p>
 </blockquote>
 <p style="text-align:center"><img src="https://images2.imgbox.com/fa/9b/1jFt8NXO_o.png" alt="ef046d288971abbb10730d18ab5b9101.png"><br></p>
 <h2> 目标</h2>
 <p>该项目的目标是建立一个深度学习模型，从分割mask生成动漫人脸肖像。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/37/33/Q9NKPRTV_o.png" alt="238cffa3b23dbc0cfde4fb637bb4a006.png"></p>
 <p style="text-align:center">segmentation mask to anime face portrait</p>
 <p>在这个项目中，首先手动标注一小批图像。然后使用数据增强和 U-Net 模型来乘以分割mask的数量来构建数据集。最后，训练一个 GauGAN 模型，用于从分割mask中合成动漫人脸。</p>
 <h2>1. 语义分割</h2>
 <p>语义分割是为图像中的每个像素分配标签（也称为类 id）的过程。它的结果是一个分割mask，它是一个大小为高度 * 宽度的数组，每个像素都包含一个类 ID。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/c6/6a/TEcLt2JZ_o.png" alt="b641aef272506cb4fd1f2db1355b561d.png"></p>
 <p style="text-align:center">class id: 0 = background, 5 = hair</p>
 <h2>1.1 Dataset</h2>
 <p>在进入图像生成任务之前，我们需要一个分割mask数据集，用于训练生成模型将mask转换为图像。</p>
 <p>不幸的是，我在互联网上找不到任何动漫人脸分割数据集。尽管如此，Danbooru2019-Portraits 上有一个动漫肖像（512 x 512px）数据集。所以我决定从 Danbooru 肖像中标注的分割mask。</p>
 <blockquote>
  <p>数据集链接：https://www.gwern.net/Crops#danbooru2019-portraits</p>
 </blockquote>
 <h2>1.2 Annotation</h2>
 <p>要标注图像，我们必须确定类。最初的想法是列出 15 个类：<br></p>
 <pre class="has"><code class="language-go">background, body, ear, face, eyeball, pupil, eyelash, nose, mouth, hair, hair_accessory, eyebrow, glasses, clothes, hand</code></pre>
 <p>后来为了简单起见，将其缩减为 7 个类，最终的类列表如下：</p>
 <pre class="has"><code class="language-go">background, skin, face, eye, mouth, hair, clothes</code></pre>
 <p>有许多不同的注释工具，这里使用的是 labelme。</p>
 <pre class="has"><code class="language-javascript">https://github.com/wkentaro/labelme</code></pre>
 <p style="text-align:center"><img src="https://images2.imgbox.com/09/a7/cEsuWOOI_o.png" alt="803536a8ac4dee684b53fe8299c8f13f.png"></p>
 <p style="text-align:center">labelme GUI</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/3f/4d/eOnjvR53_o.gif" alt="f72e19b3adc5dffac670fbbd089aebea.gif"></p>
 <p>在这项乏味的工作上辛勤工作数周后，设法标注了 200 张图像<br></p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/cc/88/tWBs23RA_o.png" alt="64dc63493ea526f181f386190bc6d5ae.png"></p>
 <p style="text-align:center">examples of annotated masks<br></p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/1a/2f/wa8wWuDf_o.png" alt="fc6fd86c5db567f41d27db4efaa2da09.png"></p>
 <p style="text-align:center">left: original image, middle: segmentation mask, right: visualization of the annotation</p>
 <h2>1.3 Data Augmentation</h2>
 <p>当然，200 张带注释的图像不足以让我们训练我们的网络。我们需要使用数据增强技术来增加数据集的大小。</p>
 <p>通过随机旋转、镜像和扭曲图像，我从这 200 个样本中生成了 3000 多个数据。换句话说，现在我有 3200 个数据。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/32/f5/0OOhZAnz_o.png" alt="cda2600a58660012f9d4ba9f2122ecef.png"></p>
 <p style="text-align:center">examples of augmented masks</p>
 <p>然而，这些数据在内容和风格方面高度重复，因为它们仅从 200 个样本中扩充而来。为了训练网络将分割掩码转换为高质量和多样化的动漫面孔，我们需要的不仅仅是 200 + 3000 个数据点。因此，我将首先使用这些数据来训练一个 U-Net 模型来学习从动漫人脸到分割掩码的翻译。然后我会将整个 Danbooru 肖像数据集输入到经过训练的 U-Net 模型中，以生成更多不同人脸的分割掩码。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/d7/53/mBOIsusl_o.png" alt="30d51df4cc8769c28a2ca885fec731ac.png"></p>
 <p style="text-align:center">anime face portraits to segmentation mask</p>
 <h2>1.4 U-Net</h2>
 <p>U-Net 最初是为了分割医学图像进行诊断而引入的。它通过使用跳跃连接来解决传统 FCN（全卷积网络）中发生的信息丢失问题，在精确分割方面做得非常好。</p>
 <p>U-Net 的架构与 Autoencoder 相似，但从下采样端到上采样端有额外的连接层。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/d4/49/KYAwq4q2_o.png" alt="9db30f884c918ad4429fb4d3bb0be81e.png"></p>
 <p style="text-align:center">source: https://arxiv.org/abs/1505.04597</p>
 <p>在下采样部分，我使用预训练的 MobileNetV2 从输入图像中提取特征。在上采样部分，我使用了由 Conv2DTranspose、Batchnorm 和 ReLU 层组成的块。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/19/87/1sZwtPi8_o.png" alt="57e1d6ac191461a78a6b0183a410b45f.png"></p>
 <p style="text-align:center">U-Net v1, v2 architecture<br></p>
 <p>在我的 U-Net 版本 1 中，输入和输出大小为 128 x 128px。经过训练的模型确实学习了从动漫人脸到分割mask的非常好的映射。但由于我想在我后来的合成模型中拥有 512 x 512px 的输入和输出，我将 U-Net 输出的大小调整为 512 x 512px 并进行插值。然而，结果看起来是像素化的，它未能捕捉到出现在小区域（例如嘴巴）中的某些类别。</p>
 <p>在版本 2 中，我只是将输入和输出大小更改为 512 x 512px（我一开始并没有这样做，因为我不希望输出嘈杂并在图像中令人困惑的区域中填充随机点，例如 衣服）。正如我所料，v2 的输出很嘈杂。不过，它们看起来比 v1 更好。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/eb/a5/kwx2DuxX_o.png" alt="be7d36bc6dc59fd36bdfe411d1c05777.png"></p>
 <p style="text-align:center">U-Net v3 architecture</p>
 <p>在版本 3 中，我尝试通过用 UpSampling2D 层替换 Conv2DTranspose 层来减轻噪音和棋盘伪影。现在的结果比 v2 的要好得多。噪音更少，棋盘伪影更少。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/a3/7c/KSSLwT0Q_o.png" alt="aec9539894dd19935236ff5ec5c1bf7b.png"></p>
 <p style="text-align:center">checkerboard artifacts of v2</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/6c/a8/jskRDBhJ_o.png" alt="69aba26cf5779ce5157a977887b99424.png"></p>
 <p style="text-align:center">U-Net segmentation results</p>
 <p>最后，我将整个 Danbooru 数据集输入 U-Net v3 以构建我的分割掩码数据集。</p>
 <h2> 2. <strong>图像语义合成</strong>
</h2>
 <p>现在，我们有了分割蒙版数据集，是时候深入研究主要任务——图像语义合成，正如之前所说，这不过是从分割mask到真实图像的转换的一个花哨的名称。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/3f/27/t1vnejyg_o.png" alt="bbfeb380055ce0b2e7fb18d8dd81e75b.png"></p>
 <p style="text-align:center">Semantic Image Synthesis: segmentation mask to anime face portrait<br></p>
 <h2>2.1 GauGAN</h2>
 <p style="text-align:center"><img src="https://images2.imgbox.com/c0/0c/39gAMjbw_o.gif" alt="eecd704a645ca6a1445cba2df8ed32c6.gif"></p>
 <p style="text-align:center">source: https://github.com/NVlabs/SPADE</p>
 <p>GauGAN 由 Nvidia 开发，用于从分割mask合成逼真的图像。在他们的展示网站上，他们展示了 GauGAN 如何出色地通过几笔画来生成逼真的风景图像。</p>
 <blockquote>
  <p>demo链接：https://www.nvidia.com/en-us/research/ai-playground/</p>
 </blockquote>
 <p style="text-align:center"><img src="https://images2.imgbox.com/50/83/QaXe9W12_o.png" alt="7df5c9d58620aafbbe6b22ad078e593c.png"></p>
 <p style="text-align:center">GauGAN architecture</p>
 <p>上图展示了 GauGAN 模型的架构。绿色块完全代表发电机。鉴别器是一个 PatchGAN。<br></p>
 <h2>2.2 SPADE</h2>
 <p style="text-align:center"><img src="https://images2.imgbox.com/4f/fa/ymJnQN0R_o.png" alt="7945cf1501f7bb101500571bda5079da.png"></p>
 <p style="text-align:center">source: https://nvlabs.github.io/SPADE/</p>
 <p><strong>GauGAN 的核心是 SPADE（Spatially-Adaptive Denormalization）模块，它是从 Batch Norm 修改而来的归一化层。它旨在克服 pix2pixHD 中的挑战：在具有统一类 ID 的大区域丢失语义信息。</strong></p>
 <p>这是通过将 Conv 层引入<strong>Batch Norm</strong>来解决的，这样它具有不同的参数集（β，γ），这些参数<strong>以分割mask为条件，并且会随着不同的区域而变化</strong>。这意味着 SPADE 允许生成器在统一标签区域中学习更多细节。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/0f/59/QAr7jzBT_o.png" alt="87c06bf3a9165d1339a35d60539e9dfb.png"></p>
 <p>因此，在我们的问题中，生成的图像可能如下所示：</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/29/a1/5D02WoJc_o.png" alt="bb6a0780575e234c2932e18c94947cb7.png"></p>
 <h2>2.3 Pretrained Encoder</h2>
 <p>encoder 实际上是可选的，因为可以直接从高斯分布中采样 z（潜在向量）而无需任何输入（就像 vanilla GAN）。这里使用了encoder ，因为我想用参考图像对生成的图像进行样式设置。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/eb/fd/grfrAf5j_o.png" alt="232c924d25a37e187dc2c4026c10b9cb.png"></p>
 <p style="text-align:center">VAE architecture</p>
 <p>由于与encoder一起训练 GauGAN 是不稳定的，需要更多的时间和资源，所以我提前使用 VAE 训练了我的编码器，然后在 GauGAN 模型的训练过程中使用预训练的encoder对 z 进行采样。</p>
 <h2>2.4 Results</h2>
 <p>以下是从不同的分割mask和参考图像生成的图像的结果。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/00/49/xFxxui4U_o.png" alt="991c0bf201aa2f6f7f417c00de790d05.png"></p>
 <p style="text-align:center">semantic image synthesis results</p>
 <h2>2.5 Latent Attribute Vectors</h2>
 <p>除了使用参考图像来控制输出图像的风格外，我们还可以直接操纵潜在向量 z 来做到这一点。为此，我们首先需要找出潜在空间中的属性向量。</p>
 <p>动漫角色面部最重要的属性之一是头发颜色。但是，由于数据集没有带有头发颜色的标签，我必须自己使用 i2v 来标记它们，i2v 是一个用于估计插图标签的库。然后，我们可以通过使用 t-SNE 将样本图像的潜在向量投影到 2D 空间来可视化潜在空间以及估计的标签。</p>
 <p style="text-align:center"><img src="https://images2.imgbox.com/e8/7a/7t9STo8o_o.png" alt="b29a26366def556335d50d6836cb385c.png"></p>
 <p style="text-align:center">t-SNE of 4000 samples (estimated hair colors are indicated by image border colors)</p>
 <p>最后，通过计算不同标签的潜在向量之间的距离和方向，我们可以得到属性向量。下面的动画演示了使用提取的属性向量在头发颜色之间进行的转换。</p>
 <h2> <strong>3. GUI</strong>
</h2>
 <p>使用 python tkinter 库创建了一个 GUI，用于编辑生成的图像和分割mask。以下是演示视频：<br></p>
 <h2> <strong>4. 总结</strong>
</h2>
 <p>这个项目还有改进的空间，尤其是语义分割模型（U-Net）和语义图像合成模型（GauGAN）。以下是未来要做的事情的清单：</p>
 <ul>
<li><p>寻找更好的模型架构以从原始图像中获得更准确的分割掩码</p></li>
<li><p>改进 GauGAN 模型以消除头发区域出现的噪声</p></li>
<li><p>训练生成模型以生成随机分割mask</p></li>
</ul>
 <h2> <strong>参考资料</strong>
</h2>
 <p>[1] D. Gwern Branwen, “Anime Crop Datasets: Faces, Figures, &amp; Hands”, Gwern.net, 2022. https://www.gwern.net/Crops#danbooru2019-portraits</p>
 <p>[2] “ wkentaro/labelme: Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).”, GitHub, 2022. https://github.com/wkentaro/labelme</p>
 <p>[3] O. Ronneberger, P. Fischer and T. Brox, “U-Net: Convolutional Networks for Biomedical Image Segmentation”, arXiv.org, 2022. https://arxiv.org/abs/1505.04597</p>
 <p>[4] Odena, et al., “Deconvolution and Checkerboard Artifacts”, Distill, 2016. http://doi.org/10.23915/distill.00003</p>
 <p>[5] “The NVIDIA AI Playground”, NVIDIA, 2022. https://www.nvidia.com/en-us/research/ai-playground/</p>
 <p>[6] “NVlabs/SPADE: Semantic Image Synthesis with SPADE”, GitHub, 2022. https://github.com/NVlabs/SPADE</p>
 <p>[7] “Semantic Image Synthesis with Spatially-Adaptive Normalization”, Nvlabs.github.io, 2022. https://nvlabs.github.io/SPADE/</p>
 <p>[8] “rezoo/illustration2vec: A simple deep learning library for estimating a set of tags and extracting semantic feature vectors from given illustrations.”, GitHub, 2022. https://github.com/rezoo/illustration2vec</p>
 <h2> <strong>推荐阅读</strong>
</h2>
 <p><a href="">科研人必备新神器，ReadPaper！爱了真好用！</a></p>
 <p><a href="">CVPR2021 最具创造力的那些工作成果！或许这就是计算机视觉的魅力！</a></p>
 <p><a href="">英伟达又一个GAN！PoE-GAN，AI绘图细节拉满，看完直接沸腾了！</a><br></p>
 <p><strong>如果文章对你有帮助，记得“在看+点赞+分享”！</strong></p> 
</div>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>