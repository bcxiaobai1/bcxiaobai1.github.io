<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>集成学习-模型融合（Lenet，Alexnet，Vgg）三个模型进行融合-附源代码-宇宙的尽头一定是融合模型而不是单个模型。 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">集成学习-模型融合（Lenet，Alexnet，Vgg）三个模型进行融合-附源代码-宇宙的尽头一定是融合模型而不是单个模型。</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p><strong>       机器学习中，有一门很有意思的提升模型accuracy的trick叫做集成学习，初次接触集成学习的时候我感觉这个方法很类似我们人类的团队，类似与我们在解决一个问题的时候需要团队不同的人各司其职一起解决，因为一个人再优秀能力也是有限的，往往一个优秀的算法只能解决我们这个问题的一方面，但是我们的问题一般都有多个方面都需要解决。就像我们做深度学习，可能我的数据集有好多种，但是lenet , vgg , alexnet对于不同的种类识别效果是不一样的，很多时候存在一种模型只对单一的几种识别效果好。</strong></p> 
<p><strong>       这个时候一味的去优化模型你会发现模型的指标提不上去，因为你的模型往往只会对几种数据比较敏感，特别当我们数据种类比较多的时候。 这个时候我们就需要用到集成学习，对在不同种类上表现好的几个模型进行融合。</strong></p> 
<p><strong>       事实上在kaggle竞赛和天池竞赛上融合模型的情况已经非常常见，在竞赛快结束提交模型的时候，排名前几的大佬一般都会去寻找同样排名靠前的人进行模型融合，一般来说模型融合的结果一定是优于单个模型的，这就是大家常说的    <span style="color:#fe2c24">宇宙的尽头一定是融合模型而不是单个模型。</span></strong></p> 
<p><strong>这期博客我们就来介绍融合模型的第一种方法，投票法。</strong></p> 
<p><img alt="" height="306" src="https://images2.imgbox.com/94/6e/U96AxTxb_o.png" width="916"></p> 
<p><strong>对于三个模型的预测结果进行投票，比如说三个模型 有两个预测的是第一种，一个预测的是第二种，那我们就取最多的，类似与我们常见的少数服从多数的思想。如果三个模型预测的结果是三种完全不一样的那我们就选取第一个（一般设置为表现最好的那个模型）为我们的预测结果。这就是本期融合模型的核心思想。</strong></p> 
<p><strong>接下来我们简单讲解一下代码：</strong></p> 
<p><strong>首先导入我们需要的库</strong></p> 
<pre><code>import torch
import torchvision
import torchvision.models
import numpy as np
from collections import Counter
from matplotlib import pyplot as plt
from tqdm import tqdm
from torch import nn
from torch.utils.data import DataLoader
from torchvision.transforms import transforms
</code></pre> 
<p><strong>图像预处理操作：</strong></p> 
<p><strong>具体函数意思可以可行百度，我这里是缩放成120*120进行处理</strong></p> 
<pre><code>data_transform = {
    "train": transforms.Compose([transforms.RandomResizedCrop(120),
                                 transforms.RandomHorizontalFlip(),
                                 transforms.ToTensor(),
                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),
    "val": transforms.Compose([transforms.Resize((120, 120)),  # 我这里是缩放成120*120进行处理的，您可以缩放成你需要的比例，但是不建议修改，因为会影响全连接成的输出
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}</code></pre> 
<p><strong>准备数据集，设置bach_size等参数：</strong></p> 
<p><strong>自己的数据放在跟代码相同的文件夹下新建一个data文件夹，data文件夹里的新建一个train文件夹用于放置训练集的图片。同理新建一个val文件夹用于放置测试集的图片。</strong></p> 
<pre><code>def main():
    train_data = torchvision.datasets.ImageFolder(root = "./data/train" ,   transform = data_transform["train"])



    traindata = DataLoader(dataset= train_data , batch_size= 32 , shuffle= True , num_workers=0 )


    test_data = torchvision.datasets.ImageFolder(root = "./data/val" , transform = data_transform["val"])

    train_size = len(train_data)  #求出训练集的长度
    test_size = len(test_data)   #求出测试集的长度
    print(train_size)
    print(test_size)
    testdata = DataLoader(dataset = test_data , batch_size= 32 , shuffle= True , num_workers=0 )</code></pre> 
<p><strong>是否使用GPU：</strong></p> 
<p><strong>有GPU则调用GPU，没有的话就调用CPU</strong></p> 
<pre><code>
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  #如果有GPU就使用GPU，否则使用CPU
    print("using {} device.".format(device))</code></pre> 
<p><strong>构建我们需要的三个模型：</strong></p> 
<p><strong>当然这里这三个模型种类可以任意修改，也可以拓展更多个模型，代码可修改。</strong></p> 
<p><strong>首先是构建的Alexnet网络：</strong></p> 
<pre><code>   class alexnet(nn.Module):      #alexnet神经网络  ，因为我的数据集是7种，因此如果你替换成自己的数据集，需要将这里的种类改成自己的
        def __init__(self):
            super(alexnet , self).__init__()
            self.model = nn.Sequential(

                nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 120, 120]  output[48, 55, 55]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]
                nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]
                nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
                nn.ReLU(inplace=True),
                nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
                nn.ReLU(inplace=True),
                nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]
                nn.Flatten(),
                nn.Dropout(p=0.5),
                nn.Linear(512, 2048),
                nn.ReLU(inplace=True),
                nn.Dropout(p=0.5),
                nn.Linear(2048, 1024),
                nn.ReLU(inplace=True),
                nn.Linear(1024, 7), #这里的7要修改成自己数据集的种类

            )
        def forward(self , x):
            x = self.model(x)
            return x
</code></pre> 
<p><strong>其次是Lenet网络：</strong></p> 
<pre><code>    class lenet(nn.Module): #Lenet神经网络
        def __init__(self):
            super(lenet , self).__init__()
            self.model = nn.Sequential(

                nn.Conv2d(3, 16,  kernel_size=5),  # input[3, 120, 120]  output[48, 55, 55]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),  # output[48, 27, 27]

                nn.Conv2d(16, 32, kernel_size=5),  # output[128, 27, 27]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),  # output[128, 13, 13]
                nn.Flatten(),
                nn.Linear(23328, 2048),
                nn.Linear(2048, 2048),
                nn.Linear(2048, 7), #这里的7要修改成自己数据集的种类

            )
        def forward(self , x):
            x = self.model(x)
            return x
</code></pre> 
<p><strong>再者是Vgg网络：</strong></p> 
<pre><code>   class VGG(nn.Module):           #VGG神经网络
        def __init__(self, features, num_classes=7, init_weights=False):  #这里的7要修改成自己数据集的种类
            super(VGG, self).__init__()
            self.features = features
            self.classifier = nn.Sequential(
                nn.Linear(4608, 4096),
                nn.ReLU(True),
                nn.Dropout(p=0.5),
                nn.Linear(4096, 4096),
                nn.ReLU(True),
                nn.Dropout(p=0.5),
                nn.Linear(4096, num_classes)
            )
            if init_weights:
                self._initialize_weights()  # 参数初始化

        def forward(self, x):
            # N x 3 x 224 x 224
            x = self.features(x)
            # N x 512 x 7 x 7
            x = torch.flatten(x, start_dim=1)
            # N x 512*7*7
            x = self.classifier(x)
            return x

        def _initialize_weights(self):
            for m in self.modules():  # 遍历各个层进行参数初始化
                if isinstance(m, nn.Conv2d):  # 如果是卷积层的话 进行下方初始化
                    # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    nn.init.xavier_uniform_(m.weight)  # 正态分布初始化
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)  # 如果偏置不是0 将偏置置成0  相当于对偏置进行初始化
                elif isinstance(m, nn.Linear):  # 如果是全连接层
                    nn.init.xavier_uniform_(m.weight)  # 也进行正态分布初始化
                    # nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)  # 将所有偏执置为0

    def make_features(cfg: list):
        layers = []
        in_channels = 3
        for v in cfg:
            if v == "M":
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
                layers += [conv2d, nn.ReLU(True)]
                in_channels = v
        return nn.Sequential(*layers)

    cfgs = {
        'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
        'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
        'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
        'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512,
                  'M'],
    }

    def vgg(model_name="vgg16", **kwargs):
        assert model_name in cfgs, "Warning: model number {} not in cfgs dict!".format(model_name)
        cfg = cfgs[model_name]

        model = VGG(make_features(cfg), **kwargs)
        return model
</code></pre> 
<p><strong>如果想了解代码，拓展的代码的注意看代码的损失和准确率的计算方式：</strong></p> 
<p><strong>先建立空数组，存储三个模型进行训练，损失和准确率也分别建立数组</strong></p> 
<p><strong>设置训练需要的参数，epoch，学习率learning 优化器。损失函数。</strong></p> 
<pre><code>
    mlps = [lenet1.to(device), alexnet1.to(device), VGGnet.to(device)] #建立一个数组，将三个模型放入

    epoch  = 5 #训练轮数
    LR = 0.0001  #学习率，我这里对于三个模型设置的是一样的学习率，事实上根据模型的不同设置成不一样的效果最好
    a = [{"params": mlp.parameters()} for mlp in mlps]  #依次读取三个模型的权重
    optimizer = torch.optim.Adam(a, lr=LR) #建立优化器
    loss_function = nn.CrossEntropyLoss()  #构建损失函数

    train_loss_all = [[] , [] , []]
    train_accur_all = [[], [], []]
    ronghe_train_loss = []  #融合模型训练集的损失
    ronghe_train_accuracy = []  #融合模型训练集的准确率



    test_loss_all = [[] , [] , []]
    test_accur_all = [[] , [] , []]

    ronghe_test_loss = []  #融合模型测试集的损失
    ronghe_test_accuracy = [] #融合模型测试集的准确
</code></pre> 
<p><strong>开始训练：</strong></p> 
<p><strong>详细注释，主要是在于如何存储和计算最多的值（即我们上述的投票）</strong></p> 
<pre><code>   for i in range(epoch): #遍历开始进行训练
        train_loss = [0 , 0 , 0]  #因为三个模型，初始化三个0存放模型的结果

        train_accuracy = [0.0 , 0.0 , 0.0]  #同上初始化三个0，存放模型的准确率
        for mlp in range(len(mlps)):
            mlps[mlp].train()    #遍历三个模型进行训练
        train_bar = tqdm(traindata)  #构建进度条，训练的时候有个进度条显示

        pre1 = [] #融合模型的损失
        vote1_correct = 0 #融合模型的准确率
        for step , data in enumerate(train_bar):  #遍历训练集


            img , target = data

            length = img.size(0)


            img, target = img.to(device), target.to(device)
            optimizer.zero_grad()
            for mlp in range(len(mlps)):  #对三个模型依次进行训练
                mlps[mlp].train()
                outputs = mlps[mlp](img)
                loss1 = loss_function(outputs, target)  # 求损失
                outputs = torch.argmax(outputs, 1)

                loss1.backward()#反向传播
                train_loss[mlp] += abs(loss1.item()) * img.size(0)
                accuracy = torch.sum(outputs == target)

                pre_num1 = outputs.cpu().numpy()
                # print(pre_num1.shape)
                train_accuracy[mlp] = train_accuracy[mlp] + accuracy

                pre1.append(pre_num1)

            arr1 = np.array(pre1)


            pre1.clear()  # 将pre进行清空

            result1 = [Counter(arr1[:, i]).most_common(1)[0][0] for i in range(length)]  # 对于每张图片，统计三个模型其中，预测的那种情况最多，就取最多的值为融合模型预测的结果，即为投票
            #投票的意思，因为是三个模型，取结果最多的
            vote1_correct += (result1 == target.cpu().numpy()).sum()

            optimizer.step()  # 更新梯度

        losshe= 0
        for mlp in range(len(mlps)):
            print("epoch："+ str(i+1) , "模型" + str(mlp) + "的损失和准确率为：", "train-Loss：{} , train-accuracy：{}".format(train_loss[mlp]/train_size , train_accuracy[mlp]/train_size))
            train_loss_all[mlp].append(train_loss[mlp]/train_size)
            train_accur_all[mlp].append(train_accuracy[mlp].double().item()/train_size)
            losshe +=  train_loss[mlp]/train_size
        losshe /= 3
        print("epoch: " + str(i+1) + "集成模型训练集的正确率" + str(vote1_correct/train_size))
        print("epoch: " + str(i+1) + "集成模型训练集的损失" + str(losshe))
        ronghe_train_loss.append(losshe)
        ronghe_train_accuracy.append(vote1_correct/train_size)
</code></pre> 
<p><strong>开始测试：</strong></p> 
<p><strong>需要将三个模型都设置为测试状态，然后进行测试</strong></p> 
<pre><code>
        test_loss = [0 , 0 , 0]
        test_accuracy = [0.0 , 0.0 , 0.0]

        for mlp in range(len(mlps)):
            mlps[mlp].eval()
        with torch.no_grad():
            pre = []
            vote_correct = 0
            test_bar = tqdm(testdata)
            vote_correct = 0
            for data in test_bar:

                length1 = 0
                img, target = data
                length1 = img.size(0)

                img, target = img.to(device), target.to(device)

                for mlp in range(len(mlps)):
                    outputs = mlps[mlp](img)

                    loss2 = loss_function(outputs, target)
                    outputs = torch.argmax(outputs, 1)


                    test_loss[mlp] += abs(loss2.item())*img.size(0)

                    accuracy = torch.sum(outputs == target)
                    pre_num = outputs.cpu().numpy()

                    test_accuracy[mlp] += accuracy

                    pre.append(pre_num)
                arr = np.array(pre)
                pre.clear()  # 将pre进行清空
                result = [Counter(arr[:, i]).most_common(1)[0][0] for i in range(length1)]  # 对于每张图片，统计三个模型其中，预测的那种情况最多，就取最多的值为融合模型预测的结果，
                vote_correct += (result == target.cpu().numpy()).sum()
        losshe1 = 0
        for mlp in range(len(mlps)):
            print("epoch："+ str(i+1), "模型" + str(mlp) + "的损失和准确率为：", "test-Loss：{} , test-accuracy：{}".format(test_loss[mlp] / test_size , test_accuracy[mlp] / test_size ))
            test_loss_all[mlp].append(test_loss[mlp]/test_size)
            test_accur_all[mlp].append(test_accuracy[mlp].double().item()/test_size )
            losshe1 += test_loss[mlp]/test_size
        losshe1 /= 3
        print("epoch: " + str(i+1) + "集成模型测试集的正确率" + str(vote_correct / test_size ))
        print("epoch: " + str(i+1) + "集成模型测试集的损失" + str(losshe1))
        ronghe_test_loss.append(losshe1)
        ronghe_test_accuracy.append(vote_correct/ test_size)
</code></pre> 
<p><strong>最后绘制曲线，我们只绘制了融合模型的曲线，单独三个模型的曲线，可以分别跑三种代码进行训练绘制。且保存了三种模型，预测的时候同时读取三个模型进行预测。</strong></p> 
<p><strong>绘制训练集loss和accuracy图 和测试集的loss和accuracy图。</strong></p> 
<pre><code>    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    # for mlp in range(len(mlps)):
    plt.plot(range(epoch) , ronghe_train_loss,
                 "ro-",label = "Train loss")
    plt.plot(range(epoch), ronghe_test_loss,
                 "bs-",label = "test loss")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("Loss")
    plt.subplot(1, 2, 2)
    plt.plot(range(epoch) , ronghe_train_accuracy,
                 "ro-",label = "Train accur")
    plt.plot(range(epoch) , ronghe_test_accuracy,
                 "bs-",label = "test accur")
    plt.xlabel("epoch")
    plt.ylabel("acc")
    plt.legend()
    plt.show()

    torch.save(alexnet1.state_dict(), "alexnet.pth")
    torch.save(lenet1.state_dict(), "lenet1.pth")
    torch.save(VGGnet.state_dict(), "VGGnet.pth")

    print("模型已保存")</code></pre> 
<p><strong>训练代码：</strong></p> 
<pre><code>import torch
import torchvision
import torchvision.models
import numpy as np
from collections import Counter
from matplotlib import pyplot as plt
from tqdm import tqdm
from torch import nn
from torch.utils.data import DataLoader
from torchvision.transforms import transforms


data_transform = {
    "train": transforms.Compose([transforms.RandomResizedCrop(120),
                                 transforms.RandomHorizontalFlip(),
                                 transforms.ToTensor(),
                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),
    "val": transforms.Compose([transforms.Resize((120, 120)),  # 我这里是缩放成120*120进行处理的，您可以缩放成你需要的比例，但是不建议修改，因为会影响全连接成的输出
                               transforms.ToTensor(),
                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}

def main():
    train_data = torchvision.datasets.ImageFolder(root = "./data/train" ,   transform = data_transform["train"])



    traindata = DataLoader(dataset= train_data , batch_size= 32 , shuffle= True , num_workers=0 )


    test_data = torchvision.datasets.ImageFolder(root = "./data/val" , transform = data_transform["val"])

    train_size = len(train_data)  #求出训练集的长度
    test_size = len(test_data)   #求出测试集的长度
    print(train_size)
    print(test_size)
    testdata = DataLoader(dataset = test_data , batch_size= 32 , shuffle= True , num_workers=0 )

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  #如果有GPU就使用GPU，否则使用CPU
    print("using {} device.".format(device))


    class alexnet(nn.Module):      #alexnet神经网络  ，因为我的数据集是7种，因此如果你替换成自己的数据集，需要将这里的种类改成自己的
        def __init__(self):
            super(alexnet , self).__init__()
            self.model = nn.Sequential(

                nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 120, 120]  output[48, 55, 55]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]
                nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]
                nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
                nn.ReLU(inplace=True),
                nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
                nn.ReLU(inplace=True),
                nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]
                nn.Flatten(),
                nn.Dropout(p=0.5),
                nn.Linear(512, 2048),
                nn.ReLU(inplace=True),
                nn.Dropout(p=0.5),
                nn.Linear(2048, 1024),
                nn.ReLU(inplace=True),
                nn.Linear(1024, 7), #这里的7要修改成自己数据集的种类

            )
        def forward(self , x):
            x = self.model(x)
            return x

    class lenet(nn.Module): #Lenet神经网络
        def __init__(self):
            super(lenet , self).__init__()
            self.model = nn.Sequential(

                nn.Conv2d(3, 16,  kernel_size=5),  # input[3, 120, 120]  output[48, 55, 55]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),  # output[48, 27, 27]

                nn.Conv2d(16, 32, kernel_size=5),  # output[128, 27, 27]
                nn.ReLU(inplace=True),
                nn.MaxPool2d(kernel_size=2, stride=2),  # output[128, 13, 13]
                nn.Flatten(),
                nn.Linear(23328, 2048),
                nn.Linear(2048, 2048),
                nn.Linear(2048, 7), #这里的7要修改成自己数据集的种类

            )
        def forward(self , x):
            x = self.model(x)
            return x

    class VGG(nn.Module):           #VGG神经网络
        def __init__(self, features, num_classes=7, init_weights=False):  #这里的7要修改成自己数据集的种类
            super(VGG, self).__init__()
            self.features = features
            self.classifier = nn.Sequential(
                nn.Linear(4608, 4096),
                nn.ReLU(True),
                nn.Dropout(p=0.5),
                nn.Linear(4096, 4096),
                nn.ReLU(True),
                nn.Dropout(p=0.5),
                nn.Linear(4096, num_classes)
            )
            if init_weights:
                self._initialize_weights()  # 参数初始化

        def forward(self, x):
            # N x 3 x 224 x 224
            x = self.features(x)
            # N x 512 x 7 x 7
            x = torch.flatten(x, start_dim=1)
            # N x 512*7*7
            x = self.classifier(x)
            return x

        def _initialize_weights(self):
            for m in self.modules():  # 遍历各个层进行参数初始化
                if isinstance(m, nn.Conv2d):  # 如果是卷积层的话 进行下方初始化
                    # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                    nn.init.xavier_uniform_(m.weight)  # 正态分布初始化
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)  # 如果偏置不是0 将偏置置成0  相当于对偏置进行初始化
                elif isinstance(m, nn.Linear):  # 如果是全连接层
                    nn.init.xavier_uniform_(m.weight)  # 也进行正态分布初始化
                    # nn.init.normal_(m.weight, 0, 0.01)
                    nn.init.constant_(m.bias, 0)  # 将所有偏执置为0

    def make_features(cfg: list):
        layers = []
        in_channels = 3
        for v in cfg:
            if v == "M":
                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
            else:
                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
                layers += [conv2d, nn.ReLU(True)]
                in_channels = v
        return nn.Sequential(*layers)

    cfgs = {
        'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
        'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
        'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
        'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512,
                  'M'],
    }

    def vgg(model_name="vgg16", **kwargs):
        assert model_name in cfgs, "Warning: model number {} not in cfgs dict!".format(model_name)
        cfg = cfgs[model_name]

        model = VGG(make_features(cfg), **kwargs)
        return model



    VGGnet = vgg(num_classes=7, init_weights=True)    #这里的7要修改成自己数据集的种类

    lenet1 = lenet()

    alexnet1 = alexnet()


    mlps = [lenet1.to(device), alexnet1.to(device), VGGnet.to(device)] #建立一个数组，将三个模型放入

    epoch  = 5 #训练轮数
    LR = 0.0001  #学习率，我这里对于三个模型设置的是一样的学习率，事实上根据模型的不同设置成不一样的效果最好
    a = [{"params": mlp.parameters()} for mlp in mlps]  #依次读取三个模型的权重
    optimizer = torch.optim.Adam(a, lr=LR) #建立优化器
    loss_function = nn.CrossEntropyLoss()  #构建损失函数

    train_loss_all = [[] , [] , []]
    train_accur_all = [[], [], []]
    ronghe_train_loss = []  #融合模型训练集的损失
    ronghe_train_accuracy = []  #融合模型训练集的准确率



    test_loss_all = [[] , [] , []]
    test_accur_all = [[] , [] , []]

    ronghe_test_loss = []  #融合模型测试集的损失
    ronghe_test_accuracy = [] #融合模型测试集的准确




    for i in range(epoch): #遍历开始进行训练
        train_loss = [0 , 0 , 0]  #因为三个模型，初始化三个0存放模型的结果

        train_accuracy = [0.0 , 0.0 , 0.0]  #同上初始化三个0，存放模型的准确率
        for mlp in range(len(mlps)):
            mlps[mlp].train()    #遍历三个模型进行训练
        train_bar = tqdm(traindata)  #构建进度条，训练的时候有个进度条显示

        pre1 = [] #融合模型的损失
        vote1_correct = 0 #融合模型的准确率
        for step , data in enumerate(train_bar):  #遍历训练集


            img , target = data

            length = img.size(0)


            img, target = img.to(device), target.to(device)
            optimizer.zero_grad()
            for mlp in range(len(mlps)):  #对三个模型依次进行训练
                mlps[mlp].train()
                outputs = mlps[mlp](img)
                loss1 = loss_function(outputs, target)  # 求损失
                outputs = torch.argmax(outputs, 1)

                loss1.backward()#反向传播
                train_loss[mlp] += abs(loss1.item()) * img.size(0)
                accuracy = torch.sum(outputs == target)

                pre_num1 = outputs.cpu().numpy()
                # print(pre_num1.shape)
                train_accuracy[mlp] = train_accuracy[mlp] + accuracy

                pre1.append(pre_num1)

            arr1 = np.array(pre1)


            pre1.clear()  # 将pre进行清空

            result1 = [Counter(arr1[:, i]).most_common(1)[0][0] for i in range(length)]  # 对于每张图片，统计三个模型其中，预测的那种情况最多，就取最多的值为融合模型预测的结果，即为投票
            #投票的意思，因为是三个模型，取结果最多的
            vote1_correct += (result1 == target.cpu().numpy()).sum()

            optimizer.step()  # 更新梯度

        losshe= 0
        for mlp in range(len(mlps)):
            print("epoch："+ str(i+1) , "模型" + str(mlp) + "的损失和准确率为：", "train-Loss：{} , train-accuracy：{}".format(train_loss[mlp]/train_size , train_accuracy[mlp]/train_size))
            train_loss_all[mlp].append(train_loss[mlp]/train_size)
            train_accur_all[mlp].append(train_accuracy[mlp].double().item()/train_size)
            losshe +=  train_loss[mlp]/train_size
        losshe /= 3
        print("epoch: " + str(i+1) + "集成模型训练集的正确率" + str(vote1_correct/train_size))
        print("epoch: " + str(i+1) + "集成模型训练集的损失" + str(losshe))
        ronghe_train_loss.append(losshe)
        ronghe_train_accuracy.append(vote1_correct/train_size)


        test_loss = [0 , 0 , 0]
        test_accuracy = [0.0 , 0.0 , 0.0]

        for mlp in range(len(mlps)):
            mlps[mlp].eval()
        with torch.no_grad():
            pre = []
            vote_correct = 0
            test_bar = tqdm(testdata)
            vote_correct = 0
            for data in test_bar:

                length1 = 0
                img, target = data
                length1 = img.size(0)

                img, target = img.to(device), target.to(device)

                for mlp in range(len(mlps)):
                    outputs = mlps[mlp](img)

                    loss2 = loss_function(outputs, target)
                    outputs = torch.argmax(outputs, 1)


                    test_loss[mlp] += abs(loss2.item())*img.size(0)

                    accuracy = torch.sum(outputs == target)
                    pre_num = outputs.cpu().numpy()

                    test_accuracy[mlp] += accuracy

                    pre.append(pre_num)
                arr = np.array(pre)
                pre.clear()  # 将pre进行清空
                result = [Counter(arr[:, i]).most_common(1)[0][0] for i in range(length1)]  # 对于每张图片，统计三个模型其中，预测的那种情况最多，就取最多的值为融合模型预测的结果，
                vote_correct += (result == target.cpu().numpy()).sum()
        losshe1 = 0
        for mlp in range(len(mlps)):
            print("epoch："+ str(i+1), "模型" + str(mlp) + "的损失和准确率为：", "test-Loss：{} , test-accuracy：{}".format(test_loss[mlp] / test_size , test_accuracy[mlp] / test_size ))
            test_loss_all[mlp].append(test_loss[mlp]/test_size)
            test_accur_all[mlp].append(test_accuracy[mlp].double().item()/test_size )
            losshe1 += test_loss[mlp]/test_size
        losshe1 /= 3
        print("epoch: " + str(i+1) + "集成模型测试集的正确率" + str(vote_correct / test_size ))
        print("epoch: " + str(i+1) + "集成模型测试集的损失" + str(losshe1))
        ronghe_test_loss.append(losshe1)
        ronghe_test_accuracy.append(vote_correct/ test_size)
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    # for mlp in range(len(mlps)):
    plt.plot(range(epoch) , ronghe_train_loss,
                 "ro-",label = "Train loss")
    plt.plot(range(epoch), ronghe_test_loss,
                 "bs-",label = "test loss")
    plt.legend()
    plt.xlabel("epoch")
    plt.ylabel("Loss")
    plt.subplot(1, 2, 2)
    plt.plot(range(epoch) , ronghe_train_accuracy,
                 "ro-",label = "Train accur")
    plt.plot(range(epoch) , ronghe_test_accuracy,
                 "bs-",label = "test accur")
    plt.xlabel("epoch")
    plt.ylabel("acc")
    plt.legend()
    plt.show()

    torch.save(alexnet1.state_dict(), "alexnet.pth")
    torch.save(lenet1.state_dict(), "lenet1.pth")
    torch.save(VGGnet.state_dict(), "VGGnet.pth")



    print("模型已保存")

if __name__ == '__main__':
    main()

</code></pre> 
<p></p> 
<p><strong>预测代码：</strong></p> 
<p><strong>需要先运行train代码，然后有生成的三个模型文件，才能进行预测这点需要注意。</strong></p> 
<pre><code>import numpy as np
import torch
from PIL import Image
from torch import nn
from torchvision.transforms import transforms
from collections import Counter
image_path = "2.JPG"#相对路径 导入图片
trans = transforms.Compose([transforms.Resize((120 , 120)),
                           transforms.ToTensor()])   #将图片缩放为跟训练集图片的大小一样 方便预测，且将图片转换为张量
image = Image.open(image_path)  #打开图片
#print(image)  #输出图片 看看图片格式
image = image.convert("RGB")  #将图片转换为RGB格式
image = trans(image)   #上述的缩放和转张量操作在这里实现
#print(image)   #查看转换后的样子
image = torch.unsqueeze(image, dim=0)  #将图片维度扩展一维

classes = ["1" , "2" , "3" , "4" , "5" , "6" , "7" ]  #预测种类


class alexnet(nn.Module):  # alexnet神经网络  ，因为我的数据集是7种，因此如果你替换成自己的数据集，需要将这里的种类改成自己的
    def __init__(self):
        super(alexnet, self).__init__()
        self.model = nn.Sequential(

            nn.Conv2d(3, 48, kernel_size=11, stride=4, padding=2),  # input[3, 120, 120]  output[48, 55, 55]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),  # output[48, 27, 27]
            nn.Conv2d(48, 128, kernel_size=5, padding=2),  # output[128, 27, 27]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 13, 13]
            nn.Conv2d(128, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
            nn.ReLU(inplace=True),
            nn.Conv2d(192, 192, kernel_size=3, padding=1),  # output[192, 13, 13]
            nn.ReLU(inplace=True),
            nn.Conv2d(192, 128, kernel_size=3, padding=1),  # output[128, 13, 13]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2),  # output[128, 6, 6]
            nn.Flatten(),
            nn.Dropout(p=0.5),
            nn.Linear(512, 2048),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5),
            nn.Linear(2048, 1024),
            nn.ReLU(inplace=True),
            nn.Linear(1024, 7),  # 这里的7要修改成自己数据集的种类

        )

    def forward(self, x):
        x = self.model(x)
        return x


class lenet(nn.Module):  # Lenet神经网络
    def __init__(self):
        super(lenet, self).__init__()
        self.model = nn.Sequential(

            nn.Conv2d(3, 16, kernel_size=5),  # input[3, 120, 120]  output[48, 55, 55]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # output[48, 27, 27]

            nn.Conv2d(16, 32, kernel_size=5),  # output[128, 27, 27]
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2),  # output[128, 13, 13]
            nn.Flatten(),
            nn.Linear(23328, 2048),
            nn.Linear(2048, 2048),
            nn.Linear(2048, 7),  # 这里的7要修改成自己数据集的种类

        )

    def forward(self, x):
        x = self.model(x)
        return x



class VGG(nn.Module):
    def __init__(self, features, num_classes=10, init_weights=False):
        super(VGG, self).__init__()
        self.features = features
        self.classifier = nn.Sequential(
            nn.Linear(4608, 4096),
            nn.ReLU(True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(True),
            nn.Dropout(p=0.5),
            nn.Linear(4096, num_classes)
        )
        if init_weights:
            self._initialize_weights()   #参数初始化

    def forward(self, x):
        # N x 3 x 224 x 224
        x = self.features(x)
        # N x 512 x 7 x 7
        x = torch.flatten(x, start_dim=1)
        # N x 512*7*7
        x = self.classifier(x)
        return x

    def _initialize_weights(self):
        for m in self.modules():         #遍历各个层进行参数初始化
            if isinstance(m, nn.Conv2d):   #如果是卷积层的话 进行下方初始化
                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                nn.init.xavier_uniform_(m.weight)  #正态分布初始化
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)     #如果偏置不是0 将偏置置成0  相当于对偏置进行初始化
            elif isinstance(m, nn.Linear):        #如果是全连接层
                nn.init.xavier_uniform_(m.weight)    #也进行正态分布初始化
                # nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)  #将所有偏执置为0


def make_features(cfg: list):
    layers = []
    in_channels = 3
    for v in cfg:
        if v == "M":
            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]
        else:
            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)
            layers += [conv2d, nn.ReLU(True)]
            in_channels = v
    return nn.Sequential(*layers)


cfgs = {
    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],
    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],
    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],
}


def vgg(model_name="vgg16", **kwargs):
    assert model_name in cfgs, "Warning: model number {} not in cfgs dict!".format(model_name)
    cfg = cfgs[model_name]

    model = VGG(make_features(cfg), **kwargs)
    return model
#以上是神经网络结构，因为读取了模型之后代码还得知道神经网络的结构才能进行预测
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  #将代码放入GPU进行训练
print("using {} device.".format(device))

VGGnet = vgg(num_classes=7, init_weights=True)  # 这里的7要修改成自己数据集的种类

lenet1 = lenet()

alexnet1 = alexnet()

VGGnet.load_state_dict(torch.load("VGGnet.pth", map_location=device))#训练得到的VGGnet模型放入当前文件夹下
lenet1.load_state_dict(torch.load("lenet1.pth", map_location=device))#训练得到的lenet模型放入当前文件夹下
alexnet1.load_state_dict(torch.load("alexnet.pth", map_location=device))#训练得到的alexnet模型放入当前文件夹下
mlps = [lenet1.to(device), alexnet1.to(device), VGGnet.to(device)] #建立一个数组，将三个模型放入

for mlp in range(len(mlps)):
    mlps[mlp].eval()#关闭梯度，将模型调整为测试模式

with torch.no_grad():  #梯度清零
    pre = []
    length1 = image.size(0)
    for mlp in range(len(mlps)):

        outputs = mlps[mlp](image.to(device))#将图片打入神经网络进行测试
        outputs = torch.argmax(outputs, 1)
        pre_num = outputs.cpu().numpy()
        pre.append(pre_num)
    arr = np.array(pre)
    print(arr)#查看三个模型输除的结果
    pre.clear()  # 将pre进行清空
    result = [Counter(arr[:, i]).most_common(1)[0][0] for i in range(length1)]  # 对于每张图片，统计三个模型其中，预测的那种情况最多，就取最多的值为融合模型预测的结果，
    print(result)  #选取最多的情况最为

    # 对应找其在种类中的序号即可然后输出即为其种类
    print(classes[result[0]-1])#因为他是输出的第几种情况，然后我们的列表是从0开始的因此，这里需要减一</code></pre> 
<p><strong>代码可拓展性很强，也可以替换成自己想要的模型进行融合，比如人resnet，googlenet均可。有兴趣的可以尝试拓展，欢迎评论交流，如何代码运行有问题可以在评论区指出来看到了会回复。</strong></p> 
<p><strong>最后觉得有用的麻烦点个赞，非常感谢！</strong></p> 
<p><strong>代码链接：https://pan.baidu.com/s/1zgdhEV6J8nOrS163Du9D6Q <br> 提取码：z0vc</strong></p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>