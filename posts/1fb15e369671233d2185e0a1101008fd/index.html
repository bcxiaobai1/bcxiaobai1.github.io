<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Attetion is all you need论文阅读笔记 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Attetion is all you need论文阅读笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="Attetion_is_all_you_need_0"></a>Attetion is all you need</h1> 
<p>参考：沐神（<a href="https://www.bilibili.com/video/BV1pu411o7BE/?spm_id_from=333.788&amp;vd_source=9d14e5d446c7c743208869eced3fcf60"><br> 沐神_论文精讲_Attention is all you need</a>）</p> 
<h2>
<a id="1Abstract_3"></a>1、Abstract</h2> 
<p>主流的序列转录模型（给一个序列生成另一个序列，比如机器翻译，给一句英文，生成一句中文）都是基于复杂的循环或卷积神经网络，一般是基于encoder and decoder的架构。在这些好的模型中，一般会在encoder和decoder之间使用注意力机制。</p> 
<p>而本文提出了一个新的简单的架构，Transformer，仅基于注意力机制，免除了循环或卷积。在俩个机器翻译任务上证明这个模型的有效性，并行度更好，训练时间少。并且Transformer模型还可以泛化到其他的任务当中。</p> 
<blockquote> 
 <p>Our model achieves 28.4 BLEU on the WMT 2014 English to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.</p> 
</blockquote> 
<h2>
<a id="2Conclusion_11"></a>2、Conclusion</h2> 
<p>Transformer是第一篇做序列转录的模型，仅使用了注意力机制，将循环层替换为multi-headed self-attetion.它在机器翻译任务重表现的很好。</p> 
<p>这种纯注意力机制的模型还可以泛化到其他领域当中。</p> 
<h2>
<a id="3Introduction_17"></a>3、Introduction</h2> 
<p>循环神经网络（RNN），长短期记忆（LSTM）和门控循环（gated recurrent）神经网络，已经成为序列建模和转导问题(如语言建模和机器翻译)的最先进方法。主要有language models 和 encoder-decoder architectures.</p> 
<p>RNN处理序列时通常会根据序列的输入输出顺序从左到右一步步前进，计算位置t时，会输出一个隐藏状态h<sub>t</sub>，该输出由之前的隐藏状态h<sub>t-1</sub>和当前位置t共同决定的。这导致使用RNN计算序列模型无法并行，它需要等到先前状态h<sub>t-1</sub>计算完成之后才能计算h<sub>t</sub>。而且由于内存的限制，会对序列长短有着限制，若序列过长会导致历史信息的丢失或者内存开销过多。虽然近期的工作对RNN有着很多的改进，比如使用分解的方法提升并行度，但是最根本的问题依旧存在着。</p> 
<p>在各种任务中，注意机制已经成为序列建模和转换模型的必要组成部分，允许模型不考虑输入或输出序列中的距离的情况。但大多情况下，注意力机制都会与RNN结合使用。</p> 
<p>在本文中，Transformer，不在使用循环神经网络，而是纯注意力机制。从而其可以并行运算，从而使得能够在更短的时间做到比之前更好的结果。</p> 
<h2>
<a id="4Background_27"></a>4、Background</h2> 
<p>可以使用卷积神经网络替换循环神经网络，使得减少时序的计算。但如果使用卷积神经网络对于较长的序列难以建模，因为卷积神经网络计算时使用的是3x3/5x5等大小的卷积核，如果要寻找间隔较长的俩个像素之间的关系，需要卷积多次才能够将这俩个像素融合起来，但是如果使用Transformer的注意力机制，可以一次可以看到所有像素。但是卷积神经网络可以卷积出多个输出通道，每个输出通道可以认为是它去识别不一样的模式，为了达到这一效果，Transformer提出了一个Multi-Head Attention来实现这一影响。</p> 
<p>自注意力机制，将单个序列不同位置联系起来的注意力机制，以便于计算序列内部之间的联系，且应用非常广泛。</p> 
<p>端到端的记忆网络基于循环注意力机制，替换了序列对齐循环，已被证明在简单的语言问题的回答和语言建模任务重表现良好。</p> 
<p>Transformer是第一个基于自注意力机制来计算输入输出的transuduction model，没有使用序列对齐的RNN模型或卷积。</p> 
<h2>
<a id="5_37"></a>5、模型架构</h2> 
<p>大多序列转导(sequence transduction)模型都使用了encoder-decoder架构。首先encoder是将一个序列(x<sub>1</sub>,…,x<sub>n</sub>)编码对应为(z<sub>1</sub>,…,z<sub>n</sub>)，随后给decoder输入<code>Z</code>生成输出序列(y<sub>1</sub>,…,y<sub>m</sub>)，其中n，m可以不相等，而且解码器是使用了自回归模型，也就是说之前的输出作为下一时刻额外的输入（解码器的输出是一个个输出，比如已知y<sub>1</sub>-y<sub>3</sub>,下一时刻生成y<sub>4</sub>）。</p> 
<p>首先Transformer结构是</p> 
<p><img src="https://images2.imgbox.com/39/88/FBlYha31_o.jpg" alt="在这里插入图片描述"></p> 
<h3>
<a id="51_Encoder_and_Decoder_Stacks_45"></a>5.1 Encoder and Decoder Stacks</h3> 
<p><strong>Encoder:</strong> N=6，重复6次，每个layer 有俩个sub-layers，第一个为multi-head自注意力机制，第二个是position-wise fully connected feed-forward network，效果也就是相当于一个MLP。对于每个子层都用了一个参差连接，子层的输出为:LayerNorm(x + Sublayer(x))，为了简单，激情每一个层的输出维度d<sub>model</sub>=512.（不同于MLP将维度减少，也不同为CNN将空间维度减少，channel维度上拉）</p> 
<p>**Decoder:**N=6,每个layer 除了有之前的俩个sub-layers之外，新加了第三个子层，一个多头注意力机制，与encoder一样，使用了参差结构，并在之后使用了LN，并使用了自回归，当前时刻的输入是上面一些时刻的输出。但因为在预测阶段，网络不能看到t时候之后的输出，只能看到t时刻之前的输出，但在注意力机制当中，每次都可以看到完整的输入（为什么训练的时候注意力机制可以看到完整的输入呢？因为训练时输入了GT，而且注意力机制不像卷积小窗口，而是可以看到所有输入，所以可以看到完整的输入），所以为了统一训练和预测阶段，我们使用了Masked Multi-Head，使得网络在t时刻只能看到t时刻之前的输出。</p> 
<h3>
<a id="52_Attention_51"></a>5.2 Attention</h3> 
<p>注意力函数是一个query，和一些key-value键值对映射成一个输出的一个函数，其中query，keys，values和output都是向量。输出是values的加权和，所以输出的维度和value的维度是一样的。其中，权重是由values对应的key和query的相似度(compatibility function)计算的来的。</p> 
<p>(形象一点的理解：key相当于名字，value是分数，query表示我想看谁的分数，所以通过query去查key对应的value，获得最终的结果,不同的注意力机制，计算相似度的方法不一样。)</p> 
<h4>
<a id="521_Scaled_DotProduct_Attention_57"></a>5.2.1 Scaled Dot-Product Attention</h4> 
<p>queries和keys的维度都为d<sub>k</sub>，values的维度为d<sub>v</sub>，具体计算时将query和keys做内积，内积越大，相似度越高，内积为0，代表垂直没有相似度。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Attention
       
       
        ⁡
       
       
        (
       
       
        Q
       
       
        ,
       
       
        K
       
       
        ,
       
       
        V
       
       
        )
       
       
        =
       
       
        softmax
       
       
        ⁡
       
       
        
         (
        
        
         
          
           Q
          
          
           
            K
           
           
            T
           
          
         
         
          
           
            d
           
           
            k
           
          
         
        
        
         )
        
       
       
        V
       
      
      
       operatorname{Attention}(Q, K, V)=operatorname{softmax}left(frac{Q K^{T}}{sqrt{d_{k}}}right) V
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mop"><span class="mord mathrm">Attention</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.8em;vertical-align: -0.65em"></span><span class="mop"><span class="mord mathrm">softmax</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size2">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.0895em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8622em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord mtight" style="padding-left: 0.833em"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail mtight" style="min-width: 0.853em;height: 1.08em">
                    
                     
                    </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1778em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9191em"><span class="" style="margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter"><span class="delimsizing size2">)</span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span></span></span></span></span></p> 
<p>常见的俩种计算注意力机制的方法，一种加型注意力机制，可以处理query和keys不等长，另一种是点积注意力机制，与作者提出的基本相同，但Transfomer的除以了<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          d
         
         
          k
         
        
       
      
      
       sqrt{d_{k}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em;vertical-align: -0.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
           
            
           </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em"><span class=""></span></span></span></span></span></span></span></span></span>。点乘更简单，高校。</p> 
<p>当d<sub>k</sub>较大/小时，点乘的值会比较大/小，会导致softmax之后的结果更加靠近0/1，更向俩端靠拢，会导致梯度比较小（因为此时该靠近1的已经趋于1，该趋于0的也都趋于0了，所以网络自认为已经收敛了）。所以/<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          d
         
         
          k
         
        
       
      
      
       sqrt{d_{k}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em;vertical-align: -0.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
           
            
           </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em"><span class=""></span></span></span></span></span></span></span></span></span>。</p> 
<p><img src="https://images2.imgbox.com/5e/77/RygNZp6L_o.jpg" alt="在这里插入图片描述"></p> 
<h4>
<a id="522_MultiHead_Attention_70"></a>5.2.2 Multi-Head Attention</h4> 
<p>与其使用单个维度为d<sub>model</sub>(keys, values, queries)的注意力函数，不如将queries，key，values投影到一个低维，投影h次，再做h次的注意力函数，得到的h个输出在使用concat并在一起。然后再通过线性投影得到最终的输出。（这里的投影相当于做了一个MLP）（为了得到更多的学习参数）</p> 
<p><img src="https://images2.imgbox.com/b8/c0/J3Vq5CeG_o.jpg" alt="在这里插入图片描述"></p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           
            MultiHead
           
           
            ⁡
           
           
            (
           
           
            Q
           
           
            ,
           
           
            K
           
           
            ,
           
           
            V
           
           
            )
           
          
         
        
        
         
          
           
           
            =
           
           
            Concat
           
           
            ⁡
           
           
            
             (
            
            
             
              
               head
              
              
               ⁡
              
             
             
              1
             
            
            
             ,
            
            
             …
            
            
             ,
            
            
             
              
               head
              
              
               ⁡
              
             
             
              h
             
            
            
             )
            
           
           
            
             W
            
            
             O
            
           
          
         
        
       
       
        
         
          
            where headi 
          
         
        
        
         
          
           
           
            =
           
           
            Attention
           
           
            ⁡
           
           
            
             (
            
            
             Q
            
            
             
              W
             
             
              i
             
             
              Q
             
            
            
             ,
            
            
             K
            
            
             
              W
             
             
              i
             
             
              K
             
            
            
             ,
            
            
             V
            
            
             
              W
             
             
              i
             
             
              V
             
            
            
             )
            
           
          
         
        
       
      
      
       begin{aligned} operatorname{MultiHead}(Q, K, V) &amp; =operatorname{Concat}left(operatorname{head}_{1}, ldots, operatorname{head}_{mathrm{h}}right) W^{O} \ text { where headi } &amp; =operatorname{Attention}left(Q W_{i}^{Q}, K W_{i}^{K}, V W_{i}^{V}right) end{aligned}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 3.6514em;vertical-align: -1.5757em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.0757em"><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mop"><span class="mord mathrm">MultiHead</span></span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mord text"><span class="mord"> where headi </span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.5757em"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.0757em"><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mop"><span class="mord mathrm">Concat</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mop"><span class="mop"><span class="mord mathrm">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mop"><span class="mop"><span class="mord mathrm">head</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathrm mtight">h</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8913em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em">O</span></span></span></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mop"><span class="mord mathrm">Attention</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size2">(</span></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9592em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2769em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8913em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em">K</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8913em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em">V</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size2">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.5757em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>h=8，因为有参差连接存在，所以输入和输出维度相等，所以投影时，投影的事输出的维度/h，之前输出的维度是512，所以512/8=64，所以就是每个head投影到一个64维。</p> 
<h4>
<a id="523_Application_of_Attention_in_our_model_84"></a>5.2.3 Application of Attention in our model</h4> 
<p><img src="https://images2.imgbox.com/bb/78/lalzFgvE_o.jpg" alt="在这里插入图片描述"></p> 
<p>三种使用方法：</p> 
<ul>
<li>在"encoder-decoder attention" layers，quries来自之前的decoder，key和values来自encoder。这是的decoder中的每个位置可以覆盖输入序列的所有位置，这模拟了序列到序列模型中典型的编码器和解码器模型。</li>
<li>encoder中的自注意力层，在自注意力层中的所有keys、values、queries都来自同一个地方，也就是来自编码器前一层的输出。编码器的每个位置都可以处理编码器前一层中的所有位置。</li>
<li>类似的，解码器的自注意力层允许解码器中的每个位置关注到所有的输入，我们要防止信息信息在decoder中向左流动，以保持自回归特性。并通过为softmax的输出设置掩码(setting -无穷)来实现scaled dot-product attention。</li>
</ul> 
<blockquote> 
 <p>The Transformer uses multi-head attention in three different ways:</p> 
 <p><em>•</em> In “encoder-decoder attention” layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].</p> 
 <p><em>•</em> The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</p> 
 <p><em>•</em> Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flflow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to <em>−∞</em>) all values in the input of the softmax which correspond to illegal connections. See Figure 2.</p> 
</blockquote> 
<h3>
<a id="53_Positionwise_FeedForward_Networks_103"></a>5.3 Position-wise Feed-Forward Networks</h3> 
<p>除了注意力子层，编码器和解码器的每一层都包含了一个fully connected feed-forward网络，这个网络分别且相同的对每一个位置（一个序列中的某一个词）都作用了一遍。包含了俩个线性转换和ReLU激活函数。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        FFN
       
       
        ⁡
       
       
        (
       
       
        x
       
       
        )
       
       
        =
       
       
        max
       
       
        ⁡
       
       
        
         (
        
        
         0
        
        
         ,
        
        
         x
        
        
         
          W
         
         
          1
         
        
        
         +
        
        
         
          b
         
         
          1
         
        
        
         )
        
       
       
        
         W
        
        
         2
        
       
       
        +
       
       
        
         b
        
        
         2
        
       
      
      
       operatorname{FFN}(x)=max left(0, x W_{1}+b_{1}right) W_{2}+b_{2}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mop"><span class="mord mathrm">FFN</span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mop">max</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></p> 
<p>x为固定维度512，W<sub>1</sub>会512投影为d<sub>ff</sub>=2048，因为有参差连接，所以W<sub>2</sub>将2048-&gt;512</p> 
<h3>
<a id="54_Embeddings_and_Softmax_111"></a>5.4 Embeddings and Softmax</h3> 
<p>Embeddings就是对一个序列中的任一一个词都学习一个长为d<sub>model</sub>的向量来表示它。d<sub>model</sub>=512，encoder、decoder输入，softmax之后也要一个embedding，这三个embedding权重相同。注意要将学习到的权重 multiply <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          d
         
         
          
           m
          
          
           o
          
          
           d
          
          
           e
          
          
           l
          
         
        
       
      
      
       sqrt{d_{model}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em;vertical-align: -0.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
           
            
           </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em"><span class=""></span></span></span></span></span></span></span></span></span>。学习embeddings是会将每个向量的l<sub>2</sub> norm学的比较小，eg=1，不管维度多大，其l<sub>2</sub> norm都等于1，所以维度一大，其权重值就会变小。所以乘<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          d
         
         
          
           m
          
          
           o
          
          
           d
          
          
           e
          
          
           l
          
         
        
       
      
      
       sqrt{d_{model}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em;vertical-align: -0.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
           
            
           </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em"><span class=""></span></span></span></span></span></span></span></span></span>变大一些。</p> 
<p>(1. L2 Norm会将向量的所有值归一化2. 维度越大的向量归一化后其单个值就越小3. 而时序信息是递增的整数(往后看会讲)4. 为了让它们的规模相匹配，故而乘了一个根号d给前面)</p> 
<p>（权重经l2正则化以后都比较小，维度越大权重会越小，为了让它能跟位置编码一个维度，乘了根号512）</p> 
<h4>
<a id="54_Positional_Encoding_121"></a>5.4 Positional Encoding</h4> 
<p>因为attention没有时序信息，输出是对value的一个加权和，权重是query和key之间的距离，和序列信息无关，所以一个序列顺序变了但经过attetion之后还是一样的，所以需要加入时序信息。</p> 
<p>RNN是将上一时刻的输出作为下一时刻的输入来传递历史信息。Attention需要在输入中加入时序信息，eg：将每个单词的位置i加到输入中。</p> 
<p>位置信息是通过一系列公式加入的，该位置信息长为512，然后结果加到输入信息中，则输入中就加入了时序信息。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           
            P
           
           
            
             E
            
            
             
              (
             
             
              p
             
             
              o
             
             
              s
             
             
              ,
             
             
              2
             
             
              i
             
             
              )
             
            
           
          
         
        
        
         
          
           
           
            =
           
           
            sin
           
           
            ⁡
           
           
            
             (
            
            
             p
            
            
             o
            
            
             s
            
            
             /
            
            
             1000
            
            
             
              0
             
             
              
               2
              
              
               i
              
              
               /
              
              
               
                d
               
               
                
                 m
                
                
                 o
                
                
                 d
                
                
                 e
                
                
                 l
                
               
              
             
            
            
             )
            
           
          
         
        
       
       
        
         
          
           
            P
           
           
            
             E
            
            
             
              (
             
             
              p
             
             
              o
             
             
              s
             
             
              ,
             
             
              2
             
             
              i
             
             
              +
             
             
              1
             
             
              )
             
            
           
          
         
        
        
         
          
           
           
            =
           
           
            cos
           
           
            ⁡
           
           
            
             (
            
            
             p
            
            
             o
            
            
             s
            
            
             /
            
            
             1000
            
            
             
              0
             
             
              
               2
              
              
               i
              
              
               /
              
              
               
                d
               
               
                
                 m
                
                
                 o
                
                
                 d
                
                
                 e
                
                
                 l
                
               
              
             
            
            
             )
            
           
          
         
        
       
      
      
       begin{aligned} P E_{(p o s, 2 i)} &amp; =sin left(p o s / 10000^{2 i / d_{mathrm{model}}}right) \ P E_{(p o s, 2 i+1)} &amp; =cos left(p o s / 10000^{2 i / d_{mathrm{model}}}right) end{aligned}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 4.2em;vertical-align: -1.85em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.35em"><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">P</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: -0.0576em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3552em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">P</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: -0.0576em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3552em"><span class=""></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.85em"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.35em"><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mop">sin</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size2">(</span></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size2">)</span></span></span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mop">cos</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size2">(</span></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/1000</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathrm mtight">model</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size2">)</span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.85em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<h2>
<a id="4_Why_SelfAttetion_134"></a>4 Why Self-Attetion</h2> 
<p>比较了自注意力与循环层、卷积层进行比较。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          
           
            
              Layer Type 
            
           
          
          
           
            
              Complexity per Layer 
            
           
          
          
           
            
             
              
               
                
                  Sequential 
                
               
              
             
             
              
               
                
                  Operations 
                
               
              
             
            
           
          
          
           
            
              Maximum Path Length 
            
           
          
         
         
          
           
            
              Self-Attention 
            
           
          
          
           
            
             
              O
             
             
              
               (
              
              
               
                n
               
               
                2
               
              
              
               ⋅
              
              
               d
              
              
               )
              
             
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              1
             
             
              )
             
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              1
             
             
              )
             
            
           
          
         
         
          
           
            
              Recurrent 
            
           
          
          
           
            
             
              O
             
             
              
               (
              
              
               n
              
              
               ⋅
              
              
               
                d
               
               
                2
               
              
              
               )
              
             
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              n
             
             
              )
             
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              n
             
             
              )
             
            
           
          
         
         
          
           
            
              Convolutional 
            
           
          
          
           
            
             
              O
             
             
              
               (
              
              
               k
              
              
               ⋅
              
              
               n
              
              
               ⋅
              
              
               
                d
               
               
                2
               
              
              
               )
              
             
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              1
             
             
              )
             
            
           
          
          
           
            
             
              O
             
             
              
               (
              
              
               
                
                 log
                
                
                 ⁡
                
               
               
                k
               
              
              
               (
              
              
               n
              
              
               )
              
              
               )
              
             
            
           
          
         
         
          
           
            
              Self-Attention (restricted) 
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              r
             
             
              ⋅
             
             
              n
             
             
              ⋅
             
             
              d
             
             
              )
             
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              1
             
             
              )
             
            
           
          
          
           
            
             
              O
             
             
              (
             
             
              n
             
             
              /
             
             
              r
             
             
              )
             
            
           
          
         
        
       
      
      
       begin{array}{lccc} hline text { Layer Type } &amp; text { Complexity per Layer } &amp; begin{array}{c} text { Sequential } \ text { Operations } end{array} &amp; text { Maximum Path Length } \ hline text { Self-Attention } &amp; Oleft(n^{2} cdot dright) &amp; O(1) &amp; O(1) \ text { Recurrent } &amp; Oleft(n cdot d^{2}right) &amp; O(n) &amp; O(n) \ text { Convolutional } &amp; Oleft(k cdot n cdot d^{2}right) &amp; O(1) &amp; Oleft(log _{k}(n)right) \ text { Self-Attention (restricted) } &amp; O(r cdot n cdot d) &amp; O(1) &amp; O(n / r) \ hline end{array}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 7.27em;vertical-align: -3.365em"></span><span class="mord"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 3.905em"><span class=""><span class="pstrut" style="height: 5.865em"></span><span class="mtable"><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 3.865em"><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord text"><span class="mord"> Layer Type </span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord text"><span class="mord"> Self-Attention </span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord text"><span class="mord"> Recurrent </span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord text"><span class="mord"> Convolutional </span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord text"><span class="mord"> Self-Attention (restricted) </span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 3.365em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 3.865em"><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord text"><span class="mord"> Complexity per Layer </span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal">d</span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord mathnormal" style="margin-right: 0.0315em">k</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0278em">r</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 3.365em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 3.865em"><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord"><span class="mtable"><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.45em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord text"><span class="mord"> Sequential </span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord text"><span class="mord"> Operations </span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.95em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 3.365em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 3.865em"><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord text"><span class="mord"> Maximum Path Length </span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord">1</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mop"><span class="mop">lo<span style="margin-right: 0.0139em">g</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.242em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2441em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mclose">)</span><span class="mclose delimcenter">)</span></span></span></span><span class=""><span class="pstrut" style="height: 3.45em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right: 0.0278em">r</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 3.365em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span></span></span><span class=""><span class="pstrut" style="height: 5.865em"></span><span class="hline" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 5.865em"></span><span class="hline" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 5.865em"></span><span class="hline" style="border-bottom-width: 0.04em"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 3.365em"><span class=""></span></span></span></span></span></span></span></span></span></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>