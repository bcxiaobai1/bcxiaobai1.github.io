<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Pytorch学习笔记（六）——Sequential类、参数管理与GPU - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch学习笔记（六）——Sequential类、参数管理与GPU</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-github-gist">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>目录</h3>
 <ul>
<li><a href="#torchnnSequential_3">一、torch.nn.Sequential</a></li>
<li>
<ul>
<li><a href="#11_Sequential__50">1.1 Sequential 的基础操作</a></li>
<li><a href="#12__Sequential_116">1.2 手动实现一个 Sequential</a></li>
<li><a href="#13_Sequential__181">1.3 Sequential 嵌套</a></li>
<li><a href="#14__241">1.4 自定义层</a></li>
<li>
<ul>
<li><a href="#141__244">1.4.1 不带参数的层</a></li>
<li><a href="#142__266">1.4.2 带参数的层</a></li>
</ul>
  </li>
</ul>
  </li>
<li><a href="#_300">二、参数管理</a></li>
<li>
<ul>
<li><a href="#21_nnParameter_301">2.1 nn.Parameter</a></li>
<li><a href="#22__373">2.2 参数访问</a></li>
<li><a href="#23__401">2.3 参数初始化</a></li>
<li>
<ul>
<li><a href="#231__408">2.3.1 使用内置初始化</a></li>
<li><a href="#232__456">2.3.2 自定义初始化</a></li>
</ul>
   </li>
<li><a href="#24__497">2.4 参数绑定</a></li>
<li><a href="#25__516">2.5 模型保存</a></li>
<li>
<ul>
<li><a href="#251__519">2.5.1 张量的保存</a></li>
<li><a href="#252__542">2.5.2 保存整个模型</a></li>
<li><a href="#253__551">2.5.3 保存模型的参数</a></li>
</ul>
  </li>
</ul>
  </li>
<li><a href="#GPU_566">三、GPU</a></li>
<li>
<ul>
<li><a href="#31_GPU_580">3.1 将数据移动到GPU</a></li>
<li><a href="#32_GPU_610">3.2 将模型移动到GPU上</a></li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<h1>
<a id="torchnnSequential_3"></a>一、torch.nn.Sequential</h1> 
<p><code>Sequential</code> 本质是一个模块（即 <code>Module</code>），根据Pytorch中的约定，模块中可以继续添加模块。这意味着我们可以在 <code>Sequential</code> 中添加其它的模块（自然也就可以添加其他的 <code>Sequential</code>）。添加完成后，<code>Sequential</code> 会将这些模块组成一个流水线，输入将依次通过这些模块得到一个输出，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/5c/7a/4DpCJJrE_o.png" alt="在这里插入图片描述"><br> 对应的代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

myseq <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    <span class="token comment"># Module 1</span>
    <span class="token comment"># Module 2</span>
    <span class="token comment"># ...</span>
    <span class="token comment"># Module n</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>因为 <code>nn.Linear</code> 和 <code>nn.ReLU</code> 也都是模块，所以我们可以将这些模块稍加组合放进 <code>myseq</code> 中以构建一个简单的神经网络。</p> 
<p>以单隐层网络为例，假设输入层、隐层和输出层神经元的个数分别为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        20
       
       
        ,
       
       
        10
       
       
        ,
       
       
        5
       
      
      
       20, 10, 5
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83888em;vertical-align: -0.19444em"></span><span class="mord">2</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">5</span></span></span></span></span>，隐层激活函数采用 ReLU，则我们的网络可写为</p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>在训练场景下，我们可以向定义好的 <code>net</code> 投喂一个 batch 的样本，假设 batch 的大小为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        3
       
      
      
       3
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>，<code>net</code> 将返回一个 batch 的输出</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>
net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token comment"># tensor([[ 0.0092, -0.3154, -0.1202, -0.2654,  0.1336],</span>
<span class="token comment">#         [-0.0042, -0.2338, -0.1788, -0.5513, -0.6258],</span>
<span class="token comment">#         [ 0.0731, -0.4427, -0.3108,  0.1791,  0.1614]],</span>
<span class="token comment">#        grad_fn=&lt;AddmmBackward0&gt;)</span>
</code></pre> 
<h2>
<a id="11_Sequential__50"></a>1.1 Sequential 的基础操作</h2> 
<p>通过打印 <code>Sequential</code> 对象来查看它的结构</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token comment"># Sequential(</span>
<span class="token comment">#   (0): Linear(in_features=20, out_features=10, bias=True)</span>
<span class="token comment">#   (1): ReLU()</span>
<span class="token comment">#   (2): Linear(in_features=10, out_features=5, bias=True)</span>
<span class="token comment"># )</span>
</code></pre> 
<p>像对待Python列表那样，我们可以使用索引来查看其子模块，也可以查看 <code>Sequential</code> 有多长</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># Linear(in_features=20, out_features=10, bias=True)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># ReLU()</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 3</span>
</code></pre> 
<p>当然，我们还可以修改、删除、添加子模块：</p> 
<pre><code class="prism language-python">net<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token comment"># Sequential(</span>
<span class="token comment">#   (0): Linear(in_features=20, out_features=10, bias=True)</span>
<span class="token comment">#   (1): Sigmoid()</span>
<span class="token comment">#   (2): Linear(in_features=10, out_features=5, bias=True)</span>
<span class="token comment"># )</span>

<span class="token keyword">del</span> net<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token comment"># Sequential(</span>
<span class="token comment">#   (0): Linear(in_features=20, out_features=10, bias=True)</span>
<span class="token comment">#   (1): Sigmoid()</span>
<span class="token comment"># )</span>

net<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 均会添加到末尾</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span>
<span class="token comment"># Sequential(</span>
<span class="token comment">#   (0): Linear(in_features=20, out_features=10, bias=True)</span>
<span class="token comment">#   (1): Sigmoid()</span>
<span class="token comment">#   (2): Linear(in_features=10, out_features=2, bias=True)</span>
<span class="token comment"># )</span>
</code></pre> 
<blockquote> 
 <p>目前（Version 1.11.0），如果使用 <code>del</code> 删除的子模块不是最后一个，可能就会出现一些 <strong>bug？</strong> 例如索引不连续，无法继续添加子模块等。</p> 
</blockquote> 
<p>当然，<code>Sequential</code> 对象本身就是一个<strong>可迭代对象</strong>，所以我们还可以使用 for 循环来打印所有子模块：</p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>

<span class="token keyword">for</span> sub_module <span class="token keyword">in</span> net<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>sub_module<span class="token punctuation">)</span>
<span class="token comment"># Linear(in_features=20, out_features=10, bias=True)</span>
<span class="token comment"># ReLU()</span>
<span class="token comment"># Linear(in_features=10, out_features=5, bias=True)</span>
</code></pre> 
<h2>
<a id="12__Sequential_116"></a>1.2 手动实现一个 Sequential</h2> 
<p>为了加深理解，接下来我们从0开始手动实现 <code>Sequential</code> （当然不会与官方的一样，只是为了便于理解）。</p> 
<p>我们需要先完成最基础的功能，即将各个模块传入 <code>Sequential</code> 后，<code>Sequential</code> 能对这些模块进行组装并拥有正向传播功能：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MySeq</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> module

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            inputs <span class="token operator">=</span> module<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> inputs
</code></pre> 
<p>尝试正向传播：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>
myseq <span class="token operator">=</span> MySeq<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span>
myseq<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token comment"># tensor([[ 0.2056, -0.5307, -0.0023, -0.0309,  0.1289],</span>
<span class="token comment">#         [ 0.0681, -0.4473,  0.2085, -0.1179,  0.1157],</span>
<span class="token comment">#         [ 0.1187, -0.5331,  0.0530, -0.0466,  0.0874]],</span>
<span class="token comment">#        grad_fn=&lt;AddmmBackward0&gt;)</span>
</code></pre> 
<p>可以看出我们实现的 <code>MySeq</code> 能够得到正确的输出。但很显然，目前实现的 <code>MySeq</code> 功能太少，还需要实现索引、赋值、删除、添加等操作：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MySeq</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token operator">*</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> idx<span class="token punctuation">,</span> module <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> module

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__setitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    	<span class="token keyword">assert</span> idx <span class="token operator">&lt;</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> module

    <span class="token keyword">def</span> <span class="token function">__delitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>idx<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>
        <span class="token keyword">del</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span>

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_modules<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">append</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> module<span class="token punctuation">)</span><span class="token punctuation">:</span>
        new_idx <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>_modules<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">(</span>new_idx<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> module

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> module <span class="token keyword">in</span> self<span class="token punctuation">.</span>_modules<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            inputs <span class="token operator">=</span> module<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
        <span class="token keyword">return</span> inputs
</code></pre> 
<p>到这里，我们的 <code>MySeq</code> 就算大功告成了，并且使用 <code>del</code> 方法不会出现bug。</p> 
<h2>
<a id="13_Sequential__181"></a>1.3 Sequential 嵌套</h2> 
<p><code>Sequential</code> 本身就是一个模块，而模块可以嵌套模块，这说明 <code>Sequential</code> 可以嵌套 <code>Sequential</code>。</p> 
<p>例如，在一个 <code>Sequential</code> 中嵌套两个 <code>Sequential</code>：</p> 
<pre><code class="prism language-python">seq_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
seq_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
seq_3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>seq_1<span class="token punctuation">,</span> seq_2<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>seq_3<span class="token punctuation">)</span>
<span class="token comment"># Sequential(</span>
<span class="token comment">#   (0): Sequential(</span>
<span class="token comment">#     (0): Linear(in_features=15, out_features=10, bias=True)</span>
<span class="token comment">#     (1): ReLU()</span>
<span class="token comment">#     (2): Linear(in_features=10, out_features=5, bias=True)</span>
<span class="token comment">#   )</span>
<span class="token comment">#   (1): Sequential(</span>
<span class="token comment">#     (0): Linear(in_features=25, out_features=15, bias=True)</span>
<span class="token comment">#     (1): Sigmoid()</span>
<span class="token comment">#     (2): Linear(in_features=15, out_features=10, bias=True)</span>
<span class="token comment">#   )</span>
<span class="token comment"># )</span>
</code></pre> 
<p>我们依然可以像列表那样使用多级索引进行访问：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>seq_3<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># Sequential(</span>
<span class="token comment">#   (0): Linear(in_features=25, out_features=15, bias=True)</span>
<span class="token comment">#   (1): Sigmoid()</span>
<span class="token comment">#   (2): Linear(in_features=15, out_features=10, bias=True)</span>
<span class="token comment"># )</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>seq_3<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token comment"># ReLU()</span>
</code></pre> 
<p>还可以使用双重循环进行遍历：</p> 
<pre><code class="prism language-python"><span class="token keyword">for</span> seq <span class="token keyword">in</span> seq_3<span class="token punctuation">:</span>
    <span class="token keyword">for</span> module <span class="token keyword">in</span> seq<span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span>
<span class="token comment"># Linear(in_features=15, out_features=10, bias=True)</span>
<span class="token comment"># ReLU()</span>
<span class="token comment"># Linear(in_features=10, out_features=5, bias=True)</span>
<span class="token comment"># Linear(in_features=25, out_features=15, bias=True)</span>
<span class="token comment"># Sigmoid()</span>
<span class="token comment"># Linear(in_features=15, out_features=10, bias=True)</span>
</code></pre> 
<p>可能会有读者好奇，给定输入 <code>inputs</code>，它是如何在 <code>seq_3</code> 中进行传递的呢？</p> 
<p>其实很显然，<code>inputs</code> 首先会进入 <code>seq_1</code> 通过一系列模块得到一个输出，该输出会作为 <code>seq_2</code> 的输入，然后通过 <code>seq_2</code> 的一系列模块后又可以得到一个输出，而这个输出就是最终的输出了。</p> 
<p>注意，本节的例子并不能将输入转化为输出，因为形状不匹配，需要修改成类似于如下这种：</p> 
<pre><code class="prism language-python">seq_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
seq_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
seq_3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>seq_1<span class="token punctuation">,</span> seq_2<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="14__241"></a>1.4 自定义层</h2> 
<p><code>Sequential</code> 中的模块又称为<strong>层</strong>，我们完全不必局限于 <code>torch.nn</code> 中提供的各种层，通过继承 <code>nn.Module</code> 我们可以自定义层并将其添加到 <code>Sequential</code> 中。</p> 
<h3>
<a id="141__244"></a>1.4.1 不带参数的层</h3> 
<p>定义一个中心化层，它能够将输入减去其均值后再返回：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">CenteredLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> X <span class="token operator">-</span> X<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>我们可以来检验一下该层是否真的起到了作用：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">,</span> CenteredLayer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># tensor(-5.2982e-09, grad_fn=&lt;MeanBackward0&gt;)</span>
</code></pre> 
<p>输出结果足够小可以近似视为0，说明自定义层起到了作用。</p> 
<h3>
<a id="142__266"></a>1.4.2 带参数的层</h3> 
<p>依旧以单隐层网络为例，大多数时候，我们希望自定义每个层的神经元个数，因此在自定义层时需要传入相应的参数。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_nodes<span class="token punctuation">,</span> hidden_nodes<span class="token punctuation">,</span> output_nodes<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>inodes <span class="token operator">=</span> input_nodes
        self<span class="token punctuation">.</span>hnodes <span class="token operator">=</span> hidden_nodes
        self<span class="token punctuation">.</span>onodes <span class="token operator">=</span> output_nodes
        self<span class="token punctuation">.</span>model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>inodes<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hnodes<span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>self<span class="token punctuation">.</span>hnodes<span class="token punctuation">,</span> self<span class="token punctuation">.</span>onodes<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>model<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
</code></pre> 
<p>分别设置输出层、隐层和输出层结点数为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        784
       
       
        ,
       
       
        256
       
       
        ,
       
       
        8
       
      
      
       784,256,8
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83888em;vertical-align: -0.19444em"></span><span class="mord">7</span><span class="mord">8</span><span class="mord">4</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">8</span></span></span></span></span>：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">42</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># tensor([[ 0.2291, -0.3913, -0.1745, -0.2685, -0.2684,  0.0760,  0.0071, -0.0337],</span>
<span class="token comment">#         [ 0.2084,  0.1235, -0.1054, -0.0508,  0.0194, -0.0429, -0.3269,  0.1890],</span>
<span class="token comment">#         [-0.0756, -0.4335, -0.1643, -0.1817, -0.2376, -0.1399,  0.2710, -0.3719],</span>
<span class="token comment">#         [ 0.4110, -0.2428, -0.1021, -0.1019, -0.0550, -0.0890,  0.1430,  0.0881],</span>
<span class="token comment">#         [ 0.0626, -0.4117,  0.0130,  0.1339, -0.2529, -0.1106, -0.2586,  0.2205]],</span>
<span class="token comment">#        grad_fn=&lt;AddmmBackward0&gt;)</span>
</code></pre> 
<h1>
<a id="_300"></a>二、参数管理</h1> 
<h2>
<a id="21_nnParameter_301"></a>2.1 nn.Parameter</h2> 
<p><code>nn.Parameter</code> 是 <code>Tensor</code> 的子类，可以被视为一种特殊的张量，它可被用作模块的参数，具体使用格式如下：</p> 
<pre><code class="prism language-python">nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>data<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<p>其中 <code>data</code> 为待传入的 <code>Tensor</code>，<code>requires_grad</code> 默认为 True。</p> 
<p>事实上，<code>torch.nn</code> 中提供的模块中的参数均是 <code>nn.Parameter</code> 类，例如：</p> 
<pre><code class="prism language-python">module <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
<span class="token builtin">type</span><span class="token punctuation">(</span>module<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token comment"># torch.nn.parameter.Parameter</span>
<span class="token builtin">type</span><span class="token punctuation">(</span>module<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
<span class="token comment"># torch.nn.parameter.Parameter</span>
</code></pre> 
<p>在我们自定义的模块中，只有使用 <code>nn.Parameter</code> 构建的参数才会被视为模块的参数，此时调用 <code>parameters()</code> 方法会显示这些参数。读者可自行体会以下两端代码：</p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">""" 代码片段一 """</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>


net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># []</span>


<span class="token triple-quoted-string string">""" 代码片段二 """</span>
<span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>


net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># [Parameter containing:</span>
<span class="token comment"># tensor([[-0.4584,  0.3815, -0.4522],</span>
<span class="token comment">#         [ 2.1236,  0.7928, -0.7095],</span>
<span class="token comment">#         [-1.4921, -0.5689, -0.2342]], requires_grad=True), Parameter containing:</span>
<span class="token comment"># tensor([-0.6971, -0.7651,  0.7897], requires_grad=True)]</span>
</code></pre> 
<p>从以上结果可以得知，如果自定义模块中有些参数必须要手动构建而不能使用现成的模块，则最好使用 <code>nn.Parameter</code> 去构建。这样后续查看模块的参数或使用优化器更新模块的参数只需调用 <code>parameters()</code> 方法即可。</p> 
<hr> 
<p><code>nn.Parameter</code> 相当于把传入的数据包装成一个参数，如果要直接访问/使用其中的数据而非参数本身，可对 <code>nn.Parameter</code> 对象调用 <code>data</code> 属性：</p> 
<pre><code class="prism language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
param <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([1., 2., 3.], requires_grad=True)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>data<span class="token punctuation">)</span>
<span class="token comment"># tensor([1., 2., 3.])</span>
</code></pre> 
<h2>
<a id="22__373"></a>2.2 参数访问</h2> 
<p><code>nn.Module</code> 中有 <code>state_dict()</code> 方法（<a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict">官网链接</a>），该方法将以字典形式返回模块的所有状态，包括模块的参数和 <code>persistent buffers</code> （博主目前还不太理解后者，暂时略过），字典的键就是对应的参数/缓冲区的名称。</p> 
<p>由于所有模块都继承 <code>nn.Module</code>，因此我们可以对任意的模块调用 <code>state_dict()</code> 方法以查看状态：</p> 
<pre><code class="prism language-python">linear_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>linear_layer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># OrderedDict([('weight', tensor([[ 0.2602, -0.2318],</span>
<span class="token comment">#         [-0.5192,  0.0130]])), ('bias', tensor([0.5890, 0.2476]))])</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>linear_layer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># odict_keys(['weight', 'bias'])</span>
</code></pre> 
<p>对于线性层，除了 <code>state_dict()</code> 之外，我们还可以对其直接调用相应的属性，如下：</p> 
<pre><code class="prism language-python">linear_layer <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>linear_layer<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([[-0.1990,  0.3394]], requires_grad=True)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>linear_layer<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([0.2697], requires_grad=True)</span>
</code></pre> 
<p>需要注意的是以上返回的均为参数对象，如需使用其中的数据，可调用 <code>data</code> 属性。</p> 
<p>当然我们还可以对 <code>nn.Linear</code> 实例调用 <code>parameters()</code> 和 <code>named_parameters()</code> 方法来获取其中的参数（对任何模块都可以调用这两个方法），具体可参考我的上一篇笔记，这里不再赘述。</p> 
<h2>
<a id="23__401"></a>2.3 参数初始化</h2> 
<p>以神经网络为例，当我们创建一个 <code>nn.Linear(a, b)</code> 的实例后，其中的参数就自动初始化了，其权重和偏置均从均匀分布 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        U
       
       
        (
       
       
        −
       
       
        1
       
       
        /
       
       
        
         a
        
       
       
        ,
       
       
        1
       
       
        /
       
       
        
         a
        
       
       
        )
       
      
      
       U(-1/sqrt{a},1/sqrt{a})
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.05028em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">U</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mord">/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.80028em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord mathdefault">a</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
           
            
           </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.23972em"><span class=""></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mord">/</span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.80028em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord mathdefault">a</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
           
            
           </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.23972em"><span class=""></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> 中随机采样而来。</p> 
<p>但有些时候，我们可能想使用其他的分布进行初始化，这时候可以考虑Pytorch中内置的初始化器 <code>torch.nn.init</code> 或自定义初始化。</p> 
<h3>
<a id="231__408"></a>2.3.1 使用内置初始化</h3> 
<p>对于下面的单隐层网络，我们想对其中的两个线性层应用内置初始化器</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
</code></pre> 
<p>假设权重从 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
       
        (
       
       
        0
       
       
        ,
       
       
        1
       
       
        )
       
      
      
       mathcal{N}(0,1)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathcal" style="margin-right: 0.14736em">N</span></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></span> 中采样，偏置全部初始化为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        0
       
      
      
       0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">0</span></span></span></span></span>，则初始化代码如下</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">init_normal</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 需要判断子模块是否为nn.Linear类，因为激活函数没有参数</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>module<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>zeros_<span class="token punctuation">(</span>module<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_normal<span class="token punctuation">)</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([[-0.3560,  0.8078, -2.4084],</span>
<span class="token comment">#         [ 0.1700, -0.3217, -1.3320]], requires_grad=True)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([0., 0.], requires_grad=True)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([[-0.8025, -1.0695],</span>
<span class="token comment">#         [-1.7031, -0.3068],</span>
<span class="token comment">#         [-0.3499,  0.4263]], requires_grad=True)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([0., 0., 0.], requires_grad=True)</span>
</code></pre> 
<p>对 <code>net</code> 调用 <code>apply</code> 方法则会递归地对其下所有的子模块应用 <code>init_normal</code> 函数。</p> 
<h3>
<a id="232__456"></a>2.3.2 自定义初始化</h3> 
<p>如果我们想要自定义初始化，例如使用以下的分布来初始化网络的权重：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         w
        
        
         ∼
        
        
         
          {
         
         
          
           
            
             
              
               U
              
              
               (
              
              
               5
              
              
               ,
              
              
               10
              
              
               )
              
              
               ,
              
             
            
           
           
            
             
              
               p
              
              
               r
              
              
               o
              
              
               b
              
              
               =
              
              
               0.25
              
             
            
           
          
          
           
            
             
              
               0
              
              
               ,
              
             
            
           
           
            
             
              
               p
              
              
               r
              
              
               o
              
              
               b
              
              
               =
              
              
               0.5
              
             
            
           
          
          
           
            
             
              
               U
              
              
               (
              
              
               −
              
              
               10
              
              
               ,
              
              
               −
              
              
               5
              
              
               )
              
              
               ,
              
             
            
           
           
            
             
              
               p
              
              
               r
              
              
               o
              
              
               b
              
              
               =
              
              
               0.25
              
             
            
           
          
         
        
       
       
         wsim begin{cases} U(5,10),&amp;prob=0.25 \ 0,&amp;prob=0.5\ U(-10,-5),&amp;prob=0.25 \ end{cases} 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">∼</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 4.32em;vertical-align: -1.91em"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.35002em"><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="delimsizinginner delim-size4"><span class="">⎩</span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="delimsizinginner delim-size4"><span class="">⎪</span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="delimsizinginner delim-size4"><span class="">⎨</span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="delimsizinginner delim-size4"><span class="">⎪</span></span></span><span class=""><span class="pstrut" style="height: 3.15em"></span><span class="delimsizinginner delim-size4"><span class="">⎧</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.85002em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.41em"><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em">U</span><span class="mopen">(</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span><span class="mpunct">,</span></span></span><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord">0</span><span class="mpunct">,</span></span></span><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.10903em">U</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">−</span><span class="mord">5</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.91em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.41em"><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mord mathdefault" style="margin-right: 0.02778em">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span></span></span><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mord mathdefault" style="margin-right: 0.02778em">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord mathdefault">p</span><span class="mord mathdefault" style="margin-right: 0.02778em">r</span><span class="mord mathdefault">o</span><span class="mord mathdefault">b</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">2</span><span class="mord">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.91em"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p>即相当于 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        w
       
      
      
       w
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span></span></span></span></span> 从 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        U
       
       
        (
       
       
        −
       
       
        10
       
       
        ,
       
       
        10
       
       
        )
       
      
      
       U(-10, 10)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">U</span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span> 中采样，如果 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        w
       
      
      
       w
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02691em">w</span></span></span></span></span> 落到 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        −
       
       
        5
       
       
        ,
       
       
        5
       
       
        )
       
      
      
       (-5, 5)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">5</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">5</span><span class="mclose">)</span></span></span></span></span> 中，则将其置为 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        0
       
      
      
       0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">0</span></span></span></span></span>。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">my_init</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>module<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
        mask <span class="token operator">=</span> module<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">&gt;=</span> <span class="token number">5</span>
        module<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data <span class="token operator">*=</span> mask
</code></pre> 
<pre><code class="prism language-python">net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>my_init<span class="token punctuation">)</span>
<span class="token keyword">for</span> param <span class="token keyword">in</span> net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([[-0.0000, -5.9610,  8.0000],</span>
<span class="token comment">#         [-0.0000, -0.0000,  7.6041]], requires_grad=True)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([ 0.4058, -0.2891], requires_grad=True)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([[ 0.0000, -0.0000],</span>
<span class="token comment">#         [-6.9569, -9.5102],</span>
<span class="token comment">#         [-9.0270, -0.0000]], requires_grad=True)</span>
<span class="token comment"># Parameter containing:</span>
<span class="token comment"># tensor([ 0.2521, -0.1500, -0.1484], requires_grad=True)</span>
</code></pre> 
<h2>
<a id="24__497"></a>2.4 参数绑定</h2> 
<p>对于一个三隐层网络：</p> 
<pre><code class="prism language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>如果我们想让第二个隐层和第三个隐层共享参数，则可以这样做：</p> 
<pre><code class="prism language-python">shared <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    shared<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    shared<span class="token punctuation">,</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="25__516"></a>2.5 模型保存</h2> 
<p>在讲解模型的保存之前，我们先来看一下张量是如何保存的。</p> 
<h3>
<a id="251__519"></a>2.5.1 张量的保存</h3> 
<p><code>torch.save()</code> 和 <code>torch.load()</code> 可以保存/加载Pytorch中的任何对象，使用格式如下:</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>obj<span class="token punctuation">,</span> path<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>path<span class="token punctuation">)</span>
</code></pre> 
<p>其中 <code>path</code> 需要包含文件名，且扩展名通常选择 <code>.pt</code>。</p> 
<p>以张量为例，保存和加载的步骤如下：</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
path <span class="token operator">=</span> <span class="token string">'./models/my_tensor.pt'</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>t<span class="token punctuation">,</span> path<span class="token punctuation">)</span>

a <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>path<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span>
<span class="token comment"># tensor([1, 2, 3])</span>
</code></pre> 
<p>需要注意的是，如果 <code>models</code> 文件夹不存在则会报错，因此需要先创建好要保存到的目录再进行保存。</p> 
<h3>
<a id="252__542"></a>2.5.2 保存整个模型</h3> 
<p>保存整个模型通常指保存模型的所有参数和整个架构，假设训练好的模型是 <code>model</code>，则保存和加载的方法如下</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'model.pt'</span><span class="token punctuation">)</span>
model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model.pt'</span><span class="token punctuation">)</span>
</code></pre> 
<p>但我们通常不这样做，这是因为保存整个模型通常会占用巨大的空间，绝大多数时候我们仅保存模型的参数。</p> 
<h3>
<a id="253__551"></a>2.5.3 保存模型的参数</h3> 
<p>我们通常会保存 <code>model.state_dict()</code>，如下：</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model_params.pt'</span><span class="token punctuation">)</span>
</code></pre> 
<p>该操作不会保存模型的架构而仅仅是保存参数。若要加载模型，需要先实例化，然后调用 <code>load_state_dict</code> 方法：</p> 
<pre><code class="prism language-python">model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_params.pt'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>注意 <code>model.eval()</code> 是必要的，它可将 <code>dropout</code> 和 <code>batch normalization</code> 层设置为评估模式。</p> 
<h1>
<a id="GPU_566"></a>三、GPU</h1> 
<p>在PyTorch中，CPU和GPU用 <code>torch.device('cpu')</code> 和 <code>torch.device('cuda')</code> 来进行表示。需要注意的是，CPU设备意味着所有物理CPU和内存， 这意味着PyTorch的计算将尝试使用所有CPU核心。 然而，GPU设备只代表一个卡和相应的显存。 如果有多个GPU，我们使用 <code>torch.device('cuda:{}'.format(i))</code> 来表示第 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.65952em;vertical-align: 0em"></span><span class="mord mathdefault">i</span></span></span></span></span> 块GPU（从0开始）。 另外，<code>cuda:0</code> 和 <code>cuda</code> 是等价的。</p> 
<p>我们可以查询可用GPU的数量：</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 1</span>
</code></pre> 
<p>为了使用GPU，需要先声明设备：</p> 
<pre><code class="prism language-python">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="31_GPU_580"></a>3.1 将数据移动到GPU</h2> 
<p>默认情况下，张量是在CPU上进行创建的。</p> 
<p>我们可以直接在创建数据的时候将其移动到GPU上</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
t<span class="token punctuation">.</span>device
<span class="token comment"># device(type='cuda', index=0)</span>
</code></pre> 
<p>也可以创建之后使用 <code>to</code> 方法进行移动（使用 <code>to</code> 方法后会返回一个新的对象）</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
t<span class="token punctuation">.</span>device
<span class="token comment"># device(type='cpu')</span>
t <span class="token operator">=</span> t<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
t<span class="token punctuation">.</span>device
<span class="token comment"># device(type='cuda', index=0)</span>
</code></pre> 
<p><strong>只有一个</strong>GPU时，我们还可以对张量调用 <code>cuda()</code> 方法来返回一个在GPU上的拷贝：</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
t <span class="token operator">=</span> t<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
t<span class="token punctuation">.</span>device
<span class="token comment"># device(type='cuda', index=0)</span>
</code></pre> 
<p>该做法的好处是不需要事先声明设备。</p> 
<h2>
<a id="32_GPU_610"></a>3.2 将模型移动到GPU上</h2> 
<p>我们只有将数据和模型全部移动到GPU上才可以在GPU上进行训练。</p> 
<pre><code class="prism language-python"><span class="token triple-quoted-string string">""" 方法一 """</span>
device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span><span class="token punctuation">)</span>
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

<span class="token triple-quoted-string string">""" 方法二 """</span>
net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span>
net<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>