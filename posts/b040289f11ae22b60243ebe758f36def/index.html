<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Transformer的基本原理 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Transformer的基本原理</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="1_Seq2Seq_0"></a>1. Seq2Seq框架</h1> 
<h2>
<a id="11_Seq2Seq_1"></a>1.1. Seq2Seq框架概述</h2> 
<p>Seq2Seq[1]框架最初是在神经机器翻译（Neural Machine Translation，NMT）领域中提出，用于将一种语言（sequence）翻译成另一种语言（sequence）。其结构如下图所示：</p> 
<p><img src="https://images2.imgbox.com/5a/c5/ERq1wvHU_o.png" alt="在这里插入图片描述"></p> 
<p>在Seq2Seq框架中包含了Encoder和Decoder两个部分。在Encoder阶段，通过神经网络将原始的输入<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        {
       
       
        
         x
        
        
         1
        
       
       
        ,
       
       
        
         x
        
        
         2
        
       
       
        ,
       
       
        ⋯
        
       
        ,
       
       
        
         x
        
        
         
          T
         
         
          x
         
        
       
       
        }
       
      
      
       left { x_1,x_2,cdots,x_{T_x}right }
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0001em;vertical-align: -0.2501em"></span><span class="minner"><span class="mopen delimcenter">{<!-- --></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1645em"><span class="" style="margin-left: -0.1389em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2501em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">}</span></span></span></span></span></span>转换成固定长度的中间向量<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        {
       
       
        
         c
        
        
         1
        
       
       
        ,
       
       
        
         c
        
        
         2
        
       
       
        ,
       
       
        ⋯
        
       
        ,
       
       
        
         c
        
        
         l
        
       
       
        }
       
      
      
       left { c_1,c_2,cdots,c_l right }
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="minner"><span class="mopen delimcenter">{<!-- --></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">}</span></span></span></span></span></span>，在Decoder阶段，将此中间向量作为输入，得到最终的输出<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        {
       
       
        
         y
        
        
         1
        
       
       
        ,
       
       
        
         y
        
        
         2
        
       
       
        ,
       
       
        ⋯
        
       
        ,
       
       
        
         y
        
        
         
          T
         
         
          y
         
        
       
       
        }
       
      
      
       left { y_1,y_2,cdots,y_{T_y} right }
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.2em;vertical-align: -0.35em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">{<!-- --></span></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1645em"><span class="" style="margin-left: -0.1389em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em">y</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2819em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3473em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size1">}</span></span></span></span></span></span></span>。</p> 
<h2>
<a id="12__8"></a>1.2. 建模方法</h2> 
<p>在Encoder和Decoder部分，需要模型能够对时序数据建模，在NLP中，通常使用两种方式对时序数据建模，一种是以RNN[2]，LSTM[3]为主的建模方法；另一种是以CNN[4]，[5]为主的建模方法。</p> 
<p>以RNN为例，其基本机构如下图所示：</p> 
<p><img src="https://images2.imgbox.com/38/fb/H9tVipwN_o.png" alt="在这里插入图片描述"></p> 
<p>在基于RNN的建模方法中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻的状态更新依赖于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
       
        −
       
       
        1
       
      
      
       t-1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6984em;vertical-align: -0.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">1</span></span></span></span></span>时刻的输出，即<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻的状态更新公式为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          h
         
         
          t
         
        
        
         =
        
        
         f
        
        
         
          (
         
         
          U
         
         
          
           h
          
          
           
            t
           
           
            −
           
           
            1
           
          
         
         
          +
         
         
          W
         
         
          
           x
          
          
           t
          
         
         
          +
         
         
          b
         
         
          )
         
        
       
       
        h_t=fleft ( Uh_{t-1}+Wx_t+b right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.1076em">f</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathnormal" style="margin-right: 0.109em">U</span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal">b</span><span class="mclose delimcenter">)</span></span></span></span></span></span></span></p> 
<p>在RNN的基础上衍生出很多优化的方案，如对于长距离依赖问题的优化，提出了LSTM以及GRU等模型；对于单向建模能力的问题，提出了双向的RNN模型，提升了对时序数据的建模能力。以简单的RNN为例，从上可以看出，RNN最大的问题是不容易并行化。因为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻的状态更新依赖于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
       
        −
       
       
        1
       
      
      
       t-1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6984em;vertical-align: -0.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">1</span></span></span></span></span>时刻的输出，所以必须先计算出<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
       
        −
       
       
        1
       
      
      
       t-1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6984em;vertical-align: -0.0833em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">1</span></span></span></span></span>时刻的输出。</p> 
<p>第二种是CNN的建模方法，以TextCNN[4]，[5]模型为例：</p> 
<p><img src="https://images2.imgbox.com/b4/5c/hySWCSek_o.png" alt="在这里插入图片描述"></p> 
<p>以最外层的红色为例，设置不同的filter的大小<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
      
      
       N
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span></span></span></span></span>，如上图中filter的大小为2，通过filter的移动，可以计算filter内<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
      
      
       N
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span></span></span></span></span>个词之间的相互依赖关系。与RNN相比，基于CNN的建模方法中，filter的计算是完全可以并行计算，是对RNN计算效率的极大提高。CNN与RNN对词的建模可以通过下图[6]进一步说明。</p> 
<p><img src="https://images2.imgbox.com/2a/24/RzC3YyKM_o.png" alt="在这里插入图片描述"></p> 
<p>从图中可以看出，CNN和RNN都是对变长序列的一种“局部编码”：卷积神经网络是基于N-gram的局部编码；而对于循环神经网络，由于梯度消失等问题也只能建立短距离依赖。要解决这种短距离依赖的“局部编码”问题，建立输入序列之间的长距离依赖关系，可以使用以下两种方法：一种方法是增加网络的层数，通过一个深层网络来获取远距离的信息交互，另一种方法是使用全连接网络[6]。全连接网络如下图所示：</p> 
<p><img src="https://images2.imgbox.com/9c/83/FfjjT3Se_o.png" alt="在这里插入图片描述"></p> 
<p>然而，全连接网络虽然可以对远距离依赖建模，但是无法处理变长的输入序列，同时，在全连接网络中，缺失了词之间的顺序信息。不同的输入长度，其连接权重的大小也是不同的。</p> 
<p>综上，基于RNN，CNN以及全连接网络建模方法存在着以下的问题：</p> 
<ul>
<li>长距离依赖问题（RNN，CNN）</li>
<li>并行问题（RNN）</li>
<li>变长输入问题，词序信息问题（全连接网络）</li>
</ul> 
<h2>
<a id="13_SelfAttention_41"></a>1.3. Self-Attention</h2> 
<p>为了能提升Seq2Seq框架的性能，在Seq2Seq框架中引入了Attention机制[7]，Attention机制通过对训练数据的学习，对其输入<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        x
       
      
      
       mathbf{x}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4444em"></span><span class="mord mathbf">x</span></span></span></span></span>的每一个特征赋予不同的权重，从而学习到对于目标更重要的信息，让模型具有更高的准确率。在Seq2Seq中引入Attention机制如下图所示：</p> 
<p><img src="https://images2.imgbox.com/35/d3/8cSefa24_o.jpg" alt="在这里插入图片描述"></p> 
<p>其中，Attention的计算体现在针对不同的Decoder输出<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         y
        
        
         t
        
       
      
      
       y_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，都有一个对应的上下文向量<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         c
        
        
         t
        
       
      
      
       c_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         y
        
        
         t
        
       
      
      
       y_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的计算公式为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          y
         
         
          t
         
        
        
         =
        
        
         f
        
        
         
          (
         
         
          
           y
          
          
           
            t
           
           
            −
           
           
            1
           
          
         
         
          ,
         
         
          
           s
          
          
           
            t
           
           
            −
           
           
            1
           
          
         
         
          ,
         
         
          
           c
          
          
           t
          
         
         
          )
         
        
       
       
        y_t=fleft ( y_{t-1},s_{t-1},c_t right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.1076em">f</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         c
        
        
         t
        
       
      
      
       c_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          c
         
         
          i
         
        
        
         =
        
        
         
          ∑
         
         
          
           j
          
          
           =
          
          
           1
          
         
         
          
           T
          
          
           x
          
         
        
        
         
          α
         
         
          
           i
          
          
           j
          
         
        
        
         
          h
         
         
          j
         
        
       
       
        c_i=sum_{j=1}^{T_x}alpha _{ij}h_j
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">c</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 3.2532em;vertical-align: -1.4138em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.8394em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1645em"><span class="" style="margin-left: -0.1389em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.4138em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0037em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         α
        
        
         
          i
         
         
          j
         
        
       
      
      
       alpha _{ij}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0037em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span></span></span></span>为归一化权重，其具体为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          α
         
         
          
           i
          
          
           j
          
         
        
        
         =
        
        
         
          
           e
          
          
           x
          
          
           p
          
          
           
            (
           
           
            
             e
            
            
             
              i
             
             
              j
             
            
           
           
            )
           
          
         
         
          
           
            ∑
           
           
            
             k
            
            
             =
            
            
             1
            
           
           
            
             T
            
            
             x
            
           
          
          
           e
          
          
           x
          
          
           p
          
          
           
            (
           
           
            
             e
            
            
             
              i
             
             
              k
             
            
           
           
            )
           
          
         
        
       
       
        alpha _{ij}=frac{expleft ( e_{ij} right )}{sum_{k=1}^{T_x}expleft ( e_{ik} right )}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0037em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 2.5979em;vertical-align: -1.1709em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.427em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.9812em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1645em"><span class="" style="margin-left: -0.1389em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">ik</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.1709em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<p>其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         e
        
        
         
          i
         
         
          j
         
        
       
      
      
       e_{ij}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span></span></span></span>表示的是第<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        i
       
      
      
       i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6595em"></span><span class="mord mathnormal">i</span></span></span></span></span>个输出前一个隐藏层状态<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         s
        
        
         
          i
         
         
          −
         
         
          1
         
        
       
      
      
       s_{i-1}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6389em;vertical-align: -0.2083em"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span></span></span></span></span>与第<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        j
       
      
      
       j
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.854em;vertical-align: -0.1944em"></span><span class="mord mathnormal" style="margin-right: 0.0572em">j</span></span></span></span></span>个输入隐层向量<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         h
        
        
         j
        
       
      
      
       h_j
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.9805em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span></span></span></span>之间的相关性，可以通过一个MLP神经网络进行计算，即：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          e
         
         
          
           i
          
          
           j
          
         
        
        
         =
        
        
         a
        
        
         
          (
         
         
          
           s
          
          
           
            i
           
           
            −
           
           
            1
           
          
         
         
          ,
         
         
          
           h
          
          
           j
          
         
         
          )
         
        
        
         =
        
        
         
          v
         
         
          a
         
         
          T
         
        
        
         t
        
        
         a
        
        
         n
        
        
         h
        
        
         
          (
         
         
          
           W
          
          
           a
          
         
         
          
           s
          
          
           
            i
           
           
            −
           
           
            1
           
          
         
         
          +
         
         
          
           U
          
          
           a
          
         
         
          
           h
          
          
           j
          
         
         
          )
         
        
       
       
        e_{ij}=aleft ( s_{i-1},h_j right )=v_a^Ttanhleft ( W_as_{i-1}+U_ah_j right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.7167em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.0361em;vertical-align: -0.2861em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.1774em;vertical-align: -0.2861em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8913em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em"><span class=""></span></span></span></span></span></span><span class="mord mathnormal">t</span><span class="mord mathnormal">anh</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.109em">U</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: -0.109em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span></span></span></span></span></span></p> 
<p>上述的公式也表示了一般性的Attention的计算过程，即：</p> 
<ul>
<li>计算Attention得分：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          e
         
         
          i
         
        
        
         =
        
        
         a
        
        
         
          (
         
         
          u
         
         
          ,
         
         
          
           v
          
          
           i
          
         
         
          )
         
        
       
       
        e_i=aleft ( mathbf{u} ,mathbf{v} _i right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathbf">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.016em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.016em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span></span></span></span></span>
</li>
<li>归一化：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          α
         
         
          i
         
        
        
         =
        
        
         
          
           e
          
          
           i
          
         
         
          
           
            ∑
           
           
            i
           
          
          
           
            e
           
           
            i
           
          
         
        
       
       
        alpha _i=frac{e_i}{sum_{i}e_i }
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5806em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0037em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.2815em;vertical-align: -0.57em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.7115em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1496em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3214em"><span class=""></span></span></span></span></span></span><span class="mspace mtight" style="margin-right: 0.1952em"></span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3281em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.57em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>
</li>
<li>输出：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         c
        
        
         =
        
        
         
          ∑
         
         
          i
         
        
        
         
          α
         
         
          i
         
        
        
         
          v
         
         
          i
         
        
       
       
        c=sum_{i} alpha _imathbf{v} _i
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">c</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.0497em;vertical-align: -0.2997em"></span><span class="mop"><span class="mop op-symbol small-op">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.162em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2997em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0037em">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.0037em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.016em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.016em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>
</li>
</ul> 
<p>通常，一个Attention函数可以被描述成一个映射，其中该映射的输入是一个query和一组key-value对，其具体过程可以通过如下的图表示[8]：</p> 
<p><img src="https://images2.imgbox.com/25/36/3CgN5X2B_o.png" alt="在这里插入图片描述"></p> 
<p>与上式对应，Query为上式中的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        u
       
      
      
       mathbf{u}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4444em"></span><span class="mord mathbf">u</span></span></span></span></span>，Key=Value为上式中的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         v
        
        
         i
        
       
      
      
       mathbf{v} _i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5944em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.016em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.016em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>。Self-Attention是一种特殊的Attention机制，即对于Query，为每一个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         v
        
        
         i
        
       
      
      
       mathbf{v} _i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.5944em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathbf" style="margin-right: 0.016em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: -0.016em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，即Query=Key=Value。以句子“The animal didn’t cross the street because it was too tired”[9]为例，计算Self-Attention的过程如下图所示：</p> 
<p><img src="https://images2.imgbox.com/30/79/95dHwWxB_o.png" alt="在这里插入图片描述"></p> 
<p>以右侧的词为Query，Query与左侧的每一个Key计算Attention得分，从上图可以看出，Query（it_）与Key（animal_）的Attention得分比较大。Self-Attention的一般形式为：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        s
       
       
        o
       
       
        f
       
       
        t
       
       
        m
       
       
        a
       
       
        x
       
       
        
         (
        
        
         X
        
        
         
          X
         
         
          T
         
        
        
         )
        
       
       
        X
       
      
      
       softmaxleft ( XX^T right ) X
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.2em;vertical-align: -0.35em"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size1">(</span></span><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span></span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size1">)</span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0785em">X</span></span></span></span></span>，Self-Attention的整个过程可由下图表示：</p> 
<p><img src="https://images2.imgbox.com/05/c3/48xZjEuQ_o.png" alt="在这里插入图片描述"><br> 与上述全连接不同的是，Self-Attention不再受变长输入的影响。</p> 
<h2>
<a id="13_Transformer_81"></a>1.3. Transformer概述</h2> 
<p>Self-Attention的提出解决了传统RNN模型的长距离依赖，不易并行的问题。虽然Self-Attention有这些优点，但是基本的Self-Attention本身并不能捕获词序信息，Google于2017年提出了解决Seq2Seq问题的Transformer模型[10]，用Self-Attention的结构完全代替了传统的基于RNN的建模方法，同时在Transformer的模块中加入了词序的信息，最终在翻译任务上取得了比RNN更好的成绩。</p> 
<p>在Transformer中依旧保留了Seq2Seq的Encoder+Decoder框架，在Encoder阶段对源文本编码，生成Embedding，记为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        c
       
      
      
       c
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">c</span></span></span></span></span>，在Decoder阶段，综合已生成的文本与Encoder阶段的Embedding，已生成的文本记为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         y
        
        
         1
        
       
       
        ,
       
       
        
         y
        
        
         2
        
       
       
        ,
       
       
        ⋯
        
       
        ,
       
       
        
         y
        
        
         
          t
         
         
          −
         
         
          1
         
        
       
      
      
       y_1,y_2,cdots ,y_{t-1}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6389em;vertical-align: -0.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span></span></span></span></span>，生成当前的词<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         y
        
        
         t
        
       
      
      
       y_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，即：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          y
         
         
          t
         
        
        
         =
        
        
         f
        
        
         
          (
         
         
          
           y
          
          
           t
          
         
         
          ∣
         
         
          
           y
          
          
           1
          
         
         
          ,
         
         
          
           y
          
          
           2
          
         
         
          ,
         
         
          ⋯
          
         
          ,
         
         
          
           y
          
          
           
            t
           
           
            −
           
           
            1
           
          
         
         
          ,
         
         
          c
         
         
          )
         
        
       
       
        y_t=fleft ( y_tmid y_1,y_2,cdots ,y_{t-1},c right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.1944em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.1076em">f</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">∣</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.0359em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal">c</span><span class="mclose delimcenter">)</span></span></span></span></span></span></span></p> 
<p>对于传统不带Attention的Seq2Seq框架中，Encoder阶段生成Embedding是固定不变的，如下图所示：</p> 
<p><img src="https://images2.imgbox.com/7d/2d/o1kN1Cuy_o.png" alt="在这里插入图片描述"></p> 
<p>对于带有Attention的Seq2Seq框架中，Encoder阶段生成Embedding会根据当前需要预测的值计算一个动态的Embedding，具体如下图所示：</p> 
<p><img src="https://images2.imgbox.com/8e/85/9k788VOI_o.png" alt="在这里插入图片描述"></p> 
<p>对于Transformer框架中的Encoder，会采用第二种方案。</p> 
<h1>
<a id="2__98"></a>2. 算法原理</h1> 
<p>Transformer的网络结构如下图所示：<br> <img src="https://images2.imgbox.com/a6/a6/xYFrznrN_o.jpg" alt="在这里插入图片描述"><br> 和大多数的Seq2Seq模型一样，在Transformer的结构中，同样是由Encoder（上图中的左侧部分）和Decoder（上图中的右侧部分）两个部分组成。以<strong>TensorFlow Core</strong>[11]的代码讲解为例子，帮助理解Transformer的整个结构。</p> 
<h2>
<a id="21_Encoder_103"></a>2.1. Encoder</h2> 
<p>Encoder部分的结构如下图所示：</p> 
<p><img src="https://images2.imgbox.com/6a/b5/VBqeHuMD_o.png" alt="在这里插入图片描述"><br> 在Encoder部分，通过堆叠多个特定模块（如图中Nx部分），在文章[10]中，选择<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
       
        =
       
       
        6
       
      
      
       N=6
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">6</span></span></span></span></span>。在该模块中，每个Layer由两个sub-layer组成，分别为Multi-Head Self-Attention和Feed Forward Network，在两个sub-layer中，都增加了残差连接和Layer Normalization操作，残差连接和Layer Normalization的作用是便于构建深层的网络，防止梯度弥散现象的出现。在参考文献[11]中的代码如下所示：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> input_vocab_size<span class="token punctuation">,</span>
               maximum_position_encoding<span class="token punctuation">,</span> rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model <span class="token comment"># 向量的维度</span>
    self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers <span class="token comment"># 编码层的层数，上图中的Nx部分</span>

    self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>input_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span> <span class="token comment"># 生成词向量</span>
    self<span class="token punctuation">.</span>pos_encoding <span class="token operator">=</span> positional_encoding<span class="token punctuation">(</span>maximum_position_encoding<span class="token punctuation">,</span> 
                                            self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span> <span class="token comment"># 生成位置编码</span>

    self<span class="token punctuation">.</span>enc_layers <span class="token operator">=</span> <span class="token punctuation">[</span>EncoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> rate<span class="token punctuation">)</span> 
                       <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># 每一个编码层函数</span>

    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span> <span class="token comment"># dropout</span>

  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> training<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>

    seq_len <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment"># 文本长度</span>

    <span class="token comment"># 将嵌入和位置编码相加。</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, input_seq_len, d_model) 词向量</span>
    x <span class="token operator">*=</span> tf<span class="token punctuation">.</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 参见下面注意点</span>
    x <span class="token operator">+=</span> self<span class="token punctuation">.</span>pos_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 与位置编码相加</span>

    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span> <span class="token comment"># dropout</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
      x <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> training<span class="token punctuation">,</span> mask<span class="token punctuation">)</span> <span class="token comment"># 每一个独立层的输出作为下一个独立层的输入</span>

    <span class="token keyword">return</span> x  <span class="token comment"># (batch_size, input_seq_len, d_model)</span>
</code></pre> 
<blockquote> 
 <p>注：在文献[10]中提到在Embedding层，乘以权重<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          
           d
          
          
           
            m
           
           
            o
           
           
            d
           
           
            e
           
           
            l
           
          
         
        
       
       
        sqrt{d_{model}}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.04em;vertical-align: -0.1828em"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
            
             
            </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em"><span class=""></span></span></span></span></span></span></span></span></span>。</p> 
</blockquote> 
<h3>
<a id="211__146"></a>2.1.1. 输入</h3> 
<p>在Transformer中摒弃了RNN的模型，使用基于Self-Attention模型，相比于RNN模型，基于Self-Attention的模型能够缓解长距离依赖以及并行的问题，然而，一般的Self-Attention模型中是无法对词序建模的，词序对于文本理解是尤为重要的，因此在文章[10]中，作者提到了两种位置编码（Positional Encoding）方法，然后将词的Embedding和位置的Embedding相加，作为最终的输入Embedding。两种位置编码分别为：</p> 
<ul>
<li>用不同频率的sin和cos函数计算</li>
<li>学习出Positional Embedding</li>
</ul> 
<p>通过经过实验发现两者的结果一样，最终作者选择了第一种方法：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         P
        
        
         
          E
         
         
          
           (
          
          
           p
          
          
           o
          
          
           s
          
          
           ,
          
          
           2
          
          
           i
          
          
           )
          
         
        
        
         =
        
        
         sin
        
        
         ⁡
        
        
         
          (
         
         
          p
         
         
          o
         
         
          s
         
         
          /
         
         
          100
         
         
          
           0
          
          
           
            2
           
           
            i
           
           
            /
           
           
            
             d
            
            
             
              m
             
             
              o
             
             
              d
             
             
              e
             
             
              l
             
            
           
          
         
         
          )
         
        
       
       
        PE_{left ( pos,2i right ) }=sin left ( pos/1000^{2i/d_{model}} right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0385em;vertical-align: -0.3552em"></span><span class="mord mathnormal" style="margin-right: 0.1389em">P</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: -0.0576em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen mtight delimcenter"><span class="mtight">(</span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mclose mtight delimcenter"><span class="mtight">)</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3552em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.8em;vertical-align: -0.65em"></span><span class="mop">sin</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size2">(</span></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/100</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size2">)</span></span></span></span></span></span></span></span></p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         P
        
        
         
          E
         
         
          
           (
          
          
           p
          
          
           o
          
          
           s
          
          
           ,
          
          
           2
          
          
           i
          
          
           +
          
          
           1
          
          
           )
          
         
        
        
         =
        
        
         cos
        
        
         ⁡
        
        
         
          (
         
         
          p
         
         
          o
         
         
          s
         
         
          /
         
         
          100
         
         
          
           0
          
          
           
            2
           
           
            i
           
           
            /
           
           
            
             d
            
            
             
              m
             
             
              o
             
             
              d
             
             
              e
             
             
              l
             
            
           
          
         
         
          )
         
        
       
       
        PE_{left ( pos,2i+1 right ) }=cos left ( pos/1000^{2i/d_{model}} right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.0385em;vertical-align: -0.3552em"></span><span class="mord mathnormal" style="margin-right: 0.1389em">P</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0576em">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: -0.0576em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="minner mtight"><span class="mopen mtight delimcenter"><span class="mtight">(</span></span><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mclose mtight delimcenter"><span class="mtight">)</span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.3552em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1.8em;vertical-align: -0.65em"></span><span class="mop">cos</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size2">(</span></span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mord">/100</span><span class="mord"><span class="mord">0</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight">i</span><span class="mord mtight">/</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mclose delimcenter"><span class="delimsizing size2">)</span></span></span></span></span></span></span></span></p> 
<p>位置编码的代码在文献[11]中为：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_angles</span><span class="token punctuation">(</span>pos<span class="token punctuation">,</span> i<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 计算函数内的部分</span>
  angle_rates <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>power<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span> <span class="token operator">*</span> <span class="token punctuation">(</span>i<span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span>float32<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> pos <span class="token operator">*</span> angle_rates

<span class="token keyword">def</span> <span class="token function">positional_encoding</span><span class="token punctuation">(</span>position<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">:</span>
  angle_rads <span class="token operator">=</span> get_angles<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>newaxis<span class="token punctuation">]</span><span class="token punctuation">,</span>
                          np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span><span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                          d_model<span class="token punctuation">)</span>

  <span class="token comment"># 将 sin 应用于数组中的偶数索引（indices）；2i</span>
  angle_rads<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>angle_rads<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token comment"># 将 cos 应用于数组中的奇数索引；2i+1</span>
  angle_rads<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> np<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>angle_rads<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  pos_encoding <span class="token operator">=</span> angle_rads<span class="token punctuation">[</span>np<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span> <span class="token comment"># 最终的位置编码向量</span>

  <span class="token keyword">return</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>pos_encoding<span class="token punctuation">,</span> dtype<span class="token operator">=</span>tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
</code></pre> 
<p>通过原始词向量和位置向量相加，便得到了最终的带有位置信息的词向量。</p> 
<h3>
<a id="222_MultiHead_SelfAttention_183"></a>2.2.2. Multi-Head Self-Attention</h3> 
<p>得到了词向量的序列后，假设为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        X
       
      
      
       mathrm{X}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathrm">X</span></span></span></span></span>，在Transformer中，通过线性变换分别得到<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Q
       
      
      
       mathrm{Q}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord mathrm">Q</span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        K
       
      
      
       mathrm{K}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathrm">K</span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        V
       
      
      
       mathrm{V}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathrm" style="margin-right: 0.0139em">V</span></span></span></span></span>，其计算过程如下图[9]所示：</p> 
<p><img src="https://images2.imgbox.com/94/7a/o2YE5R2W_o.png" alt="在这里插入图片描述"><br> 这里使用线性变换得到<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Q
       
      
      
       mathrm{Q}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord mathrm">Q</span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        K
       
      
      
       mathrm{K}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathrm">K</span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        V
       
      
      
       mathrm{V}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathrm" style="margin-right: 0.0139em">V</span></span></span></span></span>是为了进一步提升模型的拟合能力。</p> 
<p>在Transformer中，使用的是Scaled Dot-Product Attention，其具体计算方法为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         A
        
        
         t
        
        
         t
        
        
         e
        
        
         n
        
        
         t
        
        
         i
        
        
         o
        
        
         n
        
        
         
          (
         
         
          Q
         
         
          ,
         
         
          K
         
         
          ,
         
         
          V
         
         
          )
         
        
        
         =
        
        
         s
        
        
         o
        
        
         f
        
        
         t
        
        
         m
        
        
         a
        
        
         x
        
        
         
          (
         
         
          
           
            Q
           
           
            
             K
            
            
             T
            
           
          
          
           
            
             d
            
            
             k
            
           
          
         
         
          )
         
        
        
         V
        
       
       
        Attentionleft ( Q,K,V right )=softmaxleft ( frac{QK^T}{sqrt{d_k} }right )V
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 2.4684em;vertical-align: -0.95em"></span><span class="mord mathnormal">so</span><span class="mord mathnormal" style="margin-right: 0.1076em">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size3">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.5183em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8572em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord" style="padding-left: 0.833em"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail" style="min-width: 0.853em;height: 1.08em">
                    
                     
                    </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1828em"><span class=""></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em">T</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.93em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose delimcenter"><span class="delimsizing size3">)</span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span></span></span></span></span></span></p> 
<p>对于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         1
        
        
         
          
           d
          
          
           k
          
         
        
       
      
      
       frac{1}{sqrt{d_k} }
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.3831em;vertical-align: -0.538em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8451em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.8622em"><span class="svg-align"><span class="pstrut" style="height: 3em"></span><span class="mord mtight" style="padding-left: 0.833em"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: 0em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1512em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="hide-tail mtight" style="min-width: 0.853em;height: 1.08em">
                   
                    
                   </span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1778em"><span class=""></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.538em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>，其最主要的目的是对点积缩放。</p> 
<blockquote> 
 <p><strong>引参考文献[11]</strong>：假设Q和K的均值为0，方差为1。它们的矩阵乘积将有均值为0，方差为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          d
         
         
          k
         
        
       
       
        d_k
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>，因此使用<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          d
         
         
          k
         
        
       
       
        d_k
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的平方根被用于缩放。因为Q和K的矩阵乘积的均值本应该为0，方差本应该为1，这样可以获得更平缓的softmax。当维度很大时，点积结果会很大，会导致softmax的梯度很小。为了减轻这个影响，对点积进行缩放。</p> 
</blockquote> 
<p>计算过程可由下图表示：</p> 
<p><img src="https://images2.imgbox.com/59/29/fVeuXX5Q_o.jpg" alt="在这里插入图片描述"></p> 
<p>Scaled Dot-Product Attention模块的代码在文献[11]中为：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">scaled_dot_product_attention</span><span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""计算注意力权重。
  q, k, v 必须具有匹配的前置维度。
  k, v 必须有匹配的倒数第二个维度，例如：seq_len_k = seq_len_v。
  虽然 mask 根据其类型（填充或前瞻）有不同的形状，
  但是 mask 必须能进行广播转换以便求和。

  参数:
    q: 请求的形状 == (..., seq_len_q, depth)
    k: 主键的形状 == (..., seq_len_k, depth)
    v: 数值的形状 == (..., seq_len_v, depth_v)
    mask: Float 张量，其形状能转换成
          (..., seq_len_q, seq_len_k)。默认为None。

  返回值:
    输出，注意力权重
  """</span>

  matmul_qk <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> transpose_b<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token comment"># (..., seq_len_q, seq_len_k), Q*K^T</span>

  <span class="token comment"># 缩放 matmul_qk</span>
  dk <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>k<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token comment"># 计算dk</span>
  scaled_attention_logits <span class="token operator">=</span> matmul_qk <span class="token operator">/</span> tf<span class="token punctuation">.</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>dk<span class="token punctuation">)</span> <span class="token comment"># 缩放</span>

  <span class="token comment"># 将 mask 加入到缩放的张量上。</span>
  <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
    scaled_attention_logits <span class="token operator">+=</span> <span class="token punctuation">(</span>mask <span class="token operator">*</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>  

  <span class="token comment"># softmax 在最后一个轴（seq_len_k）上归一化，因此分数</span>
  <span class="token comment"># 相加等于1。</span>
  attention_weights <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scaled_attention_logits<span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># (..., seq_len_q, seq_len_k) # 归一化</span>

  output <span class="token operator">=</span> tf<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>attention_weights<span class="token punctuation">,</span> v<span class="token punctuation">)</span>  <span class="token comment"># (..., seq_len_q, depth_v) # 乘以V</span>

  <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention_weights
</code></pre> 
<p>通过多个Scaled Dot-Product Attention模块的组合，就形成了Multi-Head Self-Attention，其过程如下图所示：</p> 
<p><img src="https://images2.imgbox.com/03/bc/xO5IUsve_o.jpg" alt="在这里插入图片描述"><br> 其过程可以表示为：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         M
        
        
         u
        
        
         l
        
        
         t
        
        
         i
        
        
         H
        
        
         e
        
        
         a
        
        
         d
        
        
         
          (
         
         
          Q
         
         
          ,
         
         
          K
         
         
          ,
         
         
          V
         
         
          )
         
        
        
         =
        
        
         C
        
        
         o
        
        
         n
        
        
         c
        
        
         a
        
        
         t
        
        
         
          (
         
         
          h
         
         
          e
         
         
          a
         
         
          
           d
          
          
           1
          
         
         
          ,
         
         
          ⋯
          
         
          ,
         
         
          h
         
         
          e
         
         
          a
         
         
          
           d
          
          
           h
          
         
         
          )
         
        
        
         
          W
         
         
          o
         
        
       
       
        MultiHeadleft ( Q,K,V right ) =Concatleft ( head_1,cdots, head_h right ) W^o
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.109em">M</span><span class="mord mathnormal">u</span><span class="mord mathnormal">lt</span><span class="mord mathnormal">i</span><span class="mord mathnormal">He</span><span class="mord mathnormal">a</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">C</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner">⋯</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.7144em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">o</span></span></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>其中，每一个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        h
       
       
        e
       
       
        a
       
       
        
         d
        
        
         i
        
       
      
      
       head_i
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord mathnormal">h</span><span class="mord mathnormal">e</span><span class="mord mathnormal">a</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>就是一个Scaled Dot-Product Attention。Multi-head Attention相当于多个不同的Scaled Dot-Product Attention的集成，引入Multi-head Attention可以扩大模型的表征能力，同时这里面的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        h
       
      
      
       h
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6944em"></span><span class="mord mathnormal">h</span></span></span></span></span>个Scaled Dot-Product Attention模块是可以并行的，没有层与层之间的依赖，相比于RNN，可以提升效率。</p> 
<p>Multi-head Attention模块的代码在文献[11]中为：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadAttention</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>num_heads <span class="token operator">=</span> num_heads <span class="token comment"># h的个数</span>
    self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model <span class="token comment"># 向量的维度</span>

    <span class="token keyword">assert</span> d_model <span class="token operator">%</span> self<span class="token punctuation">.</span>num_heads <span class="token operator">==</span> <span class="token number">0</span>

    self<span class="token punctuation">.</span>depth <span class="token operator">=</span> d_model <span class="token operator">//</span> self<span class="token punctuation">.</span>num_heads

    self<span class="token punctuation">.</span>wq <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span> <span class="token comment"># 权重矩阵，用于和Q相乘</span>
    self<span class="token punctuation">.</span>wk <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span> <span class="token comment"># 权重矩阵，用于和K相乘</span>
    self<span class="token punctuation">.</span>wv <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span> <span class="token comment"># 权重矩阵，用于和V相乘</span>

    self<span class="token punctuation">.</span>dense <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span> <span class="token comment"># 权重矩阵，用于最终输出</span>

  <span class="token keyword">def</span> <span class="token function">split_heads</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""分拆最后一个维度到 (num_heads, depth).
    转置结果使得形状为 (batch_size, num_heads, seq_len, depth)
    """</span>
    x <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_heads<span class="token punctuation">,</span> self<span class="token punctuation">.</span>depth<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>x<span class="token punctuation">,</span> perm<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> v<span class="token punctuation">,</span> k<span class="token punctuation">,</span> q<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    batch_size <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>

    q <span class="token operator">=</span> self<span class="token punctuation">.</span>wq<span class="token punctuation">(</span>q<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len, d_model)</span>
    k <span class="token operator">=</span> self<span class="token punctuation">.</span>wk<span class="token punctuation">(</span>k<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len, d_model)</span>
    v <span class="token operator">=</span> self<span class="token punctuation">.</span>wv<span class="token punctuation">(</span>v<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len, d_model)</span>

    q <span class="token operator">=</span> self<span class="token punctuation">.</span>split_heads<span class="token punctuation">(</span>q<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, num_heads, seq_len_q, depth)</span>
    k <span class="token operator">=</span> self<span class="token punctuation">.</span>split_heads<span class="token punctuation">(</span>k<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, num_heads, seq_len_k, depth)</span>
    v <span class="token operator">=</span> self<span class="token punctuation">.</span>split_heads<span class="token punctuation">(</span>v<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, num_heads, seq_len_v, depth)</span>

    <span class="token comment"># scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)</span>
    <span class="token comment"># attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)</span>
    scaled_attention<span class="token punctuation">,</span> attention_weights <span class="token operator">=</span> scaled_dot_product_attention<span class="token punctuation">(</span>
        q<span class="token punctuation">,</span> k<span class="token punctuation">,</span> v<span class="token punctuation">,</span> mask<span class="token punctuation">)</span> <span class="token comment"># 计算Scaled dot product attention</span>

    scaled_attention <span class="token operator">=</span> tf<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>scaled_attention<span class="token punctuation">,</span> perm<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, num_heads, depth)</span>
	
	<span class="token comment"># 将多个scaled attention通过concat连接在一起</span>
    concat_attention <span class="token operator">=</span> tf<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>scaled_attention<span class="token punctuation">,</span> 
                                  <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, d_model)</span>

    output <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>concat_attention<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len_q, d_model) # 得到最终的输出</span>

    <span class="token keyword">return</span> output<span class="token punctuation">,</span> attention_weights
</code></pre> 
<h3>
<a id="223_Layer_Normalization_303"></a>2.2.3. Layer Normalization</h3> 
<p>Layer Normalization是针对每条样本进行归一化，可以对Transformer学习过程中由于Embedding累加可能带来的“尺度”问题加以约束，相当于对表达每个词一词多义的空间加以约束，有效降低模型方差。在TF中可以使用<code>tf.keras.layers.LayerNormalization()</code>函数直接实现Layer Normalization功能。在Transformer中，Layer Normalization是对残差连接后的结果进行归一化，具体公式如下所示：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         L
        
        
         a
        
        
         y
        
        
         e
        
        
         r
        
        
         N
        
        
         o
        
        
         r
        
        
         m
        
        
         
          (
         
         
          x
         
         
          +
         
         
          S
         
         
          u
         
         
          b
         
         
          l
         
         
          a
         
         
          y
         
         
          e
         
         
          r
         
         
          
           (
          
          
           x
          
          
           )
          
         
         
          )
         
        
       
       
        LayerNormleft ( x+Sublayerleft ( x right ) right )
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal">L</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0278em">yer</span><span class="mord mathnormal" style="margin-right: 0.109em">N</span><span class="mord mathnormal" style="margin-right: 0.0278em">or</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal" style="margin-right: 0.0576em">S</span><span class="mord mathnormal">u</span><span class="mord mathnormal">b</span><span class="mord mathnormal" style="margin-right: 0.0197em">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal" style="margin-right: 0.0278em">yer</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathnormal">x</span><span class="mclose delimcenter">)</span></span><span class="mclose delimcenter">)</span></span></span></span></span></span></span></p> 
<h3>
<a id="224_Positionwise_FFN_308"></a>2.2.4. Position-wise FFN</h3> 
<p>Position-wise Feed Forward Network就是一个全连接网络，在Transformer中，这个部分包含了两个FFN网络，可以由下述的公式表示：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         F
        
        
         F
        
        
         N
        
        
         
          (
         
         
          x
         
         
          )
         
        
        
         =
        
        
         max
        
        
         ⁡
        
        
         
          (
         
         
          0
         
         
          ,
         
         
          x
         
         
          
           W
          
          
           1
          
         
         
          +
         
         
          
           b
          
          
           1
          
         
         
          )
         
        
        
         
          W
         
         
          2
         
        
        
         +
        
        
         
          b
         
         
          2
         
        
       
       
        FFNleft ( x right )=max left ( 0, xW_1+b_1 right ) W_2+b_2
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.109em">FFN</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathnormal">x</span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mop">max</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.8444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>Position-wise FFN模块的代码在文献[11]中为：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">point_wise_feed_forward_network</span><span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dff<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">return</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
      tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>dff<span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment"># (batch_size, seq_len, dff) # 全联接1，激活函数为relu</span>
      tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>d_model<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, seq_len, d_model) # 全联接2</span>
  <span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>组合上述的多个部分，最终形成了Encoder模块部分，其代码在文献[11]中为：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>mha <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span> <span class="token comment"># 多头Attention</span>
    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> point_wise_feed_forward_network<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dff<span class="token punctuation">)</span> <span class="token comment"># position-wise FFN</span>

    self<span class="token punctuation">.</span>layernorm1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LayerNormalization<span class="token punctuation">(</span>epsilon<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span> <span class="token comment"># layer-normalization</span>
    self<span class="token punctuation">.</span>layernorm2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LayerNormalization<span class="token punctuation">(</span>epsilon<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span> <span class="token comment"># layer-normalization</span>

    self<span class="token punctuation">.</span>dropout1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span> <span class="token comment"># dropout</span>
    self<span class="token punctuation">.</span>dropout2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span> <span class="token comment"># dropout</span>

  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> training<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>

    attn_output<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>mha<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, input_seq_len, d_model) 计算Attention</span>
    attn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout1<span class="token punctuation">(</span>attn_output<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span> <span class="token comment"># 对结果dropout</span>
    out1 <span class="token operator">=</span> self<span class="token punctuation">.</span>layernorm1<span class="token punctuation">(</span>x <span class="token operator">+</span> attn_output<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, input_seq_len, d_model) # 残差连接+layer-normalization</span>

    ffn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>out1<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, input_seq_len, d_model) position-wise FFN</span>
    ffn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout2<span class="token punctuation">(</span>ffn_output<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span> <span class="token comment"># 对结果dropout</span>
    out2 <span class="token operator">=</span> self<span class="token punctuation">.</span>layernorm2<span class="token punctuation">(</span>out1 <span class="token operator">+</span> ffn_output<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, input_seq_len, d_model) # 残差连接+layer-normalization</span>

    <span class="token keyword">return</span> out2
</code></pre> 
<h2>
<a id="22_Decoder_352"></a>2.2. Decoder</h2> 
<p>Decoder部分的结构如下图所示：</p> 
<p><img src="https://images2.imgbox.com/b1/ae/uQSAuT2a_o.jpg" alt="在这里插入图片描述"><br> 在Decoder部分，通过堆叠多个特定模块（如图中Nx部分），在文章[10]中，选择<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
       
        =
       
       
        6
       
      
      
       N=6
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">6</span></span></span></span></span>。在该模块中，每个Layer由三个sub-layer组成，分别为Masked Multi-Head Self-Attention，Multi-Head Self-Attention和Feed Forward Network，与Encoder中的sub-layer一样，每个sub-layer都增加了残差连接和Layer Normalization操作。与Encoder中不一样的地方主要有两个：</p> 
<ul>
<li>增加了Masked Multi-Head Self-Attention这个sub-layer</li>
<li>Multi-Head Self-Attention的输入不一样，在Encoder中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         Q
        
       
       
        Q
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span>，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         K
        
       
       
        K
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         V
        
       
       
        V
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span></span></span></span></span>的值都是一样的，而在Decoder中的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         Q
        
       
       
        Q
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span>的值来自本身的输入向量，而<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         K
        
       
       
        K
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         V
        
       
       
        V
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span></span></span></span></span>则来自于Encoder的输出</li>
</ul> 
<p>Decoder部分的代码在参考文献[11]中如下所示：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> target_vocab_size<span class="token punctuation">,</span>
               maximum_position_encoding<span class="token punctuation">,</span> rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>d_model <span class="token operator">=</span> d_model <span class="token comment"># 向量维度</span>
    self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers <span class="token comment"># 解码器的层数，上图中的Nx</span>

    self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>target_vocab_size<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span> <span class="token comment"># 输入的词映射成Embedding</span>
    self<span class="token punctuation">.</span>pos_encoding <span class="token operator">=</span> positional_encoding<span class="token punctuation">(</span>maximum_position_encoding<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span> <span class="token comment"># 输入词的位置编码</span>

    self<span class="token punctuation">.</span>dec_layers <span class="token operator">=</span> <span class="token punctuation">[</span>DecoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> rate<span class="token punctuation">)</span> 
                       <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token comment"># 解码层</span>
    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span> <span class="token comment"># dropout</span>

  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> enc_output<span class="token punctuation">,</span> training<span class="token punctuation">,</span> 
           look_ahead_mask<span class="token punctuation">,</span> padding_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>

    seq_len <span class="token operator">=</span> tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token comment"># 输入句子长度</span>
    attention_weights <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span><span class="token punctuation">}</span>

    x <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, target_seq_len, d_model) # 输入词向量</span>
    x <span class="token operator">*=</span> tf<span class="token punctuation">.</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 同encoder的输入操作</span>
    x <span class="token operator">+=</span> self<span class="token punctuation">.</span>pos_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 与位置编码相加</span>

    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span> <span class="token comment"># dropout</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 解码层</span>
      x<span class="token punctuation">,</span> block1<span class="token punctuation">,</span> block2 <span class="token operator">=</span> self<span class="token punctuation">.</span>dec_layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> enc_output<span class="token punctuation">,</span> training<span class="token punctuation">,</span>
                                             look_ahead_mask<span class="token punctuation">,</span> padding_mask<span class="token punctuation">)</span>

      attention_weights<span class="token punctuation">[</span><span class="token string">'decoder_layer{}_block1'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> block1
      attention_weights<span class="token punctuation">[</span><span class="token string">'decoder_layer{}_block2'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> block2

    <span class="token comment"># x.shape == (batch_size, target_seq_len, d_model)</span>
    <span class="token keyword">return</span> x<span class="token punctuation">,</span> attention_weights
</code></pre> 
<p>最终，在Encoder阶段，会针对输入中的每一个词产出一个Embedding表示，正如上述代码中的<code>x</code>的大小为<code>[target_seq_len, d_model]</code>。</p> 
<h3>
<a id="221_Decoder_404"></a>2.2.1. Decoder部分的输入</h3> 
<p>Decoder部分的作用是在<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻，根据上下文信息（即Encoder中对源文本的编码信息）以及<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻之前已生成好的文本，得到当前<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻的输出，因此，在Decoder模块中，输入分为两个部分，一部分是Encoder部分的输出，一部分是Seq2Seq的目标Seq的Embedding，其中源Seq的Embedding输入到Encoder中。对于Seq的处理与Encoder中一致，详细为：</p> 
<pre><code class="prism language-python">x <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, target_seq_len, d_model) # 输入词向量</span>
x <span class="token operator">*=</span> tf<span class="token punctuation">.</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 同encoder的输入操作</span>
x <span class="token operator">+=</span> self<span class="token punctuation">.</span>pos_encoding<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token comment"># 与位置编码相加</span>
</code></pre> 
<h3>
<a id="222_Masked_MultiHead_SelfAttention_413"></a>2.2.2. Masked Multi-Head Self-Attention</h3> 
<p>Masked是Transformer中很重要的概念，其实在Transformer中存在两种Mask。Mask的含义是掩码，它能掩藏某些值，使得模型在参数更新时对模型掩藏。Transformer中包含了两种Mask，分别是padding mask和sequence mask。其中，padding mask在所有的Scaled Dot-Product Attention里面都需要用到，而sequence mask只有在Decoder的Masked Multi-Head Self-Attention里面用到。</p> 
<blockquote> 
 <p>Masked Language Model：即对文本中随机掩盖（mask）部分词，并通过训练语言模型，将masked掉的词填充好，以此训练语言模型。</p> 
</blockquote> 
<ul><li>padding mask</li></ul> 
<p>对于输入序列都要进行padding补齐，也就是说设定一个统一的句子长度<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
      
      
       N
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span></span></span></span></span>，对于橘子长度不满<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
      
      
       N
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span></span></span></span></span>的序列后面填充<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        0
       
      
      
       0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">0</span></span></span></span></span>，如果输入的序列长度大于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
      
      
       N
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span></span></span></span></span>，则截取左边长度为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
      
      
       N
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span></span></span></span></span>的内容，把多余的直接舍弃。对于padding补齐，对于填充值<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        0
       
      
      
       0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">0</span></span></span></span></span>的位置，最终在该位置mask输出为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        1
       
      
      
       1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">1</span></span></span></span></span>，否则输出为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        0
       
      
      
       0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">0</span></span></span></span></span>。在参考文献[11]中代码为：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">create_padding_mask</span><span class="token punctuation">(</span>seq<span class="token punctuation">)</span><span class="token punctuation">:</span>
  seq <span class="token operator">=</span> tf<span class="token punctuation">.</span>cast<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>math<span class="token punctuation">.</span>equal<span class="token punctuation">(</span>seq<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token comment"># 填充0</span>

  <span class="token comment"># 添加额外的维度来将填充加到</span>
  <span class="token comment"># 注意力对数（logits）。</span>
  <span class="token keyword">return</span> seq<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> tf<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> tf<span class="token punctuation">.</span>newaxis<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># (batch_size, 1, 1, seq_len)</span>
</code></pre> 
<p>在Self-Attention的计算过程中，对于mask为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        1
       
      
      
       1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">1</span></span></span></span></span>的位置，具体的做法是，把这些位置的值加上一个非常大的负数，这样经过Softmax后，这些位置的权重就会接近<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        0
       
      
      
       0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">0</span></span></span></span></span>，具体如<code>scaled_dot_product_attention</code>函数中所示：</p> 
<pre><code class="prism language-python"><span class="token comment"># 将 mask 加入到缩放的张量上。</span>
<span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
  scaled_attention_logits <span class="token operator">+=</span> <span class="token punctuation">(</span>mask <span class="token operator">*</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>  
</code></pre> 
<ul><li>sequence mask</li></ul> 
<p>sequence mask是为了使Decoder模块不能看见未来的信息。在Decoder模块中，希望在<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻，只利用Encoder的输出以及<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻之前的输出，而需要对<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻以及<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻之后的信息隐藏起来。具体做法就是产生一个上三角矩阵，上三角的值全为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        0
       
      
      
       0
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6444em"></span><span class="mord">0</span></span></span></span></span>，把这个矩阵作用在每一个序列上。在参考文献[11]中代码为：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">create_look_ahead_mask</span><span class="token punctuation">(</span>size<span class="token punctuation">)</span><span class="token punctuation">:</span>
  mask <span class="token operator">=</span> <span class="token number">1</span> <span class="token operator">-</span> tf<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>band_part<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span>size<span class="token punctuation">,</span> size<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
  <span class="token keyword">return</span> mask  <span class="token comment"># (seq_len, seq_len)</span>
</code></pre> 
<p>两个部分的mask组合在一起，在参考文献[11]中的代码如下所示：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">create_masks</span><span class="token punctuation">(</span>inp<span class="token punctuation">,</span> tar<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token comment"># 编码器填充遮挡</span>
  enc_padding_mask <span class="token operator">=</span> create_padding_mask<span class="token punctuation">(</span>inp<span class="token punctuation">)</span> <span class="token comment"># 编码器的padding</span>

  <span class="token comment"># 在解码器的第二个注意力模块使用。</span>
  <span class="token comment"># 该填充遮挡用于遮挡编码器的输出。</span>
  dec_padding_mask <span class="token operator">=</span> create_padding_mask<span class="token punctuation">(</span>inp<span class="token punctuation">)</span> <span class="token comment"># 解码器的padding</span>

  <span class="token comment"># 在解码器的第一个注意力模块使用。</span>
  <span class="token comment"># 用于填充（pad）和遮挡（mask）解码器获取到的输入的后续标记（future tokens）。</span>
  look_ahead_mask <span class="token operator">=</span> create_look_ahead_mask<span class="token punctuation">(</span>tf<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>tar<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment"># 编码器的mask</span>
  dec_target_padding_mask <span class="token operator">=</span> create_padding_mask<span class="token punctuation">(</span>tar<span class="token punctuation">)</span> <span class="token comment"># 解码器对target做padding</span>
  combined_mask <span class="token operator">=</span> tf<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span>dec_target_padding_mask<span class="token punctuation">,</span> look_ahead_mask<span class="token punctuation">)</span> <span class="token comment"># mask</span>

  <span class="token keyword">return</span> enc_padding_mask<span class="token punctuation">,</span> combined_mask<span class="token punctuation">,</span> dec_padding_mask
</code></pre> 
<h3>
<a id="223_Decoder_469"></a>2.2.3. Decoder的核心部分</h3> 
<p>对于Decoder的核心部分，包含了<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        N
       
       
        x
       
      
      
       Nx
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.109em">N</span><span class="mord mathnormal">x</span></span></span></span></span>组的多种Attention的堆叠，对于每一组的结构如下图所示：</p> 
<p><img src="https://images2.imgbox.com/0c/07/bIf6Y0TR_o.png" alt="在这里插入图片描述"></p> 
<p>对于Masked Multi-Head Attention，其输入为target的Embedding与Position Embedding的和，其输出作为Multi-Head Attention的输入<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Q
       
      
      
       Q
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord mathnormal">Q</span></span></span></span></span>，而Multi-Head Attention的输入<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        K
       
      
      
       K
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        V
       
      
      
       V
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span></span></span></span></span>则是来自于Encoder的输出。Decoder阶段与Encoder阶段的不同的是，在Encoder阶段可以做并行计算，但是在Decoder阶段，需要根据<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻前面的输出预测<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻的值，而<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6151em"></span><span class="mord mathnormal">t</span></span></span></span></span>时刻前面的输出即为Masked Multi-Head Attention的输入。Decoder过程在参考文献[11]中的代码如下所示：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Layer<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>mha1 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>mha2 <span class="token operator">=</span> MultiHeadAttention<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>ffn <span class="token operator">=</span> point_wise_feed_forward_network<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dff<span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>layernorm1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LayerNormalization<span class="token punctuation">(</span>epsilon<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>layernorm2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LayerNormalization<span class="token punctuation">(</span>epsilon<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>layernorm3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>LayerNormalization<span class="token punctuation">(</span>epsilon<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>dropout1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dropout2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dropout3 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>rate<span class="token punctuation">)</span>


  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> enc_output<span class="token punctuation">,</span> training<span class="token punctuation">,</span> 
           look_ahead_mask<span class="token punctuation">,</span> padding_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># enc_output.shape == (batch_size, input_seq_len, d_model)</span>

    attn1<span class="token punctuation">,</span> attn_weights_block1 <span class="token operator">=</span> self<span class="token punctuation">.</span>mha1<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> look_ahead_mask<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, target_seq_len, d_model) # Decoder部分的输入，经过第一个Masked Multi-Head Attention</span>
    attn1 <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout1<span class="token punctuation">(</span>attn1<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span> <span class="token comment"># dropout</span>
    out1 <span class="token operator">=</span> self<span class="token punctuation">.</span>layernorm1<span class="token punctuation">(</span>attn1 <span class="token operator">+</span> x<span class="token punctuation">)</span> <span class="token comment"># layer normalization和残差连接</span>

    attn2<span class="token punctuation">,</span> attn_weights_block2 <span class="token operator">=</span> self<span class="token punctuation">.</span>mha2<span class="token punctuation">(</span>
        enc_output<span class="token punctuation">,</span> enc_output<span class="token punctuation">,</span> out1<span class="token punctuation">,</span> padding_mask<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, target_seq_len, d_model) # 第二个Multi-Head Attention</span>
    attn2 <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout2<span class="token punctuation">(</span>attn2<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span> <span class="token comment"># dropout</span>
    out2 <span class="token operator">=</span> self<span class="token punctuation">.</span>layernorm2<span class="token punctuation">(</span>attn2 <span class="token operator">+</span> out1<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, target_seq_len, d_model) # layer normalization和残差连接</span>

    ffn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>ffn<span class="token punctuation">(</span>out2<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, target_seq_len, d_model) # 全连接</span>
    ffn_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout3<span class="token punctuation">(</span>ffn_output<span class="token punctuation">,</span> training<span class="token operator">=</span>training<span class="token punctuation">)</span> <span class="token comment"># dropout</span>
    out3 <span class="token operator">=</span> self<span class="token punctuation">.</span>layernorm3<span class="token punctuation">(</span>ffn_output <span class="token operator">+</span> out2<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, target_seq_len, d_model) # layer normalization和残差连接</span>

    <span class="token keyword">return</span> out3<span class="token punctuation">,</span> attn_weights_block1<span class="token punctuation">,</span> attn_weights_block2
</code></pre> 
<h2>
<a id="23__515"></a>2.3. 模型训练</h2> 
<p>有了上述的Encoder和Decoder模块，对于一个完整的Seq2Seq框架，需要综合这两个部分的逻辑，完整的Transformer的代码在参考文献[11]为：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Transformer</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_layers<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> input_vocab_size<span class="token punctuation">,</span> 
               target_vocab_size<span class="token punctuation">,</span> pe_input<span class="token punctuation">,</span> pe_target<span class="token punctuation">,</span> rate<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>Transformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span>num_layers<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> 
                           input_vocab_size<span class="token punctuation">,</span> pe_input<span class="token punctuation">,</span> rate<span class="token punctuation">)</span> <span class="token comment"># Encoder模块</span>

    self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span>num_layers<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> num_heads<span class="token punctuation">,</span> dff<span class="token punctuation">,</span> 
                           target_vocab_size<span class="token punctuation">,</span> pe_target<span class="token punctuation">,</span> rate<span class="token punctuation">)</span> <span class="token comment"># Decoder模块</span>

    self<span class="token punctuation">.</span>final_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>target_vocab_size<span class="token punctuation">)</span> <span class="token comment"># 最后的全连接层</span>

  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inp<span class="token punctuation">,</span> tar<span class="token punctuation">,</span> training<span class="token punctuation">,</span> enc_padding_mask<span class="token punctuation">,</span> 
           look_ahead_mask<span class="token punctuation">,</span> dec_padding_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>

    enc_output <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>inp<span class="token punctuation">,</span> training<span class="token punctuation">,</span> enc_padding_mask<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, inp_seq_len, d_model) # Encoder模块的输出</span>

    <span class="token comment"># dec_output.shape == (batch_size, tar_seq_len, d_model)</span>
    dec_output<span class="token punctuation">,</span> attention_weights <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>
        tar<span class="token punctuation">,</span> enc_output<span class="token punctuation">,</span> training<span class="token punctuation">,</span> look_ahead_mask<span class="token punctuation">,</span> dec_padding_mask<span class="token punctuation">)</span> <span class="token comment"># Decoder模块的输出</span>

    final_output <span class="token operator">=</span> self<span class="token punctuation">.</span>final_layer<span class="token punctuation">(</span>dec_output<span class="token punctuation">)</span>  <span class="token comment"># (batch_size, tar_seq_len, target_vocab_size) # 全连接输出</span>

    <span class="token keyword">return</span> final_output<span class="token punctuation">,</span> attention_weights
</code></pre> 
<h1>
<a id="3__546"></a>3. 总结</h1> 
<p>Transformer对基于递归神经网络RNN的Seq2Seq模型的巨大改进。在文本序列的学习中能够更好的提取文本中的信息，在Seq2Seq的任务中取得较好的结果。但Transformer自身也存在一定的局限性，最主要的是注意力只能处理固定长度的文本字符串，这对于长文本来说会丢失很多信息。</p> 
<h1>
<a id="_549"></a>参考文献</h1> 
<p>[1] Cho K, Merrienboer B V, Gulcehre C, et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation[J]. Computer Science, 2014.</p> 
<p>[2] <a href="http://felixzhao.cn/Articles/article/6">循环神经网络RNN</a></p> 
<p>[3] <a href="http://felixzhao.cn/Articles/article/7">长短期记忆网络LSTM</a></p> 
<p>[4] <a href="http://felixzhao.cn/Articles/article/12">CNN在文本建模中的应用TextCNN</a></p> 
<p>[5] Y. Kim, “Convolutional neural networks for sentence classification,” in Proceedings of EMNLP 2014</p> 
<p>[6] <a href="https://nndl.github.io/">神经网络与深度学习</a></p> 
<p>[7] Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.</p> 
<p>[8] <a href="https://zhuanlan.zhihu.com/p/265108616">Attention注意力机制与self-attention自注意力机制</a></p> 
<p>[9] <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p> 
<p>[10] Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[J]. Advances in neural information processing systems, 2017, 30.</p> 
<p>[11] <a href="https://tensorflow.google.cn/tutorials/text/transformer?hl=zh-cn">理解语言的 Transformer 模型</a></p> 
<p>[11] <a href="https://zhuanlan.zhihu.com/p/363466672">transformer面试题的简单回答</a></p> 
<p>[12] <a href="https://zhuanlan.zhihu.com/p/53682800">nlp中的Attention注意力机制+Transformer详解</a></p> 
<p>[13] <a href="https://zhuanlan.zhihu.com/p/37601161">深度学习中的注意力模型（2017版）</a></p> 
<p>[14] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016.</p> 
<p>[15] <a href="https://zhuanlan.zhihu.com/p/54530247">模型优化之Layer Normalization</a></p> 
<p>[16] <a href="https://zhuanlan.zhihu.com/p/410776234">超详细图解Self-Attention</a></p> 
<p>[17] <a href="https://zhuanlan.zhihu.com/p/104393915">【经典精读】Transformer模型和Attention机制</a></p> 
<p>[18] <a href="https://www.cnblogs.com/zhouxiaosong/p/11032431.html">Transformer解析与tensorflow代码解读</a></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>