<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>OTA: Optimal Transport Assignment for Object Detection原理与代码解读 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">OTA: Optimal Transport Assignment for Object Detection原理与代码解读</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p><strong>paper：<a href="https://arxiv.org/abs/2103.14259" title="OTA: Optimal Transport Assignment for Object Detection">OTA: Optimal Transport Assignment for Object Detection</a></strong></p> 
<p><strong>code：<a href="https://github.com/Megvii-BaseDetection/OTA" title="https://github.com/Megvii-BaseDetection/OTA">https://github.com/Megvii-BaseDetection/OTA</a> </strong></p> 
<h1>背景</h1> 
<p>标签分配（Label Assignment）是目标检测中重要的一环，经典的标签分配策略采用预定义的规则为每个anchor匹配对应的gt或背景类。比如RetinaNet采用IoU作为划分正负样本的阈值标准，anchor-free检测器比如FCOS将ground truth物体的bbox内或bbox中心区域内的anchor point作为正样本。这种静态分配策略忽略了这样一个事实，即对于不同大小、形状、遮挡状态的对象，最适合的正负样本划分的边界可能是不同的。 </p> 
<p id="ufa9b2cbd">基于此很多动态分配方法被提出，比如ATSS基于统计特征为每个gt设置划分边界，Freeanchor、Autoassign、PAA等方法提出anchor的预测分数可以作为一个合适的指标用来设计动态分配策略。</p> 
<p id="u6f965675">但是，不考虑上下文单独的为每个gt分配正负样本的方法可能不是最优的。对于模糊的anchor，即可能作为正样本分配给多个gt的anchor，现有的策略都是基于人工定义的准则，比如Min Area或Max IoU。作者指出把ambiguous anchor分配给任一个gt，对其他gt的学习都是不利的（introduce harmful gradients w.r.t. other gts），因此分配还需要更多的信息。一个更好的分配策略应该摆脱对每个gt单独追求最优分配的思想，转而全局最优的思想，找到一张图像中所有gt的综合最优分配策略。</p> 
<h1>本文的创新点</h1> 
<p id="u50ae241b">本文提出把标签分配当做最优传输问题，具体是把每个gt定义成一个supplier，它可以提供一定数量的label。把每个anchor定义成demander，它需要一个label。如果一个anchor从某个gt那得到了足够数量的positive label，这个anchor就被当做这个gt的一个正样本。每个gt可以提供的positive label的数量可以理解为这个gt在训练过程中需要多少个正样本来更好的收敛。每对anchor-gt的传输cost定义为它们之间的分类和回归loss的加权和。此外，背景类也被定义为supplier，它提供negative label，anchor-background之间的传输cost定义为它们之间的分类loss。这样标签分配问题就被转化为了最优传输问题，最终是为了找到全局最优的分配方法而不再是为每个gt单独寻找最优anchor。</p> 
<h1>具体方法</h1> 
<h2>Optimal Transport</h2> 
<p>最优传输问题可以表述为：假设有 (m) 个supplier和 (n) 个demander，第 (i) 个supplier有 (s_{i}) 个物品，第 (j) 个demander需要 (d_{j}) 个物品，每个物品从第 (i) 个supplier运到第 (j) 个demander的运输运输成本为 (c_{ij})，最优传输的目标是找到一个最优传输方案 (pi^{*}=left { pi_{i,j}|i=1,2,...m,j=1,2,...n right } ) 能以最小的运输成本把所有的物品从supplier运输到demander。</p> 
<h2>OT for Label Assignment</h2> 
<p>对于目标检测问题，假设一张图片有 (m) 个gt和 (n) 个anchor（所有FPN level加起来），每个gt当做一个supplier，持有 (k) 个正标签 ((i.e.,s_{i}=k,i=1,2,...,m))，每个anchor当做一个demander，需要一个标签 ((i.e.,d_{j}=1,j=1,2,...,n))。从 (gt_{i}) 传输一个正标签到anchor (a_{j}) 的运输成本 (f^{fg}) 定义为它们之间的分类损失和回归损失的加权和</p> 
<p style="text-align:center"><img alt="" height="83" src="https://images2.imgbox.com/c8/12/TqmuqeME_o.png" width="553"></p> 
<p>其中 (theta) 是模型参数，(P_{j}^{cls}) 和 (P_{j}^{reg}) 分别表示anchor (a_{j}) 的预测的分类得分和bounding box。(G_{i}^{cls}) 和 (G_{i}^{box})  分别表示 (gt_{i}) 的ground truth类别和bounding box。(L_{cls}) 和 (L_{reg}) 分别表示交叉熵loss和IoU loss，也可以分别替换成Focal loss和GIoU/Smooth L1 loss，(alpha) 是权重系数。</p> 
<p>此外，还有另一种提供负标签的supplier，背景类。在标准的最优传输问题中，<strong>supply的数量和demand的数量是相等的</strong>。因此背景类一共可以提供 (n-mtimes k) 个负标签，从背景类传输一个负标签到 (a_{j}) 的成本为</p> 
<p style="text-align:center"><img alt="" height="40" src="https://images2.imgbox.com/05/22/clfnn5VO_o.png" width="521"></p> 
<p>其中 (oslash) 表示背景类，把 (c^{bg}in mathbb{R}^{1times n}) 拼接到 (c^{fg}in mathbb{R}^{mtimes n}) 的最后一行即得到了完整的cost matrix (cin mathbb{R}^{(m+1)times n})。supply vector (s) 需要按下式更新</p> 
<p style="text-align:center"><img alt="" height="75" src="https://images2.imgbox.com/44/16/w3bWzs9p_o.png" width="504"></p> 
<p>现在有了cost matrix (c)，supply vector (sin mathbb{R}^{m+1})，demand vector (din mathbb{R}^{n})，则最优传输路径 (pi^{*}in mathbb{R}^{(m+1)times n}) 可通过现有的Sinkhorn-Knopp Iteration算法求得。得到 (pi^{*}) 后，对应的标签分配就是<strong>将每个anchor分配给传输给这个anchor最多标签的gt</strong>。 </p> 
<h2>Advanced Designs</h2> 
<h3>Center Prior</h3> 
<p id="uc1ca4247">center prior即只从gt的中心有限区域挑选正样本，而不是整个bounding box范围内选择。强迫模型关注潜在positive areas即中心区域有助于稳定训练，特别是在训练的早期阶段，模型的最终性能也会更好。作者发现center prior对OTA的训练也有帮助，因此引入了center prior策略。</p> 
<p>具体做法是，对于每个gt，只挑选每个FPN层中距离bounding box中心最近的 (r^{2}) 个anchor，对于bounding box内 (r^{2}) 之外的anchor，cost matrix中对应的cost会加上一个<strong>额外的常数项cost</strong>，这样就减少了训练阶段它们被分配为正样本的概率。 </p> 
<h3>Dynamic (k) Estimation</h3> 
<p>每个gt需要的正样本数量应该是不同的并且基于很多因素，比如物体大小、尺度、遮挡情况等。由于很难将这些因素和所需anchor数量直接映射起来，本文提出了一种简单有效的方法，根据预测框和对应gt的IoU值来粗略估计每个gt合适的正样本数量。具体来说，对于每个gt，选择IoU最大的 (q) 个个预测，将这 (q) 个IoU值的和作为这个gt正样本数量的粗略估计值。这样做是基于直觉：某个gt的所需合适的postive anchor数量与和这个gt拟合的很好的anchor的数量正相关。</p> 
<p></p> 
<p id="u529960f3">OTA的完整流程如下图所示</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/81/8f/bg6wvUPh_o.png"></p> 
<p>包含center prior和dynamic k estimation的完整流程伪代码如下所示</p> 
<p style="text-align:center"><img alt="" height="619" src="https://images2.imgbox.com/2c/03/oDiGsAf6_o.png" width="433"></p> 
<p></p> 
<h1>代码解读</h1> 
<p>这里batch_size=2，输入shape=(2, 3, 1085, 800)，前景loss权重系数 (alpha=1.5)，center prior超参 (r=2.5)，dynamic (k) estmation中 (q=20)。</p> 
<p>其中line96计算前景loss和中的<strong> 1e6*(1-is_in_boxes.float()) </strong>就是中心区域外的anchor额外加的常数项cost，line105将背景的cost拼接到前景cost矩阵最后就得到了最终的cost matrix，这里的loss就是cost matrix。mu和nu分别是上面的supply vector (s) 和 demand vector (d)。</p> 
<p id="u273d1395">核心代码如下，加了一些注释，其中sinkhorn算法没有专门了解原理，这里就直接用吧。</p> 
<pre><code class="language-python">    def get_ground_truth(self, shifts, targets, box_cls, box_delta, box_iou):
        # shifts
        # [[(13600,2),(3400,2),(850,2),(221,2),(63,2)],
        #  [(13600,2),(3400,2),(850,2),(221,2),(63,2)]]
        # targets
        # [Instances(num_instances=2, image_height=1085, image_width=800,
        #     fields=[gt_boxes = Boxes(tensor([[216.9492, 217.0000, 605.6497, 965.1979], [246.3277, 160.4896, 501.6949, 641.9583]], device='cuda:0')),
        #             gt_classes = tensor([12, 14], device='cuda:0'), ]),
        #  Instances(num_instances=2, image_height=1085, image_width=800,
        #     fields=[gt_boxes = Boxes(tensor([[216.9492, 217.0000, 605.6497, 965.1979], [246.3277, 160.4896, 501.6949, 641.9583]], device='cuda:0')),
        #             gt_classes = tensor([12, 14], device='cuda:0'), ])]

        gt_classes = []
        gt_shifts_deltas = []
        gt_ious = []
        assigned_units = []

        box_cls = [permute_to_N_HWA_K(x, self.num_classes) for x in box_cls]
        # [(2,13600,20),(2,3400,20),(2,850,20),(2,221,20),(2,63,20)]
        box_delta = [permute_to_N_HWA_K(x, 4) for x in box_delta]
        # [(2,13600,4),(2,3400,4),(2,850,4),(2,221,4),(2,63,4)]
        box_iou = [permute_to_N_HWA_K(x, 1) for x in box_iou]
        # [(2,13600,1),(2,3400,1),(2,850,1),(2,221,1),(2,63,1)]

        box_cls = torch.cat(box_cls, dim=1)  # (2,18134,20)
        box_delta = torch.cat(box_delta, dim=1)  # (2,18134,4)
        box_iou = torch.cat(box_iou, dim=1)  # (2,18134,1)

        for shifts_per_image, targets_per_image, box_cls_per_image, 
                box_delta_per_image, box_iou_per_image in zip(
                    shifts, targets, box_cls, box_delta, box_iou):

            shifts_over_all = torch.cat(shifts_per_image, dim=0)  # (18134,2)

            gt_boxes = targets_per_image.gt_boxes  # (2,4)

            # In gt box and center.
            deltas = self.shift2box_transform.get_deltas(
                shifts_over_all, gt_boxes.tensor.unsqueeze(1))  # (18134,2),(2,1,4) -&gt; (2,18134,4)
            is_in_boxes = deltas.min(dim=-1).values &gt; 0.01  # (2,18134)

            center_sampling_radius = 2.5
            centers = gt_boxes.get_centers()  # (2,2),
            # tensor([[388.7006, 591.0990],
            #         [425.9887, 401.2239]], device='cuda:0')
            # 因为数据增强的, gt_bboxes和centers每次运行结果都会变化

            is_in_centers = []
            for stride, shifts_i in zip(self.fpn_strides, shifts_per_image):  # [8, 16, 32, 64, 128], _
                radius = stride * center_sampling_radius
                center_boxes = torch.cat((
                    torch.max(centers - radius, gt_boxes.tensor[:, :2]),
                    torch.min(centers + radius, gt_boxes.tensor[:, 2:]),
                ), dim=-1)  # (2,4)
                center_deltas = self.shift2box_transform.get_deltas(
                    shifts_i, center_boxes.unsqueeze(1))  # (13600,2),(2,1,4) -&gt; (2,13600,4)
                is_in_centers.append(center_deltas.min(dim=-1).values &gt; 0)
            is_in_centers = torch.cat(is_in_centers, dim=1)  # (2,18134)
            del centers, center_boxes, deltas, center_deltas
            is_in_boxes = (is_in_boxes &amp; is_in_centers)

            num_gt = len(targets_per_image)
            num_anchor = len(shifts_over_all)
            shape = (num_gt, num_anchor, -1)  # (2,18134,-1)

            gt_cls_per_image = F.one_hot(
                targets_per_image.gt_classes, self.num_classes
            ).float()  # (2,20)

            with torch.no_grad():
                loss_cls = sigmoid_focal_loss_jit(
                    box_cls_per_image.unsqueeze(0).expand(shape),  # (18134,20)-&gt;(1,18134,20)-&gt;(2,18134,20)
                    gt_cls_per_image.unsqueeze(1).expand(shape),  # (2,20)-&gt;(2,1,20)-&gt;(2,18134,20)
                    alpha=self.focal_loss_alpha,  # 0.25
                    gamma=self.focal_loss_gamma,  # 2
                ).sum(dim=-1)  # (2,18134,20)-&gt;(2,18134)

                loss_cls_bg = sigmoid_focal_loss_jit(
                    box_cls_per_image,  # (18134,20)
                    torch.zeros_like(box_cls_per_image),
                    alpha=self.focal_loss_alpha,
                    gamma=self.focal_loss_gamma,
                ).sum(dim=-1)  # (18134,20)-&gt;(18134)

                gt_delta_per_image = self.shift2box_transform.get_deltas(
                    shifts_over_all, gt_boxes.tensor.unsqueeze(1)  # (18134,2), (2,4)-&gt;(2,1,4)
                )  # (2,18134,4)

                ious, loss_delta = get_ious_and_iou_loss(
                    box_delta_per_image.unsqueeze(0).expand(shape),  # (18134,4)-&gt;(1,18134,4)-&gt;(2,18134,4)
                    gt_delta_per_image,
                    box_mode="ltrb",
                    loss_type='iou'
                )  # (2,18134),(2,18134)

                loss = loss_cls + self.reg_weight * loss_delta + 1e6 * (1 - is_in_boxes.float())  # 1.5
                # (2,18134)

                # Performing Dynamic k Estimation
                topk_ious, _ = torch.topk(ious * is_in_boxes.float(), self.top_candidates, dim=1)  # (2,18134),20 -&gt; (2,20)
                mu = ious.new_ones(num_gt + 1)  # torch.Size([3]), tensor([1., 1., 1.], device='cuda:0')
                mu[:-1] = torch.clamp(topk_ious.sum(1).int(), min=1).float()  # s_{i}(i=1,...,m)
                mu[-1] = num_anchor - mu[:-1].sum()  # s_{m+1}
                nu = ious.new_ones(num_anchor)  # (18134), d_{j}(j=1,..,n)
                loss = torch.cat([loss, loss_cls_bg.unsqueeze(0)], dim=0)  # (2,18134),(18134)-&gt;(1,18134), -&gt; (3,18134)

                # Solving Optimal-Transportation-Plan pi via Sinkhorn-Iteration.
                _, pi = self.sinkhorn(mu, nu, loss)  # (3,),(18134,),(3,18134) -&gt; (3,18134)

                # Rescale pi so that the max pi for each gt equals to 1.
                rescale_factor, _ = pi.max(dim=1)  # (3,)
                pi = pi / rescale_factor.unsqueeze(1)  # (3,18134)

                max_assigned_units, matched_gt_inds = torch.max(pi, dim=0)
                gt_classes_i = targets_per_image.gt_classes.new_ones(num_anchor) * self.num_classes
                fg_mask = matched_gt_inds != num_gt
                gt_classes_i[fg_mask] = targets_per_image.gt_classes[matched_gt_inds[fg_mask]]
                gt_classes.append(gt_classes_i)
                assigned_units.append(max_assigned_units)

                box_target_per_image = gt_delta_per_image.new_zeros((num_anchor, 4))
                box_target_per_image[fg_mask] = 
                    gt_delta_per_image[matched_gt_inds[fg_mask], torch.arange(num_anchor)[fg_mask]]
                gt_shifts_deltas.append(box_target_per_image)

                gt_ious_per_image = ious.new_zeros((num_anchor, 1))
                gt_ious_per_image[fg_mask] = ious[matched_gt_inds[fg_mask],
                                                  torch.arange(num_anchor)[fg_mask]].unsqueeze(1)
                gt_ious.append(gt_ious_per_image)

        return torch.cat(gt_classes), torch.cat(gt_shifts_deltas), torch.cat(gt_ious)</code></pre> 
<h1>Experiments</h1> 
<h2>Alation Studies and Analysis</h2> 
<h3>Effects of Individual Components</h3> 
<p>OTA可以既可以用于anchor-based detector也可以用于anchor-free detector，本文采用FCOS，同时额外加入了IoU分支，从下图可以看出随着添加IoU branch、center prior、dynamic k estimation，性能持续提升，并且比对应的原始FCOS的精度要高。 </p> 
<p style="text-align:center"><img alt="" height="299" src="https://images2.imgbox.com/ff/55/xtwyrsJc_o.png" width="401"></p> 
<h3>Effects of (r)  </h3> 
<p>center prior的半径 (r) 控制每个gt的正样本数量，(r) 值小，只有最靠近gt中心的高质量anchor才被当做正样本，有助于模型的学习。(r) 越大，引入的低质量的正样本anchor越多，导致了优化过程中潜在的不稳定。从下表可以看出，随着 (r) 的增大，三种模型的精度都出现了不同程度的下降，但OTA下降的最少，表明OTA对 (r) 值的变化不那么敏感，同时不同的 (r) 值下，OTA的精度也是最高的。</p> 
<p style="text-align:center"><img alt="" height="246" src="https://images2.imgbox.com/0d/e0/gVX3Ucyo_o.png" width="501"></p> 
<h3>Ambiguous Anchors Handling</h3> 
<p>当发生遮挡或者多个对象靠的非常近时，一个anchor可能是多个ground truth的合格候选对象（比如Faster RCNN中一个anchor与多个gt的IoU都大于0.5），这种anchor定义为<strong>ambiguous anchor</strong>。之前的方法主要通过人工设定的规则来处理这种情况，比如Min Area、Max IoU、Min Loss等。本文将 (max pi^{*}_{j}&lt;0.9) 的anchor (a_{j}) 定义为ambiguous anchor，然后统计在不同的 (r) 值下ATSS、PAA、OTA的ambiguous anchor的数量以及对应的精度。从上表(2)中可以看出，随着 (r) 的增大，ATSS中ambiguous anchor的数量显著增加，AP也降了1.8个点。PAA中ambiguous anchor的数量对 (r) 的变化不那么敏感，但AP也降了0.8个点。而OTA中ambiguous anchor的数量既对 (r) 的变化不敏感，和ATSS、PAA相比数量也是最少的，同时AP也只下降了0.3个点。这是因为当多个gt试图将positive label传输到同一个anchor时，OT算法会基于全局最小传输成本的准则自动解决它们之间的冲突。 </p> 
<h3>Effects of (k)</h3> 
<p>如下表所示，作者对比了 (k) 设置为不同的常数值以及采用dynamic (k) 时模型的精度，可以看出随着 (k) 的增大，模型精度越来越高，当 (k) 取10或12时，模型达到最高的精度，随后开始下降。但最高的精度也比采用dynamic (k) 的精度低。从直觉上讲，每个gt的大小、尺度、遮挡情况都不同，因此每个gt所需的postive anchor的数量应该也是不同的。</p> 
<p style="text-align:center"><img alt="" height="247" src="https://images2.imgbox.com/d2/a6/7zPBUNyp_o.png" width="445"></p> 
<h2>Comparison with State-of-the-art Methods</h2> 
<p>从下表可以看出，采用ResNet-101-FPN结构，OTA的AP达到了45.3%，超过了其它所有相同backbone的方法，如ATSS（43.6% AP）、AutoAssign（44.5% AP）、PAA（44.6% AP）。</p> 
<p style="text-align:center"><img alt="" height="529" src="https://images2.imgbox.com/db/e9/sgdmAXCi_o.png" width="676"></p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>