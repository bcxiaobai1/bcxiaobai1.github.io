<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>TensorRT安装记录（8.2.5） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">TensorRT安装记录（8.2.5）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-tomorrow-night">
                    
                        
                    
                    <p>官网链接：<a href="https://developer.nvidia.com/tensorrt">https://developer.nvidia.com/tensorrt</a></p> 
<hr> 
<p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul><li>
<ul>
<li><a href="#0_TensorRT_7">0 TensorRT简介</a></li>
<li><a href="#1_TensorRT_21">1 安装TensorRT</a></li>
<li>
<ul>
<li><a href="#11_piptrtexec_34">1.1 pip安装（trtexec无法使用）</a></li>
<li><a href="#12_TAR_Package_66">1.2 TAR Package安装</a></li>
</ul>
   </li>
<li><a href="#2_TensorRT_114">2 将模型转换成TensorRT的流程</a></li>
<li><a href="#3_PytorchTensorRT_132">3 将Pytorch模型转成TensorRT案例</a></li>
<li>
<ul>
<li><a href="#31_PytorchONNX_134">3.1 将Pytorch模型转成ONNX格式</a></li>
<li><a href="#32_ONNXTensorRT_200">3.2 将ONNX格式转成TensorRT格式</a></li>
<li><a href="#33_TensorRT_255">3.3 载入TensorRT模型</a></li>
</ul>
   </li>
<li><a href="#34__353">3.4 其他</a></li>
</ul>
 </li></ul>
</div>
<p></p> 
<hr> 
<h2>
<a id="0_TensorRT_7"></a>0 TensorRT简介</h2> 
<blockquote> 
 <p>NVIDIA® TensorRT™ is an SDK for optimizing trained deep learning models to enable high-performance inference. TensorRT contains a deep learning inference optimizer for trained deep learning models, and a runtime for execution. After you have trained your deep learning model in a framework of your choice, TensorRT enables you to run it with higher throughput and lower latency.</p> 
</blockquote> 
<p>根据官方对于<code>TensorRT</code>的介绍可知，<code>TensorRT</code>是一个针对<strong>已训练好模型</strong>的SDK，通过该SDK能够在NVIDIA的设备上进行高性能的推理。那么<code>TensorRT</code>具体会对我们训练好的模型做哪些优化呢，可以参考<code>TensorRT</code>官网中的一幅图，如下图所示：<br> <img src="https://images2.imgbox.com/94/e4/NHz3Enwe_o.png" alt="在这里插入图片描述">总结下来主要有以下6点：</p> 
<ol>
<li>
<code>Reduced Precision</code>：将模型量化成<code>INT8</code>或者<code>FP16</code>的数据类型（在保证精度不变或略微降低的前提下），以提升模型的推理速度。</li>
<li>
<code>Layer and Tensor Fusion</code>：通过将多个层结构进行融合（包括横向和纵向）来优化GPU的显存以及带宽。</li>
<li>
<code>Kernel Auto-Tuning</code>：根据当前使用的GPU平台选择最佳的数据层和算法。</li>
<li>
<code>Dynamic Tensor Memory</code>：最小化内存占用并高效地重用张量的内存。</li>
<li>
<code>Multi-Stream Execution</code>：使用可扩展设计并行处理多个输入流。</li>
<li>
<code>Time Fusion</code>：使用动态生成的核去优化随时间步长变化的RNN网络。</li>
</ol> 
<hr> 
<h2>
<a id="1_TensorRT_21"></a>1 安装TensorRT</h2> 
<p>安装<code>TensorRT</code>建议直接按照官方的教程来，官方最新<code>TensorRT</code>快速开始文档：<br> <a href="https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html">https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/index.html</a><br> 或者指定某一版本的<code>TensorRT</code>快速开始文档（以当前最新稳定版<code>8.2.5</code>为例）：<br> <a href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-825/quick-start-guide/index.html">https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-825/quick-start-guide/index.html</a></p> 
<p>对于安装<code>TensorRT</code>官方列出了下面三种安装方式，但我个人还是喜欢<code>TAR Package</code>安装（其他安装方式参考<a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html">https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html</a>）：</p> 
<ul>
<li><code>Container Installation</code></li>
<li><code>Debian Installation</code></li>
<li><code>pip Wheel File Installation</code></li>
</ul> 
<hr> 
<h3>
<a id="11_piptrtexec_34"></a>1.1 pip安装（trtexec无法使用）</h3> 
<p>如果会使用Docker的建议用<code>Container Installation</code>，本文先以<code>pip Wheel File Installation</code>安装方式为例。在官方快速开始文档<code>pip Wheel File Installation</code>中（<code>8.2.5</code>）明确说明Python的版本只支持3.6至3.9，CUDA版本只支持<code>11.x</code>，并且只支持Linux操作系统以及x86_64的CPU架构，官方建议使用<code>Centos 7</code>或者<code>Ubuntu 18.04</code>。</p> 
<blockquote> 
 <p>The pip-installable nvidia-tensorrt Python wheel files only support Python versions 3.6 to 3.9 and CUDA 11.x at this time and will not work with other Python or CUDA versions. Only the Linux operating system and x86_64 CPU architecture is currently supported. These wheel files are expected to work on CentOS 7 or newer and Ubuntu 18.04 or newer.</p> 
</blockquote> 
<p>除了以上说的要求外，还需要注意下GPU的驱动版本，因为不同的CUDA版本对GPU的驱动有不同的要求，而这里安装的<code>TensorRT（8.2.5）</code>要求使用CUDA 11.x版本，所以需要看下自己GPU的驱动版本是否满足，可通过<code>nvidia-smi</code>指令查看自己的驱动版本。这里可以直接在NVIDIA官网，看下CUDA版本以及GPU驱动的对应关系：<br> <a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html</a></p> 
<p><img src="https://images2.imgbox.com/5a/7f/GmFiVYmw_o.png" alt="在这里插入图片描述"></p> 
<p>在保证GPU驱动满足要求的前提下，建议先用conda创建一个新的虚拟环境（不影响其他环境）。这里就直接创建一个名为<code>tensorrt</code>的虚拟环境，然后采用<code>python</code>的版本为<code>3.8</code>。</p> 
<pre><code>conda create -n tensorrt python=3.8
</code></pre> 
<p>创建好虚拟环境以后，激活进入虚拟环境：</p> 
<pre><code>conda activate tensorrt
</code></pre> 
<p>接着安装<code>nvidia-pyindex</code>和<code>nvidia-tensorrt</code>，注意，如果不指定<code>nvidia-tensorrt</code>的版本号默认安装最新版本，本文是以<code>8.2.5</code>版本为例，所以这里安装的是当前可用的<code>8.2.5.1</code>：</p> 
<pre><code>pip install nvidia-pyindex
pip install nvidia-tensorrt==8.2.5.1
</code></pre> 
<p>安装完成后，按照官方的步骤检查下是否安装成功，只需进入Python环境，然后简单打印下版本号等信息，只要不报错就说明安装成功。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> tensorrt
<span class="token keyword">print</span><span class="token punctuation">(</span>tensorrt<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
<span class="token keyword">assert</span> tensorrt<span class="token punctuation">.</span>Builder<span class="token punctuation">(</span>tensorrt<span class="token punctuation">.</span>Logger<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/79/c8/e0dJT3BH_o.png" alt="在这里插入图片描述"><br> 但后面按照官方教程使用<code>trtexec</code>转换模型格式时发现找不到这个工具，我怀疑通过pip安装方式只是安装了TensorRT的运行时，没有提供<code>trtexec</code>工具。</p> 
<hr> 
<h3>
<a id="12_TAR_Package_66"></a>1.2 TAR Package安装</h3> 
<p>安装过程主要按照官网流程：<a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar">https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar</a><br> 在安装之前需要准备好以下环境：</p> 
<ul>
<li>CUDA 10.2, 11.0 update 1, 11.1 update 1, 11.2 update 2, 11.3 update 1, 11.4 update 3, 11.5 update 1 or 11.6</li>
<li>cuDNN 8.3.2</li>
<li>Python 3 (Optional)</li>
</ul> 
<p>进入官方TensorRT的下载页面（需要登录）</p> 
<p><img src="https://images2.imgbox.com/4d/d0/VRfehHLa_o.png" alt="在这里插入图片描述"></p> 
<p>下载对应的包，这里我下载的是<code>TensorRT 8.2 GA Update 4 for Linux x86_64 and CUDA 11.0, 11.1, 11.2, 11.3, 11.4 and 11.5 TAR Package</code>：</p> 
<p><img src="https://images2.imgbox.com/7c/e7/Mfht0Uy5_o.png" alt="在这里插入图片描述"></p> 
<p>下载完成后解压文件：</p> 
<pre><code>tar -xzvf TensorRT-8.2.5.1.Linux.x86_64-gnu.cuda-11.4.cudnn8.2.tar.gz
</code></pre> 
<p>解压后会生成<code>TensorRT-8.2.5.1</code>文件夹，接着将<code>TensorRT-8.2.5.1/lib</code>文件夹路径添加到环境变量<code>LD_LIBRARY_PATH</code>中，注意我是将<code>TensorRT-8.2.5.1</code>文件夹放在<code>root</code>路径下所以设置的是<code>/root/TensorRT-8.2.5.1/lib</code>，这里需要根据自己解压的路径设置。同理将<code>TensorRT-8.2.5.1/bin</code>文件夹路径添加到环境变量<code>PATH</code>中，其中包含后面需要用到的<code>trtexec</code>工具：</p> 
<pre><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/root/TensorRT-8.2.5.1/lib
export PATH=$PATH:/root/TensorRT-8.2.5.1/bin
</code></pre> 
<p>接着进入<code>TensorRT-8.2.5.1/python</code>文件夹下安装<code>TensorRT wheel</code>文件，在该文件夹里有针对不同python版本的whl文件，由于我采用的虚拟环境中的python版本是<code>3.8</code>所以安转<code>cp38</code>对应的whl文件：</p> 
<pre><code>cd TensorRT-8.2.5.1/python
pip install tensorrt-8.2.5.1-cp38-none-linux_x86_64.whl
</code></pre> 
<p>接着进入<code>TensorRT-8.2.5.1/graphsurgeon</code>文件夹下安装<code>graphsurgeon wheel</code>文件：</p> 
<pre><code>cd TensorRT-8.2.5.1/graphsurgeon
pip install graphsurgeon-0.4.5-py2.py3-none-any.whl
</code></pre> 
<p>接着进入<code>TensorRT-8.2.5.1/onnx-graphsurgeon</code>文件夹下安装<code>onnx-graphsurgeon wheel</code>文件：</p> 
<pre><code>cd TensorRT-8.2.5.1/onnx-graphsurgeon
pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl
</code></pre> 
<p>安转完后，同样可以进入Python环境，然后简单打印下版本号等信息，只要不报错就说明安装成功。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> tensorrt
<span class="token keyword">print</span><span class="token punctuation">(</span>tensorrt<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
<span class="token keyword">assert</span> tensorrt<span class="token punctuation">.</span>Builder<span class="token punctuation">(</span>tensorrt<span class="token punctuation">.</span>Logger<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<hr> 
<h2>
<a id="2_TensorRT_114"></a>2 将模型转换成TensorRT的流程</h2> 
<p>根据官网的介绍，转换TensorRT的工作流程主要有以下6个步骤：</p> 
<ol>
<li>
<code>Export the Model</code>: 导出模型</li>
<li>
<code>Select A Batch Size</code>: 根据自己的实际项目选择一个合适的<code>Batch Size</code>
</li>
<li>
<code>Select A Precision</code>: 选择一个精度类型，比如<code>INT8</code>，<code>FLOAT16</code>，<code>FLOAT32</code>
</li>
<li>
<code>Convert The Model</code>: 转换模型</li>
<li>
<code>Deploy The Model</code>: 部署模型</li>
</ol> 
<p>那哪些格式的模型能够导出并转换成TensorRT模型呢，官方提到了三种方式：</p> 
<ol>
<li>
<code>using TF-TRT</code>： 使用TF-TRT(<code>TensorFlow-TensorRT</code> )</li>
<li>
<code>automatic ONNX conversion from .onnx files</code>：从ONNX通用格式转换得到（注意，这里需要自己提前将模型转成ONNX格式）</li>
<li>
<code>manually constructing a network using the TensorRT API (either in C++ or Python)</code>：自己用<code>TensorRT API</code>构建模型（这个对新人不太友好，难度有点大）</li>
</ol> 
<p>也可以参考下面这幅图，比如说对于Pytorch的模型，我们一般需要先转成ONNX通用格式，然后再转成TensorRT模型，最后部署的时候可以选择<code>C++</code>或者<code>Python</code>：<br> <img src="https://images2.imgbox.com/6e/e1/Xc7p7uJW_o.png" alt="在这里插入图片描述"></p> 
<hr> 
<h2>
<a id="3_PytorchTensorRT_132"></a>3 将Pytorch模型转成TensorRT案例</h2> 
<p>按照上述内容，我们知道一般将Pytorch模型转成TensorRT格式的流程是先转ONNX通用格式，再转TensorRT。</p> 
<h3>
<a id="31_PytorchONNX_134"></a>3.1 将Pytorch模型转成ONNX格式</h3> 
<p>这里以Pytorch官方提供的<code>ResNet34</code>为例，直接从<code>torchvision</code>中实例化<code>ResNet34</code>并载入自己在<code>flower_photos</code>数据集上训练好的权重，然后在转成ONNX格式，示例代码如下：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>onnx
<span class="token keyword">import</span> onnx
<span class="token keyword">import</span> onnxruntime
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">import</span> resnet34

device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">to_numpy</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> tensor<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">if</span> tensor<span class="token punctuation">.</span>requires_grad <span class="token keyword">else</span> tensor<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    weights_path <span class="token operator">=</span> <span class="token string">"resNet34(flower).pth"</span>
    onnx_file_name <span class="token operator">=</span> <span class="token string">"resnet34.onnx"</span>
    batch_size <span class="token operator">=</span> <span class="token number">1</span>
    img_h <span class="token operator">=</span> <span class="token number">224</span>
    img_w <span class="token operator">=</span> <span class="token number">224</span>
    img_channel <span class="token operator">=</span> <span class="token number">3</span>

    <span class="token comment"># create model and load pretrain weights</span>
    model <span class="token operator">=</span> resnet34<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>weights_path<span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token string">'cpu'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># input to the model</span>
    <span class="token comment"># [batch, channel, height, width]</span>
    x <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> img_channel<span class="token punctuation">,</span> img_h<span class="token punctuation">,</span> img_w<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    torch_out <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

    <span class="token comment"># export the model</span>
    torch<span class="token punctuation">.</span>onnx<span class="token punctuation">.</span>export<span class="token punctuation">(</span>model<span class="token punctuation">,</span>             <span class="token comment"># model being run</span>
                      x<span class="token punctuation">,</span>                 <span class="token comment"># model input (or a tuple for multiple inputs)</span>
                      onnx_file_name<span class="token punctuation">,</span>    <span class="token comment"># where to save the model (can be a file or file-like object)</span>
                      input_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"input"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                      output_names<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"output"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                      verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

    <span class="token comment"># check onnx model</span>
    onnx_model <span class="token operator">=</span> onnx<span class="token punctuation">.</span>load<span class="token punctuation">(</span>onnx_file_name<span class="token punctuation">)</span>
    onnx<span class="token punctuation">.</span>checker<span class="token punctuation">.</span>check_model<span class="token punctuation">(</span>onnx_model<span class="token punctuation">)</span>

    ort_session <span class="token operator">=</span> onnxruntime<span class="token punctuation">.</span>InferenceSession<span class="token punctuation">(</span>onnx_file_name<span class="token punctuation">)</span>

    <span class="token comment"># compute ONNX Runtime output prediction</span>
    ort_inputs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>ort_session<span class="token punctuation">.</span>get_inputs<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>name<span class="token punctuation">:</span> to_numpy<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">}</span>
    ort_outs <span class="token operator">=</span> ort_session<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> ort_inputs<span class="token punctuation">)</span>

    <span class="token comment"># compare ONNX Runtime and Pytorch results</span>
    <span class="token comment"># assert_allclose: Raises an AssertionError if two objects are not equal up to desired tolerance.</span>
    np<span class="token punctuation">.</span>testing<span class="token punctuation">.</span>assert_allclose<span class="token punctuation">(</span>to_numpy<span class="token punctuation">(</span>torch_out<span class="token punctuation">)</span><span class="token punctuation">,</span> ort_outs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> rtol<span class="token operator">=</span><span class="token number">1e-03</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Exported model has been tested with ONNXRuntime, and the result looks good!"</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>注意，这里将Pytorch模型转成ONNX后，又利用ONNXRUNTIME载入导出的模型，然后输入同样的数据利用<code>np.testing.assert_allclose</code>方法对比转换前后输出的差异，其中<code>rtol</code>代表相对偏差，<code>atol</code>代表绝对偏差，如果两者的差异超出指定的精度则会报错。在转换后，会在当前文件夹中生成一个<code>resnet34.onnx</code>文件。</p> 
<hr> 
<h3>
<a id="32_ONNXTensorRT_200"></a>3.2 将ONNX格式转成TensorRT格式</h3> 
<p>将ONNX转成TensorRT engine的方式有多种，其中最简单的就是使用<code>trtexec</code>工具。在上面<code>3.1</code>章节中已经将Pyotrch中的<code>Resnet34</code>转成ONNX格式了，接下来可以直接使用<code>trtexec</code>工具将其转为TensorRT engine格式：</p> 
<pre><code>trtexec --onnx=resnet34.onnx --saveEngine=trt_output/resnet34.trt
</code></pre> 
<p>其中：</p> 
<ul>
<li>
<code>--onnx</code>是指向生成的onnx模型文件路径</li>
<li>
<code>--saveEngine</code>是保存TensorRT engine的文件路径（发现一个小问题，就是保存的目录必须提前创建好，如果没有创建的话就会报错）</li>
</ul> 
<p>转化过程中终端会输出如下信息：</p> 
<pre><code>[06/23/2022-08:08:14] [I] === Model Options ===
[06/23/2022-08:08:14] [I] Format: ONNX
[06/23/2022-08:08:14] [I] Model: /root/project/resnet34.onnx
[06/23/2022-08:08:14] [I] Output:
[06/23/2022-08:08:14] [I] === Build Options ===
[06/23/2022-08:08:14] [I] Max batch: explicit batch
[06/23/2022-08:08:14] [I] Workspace: 16 MiB
[06/23/2022-08:08:14] [I] minTiming: 1
[06/23/2022-08:08:14] [I] avgTiming: 8
[06/23/2022-08:08:14] [I] Precision: FP32
[06/23/2022-08:08:14] [I] Calibration:
[06/23/2022-08:08:14] [I] Refit: Disabled
[06/23/2022-08:08:14] [I] Sparsity: Disabled
[06/23/2022-08:08:14] [I] Safe mode: Disabled
[06/23/2022-08:08:14] [I] DirectIO mode: Disabled
[06/23/2022-08:08:14] [I] Restricted mode: Disabled
[06/23/2022-08:08:14] [I] Save engine: trt_ouput/resnet34.trt
[06/23/2022-08:08:14] [I] Load engine:
[06/23/2022-08:08:14] [I] Profiling verbosity: 0
[06/23/2022-08:08:14] [I] Tactic sources: Using default tactic sources
[06/23/2022-08:08:14] [I] timingCacheMode: local
[06/23/2022-08:08:14] [I] timingCacheFile:
[06/23/2022-08:08:14] [I] Input(s)s format: fp32:CHW
[06/23/2022-08:08:14] [I] Output(s)s format: fp32:CHW
[06/23/2022-08:08:14] [I] Input build shapes: model
[06/23/2022-08:08:14] [I] Input calibration shapes: model
......
[06/23/2022-08:08:41] [I] === Performance summary ===
[06/23/2022-08:08:41] [I] Throughput: 550.406 qps
[06/23/2022-08:08:41] [I] Latency: min = 1.85938 ms, max = 2.23706 ms, mean = 1.87513 ms, median = 1.87372 ms, percentile(99%) = 1.90234 ms
[06/23/2022-08:08:41] [I] End-to-End Host Latency: min = 1.87573 ms, max = 3.56226 ms, mean = 3.38754 ms, median = 3.47742 ms, percentile(99%) = 3.50659 ms
[06/23/2022-08:08:41] [I] Enqueue Time: min = 0.402954 ms, max = 2.53369 ms, mean = 0.68202 ms, median = 0.653564 ms, percentile(99%) = 0.830811 ms
[06/23/2022-08:08:41] [I] H2D Latency: min = 0.0581055 ms, max = 0.0943298 ms, mean = 0.063807 ms, median = 0.0615234 ms, percentile(99%) = 0.0910645 ms
[06/23/2022-08:08:41] [I] GPU Compute Time: min = 1.79099 ms, max = 2.14551 ms, mean = 1.80203 ms, median = 1.80127 ms, percentile(99%) = 1.8125 ms
[06/23/2022-08:08:41] [I] D2H Latency: min = 0.00610352 ms, max = 0.0129395 ms, mean = 0.00928149 ms, median = 0.00949097 ms, percentile(99%) = 0.0119934 ms
[06/23/2022-08:08:41] [I] Total Host Walltime: 3.00324 s
[06/23/2022-08:08:41] [I] Total GPU Compute Time: 2.97876 s
[06/23/2022-08:08:41] [I] Explanations of the performance metrics are printed in the verbose logs.
</code></pre> 
<p>有关<code>trtexec</code>工具的使用方法，可以通过<code>trtexec --help</code>查看详细介绍，比如要使用<code>FP16</code>精度转模型时加上<code>--fp16</code>参数即可。</p> 
<hr> 
<h3>
<a id="33_TensorRT_255"></a>3.3 载入TensorRT模型</h3> 
<p>这里主要参考官方提供的<code>notebook</code>教程：<a href="https://github.com/NVIDIA/TensorRT/blob/main/quickstart/SemanticSegmentation/tutorial-runtime.ipynb">https://github.com/NVIDIA/TensorRT/blob/main/quickstart/SemanticSegmentation/tutorial-runtime.ipynb</a></p> 
<p>下面是我参考官方demo写的一个样例，在样例中对比<code>ONNX</code>和<code>TensorRT</code>的输出结果。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> tensorrt <span class="token keyword">as</span> trt
<span class="token keyword">import</span> onnxruntime
<span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>driver <span class="token keyword">as</span> cuda
<span class="token keyword">import</span> pycuda<span class="token punctuation">.</span>autoinit


<span class="token keyword">def</span> <span class="token function">normalize</span><span class="token punctuation">(</span>image<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Normalize the image to the given mean and standard deviation
    """</span>
    image <span class="token operator">=</span> image<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>
    mean <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">)</span>
    std <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">)</span>
    image <span class="token operator">/=</span> <span class="token number">255.0</span>
    image <span class="token operator">-=</span> mean
    image <span class="token operator">/=</span> std
    <span class="token keyword">return</span> image


<span class="token keyword">def</span> <span class="token function">onnx_inference</span><span class="token punctuation">(</span>onnx_path<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> image<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># load onnx model</span>
    ort_session <span class="token operator">=</span> onnxruntime<span class="token punctuation">.</span>InferenceSession<span class="token punctuation">(</span>onnx_path<span class="token punctuation">)</span>

    <span class="token comment"># compute onnx Runtime output prediction</span>
    ort_inputs <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>ort_session<span class="token punctuation">.</span>get_inputs<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>name<span class="token punctuation">:</span> image<span class="token punctuation">}</span>
    res_onnx <span class="token operator">=</span> ort_session<span class="token punctuation">.</span>run<span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span> ort_inputs<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">return</span> res_onnx


<span class="token keyword">def</span> <span class="token function">trt_inference</span><span class="token punctuation">(</span>trt_path<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> image<span class="token punctuation">:</span> np<span class="token punctuation">.</span>ndarray<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># Load the network in Inference Engine</span>
    trt_logger <span class="token operator">=</span> trt<span class="token punctuation">.</span>Logger<span class="token punctuation">(</span>trt<span class="token punctuation">.</span>Logger<span class="token punctuation">.</span>WARNING<span class="token punctuation">)</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>trt_path<span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">,</span> trt<span class="token punctuation">.</span>Runtime<span class="token punctuation">(</span>trt_logger<span class="token punctuation">)</span> <span class="token keyword">as</span> runtime<span class="token punctuation">:</span>
        engine <span class="token operator">=</span> runtime<span class="token punctuation">.</span>deserialize_cuda_engine<span class="token punctuation">(</span>f<span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">with</span> engine<span class="token punctuation">.</span>create_execution_context<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> context<span class="token punctuation">:</span>
        <span class="token comment"># Set input shape based on image dimensions for inference</span>
        context<span class="token punctuation">.</span>set_binding_shape<span class="token punctuation">(</span>engine<span class="token punctuation">.</span>get_binding_index<span class="token punctuation">(</span><span class="token string">"input"</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> image<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment"># Allocate host and device buffers</span>
        bindings <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> binding <span class="token keyword">in</span> engine<span class="token punctuation">:</span>
            binding_idx <span class="token operator">=</span> engine<span class="token punctuation">.</span>get_binding_index<span class="token punctuation">(</span>binding<span class="token punctuation">)</span>
            size <span class="token operator">=</span> trt<span class="token punctuation">.</span>volume<span class="token punctuation">(</span>context<span class="token punctuation">.</span>get_binding_shape<span class="token punctuation">(</span>binding_idx<span class="token punctuation">)</span><span class="token punctuation">)</span>
            dtype <span class="token operator">=</span> trt<span class="token punctuation">.</span>nptype<span class="token punctuation">(</span>engine<span class="token punctuation">.</span>get_binding_dtype<span class="token punctuation">(</span>binding<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> engine<span class="token punctuation">.</span>binding_is_input<span class="token punctuation">(</span>binding<span class="token punctuation">)</span><span class="token punctuation">:</span>
                input_buffer <span class="token operator">=</span> np<span class="token punctuation">.</span>ascontiguousarray<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
                input_memory <span class="token operator">=</span> cuda<span class="token punctuation">.</span>mem_alloc<span class="token punctuation">(</span>image<span class="token punctuation">.</span>nbytes<span class="token punctuation">)</span>
                bindings<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>input_memory<span class="token punctuation">)</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                output_buffer <span class="token operator">=</span> cuda<span class="token punctuation">.</span>pagelocked_empty<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dtype<span class="token punctuation">)</span>
                output_memory <span class="token operator">=</span> cuda<span class="token punctuation">.</span>mem_alloc<span class="token punctuation">(</span>output_buffer<span class="token punctuation">.</span>nbytes<span class="token punctuation">)</span>
                bindings<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">(</span>output_memory<span class="token punctuation">)</span><span class="token punctuation">)</span>

        stream <span class="token operator">=</span> cuda<span class="token punctuation">.</span>Stream<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># Transfer input data to the GPU.</span>
        cuda<span class="token punctuation">.</span>memcpy_htod_async<span class="token punctuation">(</span>input_memory<span class="token punctuation">,</span> input_buffer<span class="token punctuation">,</span> stream<span class="token punctuation">)</span>
        <span class="token comment"># Run inference</span>
        context<span class="token punctuation">.</span>execute_async_v2<span class="token punctuation">(</span>bindings<span class="token operator">=</span>bindings<span class="token punctuation">,</span> stream_handle<span class="token operator">=</span>stream<span class="token punctuation">.</span>handle<span class="token punctuation">)</span>
        <span class="token comment"># Transfer prediction output from the GPU.</span>
        cuda<span class="token punctuation">.</span>memcpy_dtoh_async<span class="token punctuation">(</span>output_buffer<span class="token punctuation">,</span> output_memory<span class="token punctuation">,</span> stream<span class="token punctuation">)</span>
        <span class="token comment"># Synchronize the stream</span>
        stream<span class="token punctuation">.</span>synchronize<span class="token punctuation">(</span><span class="token punctuation">)</span>

        res_trt <span class="token operator">=</span> np<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>output_buffer<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> res_trt


<span class="token keyword">def</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    image_h <span class="token operator">=</span> <span class="token number">224</span>
    image_w <span class="token operator">=</span> <span class="token number">224</span>
    onnx_path <span class="token operator">=</span> <span class="token string">"resnet34.onnx"</span>
    trt_path <span class="token operator">=</span> <span class="token string">"trt_output/resnet34.trt"</span>

    image <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>image_h<span class="token punctuation">,</span> image_w<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
    normalized_image <span class="token operator">=</span> normalize<span class="token punctuation">(</span>image<span class="token punctuation">)</span>

    <span class="token comment"># Convert the resized images to network input shape</span>
    <span class="token comment"># [h, w, c] -&gt; [c, h, w] -&gt; [1, c, h, w]</span>
    normalized_image <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>np<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>normalized_image<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

    onnx_res <span class="token operator">=</span> onnx_inference<span class="token punctuation">(</span>onnx_path<span class="token punctuation">,</span> normalized_image<span class="token punctuation">)</span>
    ir_res <span class="token operator">=</span> trt_inference<span class="token punctuation">(</span>trt_path<span class="token punctuation">,</span> normalized_image<span class="token punctuation">)</span>
    np<span class="token punctuation">.</span>testing<span class="token punctuation">.</span>assert_allclose<span class="token punctuation">(</span>onnx_res<span class="token punctuation">,</span> ir_res<span class="token punctuation">,</span> rtol<span class="token operator">=</span><span class="token number">1e-03</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e-05</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Exported model has been tested with TensorRT Runtime, and the result looks good!"</span><span class="token punctuation">)</span>


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>
    main<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<hr> 
<h2>
<a id="34__353"></a>3.4 其他</h2> 
<p>最后提下模型的量化，关于量化可以简单的分成两类（不严谨）：QAT（Quantiztion Aware Training）在训练过程中同时进行量化，PTQ（Post Training Quantization）训练后量化。由于现在深度学习框架非常多以及各种runtime（比如tensorflow的tf-lite，pytorch的torchscript，onnx，tensorrt，openvino等等），量化的工具也一堆。这里对于QAT推荐nvidia的<code>pytorch-quantization</code>工具。对于PTQ，如果是部署在nvidia卡上推荐<code>tensorrt</code>，如果部署在cpu上可以尝试openvino。</p> 
<ul>
<li><a href="https://github.com/NVIDIA/TensorRT/tree/main/tools/pytorch-quantization">https://github.com/NVIDIA/TensorRT/tree/main/tools/pytorch-quantization</a></li>
<li><a href="https://developer.nvidia.com/blog/int8-inference-autonomous-vehicles-tensorrt">https://developer.nvidia.com/blog/int8-inference-autonomous-vehicles-tensorrt</a></li>
</ul>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>