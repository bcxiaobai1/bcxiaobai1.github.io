<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Autoformer算法与代码分析 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Autoformer算法与代码分析</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="Autoformer_1"></a>Autoformer算法与代码分析</h1> 
<p><img src="https://images2.imgbox.com/c1/59/lWDvH8Jg_o.jpg" alt="在这里插入图片描述"><br> 原文链接：https://arxiv.org/pdf/2106.13008.pdf<br> 代码链接：https://github.com/thuml/Autoformer</p> 
<h2>
<a id="_7"></a>简介</h2> 
<p>Autoformer本质上是<strong>基于Transformer结构</strong>的时序预测模型，因此如果有Transformer基础，对于理解Autoformer以及所有基于Transformer的时序预测模型都比较有帮助。</p> 
<p>所有基于Transfomer结构的时序预测模型，本质上都是对于其中最重要的模块：<strong>Self-attention</strong>进行创新。例如LogsTransformer，reformer，informer，fedformer无一例外都是对于Self-attention模块进行创新。其中LogsTransformer，reformer，informer都是对于Self-attention的计算复杂度方面进行创新，并且提出各自的modified-self-attention。而到了Autoformer首先对于Self-attention的计算方式进行创新。</p> 
<h2>
<a id="_15"></a>模型结构</h2> 
<p>Autoformer相对于普通的Transformer结构无非就是加了时序拆解模块，修改了Self-attention模块。其他的例如Feed Forward模块并没有改动，那么接下来就从这两个新的模块进行详细拆解：</p> 
<h3>
<a id="Series_Decmop_21"></a>Series Decmop</h3> 
<p>时序拆解器本质上来自于传统的时序预测算法，例如Arima，Fbprophet。Arima等传统算法本质上思考的就是从统计的角度对于时间序列进行拆解，并赋予拆解的子项以不同的物理意义例如：趋势项，季节项，残差项等。</p> 
<p>因此传统的时序拆解的普遍形式是：</p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         X
        
        
         (
        
        
         t
        
        
         )
        
        
         =
        
        
         T
        
        
         (
        
        
         t
        
        
         )
        
        
         +
        
        
         S
        
        
         (
        
        
         t
        
        
         )
        
        
         +
        
        
         R
        
        
         (
        
        
         t
        
        
         )
        
       
       
         X(t) = T(t)+S(t)+R(t) 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.1389em">T</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0576em">S</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0077em">R</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span></span><br> 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        X
       
       
        (
       
       
        t
       
       
        )
       
      
      
       X(t)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span>是待拆解的时间序列，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        T
       
       
        (
       
       
        t
       
       
        )
       
       
        ,
       
       
        S
       
       
        (
       
       
        t
       
       
        )
       
       
        ,
       
       
        R
       
       
        (
       
       
        t
       
       
        )
       
      
      
       T(t),S(t),R(t)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.1389em">T</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0576em">S</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0077em">R</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span>是对应拆解后的趋势项，季节项，残差项。每一种算法对于其中每一拆解项的计算方式都千差万别，因为这是它们的主要创新点。以及每个子项的组合方式也各不相同，上述介绍的是最普遍的加法模型，实际上还有乘法模型以及混合模型。</p> 
<p><strong>从这个角度看，Autoformer的时序拆解模块其实是最简单的一种：</strong><br> <img src="https://images2.imgbox.com/4a/fb/DMXBZEnk_o.png" alt="在这里插入图片描述"></p> 
<p>第一行的式子解释了趋势项<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         t
        
       
      
      
       X_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的具体算法，第二行的式子解释了Autoformer采用的是最简单的加法模型，并且仅仅拆解两个子项：趋势项和季节项。趋势项的具体算法是：使用一个均值滤波器处理时序，也可以看做使用了一个等长的卷积(same convolution)只不过卷积核的参数是确定的。</p> 
<p>时序拆解部分的算法比较简单，没有什么创新点。作者的思路可能来源于传统算法对于时序拆解的使用，试图移植到Transformer结构中。但实际上该部分还有很多可以深挖的部分，例如使用更复杂的混合模型，以及拆解出多个子项。实际上后来的Fedformer也在这个部分作出了相应的优化和改进。</p> 
<h3>
<a id="Autocorrelation_42"></a>Auto-correlation</h3> 
<p>和大部分的基于Transformer的时序预测模型一样，Autoformer也是将Self-attention的改进作为主要的创新点。和reformer，informer等在它之前的模型不同的是，Autoformer的注意力不主要在于对于Self-attention计算量的减小上，它更加关注将Self-attention移植到时序计算上，设计一个更贴合时序分析的attention结构。</p> 
<p>而时序数据的分析无外乎为：找周期特性，频域分析等，而Auto-correlation关注的就是寻找时序数据的周期特性，它希望模型能够更好的记住时序的周期特性，从而预测时序的未来值。</p> 
<p><img src="https://images2.imgbox.com/8e/0e/BEOXAEVi_o.png" alt="在这里插入图片描述"></p> 
<p>Attention注意力的计算核心在于：相似性的度量。即通过向量内积来衡量向量之间的相似性。而Autoformer度量时序周期性的方式是，对于时间序列进行平移，度量平移前后的时间序列的相似性，度量的具体方式类似于向量的内积。基于这种方法，Autoformer认为，相似性高的平移序列对应的平移量就是潜在的周期。</p> 
<p>当我们每次的平移量为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        τ
       
      
      
       tau
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal" style="margin-right: 0.1132em">τ</span></span></span></span></span>时，上述的相似性计算可以表示为：</p> 
<p><img src="https://images2.imgbox.com/28/d9/oEWlz1Tr_o.png" alt="在这里插入图片描述"></p> 
<p>其中<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         t
        
       
      
      
       X_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>为需要寻找周期性的时间序列,<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         
          t
         
         
          −
         
         
          τ
         
        
       
      
      
       X_{t-tau }
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em;vertical-align: -0.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right: 0.1132em">τ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span></span></span></span></span>表示原时间序列平移<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        τ
       
      
      
       tau
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal" style="margin-right: 0.1132em">τ</span></span></span></span></span>后的序列。也就是说如果<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         
          t
         
         
          −
         
         
          τ
         
        
       
      
      
       X_{t-tau}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8917em;vertical-align: -0.2083em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right: 0.1132em">τ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2083em"><span class=""></span></span></span></span></span></span></span></span></span></span>和 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         t
        
       
      
      
       X_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的相似性比较高，那么我们可以将<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        τ
       
      
      
       tau
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal" style="margin-right: 0.1132em">τ</span></span></span></span></span>看做<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         t
        
       
      
      
       X_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>的潜在周期。</p> 
<p>如果我们迭代计算<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         t
        
       
      
      
       X_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>和所有平移变量之间的相似性，且每次的平移增量为1，那么迭代计算可以用频域计算表示为：<br> <img src="https://images2.imgbox.com/ac/66/oAqWxap0_o.png" alt="在这里插入图片描述"></p> 
<p>因此在实际的Auto-correlation中，输入时间序列<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         t
        
       
      
      
       X_t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em"><span class="" style="margin-left: -0.0785em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>经过<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Q
       
       
        ,
       
       
        K
       
       
        ,
       
       
        V
       
      
      
       Q,K,V
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8778em;vertical-align: -0.1944em"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.2222em">V</span></span></span></span></span>的映射后，先变换到频域，进而在频域中计算平移相似性。</p> 
<h2>
<a id="_66"></a>代码讲解</h2> 
<p>Autoformer的主要创新点以及独特的模块已经在原理层面介绍完毕，接下来笔者将会在实际的代码层面进行细致的讲解。具体的代码细节将会在注释中注明，而需要注意的点将会着重说明</p> 
<h3>
<a id="_70"></a>模块初始化</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Autoformer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Autoformer is the first method to achieve the series-wise connection,
    with inherent O(LlogL) complexity
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> configs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Autoformer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment">#利用历史时间序列的时间戳长度，编码器输入的时间维度</span>
        self<span class="token punctuation">.</span>seq_len <span class="token operator">=</span> configs<span class="token punctuation">.</span>seq_len
        <span class="token comment">#解码器输入的历史时间序列的时间戳长度。</span>
        self<span class="token punctuation">.</span>label_len <span class="token operator">=</span> configs<span class="token punctuation">.</span>label_len
        self<span class="token punctuation">.</span>pred_len <span class="token operator">=</span> configs<span class="token punctuation">.</span>pred_len
        self<span class="token punctuation">.</span>output_attention <span class="token operator">=</span> <span class="token boolean">False</span>

        <span class="token comment"># Decomp，传入参数均值滤波器的核大小</span>
        kernel_size <span class="token operator">=</span> configs<span class="token punctuation">.</span>moving_avg
        self<span class="token punctuation">.</span>decomp <span class="token operator">=</span> series_decomp<span class="token punctuation">(</span>kernel_size<span class="token punctuation">)</span>

        <span class="token comment"># Embedding</span>
        <span class="token comment"># embedding操作，由于时间序列天然在时序上具有先后关系，因此这里embedding的作用更多的是为了调整维度</span>
        self<span class="token punctuation">.</span>enc_embedding <span class="token operator">=</span> DataEmbedding_wo_pos<span class="token punctuation">(</span>configs<span class="token punctuation">.</span>d_feature<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>embed<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>freq<span class="token punctuation">,</span>
                                                  configs<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dec_embedding <span class="token operator">=</span> DataEmbedding_wo_pos<span class="token punctuation">(</span>configs<span class="token punctuation">.</span>d_feature<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>embed<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>freq<span class="token punctuation">,</span>
                                                  configs<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>



        <span class="token comment"># Encoder，采用的是多编码层堆叠</span>
        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> Encoder<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>
                EncoderLayer<span class="token punctuation">(</span>
                    AutoCorrelationLayer<span class="token punctuation">(</span>
                        <span class="token comment">#这里的第一个False表明是否使用mask机制。</span>
                        AutoCorrelation<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">,</span> configs<span class="token punctuation">.</span>factor<span class="token punctuation">,</span> attention_dropout<span class="token operator">=</span>configs<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span>
                                        output_attention<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token comment">#编码过程中的特征维度设置</span>
                    configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>
                    configs<span class="token punctuation">.</span>d_ff<span class="token punctuation">,</span>
                    moving_avg<span class="token operator">=</span>configs<span class="token punctuation">.</span>moving_avg<span class="token punctuation">,</span>
                    dropout<span class="token operator">=</span>configs<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span>
                    <span class="token comment">#激活函数</span>
                    activation<span class="token operator">=</span>configs<span class="token punctuation">.</span>activation
                <span class="token punctuation">)</span> <span class="token keyword">for</span> l <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>configs<span class="token punctuation">.</span>e_layers<span class="token punctuation">)</span>
            <span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token comment">#时间序列通常采用Layernorm而不适用BN层</span>
            norm_layer<span class="token operator">=</span>my_Layernorm<span class="token punctuation">(</span>configs<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        <span class="token comment"># Decoder也是才是用多解码器堆叠</span>
        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> Decoder<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>
                DecoderLayer<span class="token punctuation">(</span>
                    <span class="token comment">#如同传统的Transformer结构，decoder的第一个attention需要mask，保证当前的位置的预测不能看到之前的内容</span>
                    <span class="token comment">#这个做法是来源于NLP中的作法，但是换成时序预测，实际上应该是不需要使用mask机制的。</span>
                    <span class="token comment">#而在后续的代码中可以看出，这里的attention模块实际上都没有使用mask机制。</span>

                    <span class="token comment">#self-attention，输入全部来自于decoder自身</span>
                    AutoCorrelationLayer<span class="token punctuation">(</span>
                        AutoCorrelation<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">,</span> configs<span class="token punctuation">.</span>factor<span class="token punctuation">,</span> attention_dropout<span class="token operator">=</span>configs<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span>
                                        output_attention<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    <span class="token comment">#cross-attention，输入一部分来自于decoder，另一部分来自于encoder的输出</span>
                    AutoCorrelationLayer<span class="token punctuation">(</span>
                        AutoCorrelation<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">,</span> configs<span class="token punctuation">.</span>factor<span class="token punctuation">,</span> attention_dropout<span class="token operator">=</span>configs<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span>
                                        output_attention<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                        configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>n_heads<span class="token punctuation">)</span><span class="token punctuation">,</span>
                    configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span>
                    <span class="token comment">#任务要求的输出特征维度</span>
                    configs<span class="token punctuation">.</span>c_out<span class="token punctuation">,</span>
                    configs<span class="token punctuation">.</span>d_ff<span class="token punctuation">,</span>
                    moving_avg<span class="token operator">=</span>configs<span class="token punctuation">.</span>moving_avg<span class="token punctuation">,</span>
                    dropout<span class="token operator">=</span>configs<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span>
                    activation<span class="token operator">=</span>configs<span class="token punctuation">.</span>activation<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
                <span class="token keyword">for</span> l <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>configs<span class="token punctuation">.</span>d_layers<span class="token punctuation">)</span>
            <span class="token punctuation">]</span><span class="token punctuation">,</span>
            norm_layer<span class="token operator">=</span>my_Layernorm<span class="token punctuation">(</span>configs<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span><span class="token punctuation">,</span>
            projection<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>configs<span class="token punctuation">.</span>d_model<span class="token punctuation">,</span> configs<span class="token punctuation">.</span>c_out<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

</code></pre> 
<p>各个模块的初始化没有什么特别的，参照Autoformer给出的结构图即可一一对应的找到各个模块的具体含义。值得一提的是，这里的初始化考虑了attention模块是否需要使用mask机制。mask-attention来源于NLP任务中的特殊要求，而在时序预测中，attention其实没有mask的必要。因此在后续的代码也可以看出，实际上所有的attention结构都没用使用mask机制。</p> 
<h3>
<a id="Autoformer_159"></a>Autoformer的前向传播</h3> 
<p>首先介绍Autoformer的原始输入：</p> 
<pre><code class="prism language-python">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x_enc<span class="token punctuation">,</span> x_mark_enc<span class="token punctuation">,</span> x_dec<span class="token punctuation">,</span> x_mark_dec<span class="token punctuation">,</span>
                enc_self_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dec_self_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dec_enc_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#x_enc表示编码器的输入</span>
        <span class="token comment">#x_mark_enc表示x_enc中各个时间戳的先后关系</span>
        <span class="token comment">#x_dec表示解码器的输入</span>
        <span class="token comment">#x_mark_dec表示x_dec中各个时间戳的先后关系</span>
</code></pre> 
<p>这里x_mark_enc,x_mark_dec都是为了后续输入的Embedding操作服务的。作者的想法可能来自于原始Transformer的输入Embedding操作。因为在原始Transformer使用的NLP领域，词向量的输入是需要通过Embedding操作来表征词之间的先后关系。但是在时间序列预测领域中，时间序列的时间戳之间天然的具有先后关系，因此笔者认为这里的Embedding操作仅仅起到调整输入的特征维度的作用，对于表明先后关系没有太多作用。</p> 
<p>接下来介绍编码器和解码器的输入：</p> 
<pre><code class="prism language-python">        <span class="token comment"># decomp init</span>
        <span class="token comment"># 因为需要使用生成式预测，所以需要用均值和0来占位，占住预测部分的位置。</span>
        mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x_enc<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>pred_len<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        zeros <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>x_dec<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>pred_len<span class="token punctuation">,</span> x_dec<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>x_enc<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
        seasonal_init<span class="token punctuation">,</span> trend_init <span class="token operator">=</span> self<span class="token punctuation">.</span>decomp<span class="token punctuation">(</span>x_enc<span class="token punctuation">)</span>
        <span class="token comment"># decoder input</span>
        trend_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>trend_init<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>label_len<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mean<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        seasonal_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>seasonal_init<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>label_len<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> zeros<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>因为Autoformer沿用了Informer的生成式预测的方式，所以直接将空有预测部分的输入送到模型中。所以需要初始化预测部分的输入，编码器的输入正常，解码器的输入由Trend和Season构成：</p> 
<ul>
<li>Trend部分：前半部分来自于时序拆解，后半部分即预测部分用均值占位；</li>
<li>Season部分：前半部分来自于时序拆解，后半部分即预测部分用零值占位；</li>
</ul> 
<p>接下来的代码都没有什么特别的，根据paper中的流程图就可以轻松理解：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x_enc<span class="token punctuation">,</span> x_mark_enc<span class="token punctuation">,</span> x_dec<span class="token punctuation">,</span> x_mark_dec<span class="token punctuation">,</span>
            enc_self_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dec_self_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dec_enc_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># decomp init</span>
    <span class="token comment"># 因为需要使用生成式预测，所以需要用均值和0来占位，占住预测部分的位置。</span>
    mean <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>x_enc<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>pred_len<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    zeros <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">[</span>x_dec<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>pred_len<span class="token punctuation">,</span> x_dec<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span>x_enc<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    seasonal_init<span class="token punctuation">,</span> trend_init <span class="token operator">=</span> self<span class="token punctuation">.</span>decomp<span class="token punctuation">(</span>x_enc<span class="token punctuation">)</span>
    <span class="token comment"># decoder input</span>
    trend_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>trend_init<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>label_len<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> mean<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    seasonal_init <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>seasonal_init<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>label_len<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> zeros<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># enc</span>
    enc_out <span class="token operator">=</span> self<span class="token punctuation">.</span>enc_embedding<span class="token punctuation">(</span>x_enc<span class="token punctuation">,</span> x_mark_enc<span class="token punctuation">)</span>
    enc_out<span class="token punctuation">,</span> attns <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>enc_out<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span>enc_self_mask<span class="token punctuation">)</span>
    <span class="token comment"># dec</span>
    dec_out <span class="token operator">=</span> self<span class="token punctuation">.</span>dec_embedding<span class="token punctuation">(</span>seasonal_init<span class="token punctuation">,</span> x_mark_dec<span class="token punctuation">)</span>
    seasonal_part<span class="token punctuation">,</span> trend_part <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>dec_out<span class="token punctuation">,</span> enc_out<span class="token punctuation">,</span> x_mask<span class="token operator">=</span>dec_self_mask<span class="token punctuation">,</span> cross_mask<span class="token operator">=</span>dec_enc_mask<span class="token punctuation">,</span>
                                             trend<span class="token operator">=</span>trend_init<span class="token punctuation">)</span>
    <span class="token comment"># final</span>
    dec_out <span class="token operator">=</span> trend_part <span class="token operator">+</span> seasonal_part

    <span class="token keyword">if</span> self<span class="token punctuation">.</span>output_attention<span class="token punctuation">:</span>
        <span class="token keyword">return</span> dec_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>pred_len<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> attns
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> dec_out<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span>self<span class="token punctuation">.</span>pred_len<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>
</code></pre> 
<h3>
<a id="_223"></a>时序拆解器</h3> 
<p>时序拆解器的算法原理可以参照上述的讲解以及paper，它的代码如下所示：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">moving_avg</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Moving average block to highlight the trend of time series
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> stride<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>moving_avg<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> kernel_size
        self<span class="token punctuation">.</span>avg <span class="token operator">=</span> nn<span class="token punctuation">.</span>AvgPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span>kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># padding on the both ends of time series</span>
        <span class="token comment"># 这里的判断语句主要是为了和Fedformer的部分进行区分，如果只考虑autoformer可以默认这里的判断全是True</span>
        <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>kernel_size<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token builtin">list</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>kernel_size<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>kernel_size <span class="token operator">=</span> self<span class="token punctuation">.</span>kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        front <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token operator">-</span>math<span class="token punctuation">.</span>floor<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        end <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> math<span class="token punctuation">.</span>floor<span class="token punctuation">(</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>front<span class="token punctuation">,</span> x<span class="token punctuation">,</span> end<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>avg<span class="token punctuation">(</span>x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token keyword">class</span> <span class="token class-name">series_decomp</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Series decomposition block
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> kernel_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>series_decomp<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>moving_avg <span class="token operator">=</span> moving_avg<span class="token punctuation">(</span>kernel_size<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        moving_mean <span class="token operator">=</span> self<span class="token punctuation">.</span>moving_avg<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        res <span class="token operator">=</span> x <span class="token operator">-</span> moving_mean
        <span class="token keyword">return</span> res<span class="token punctuation">,</span> moving_mean
</code></pre> 
<p>时序拆解器就是一个简单的均值滤波器，值得注意的是这里的滤波全部沿用的是same conv 即不改变输入的时间维度大小。其中一个目的是为了后续的维度统一，笔者认为在传统的LSTM或GRU模块中，一维卷积还是valid conv会好一些因为如果传入LSTM的时间戳个数过多，会加剧梯度爆炸和梯度弥散等问题。</p> 
<h3>
<a id="Autocorrelation_269"></a>Autocorrelation</h3> 
<p>Autocorrelation是autoformer的最大创新点，算法细节如前所述，这里主要讲解实际的代码细节，一边参照流程图，一边看代码会更好理解：<br> <img src="https://images2.imgbox.com/42/ea/PAYXz0N3_o.png" alt="在这里插入图片描述"></p> 
<p>实际的代码也基本是照着流程图走的，只是有些实现的小细节第一遍看来不太好懂，首先是qkv的处理，需要注意的是encoder和decoder中qkv的来源不太相同，encoder中的qkv全部来源一个输入，而decoder的qkv不知来源于decoder本身还来源于encoder的输出：</p> 
<pre><code class="prism language-python">    <span class="token comment">#首先对于q，k进行FFT变换</span>
    q_fft <span class="token operator">=</span> torch<span class="token punctuation">.</span>fft<span class="token punctuation">.</span>rfft<span class="token punctuation">(</span>queries<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># size=[B, H, E, L]</span>
    k_fft <span class="token operator">=</span> torch<span class="token punctuation">.</span>fft<span class="token punctuation">.</span>rfft<span class="token punctuation">(</span>keys<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment">#对k进行求共轭，并与q点乘</span>
    res <span class="token operator">=</span> q_fft <span class="token operator">*</span> torch<span class="token punctuation">.</span>conj<span class="token punctuation">(</span>k_fft<span class="token punctuation">)</span>
    <span class="token comment">#傅里叶反变换计算得出的值就是每一个时移因子的相似性大小</span>
    corr <span class="token operator">=</span> torch<span class="token punctuation">.</span>fft<span class="token punctuation">.</span>irfft<span class="token punctuation">(</span>res<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># size=[B, H, E, L]</span>
    V <span class="token operator">=</span> self<span class="token punctuation">.</span>time_delay_agg_training<span class="token punctuation">(</span>values<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> corr<span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
</code></pre> 
<p>计算出了每个时移因子的相似性大小，接下来就要基于此对于V进行处理：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">time_delay_agg_full</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> values<span class="token punctuation">,</span> corr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Standard version of Autocorrelation
    """</span>
    batch <span class="token operator">=</span> values<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    head <span class="token operator">=</span> values<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    channel <span class="token operator">=</span> values<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
    length <span class="token operator">=</span> values<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>
    <span class="token comment"># index init，索引值初始化，类似于初始化自变量x，由于是多维处理，因此需要在各个维度上进行repeat</span>
    init_index <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>length<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>batch<span class="token punctuation">,</span> head<span class="token punctuation">,</span> channel<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># find top k，选取最大的几个时移因子</span>
    top_k <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>factor <span class="token operator">*</span> math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>length<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment">#[0]返回的是权重值,[1]返回的是对应的索引d</span>
    weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>corr<span class="token punctuation">,</span> top_k<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    delay <span class="token operator">=</span> torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>corr<span class="token punctuation">,</span> top_k<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token comment"># update corr</span>
    <span class="token comment"># 使用softmax对权重值进行归一化处理</span>
    tmp_corr <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>weights<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token comment"># aggregation</span>
    <span class="token comment"># 这里用的操作比较巧，笔者一开始看的也不明所以，这里之所以最后一个维度repeat两次是因为流程图中Roll(V)的操作，也就是将前一部分的序列位置平移拼接到原序列的末尾，因此这里repeat两次是为了后续通过截取直接实现Roll(V)操作</span>
    tmp_values <span class="token operator">=</span> values<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
    delays_agg <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>values<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>top_k<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#init_index+delay就相当于自变量给一个平移增量即 x = x+d</span>
        tmp_delay <span class="token operator">=</span> init_index <span class="token operator">+</span> delay<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token comment">#这里就是Roll(V)操作，相当于根据索引x直接对于V进行截取</span>
        pattern <span class="token operator">=</span> torch<span class="token punctuation">.</span>gather<span class="token punctuation">(</span>tmp_values<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> index<span class="token operator">=</span>tmp_delay<span class="token punctuation">)</span>
        delays_agg <span class="token operator">=</span> delays_agg <span class="token operator">+</span> pattern <span class="token operator">*</span> <span class="token punctuation">(</span>tmp_corr<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> delays_agg
</code></pre> 
<h3>
<a id="Encoder_324"></a>Encoder</h3> 
<p>编码器和原始的Transformer结构没差，除了将self-attention改为auto-correlation外，采用的多个编码层堆叠的结构。我们直接上代码：</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Autoformer encoder layer with the progressive decomposition architecture
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> attention<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> d_ff<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> moving_avg<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        d_ff <span class="token operator">=</span> d_ff <span class="token keyword">or</span> <span class="token number">4</span> <span class="token operator">*</span> d_model

        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> attention
        <span class="token comment">#这里的卷积是为了替代feed forward模块</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>d_model<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>d_ff<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>d_ff<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>d_model<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>decomp1 <span class="token operator">=</span> series_decomp<span class="token punctuation">(</span>moving_avg<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decomp2 <span class="token operator">=</span> series_decomp<span class="token punctuation">(</span>moving_avg<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> F<span class="token punctuation">.</span>relu <span class="token keyword">if</span> activation <span class="token operator">==</span> <span class="token string">"relu"</span> <span class="token keyword">else</span> F<span class="token punctuation">.</span>gelu
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> attn_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

		<span class="token comment">#Autocorrelation</span>
        new_x<span class="token punctuation">,</span> attn <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>
            x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span>
            attn_mask<span class="token operator">=</span>attn_mask
        <span class="token punctuation">)</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>new_x<span class="token punctuation">)</span>
        x<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>decomp1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> x
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>y<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        res<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>decomp2<span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>
        <span class="token keyword">return</span> res<span class="token punctuation">,</span> attn

</code></pre> 
<p>只要理解了Autocorrelation，autoformer的代码就没有什么复杂的地方，完全是按照正常的Transformer模块进行的设计。对着autoformer的流程图就可以很好的理解。</p> 
<h3>
<a id="Decoder_370"></a>Decoder</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Autoformer Decoder
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layers<span class="token punctuation">,</span> norm_layer<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> projection<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>layers<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> norm_layer
        self<span class="token punctuation">.</span>projection <span class="token operator">=</span> projection

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> cross<span class="token punctuation">,</span> x_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> cross_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> trend<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>
            <span class="token comment">#每一次都将趋势项累加，季节项传递给更深的网络进行处理</span>
            x<span class="token punctuation">,</span> residual_trend <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> cross<span class="token punctuation">,</span> x_mask<span class="token operator">=</span>x_mask<span class="token punctuation">,</span> cross_mask<span class="token operator">=</span>cross_mask<span class="token punctuation">)</span>
            trend <span class="token operator">=</span> trend <span class="token operator">+</span> residual_trend

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>norm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>projection <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment">#从高维映射回输出维度</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> trend
</code></pre> 
<p>Decoder也是多个decoder层进行堆叠，而每一个decoder层都共享encoder的输出</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Autoformer decoder layer 
    """</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> self_attention<span class="token punctuation">,</span> cross_attention<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> c_out<span class="token punctuation">,</span> d_ff<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                 moving_avg<span class="token operator">=</span><span class="token number">25</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">"relu"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        d_ff <span class="token operator">=</span> d_ff <span class="token keyword">or</span> <span class="token number">4</span> <span class="token operator">*</span> d_model
        self<span class="token punctuation">.</span>self_attention <span class="token operator">=</span> self_attention
        self<span class="token punctuation">.</span>cross_attention <span class="token operator">=</span> cross_attention
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>d_model<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>d_ff<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>d_ff<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>d_model<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>decomp1 <span class="token operator">=</span> series_decomp<span class="token punctuation">(</span>moving_avg<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decomp2 <span class="token operator">=</span> series_decomp<span class="token punctuation">(</span>moving_avg<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>decomp3 <span class="token operator">=</span> series_decomp<span class="token punctuation">(</span>moving_avg<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>projection <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span>d_model<span class="token punctuation">,</span> out_channels<span class="token operator">=</span>c_out<span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                                    padding_mode<span class="token operator">=</span><span class="token string">'circular'</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>activation <span class="token operator">=</span> F<span class="token punctuation">.</span>relu <span class="token keyword">if</span> activation <span class="token operator">==</span> <span class="token string">"relu"</span> <span class="token keyword">else</span> F<span class="token punctuation">.</span>gelu

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> cross<span class="token punctuation">,</span> x_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> cross_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#self-attention输入全部来自于decoder自身</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>self_attention<span class="token punctuation">(</span>
            x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span>
            attn_mask<span class="token operator">=</span>x_mask
        <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        x<span class="token punctuation">,</span> trend1 <span class="token operator">=</span> self<span class="token punctuation">.</span>decomp1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment">#cross-attention：q来自于decoder，k，v则来自于encoder的输出</span>
        x <span class="token operator">=</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>cross_attention<span class="token punctuation">(</span>
            x<span class="token punctuation">,</span> cross<span class="token punctuation">,</span> cross<span class="token punctuation">,</span>
            attn_mask<span class="token operator">=</span>cross_mask
        <span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        x<span class="token punctuation">,</span> trend2 <span class="token operator">=</span> self<span class="token punctuation">.</span>decomp2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        y <span class="token operator">=</span> x
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>activation<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>y<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        y <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        x<span class="token punctuation">,</span> trend3 <span class="token operator">=</span> self<span class="token punctuation">.</span>decomp3<span class="token punctuation">(</span>x <span class="token operator">+</span> y<span class="token punctuation">)</span>
		
        <span class="token comment">#趋势项累加，季节项通过模块进行处理</span>
        residual_trend <span class="token operator">=</span> trend1 <span class="token operator">+</span> trend2 <span class="token operator">+</span> trend3
        residual_trend <span class="token operator">=</span> self<span class="token punctuation">.</span>projection<span class="token punctuation">(</span>residual_trend<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> residual_trend
</code></pre> 
<p>Decoder部分的代码也是比较清晰明了的，至此autoformer部分的代码已经介绍完毕，下一篇会介绍基于autoformer进行改进的Fedformer。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>