<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>命名实体识别（NER）算法 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">命名实体识别（NER）算法</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#_4">标注方案</a></li>
<li><a href="#_14">问题建模</a></li>
<li><a href="#_17">评价指标</a></li>
<li><a href="#NER_21">常用的NER方法</a></li>
<li>
<ul>
<li><a href="#in_survey_31">深度学习方法（in survey）</a></li>
<li>
<ul>
<li><a href="#_35">输入的分布式表示</a></li>
<li><a href="#_48">上下文编码</a></li>
<li><a href="#_60">解码器</a></li>
</ul>
   </li>
<li><a href="#_76">实践</a></li>
<li>
<ul>
<li><a href="#Bert__Softmax_77">Bert + Softmax</a></li>
<li><a href="#Bert__CRF_109">Bert + CRF</a></li>
</ul>
  </li>
</ul>
  </li>
<li><a href="#_233">参考</a></li>
</ul>
</div>
<p></p> 
<p>在论文<a href="https://ieeexplore.ieee.org/abstract/document/9039685/">A Survey on Deep Learning for Named Entity Recognition</a> 中对NER工作进行了详尽的介绍。本文根据综述的行文顺序，快速的介绍NER使用的技术，并在最后一小节中附上bert+softmax与bert+crf的相关代码。</p> 
<h1>
<a id="_4"></a>标注方案</h1> 
<ul>
<li>BIO</li>
<li>BIOES</li>
</ul> 
<blockquote> 
 <p>B，开始位置<br> I，中间位置<br> E，结束位置<br> O，other<br> S，single</p> 
</blockquote> 
<h1>
<a id="_14"></a>问题建模</h1> 
<p>给定一个符号（Token）序列 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        s
       
       
        =
       
       
        (
       
       
        
         w
        
        
         1
        
       
       
        ,
       
       
        
         w
        
        
         2
        
       
       
        ,
       
       
        .
       
       
        .
       
       
        .
       
       
        ,
       
       
        
         w
        
        
         N
        
       
       
        )
       
      
      
       s = (w_1, w_2, ..., w_N)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">s</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.328331em"><span class="" style="margin-left: -0.02691em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.10903em">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span>， NER的任务就是要输出一个元组序列 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        
         I
        
        
         s
        
       
       
        ,
       
       
        
         I
        
        
         e
        
       
       
        ,
       
       
        t
       
       
        )
       
      
      
       (I_s, I_e, t)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span></span></span>, 其中每一个元组表示序列s中出现的一个命名实体。 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         I
        
        
         s
        
       
       
        ∈
       
       
        [
       
       
        1
       
       
        ,
       
       
        N
       
       
        ]
       
       
        ,
       
       
        
         I
        
        
         e
        
       
       
        ∈
       
       
        [
       
       
        1
       
       
        ,
       
       
        N
       
       
        ]
       
      
      
       I_s in [1,N], I_e in [1,N]
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.83333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">s</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">N</span><span class="mclose">]</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">I</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: -0.07847em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">e</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">N</span><span class="mclose">]</span></span></span></span></span>, 分别代表实体在符号序列中的位置，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        t
       
      
      
       t
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.61508em;vertical-align: 0em"></span><span class="mord mathdefault">t</span></span></span></span></span>表示该实体的类别。比如：<br> <img src="https://images2.imgbox.com/a9/9b/35PAzfn3_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="_17"></a>评价指标</h1> 
<ul>
<li>宽松匹配评估（Relaxed-match Evaluation）：当实体位置区间部分重叠,或位置正确类别错误的,都记为正确或部分正确。但因为这样的评估方案不够直观，对于错误分析很不友好，所以并没有被广泛采用。</li>
<li>严格匹配评估（Exact-match Evaluation）：当且仅当实体位置区间和类别都正确时判定其为正确。<strong>其中F1值又可以分为macro-averaged和micro-averaged，前者是按照不同实体类别计算F1，然后取平均；后者是把所有识别结果合在一起，再计算F1。</strong> 这两者的区别在于实体类别数目不均衡，因为通常语料集中类别数量分布不均衡，模型往往对于大类别的实体学习较好。</li>
</ul> 
<h1>
<a id="NER_21"></a>常用的NER方法</h1> 
<p><img src="https://images2.imgbox.com/90/26/LGVpEhJw_o.png" alt="在这里插入图片描述"><br> 如上图所示，常用的NER方法主要分为四个大类：</p> 
<ul>
<li>基于规则</li>
<li>无监督学习</li>
<li>基于人工特征的监督学习方法</li>
<li>深度学习方法</li>
</ul> 
<p>目前，深度学习方法是NER常用的方法，本文具体介绍深度学习方法。</p> 
<h2>
<a id="in_survey_31"></a>深度学习方法（in survey）</h2> 
<p>本小结按照 <a href="https://arxiv.org/pdf/1812.09449.pdf">综述</a> 的顺序简要介绍 NER中常用的深度学习方法。</p> 
<p><img src="https://images2.imgbox.com/20/17/Kfo8uITz_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="_35"></a>输入的分布式表示</h3> 
<p>输入的分布式可以分为：词语级别表示、字符级别表示和混合表示。</p> 
<ul>
<li> <p>词语级别表示： (word2vec/fasttest/glove等)，对于英文而言就是单词，比如 hello。<strong>而对中文而言，可能字符级别的表示要更好一些，这是因为基于此的中文NER会受分词效果的影响，切词错误会传递到NER中，会造成累计误差。</strong> 但是也有的论文利用了中文词级别的信息来做的，比如：<a href="https://arxiv.org/pdf/1805.02023.pdf'%20rel=">Lattice LSTM</a>。</p> </li>
<li> <p>字符级别的表示：（CNN/RNN）。字符级别的表示方法可以比较有效地挖掘如前缀、后缀等sub-word级别的信息，另一方面的优势在于其天然地可以解决OOV问题。<br> <img src="https://images2.imgbox.com/ea/21/dlobUU3f_o.png" alt="在这里插入图片描述"><br> 对于中文而言，字符级别是比较常用的表示，但是每个字通常具有多重含义，比如说：“花哨”、“花茶”中的“花”显然含义不同；因此，想要捕捉语义层面的信息的话，就需要对上下文语义进行建模从而充分表达字的含义，常见的就是用CNN来做，RNN也可以。</p> <p>比如，18年的一项工作《Contextual String Embeddings for Sequence Labeling》，使用字符级别的神经语言模型产生上下文相关的文本嵌入。大致思路为使用双向RNN编码字符级别嵌入，将一个词的前向和后向隐层状态与词嵌入拼接作为最终词嵌入向量，如下图所示。<a href="https://github.com/flairNLP/flair">源码位置</a><br> <img src="https://images2.imgbox.com/b3/92/5dKRATqD_o.png" alt="在这里插入图片描述"></p> </li>
<li> <p>混合表示：部分研究在词语级别和字符级别外还引入了其他额外信息，如词语相似度、语义依存关系、视觉特征、知识图谱等。<strong>某种程度上而言，使用Token Embedding、Position Embedding等预训练得到的类BERT模型也可以划分到混合表示方法中。</strong></p> </li>
</ul> 
<h3>
<a id="_48"></a>上下文编码</h3> 
<ul>
<li>CNN： 如下图所示，主要用于捕捉局部依赖特征。虽然可以通过多层CNN的叠加加大感受野而得到全局特征，但是，对于NER来说，这种长程依赖性还是不足。因此，CNN大多后面可以接LSTM或者attention来强化长程依赖能力。<br> <img src="https://images2.imgbox.com/fe/ff/8Aq5pECb_o.png" alt="在这里插入图片描述">
</li>
<li>RNN，常用的有LSTM和GRU，可以获得较好的长程依赖能力。而且由于门机制的存在，也能获得较好的局部依赖性。在NLP中常用BILSTM获得上下文特征。<br> <img src="https://images2.imgbox.com/d7/5c/VGN2pH7B_o.png" alt="在这里插入图片描述">
</li>
<li>递归神经网络。把句子当作树状结构而非序列进行处理，从理论上而言具有更强的表示能力，但其存在样本标注难度大（需标注语法解析树）、深层易梯度消失、难以并行计算等弱点，因此在实际应用中使用较少。<br> <img src="https://images2.imgbox.com/8f/07/B7JOV2kQ_o.png" alt="在这里插入图片描述">
</li>
<li>Transformer：是近年来使用广泛的网络，transformer在长距离文本依赖上相较RNN有更好的效果。</li>
</ul> 
<blockquote> 
 <p>但是将transformer直接应用于NER任务时，可能效果不那么理想，原因可能是对于NER而言，它需要的是：（1）局部依赖的捕捉；（2）位置信息；（3）更 sharp 的attention 分布（NER不需要太多的全局信息）。有论文表明，这些问题解决后，呈现出比CNN+BiLSTM+CRF更好的效果。</p> 
</blockquote> 
<ul><li>语言模型：大规模预训练语言模型近年来在各种NLP任务中取得巨大成功，如GPT（Generative Pre-trained Transformer）、ELMo、BERT等。这些预训练语言模型不但有效地捕捉了文本中的上下文关系，且不需要如Word2Vec、GloVe等传统词向量进行分布式表示。</li></ul> 
<h3>
<a id="_60"></a>解码器</h3> 
<p>标注解码器是NER模型的最后一部分，其将经过上下文编码器得到的表征作为输入，常用的解码器组合有：MLP+Softmax，CRF，RNN，Pointer Networks等。</p> 
<p><strong>先说MLP+softmax和CRF：</strong></p> 
<p>CRF有一系列特征函数，相当于增加了一系列约束条件。比如说：E 后面跟 B的可能性比E后面跟I的可能性（这个几乎不可能）要大得多。但是softmax就不存在这样的约束，这样的话就相当于这一部分需要在放在前面的模型去学习。这也就是为什么CRF解码器如此常见的原因。</p> 
<p>但是，为什么Bert+CRF相对于Bert+softmax性能没什么提升呢？甚至有的时候会带来一些负面影响？</p> 
<blockquote> 
 <p>可以参考 <a href="https://mp.weixin.qq.com/s/g-5MWmh3ys5wyXpIjBG8oQ">你的 CRF 层的学习率可能不够大</a> 这篇文章。</p> 
</blockquote> 
<p>简单来说就是，BERT 的拟合能力太强了，导致不需要转移矩阵效果都很好。而不好的转移矩阵会给结果带来负面的影响。</p> 
<p><strong>RNN：</strong><br> <img src="https://images2.imgbox.com/62/54/wwFoXy8o_o.png" alt="在这里插入图片描述"><br> <strong>Pointer Networks：使用指针网络解码，是将NER任务当作先识别“块”即实体范围，然后再对其进行分类。指针网络通常是在Seq2seq框架中</strong><br> <img src="https://images2.imgbox.com/9d/9d/GwoCMBGR_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="_76"></a>实践</h2> 
<h3>
<a id="Bert__Softmax_77"></a>Bert + Softmax</h3> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertSoftmax</span><span class="token punctuation">(</span>BertPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># 大部分与transformer 提供的BertForTokenClassification相同，这里只是改了点loss</span>
  <span class="token comment"># config 来自bertconfig,其实本质是一个分类问题。即每个token属于什么类别</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>config<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>BertSoftmax<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> config<span class="token punctuation">.</span>num_labels
    self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_dropout_prob<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>classifier<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>config<span class="token punctuation">.</span>num_labels<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>loss_type<span class="token operator">=</span>config<span class="token punctuation">.</span>loss_type
    self<span class="token punctuation">.</span>init_weights<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> labels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># last hidden state,pooler output, past_key_values, hidden_states,attentions,cross_attentions</span>
    outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>input_ids<span class="token operator">=</span>input_ids<span class="token punctuation">,</span> attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">)</span>
    sequence_output <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    sequence_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>sequence_output<span class="token punctuation">)</span>
    logits <span class="token operator">=</span> self<span class="token punctuation">.</span>classifier<span class="token punctuation">(</span>sequence_output<span class="token punctuation">)</span>
    outputs <span class="token operator">=</span> <span class="token punctuation">(</span>logits<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> outputs<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># add hidden states and attention if they are here</span>
    <span class="token keyword">if</span> labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
      loss_fct <span class="token operator">=</span> CrossEntropyLoss<span class="token punctuation">(</span>ignore_index<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
      <span class="token keyword">if</span> attention_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        active_loss <span class="token operator">=</span> attention_mask<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">1</span>
        active_logits <span class="token operator">=</span> logits<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_labels<span class="token punctuation">)</span><span class="token punctuation">[</span>active_loss<span class="token punctuation">]</span>
        active_labels <span class="token operator">=</span> labels<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">[</span>active_loss<span class="token punctuation">]</span>
        loss <span class="token operator">=</span> loss_fct<span class="token punctuation">(</span>active_logits<span class="token punctuation">,</span> active_labels<span class="token punctuation">)</span>
      <span class="token keyword">else</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> loss_fct<span class="token punctuation">(</span>logits<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_labels<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
      outputs <span class="token operator">=</span> <span class="token punctuation">(</span>loss<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> outputs
      <span class="token keyword">return</span> outputs  <span class="token comment"># (loss), scores, (hidden_states), (attentions)</span>
</code></pre> 
<h3>
<a id="Bert__CRF_109"></a>Bert + CRF</h3> 
<p>CRF: 这里展示的代码没展示解码（也就是维特比算法）：</p> 
<pre><code class="prism language-python"><span class="token comment"># CRF class</span>
<span class="token keyword">class</span> <span class="token class-name">CRF</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token triple-quoted-string string">"""
  发射矩阵（来自于bert层），其shape为(seq_length,batch_size,num_tags)，如果batch_first=true,那么应该是(batch_size,seq_length,num_tags)
  转移矩阵：shape是(num_tags,num_tags)
  训练： 主要是计算max的正确路径/总路径的大小
  预测： 维特比算法
  """</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>num_tags<span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>num_tags<span class="token operator">=</span>num_tags
    self<span class="token punctuation">.</span>batch_first<span class="token operator">=</span>batch_first
    <span class="token comment"># 转移矩阵</span>
    self<span class="token punctuation">.</span>transitions<span class="token operator">=</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span>num_tags<span class="token punctuation">,</span> num_tags<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token comment"># 初始化</span>
    self<span class="token punctuation">.</span>init_parameter<span class="token punctuation">(</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">init_parameter</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>transitions<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> emissions<span class="token punctuation">,</span>tags<span class="token punctuation">,</span>mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    batch_first表征batch是不是在头部
    emissions: 从bert层传过来的发射矩阵。
    tags: seq labels, shape:(seq_length, batch_size)
    mask: mask tensor, shape:(seq_length,batch_size)
    返回值： log likelihood, shape: (batch_size,)
    """</span>
    <span class="token comment"># 处理mask的格式</span>
    <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
      <span class="token comment"># 如果mask是None,那么所有的分数都是有效的</span>
      mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>tags<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">,</span> device<span class="token operator">=</span>tags<span class="token punctuation">.</span>device<span class="token punctuation">)</span>
    <span class="token keyword">if</span> mask<span class="token punctuation">.</span>dtype <span class="token operator">!=</span> torch<span class="token punctuation">.</span>uint8<span class="token punctuation">:</span>
      mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>byte<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 处理batch first</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>batch_first<span class="token punctuation">:</span>
      emissions <span class="token operator">=</span> emissions<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
      tags <span class="token operator">=</span> tags<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
      mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    
    <span class="token comment"># 计算正确的label的分数</span>
    <span class="token comment"># shape: (batch_size,)</span>
    numerator <span class="token operator">=</span> self<span class="token punctuation">.</span>_compute_score<span class="token punctuation">(</span>emissions<span class="token punctuation">,</span> tags<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
    <span class="token comment"># 计算归一化因子</span>
    denominator <span class="token operator">=</span> self<span class="token punctuation">.</span>_compute_normalizer<span class="token punctuation">(</span>emissions<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>
    <span class="token comment"># shape: (batch_size,)</span>
    llh <span class="token operator">=</span> numerator <span class="token operator">-</span> denominator
    <span class="token keyword">return</span> llh<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">/</span> mask<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>


  <span class="token keyword">def</span> <span class="token function">_compute_score</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>emissions<span class="token punctuation">,</span>tags<span class="token punctuation">,</span>mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 需要计算point label 和 trans_label</span>
    seq_length<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> tags<span class="token punctuation">.</span>shape
    mask <span class="token operator">=</span> mask<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

    score <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    <span class="token comment"># shape:(batch_size,)</span>
    <span class="token comment"># 第一步，加的是所有batch的seq第0个位置的point score</span>
    <span class="token comment"># tags[0]:(batch_size,)</span>
    <span class="token comment"># emissions (seq_len,batch_size,num_tags)</span>
    score <span class="token operator">+=</span> emissions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">,</span>tags<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span>

    <span class="token comment"># 开始计算转移分数 与 余下的point分数</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>seq_length<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token comment"># 如果这个分数是有效的，则加入（mask=1）</span>
      score <span class="token operator">+=</span> emissions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">,</span>tags<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
      <span class="token comment"># 开始计算转移分数</span>
      score <span class="token operator">+=</span> self<span class="token punctuation">.</span>transitions<span class="token punctuation">[</span>tags<span class="token punctuation">[</span>i <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> tags<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">*</span> mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
    
    <span class="token keyword">return</span> score
  
  <span class="token keyword">def</span> <span class="token function">_compute_normalizer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>emissions<span class="token punctuation">,</span>mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># 计算归一化因子，逐标签得分 + 转移概率得分（考虑的是全状态）</span>
    <span class="token comment"># return：归一化分数，shape:(batch_size,)</span>
    <span class="token comment"># emissions (seq_len,batch_size,num_tags)</span>
    seq_length <span class="token operator">=</span> emissions<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token comment"># 考虑某一个batch，每一个num_tags可以作为一个hidden state(考虑rnn的图，所以一开始应该是 batch_size, num_tags)</span>
    score <span class="token operator">=</span> emissions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>  <span class="token comment"># shape:(batch_size,num_tags) 这个是在位置0每个num_tags的标签得分</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span>seq_length<span class="token punctuation">)</span><span class="token punctuation">:</span>
      <span class="token comment"># shape : (batch_size, num_tags, 1)</span>
      <span class="token comment"># 之前的state h_t</span>
      broadcast_score <span class="token operator">=</span> score<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
      <span class="token comment"># shape: (batch_size, 1, num_tags)</span>
      <span class="token comment"># x_t+1</span>
      broadcast_emissions <span class="token operator">=</span> emissions<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
      <span class="token comment"># h_t+1 = f(h_t,x_t+1),  h_t+1=h_t*x_t+1*g</span>
      next_score <span class="token operator">=</span> broadcast_score <span class="token operator">+</span> self<span class="token punctuation">.</span>transitions <span class="token operator">+</span> broadcast_emissions
      <span class="token comment"># (batch_size,num_tags,num_tags)</span>
      next_score <span class="token operator">=</span> torch<span class="token punctuation">.</span>logsumexp<span class="token punctuation">(</span>next_score<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
      <span class="token comment"># Set score to the next score if this timestep is valid (mask == 1)</span>
      <span class="token comment"># (seq_length,1, batch_size)</span>
      score <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>mask<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> next_score<span class="token punctuation">,</span> score<span class="token punctuation">)</span>
    <span class="token comment"># (batch_size,)</span>
    <span class="token keyword">return</span> torch<span class="token punctuation">.</span>logsumexp<span class="token punctuation">(</span>score<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p>Bert+CRF:</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">BertCRF</span><span class="token punctuation">(</span>BertPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>config<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>BertCRF<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>bert<span class="token operator">=</span>BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dropout<span class="token operator">=</span>nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_dropout_prob<span class="token punctuation">)</span>
    <span class="token comment"># 这是crf的发射概率</span>
    self<span class="token punctuation">.</span>classifier<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span>config<span class="token punctuation">.</span>num_labels<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>crf <span class="token operator">=</span> CRF<span class="token punctuation">(</span>num_tags<span class="token operator">=</span>config<span class="token punctuation">.</span>num_labels<span class="token punctuation">,</span> batch_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>init_weights<span class="token punctuation">(</span><span class="token punctuation">)</span>
  
  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_ids<span class="token punctuation">,</span> token_type_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>labels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token comment"># last hidden state,pooler output, past_key_values, hidden_states,attentions,cross_attentions</span>
    outputs <span class="token operator">=</span>self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span>input_ids <span class="token operator">=</span> input_ids<span class="token punctuation">,</span>attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span>token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">)</span>
    sequence_output <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    sequence_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>sequence_output<span class="token punctuation">)</span>
    logits <span class="token operator">=</span> self<span class="token punctuation">.</span>classifier<span class="token punctuation">(</span>sequence_output<span class="token punctuation">)</span>
    outputs <span class="token operator">=</span> <span class="token punctuation">(</span>logits<span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> labels <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        loss <span class="token operator">=</span> self<span class="token punctuation">.</span>crf<span class="token punctuation">(</span>emissions <span class="token operator">=</span> logits<span class="token punctuation">,</span> tags<span class="token operator">=</span>labels<span class="token punctuation">,</span> mask<span class="token operator">=</span>attention_mask<span class="token punctuation">)</span>
        outputs <span class="token operator">=</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token operator">*</span>loss<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token operator">+</span>outputs
    <span class="token keyword">return</span> outputs <span class="token comment"># (loss), scores</span>
</code></pre> 
<h1>
<a id="_233"></a>参考</h1> 
<p><a href="https://arxiv.org/pdf/1812.09449.pdf">A Survey on Deep Learning for Named Entity Recognition</a><br> <a href="https://zhuanlan.zhihu.com/p/44042528">最通俗易懂的BILSTM-CRF的CRF层介绍</a><br> <a href="https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;mid=2247489378&amp;idx=1&amp;sn=0e0ed4424bb336022f36d8e2236f96cc&amp;chksm=96e9c8e2a19e41f4d1fb67254ee3c057ce66a4eaa4084db89d53f314c833b73fb79b8ee3c0dd&amp;scene=21#wechat_redirect">简明条件随机场CRF介绍 | 附带纯Keras实现</a><br> <a href="https://github.com/bojone/crf">keras实现源码</a><br> <a href="https://www.zhihu.com/question/358892919">BERT标注为何不使用CRF</a><br> <a href="https://zhuanlan.zhihu.com/p/141088583">NER综述</a><br> <a href="https://zhuanlan.zhihu.com/p/346366561">命名实体识别NER</a><br> <a href="https://zhuanlan.zhihu.com/p/100087966">NER论文大礼包</a></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>