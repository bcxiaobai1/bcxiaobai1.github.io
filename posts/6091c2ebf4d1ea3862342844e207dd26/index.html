<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>MXNet中使用双向循环神经网络BiRNN对文本进行情感分类 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">MXNet中使用双向循环神经网络BiRNN对文本进行情感分类</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <div class="kdocs-document">
 <p style="text-indent:1.4em">文本分类类似于图片分类，也是很常见的一种分类任务，将一段不定长的文本序列变换为文本的类别。这节主要就是关注文本的<span class="kdocs-bold" style="font-weight:bold">情感分析(sentiment analysis)</span>，对电影的评论进行一个正面情绪与负面情绪的分类。</p>
 <h2>整理数据集</h2>
 <p>第一步都是将数据集整理好，这里我们使用<span class="kdocs-bold" style="font-weight:bold">"大型电影评论数据集"LMDB(Large Movie Review Dataset v1.0)</span>，该数据集包含电影评论及其相关二进制情感标签。标签的整体分布是平衡的，一半的正类标签和一半的负类标签，另外有一些未贴标签的用于无监督学习。电影评分满分是10分，将评分&gt;=7分的判定为正面评论，评论得分&lt;= 4分则为负面评论。</p>
 <p>下载数据集，可以使用自带的函数</p>
 <pre class="kdocs-python"><code class="language-python">import d2lzh as d2l
d2l.download_imdb(data_dir='data')</code></pre>
 <p>或者手动下载：<a class="kdocs-link" style="color:#0A6CFF" href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz" target="_blank" rel="noopener noreferrer">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></p>
 <p>自动下载虽然只有80M的大小，但是下载特别慢。这里依然<span class="kdocs-color" style="color:#C21C13">推荐迅雷下载</span>，下载下来之后就手动解压(自动下载的函数包括自动解压)</p>
 <p>我们先来看下这个数据集里面有一些什么内容，本人地址截图如下：</p>
 <div class="kdocs-line-container">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:1105px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:26.063349%;height:0">
    <img src="https://images2.imgbox.com/4d/27/4NWs1ZT0_o.png" style="margin-left:;width:1105px;margin-top:-26.063349%;height:auto">
   </div>
  </div>
 </div>
 <div class="kdocs-line-container">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:1170px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:31.880342%;height:0">
    <img src="https://images2.imgbox.com/cf/7a/Pp7k1TBY_o.png" style="margin-left:;width:1170px;margin-top:-31.880342%;height:auto">
   </div>
  </div>
 </div>
 <p>可以看到有<span class="kdocs-color" style="color:#C21C13"><span class="kdocs-bold" style="font-weight:bold">train</span></span>和<span class="kdocs-color" style="color:#C21C13"><span class="kdocs-bold" style="font-weight:bold">test</span></span>两个数据集，里面都有<span class="kdocs-color" style="color:#C21C13"><span class="kdocs-bold" style="font-weight:bold">neg</span></span>和<span class="kdocs-color" style="color:#C21C13"><span class="kdocs-bold" style="font-weight:bold">pos</span></span>的评论，分别表示负面和正面的评论：</p>
 <div class="kdocs-line-container">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:811px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:57.33662%;height:0">
    <img src="https://images2.imgbox.com/ae/b7/ijfeRvMW_o.png" style="margin-left:;width:811px;margin-top:-57.33662%;height:auto">
   </div>
  </div>
 </div>
 <p>每个文本是一条影评，文本名称构造：id_评分，比如上面图中的200_8.txt表示id为200的这条影评的评分是8分。</p>
 <p>还有一种feat文件，如下图：</p>
 <div class="kdocs-line-container">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:1784px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:28.643497%;height:0">
    <img src="https://images2.imgbox.com/ca/de/gHCDAEPs_o.png" style="margin-left:;width:1784px;margin-top:-28.643497%;height:auto">
   </div>
  </div>
 </div>
 <p>这种.feat文件的格式为<span class="kdocs-bold" style="font-weight:bold">LIBSVM</span>，是一种用于标记的ascii稀疏向量格式数据，比如图片中红色划线处的第200条评论，8后面的数字表示什么意思呢？</p>
 <p><span class="kdocs-color" style="background-color:#FBF5B3;color:#C21C13">8 0:5 1:2 3:1 4:2 6:4 7:7 8:4 9:2 10:2 11:3 16:1 17:3 ... ...</span></p>
 <p>这里的0:5表示第一个单词出现了5次，1:2就是第二个单词出现了2次，后面依次类推。</p>
 <p>接下来使用自带的<span class="kdocs-bold" style="font-weight:bold">read_imdb</span>函数来读取训练集和测试集，当然这里使用自带的函数需要注意目录的位置，将aclImdb整个目录剪切到上级目录data里面，比如本人电脑上的地址：<span class="kdocs-color" style="background-color:#FBF5B3">D:dataaclImdb</span></p>
 <pre class="kdocs-python"><code class="language-python">train_data, test_data = d2l.read_imdb("train"), d2l.read_imdb("test")
print(train_data[1])
'''
(pygpu) D:DOG-BREED&gt;python test.py
["i went to this movie expecting an artsy scary film. what i got was scare after scare. it's a horror film at it's core. it's not dull like other horror films where a haunted house just has ghosts and gore. this film doesn't even show you the majority of the deaths it shows the fear of the characters. i think one of the best things about the concept where it's not just the house thats haunted its whoever goes into the house. they become haunted no matter where they are. office buildings, police stations, hotel rooms... etc. after reading some of the external reviews i am really surprised that critics didn't like this film. i am going to see it again this week and am excited about it.&lt;br /&gt;&lt;br /&gt;i gave this film 10 stars because it did what a horror film should. it scared the s**t out of me.", 1]
'''</code></pre>
 <p>返回的结果是列表，里面元素是<span class="kdocs-bold" style="font-weight:bold">评论加一个正负类标签</span>。这里是赞叹这部恐怖片拍的很不错，后面的1表示正类评价。</p>
 <p>上面两个函数的源码附上<span class="kdocs-color" style="background-color:#FBF5B3">[../envs/pygpu/Lib/site-packages/d2lzh/utils.py]</span>：</p>
 <pre class="kdocs-python"><code class="language-python">def download_imdb(data_dir='../data'):
    """Download the IMDB data set for sentiment analysis."""
    url = ('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')
    sha1 = '01ada507287d82875905620988597833ad4e0903'
    fname = gutils.download(url, data_dir, sha1_hash=sha1)
    with tarfile.open(fname, 'r') as f:
        f.extractall(data_dir)


def read_imdb(folder='train'):
    """Read the IMDB data set for sentiment analysis."""
    data = []
    for label in ['pos', 'neg']:
        folder_name = os.path.join('../data/aclImdb/', folder, label)
        for file in os.listdir(folder_name):
            with open(os.path.join(folder_name, file), 'rb') as f:
                review = f.read().decode('utf-8').replace('n', '').lower()
                data.append([review, 1 if label == 'pos' else 0])
    random.shuffle(data)
    return data</code></pre>
 <h2>预处理数据集</h2>
 <p>数据集和测试集读取没有问题之后，我们对评论进行分词，这里基于空格分词，也是自带的函数<span class="kdocs-bold" style="font-weight:bold">get_tokenized_imdb</span>进行分词并做了小写处理。</p>
 <pre class="kdocs-python"><code class="language-python">def get_tokenized_imdb(data):
    """Get the tokenized IMDB data set for sentiment analysis."""
    def tokenizer(text):
        return [tok.lower() for tok in text.split(' ')]
    return [tokenizer(review) for review, _ in data]</code></pre>
 <p>然后将分好词的训练数据集创建Vocabulary词典，我们这里过滤掉出现次数少于5的词，min_freq=5。</p>
 <pre class="kdocs-python"><code class="language-python">def get_vocab_imdb(data):
    """Get the vocab for the IMDB data set for sentiment analysis."""
    tokenized_data = get_tokenized_imdb(data)
    counter = collections.Counter([tk for st in tokenized_data for tk in st])
    return text.vocab.Vocabulary(counter, min_freq=5)

tokenized_data = d2l.get_tokenized_imdb(train_data)
vocab=d2l.get_vocab_imdb(train_data)
print(len(vocab))#46151</code></pre>
 <p>可以看到过滤掉次数少的之后，词汇量从25000降低到了46151，这里返回的变量<span class="kdocs-color" style="background-color:#FBF5B3;color:#C21C13">vocab</span>是<span class="kdocs-color" style="background-color:#FBF5B3;color:#C21C13">mxnet.contrib.text.vocab.Vocabulary</span>类型，我们可以查看它里面有哪些属性与方法：</p>
 <pre class="kdocs-python"><code class="language-python">dir(mxnet.contrib.text.vocab.Vocabulary)
'''
['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_index_counter_keys', '_index_unknown_and_reserved_tokens', 'idx_to_token', 'reserved_tokens', 'to_indices', 'to_tokens', 'token_to_idx', 'unknown_token']
'''
print(vocab.idx_to_token[1])#the</code></pre>
 <p>由于每条评论的字数或说长度不一样，所以不能直接组合成小批量，我们通过一个辅助函数让它的长度固定在500，超出的进行截断，不足的进行'&lt;pad&gt;'补足。这个函数<span class="kdocs-bold" style="font-weight:bold">preprocess_imdb</span>在d2lzh包中也自带有</p>
 <pre class="kdocs-python"><code class="language-python">features, labels = d2l.preprocess_imdb(train_data, vocab)
print(features.shape, labels.shape)#(25000, 500) (25000,)</code></pre>
 <p>从形状可以看到每条评论都固定到了长度为500</p>
 <pre class="kdocs-python"><code class="language-python">print(features)
'''
[[5.0000e+00 5.3200e+02 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]
 [2.0100e+02 5.4810e+03 4.2891e+04 ... 1.6000e+01 2.9200e+02 1.1000e+01]
 [0.0000e+00 0.0000e+00 3.6000e+01 ... 0.0000e+00 0.0000e+00 0.0000e+00]
 ...
 [9.0000e+00 2.2600e+02 3.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]
 [2.8690e+03 1.2220e+03 1.4000e+01 ... 1.1538e+04 5.2700e+02 2.9000e+01]
 [9.0000e+00 1.9900e+02 1.2108e+04 ... 0.0000e+00 0.0000e+00 0.0000e+00]]
&lt;NDArray 25000x500 @cpu(0)&gt;
'''</code></pre>
 <p>附上源码：</p>
 <pre class="kdocs-python"><code class="language-python">def preprocess_imdb(data, vocab):
    """Preprocess the IMDB data set for sentiment analysis."""
    max_l = 500

    def pad(x):
        return x[:max_l] if len(x) &gt; max_l else x + [0] * (max_l - len(x))

    tokenized_data = get_tokenized_imdb(data)
    features = nd.array([pad(vocab.to_indices(x)) for x in tokenized_data])
    labels = nd.array([score for _, score in data])
    return features, labels</code></pre>
 <p>当然如果想要查看'&lt;pad&gt;'对应的值，print(vocab.token_to_idx['&lt;pad&gt;'])会报错：</p>
 <blockquote class="kdocs-blockquote">
  Traceback (most recent call last):
  <br> File "test.py", line 19, in &lt;module&gt;
  <br> print(vocab.token_to_idx['&lt;pad&gt;'])
  <br>KeyError: '&lt;pad&gt;'
 </blockquote>
 <p>所以在创建词典Vocabulary的时候，需指定参数<span class="kdocs-color" style="background-color:#FBF5B3">reserved_tokens=['&lt;pad&gt;']</span>保留这个词</p>
 <pre class="kdocs-python"><code class="language-python">def get_vocab_imdb(data):
    """Get the vocab for the IMDB data set for sentiment analysis."""
    tokenized_data = d2l.get_tokenized_imdb(data)
    counter = collections.Counter([tk for st in tokenized_data for tk in st])
    return text.vocab.Vocabulary(counter, min_freq=5,reserved_tokens=['&lt;pad&gt;'])</code></pre>
 <h2>创建数据迭代器</h2>
 <p>数据集都整理好了之后，就开始做数据迭代器，每次迭代将返回一个小批量的数据</p>
 <pre class="kdocs-python"><code class="language-python">batch_size = 64
#train_set = gdata.ArrayDataset(*d2l.preprocess_imdb(train_data, vocab))
train_set=gdata.ArrayDataset(*[features,labels])
test_set = gdata.ArrayDataset(*d2l.preprocess_imdb(test_data, vocab))
train_iter = gdata.DataLoader(train_set, batch_size, shuffle=True)
test_ieter = gdata.DataLoader(test_set, batch_size)

print(len(train_iter))
for X,y in train_iter:
    print(X.shape,y.shape)
    break
'''
391
(64, 500) (64,)
'''</code></pre>
 <h2>创建RNN模型</h2>
 <p>数据迭代器测试没有问题之后，接下来就是选择循环神经网络模型来试下效果怎么样了。</p>
 <p>首先就是将每个词做嵌入，也就是通过嵌入层得到特征向量，然后我们使用双向循环神经网络对特征序列进一步编码得到序列信息，最后将编码的序列信息通过全连接层变换成输出。</p>
 <p>具体来说，我们可以将双向长短期记忆在最初时间步和最终时间步的隐藏状态连结，作为特征序列的表征传递给输出层分类。<span class="kdocs-color" style="background-color:#FBF5B3">在下面实现BiRNN类中，Embedding实例就是嵌入层，LSTM实例即为序列编码的隐藏层，Dense实例即生成分类结果的输出层。</span></p>
 <pre class="kdocs-python"><code class="language-python">class BiRNN(nn.Block):
    def __init__(self, vocab, embed_size, num_hiddens, num_layers, **kwargs):
        super(BiRNN, self).__init__(**kwargs)
        # 词嵌入层
        self.embedding = nn.Embedding(input_dim=len(vocab), output_dim=embed_size)
        # bidirectional设为True就是双向循环神经网络
        self.encoder = rnn.LSTM(
            hidden_size=num_hiddens,
            num_layers=num_layers,
            bidirectional=True,
            input_size=embed_size,
        )
        self.decoder = nn.Dense(2)

    def forward(self, inputs):
        # LSTM需要序列长度(词数)作为第一维，所以inputs[形状为：(批量大小,词数)]需做转置
        embeddings = self.embedding(inputs.T)
        print(embeddings.shape)
        outputs = self.encoder(embeddings)
        print(outputs.shape)
        # 将初始时间步和最终时间步的隐藏状态作为全连接层输入
        encoding = nd.concat(outputs[0], outputs[-1])
        print(encoding.shape)
        outs = self.decoder(encoding)
        return outs


# 创建一个含2个隐藏层的双向循环神经网络
embed_size, num_hiddens, num_layers, ctx = 100, 100, 2, d2l.try_all_gpus()
net = BiRNN(
    vocab=vocab, embed_size=embed_size, num_hiddens=num_hiddens, num_layers=num_layers
)
net.initialize(init.Xavier(), ctx=ctx)
#print(net)
'''
BiRNN(
  (embedding): Embedding(46152 -&gt; 100, float32)
  (encoder): LSTM(100 -&gt; 100, TNC, num_layers=2, bidirectional)
  (decoder): Dense(None -&gt; 2, linear)
)
'''</code></pre>
 <p>其中LSTM长短期记忆的公式如下(来自源码)：</p>
 <p><img class="kdocs-latex-img" src="https://images2.imgbox.com/24/2b/67tpNXJB_o.png"></p>
 <h2>训练模型</h2>
 <p>由于情感分类的训练数据集并不大，<span class="kdocs-bold" style="font-weight:bold"><span class="kdocs-underline" style="text-decoration:underline">容易过拟合</span></span>，所以这里将使用<span class="kdocs-color" style="color:#C21C13"><span class="kdocs-bold" style="font-weight:bold">glove.6B.100d.txt</span></span>的语料库，将这个预训练的词向量作为每个词的特征向量。</p>
 <p>需要注意的是，这里选择的预训练词向量维度是100，需要跟创建的模型中的嵌入层输出层大小embed_size一致，以及在训练中就不再需要更新这些词向量。</p>
 <pre class="kdocs-python"><code class="language-python">glove_embedding = text.embedding.create(
    "glove", pretrained_file_name="glove.6B.100d.txt", vocabulary=vocab
)
net.embedding.weight.set_data(glove_embedding.idx_to_vec)
net.embedding.collect_params().setattr('grad_req','null')

lr,num_epochs=0.01,5
trainer=gluon.Trainer(net.collect_params(),'adam',{'learning_rate':lr})
loss=gloss.SoftmaxCrossEntropyLoss()
d2l.train(train_iter,test_ieter,net,loss,trainer,ctx,num_epochs)


print(d2l.predict_sentiment(net,vocab,['this','movie','is','so','good']))
print(d2l.predict_sentiment(net,vocab,['this','movie','is','so','bad']))
'''
training on [gpu(0)]
epoch 1, loss 0.6553, train acc 0.605, test acc 0.738, time 65.4 sec
epoch 2, loss 0.4273, train acc 0.807, test acc 0.809, time 65.4 sec
epoch 3, loss 0.3514, train acc 0.851, test acc 0.849, time 65.5 sec
epoch 4, loss 0.3054, train acc 0.874, test acc 0.859, time 65.6 sec
epoch 5, loss 0.2765, train acc 0.887, test acc 0.843, time 65.6 sec
positive
negative
'''</code></pre>
 <p>其中预测函数的源码如下：</p>
 <pre class="kdocs-python"><code class="language-python">def predict_sentiment(net, vocab, sentence):
    """Predict the sentiment of a given sentence."""
    sentence = nd.array(vocab.to_indices(sentence), ctx=try_gpu())
    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)
    return 'positive' if label.asscalar() == 1 else 'negative'</code></pre>
 <p></p>
</div>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>