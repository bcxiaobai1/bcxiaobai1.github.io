<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>图像中的注意力机制详解(SEBlock | ECABlock | CBAM) - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">图像中的注意力机制详解(SEBlock | ECABlock | CBAM)</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="centercenter_2"></a>
 
  图像中的注意力机制详解
 
</h1> 
<p>注意力机制目前主要有<strong>通道注意力机制</strong>和<strong>空间注意力机制</strong>两种</p> 
<h2>
<a id="__6"></a>一、 前言</h2> 
<p>我们知道，输入一张图片，神经网络会提取图像特征，每一层都有不同大小的特征图。如图1所示，展示了 VGG网络在提取图像特征时特征图的大小变化。</p> 
<p><img src="https://images2.imgbox.com/f6/9c/5ZTrJwH6_o.png" alt="在这里插入图片描述"></p> 

 图1 VGG网络特征结构图
 
<p>其中，特征图常见的矩阵形状为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        C
       
       
        ,
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[C,H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>(图1中的数字为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        H
       
       
        ,
       
       
        W
       
       
        ,
       
       
        C
       
       
        ]
       
      
      
       {[H,W,C]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mclose">]</span></span></span></span></span></span>格式)。当<strong>model在training时，特征图的矩阵形状为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         B
        
        
         ,
        
        
         C
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[B,C,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.05017em">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span></strong>。其中B表示为batch size(批处理大小)，C表示为channels(通道数)，H表示为特征图的high(高度)，W表示为特征图的weight(宽度)</p> 
<p>提问：为什么特征图的维度就是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        B
       
       
        ,
       
       
        C
       
       
        ,
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[B,C,H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.05017em">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>，而不是其他什么维度格式？</p> 
<p>回答：pytorch在处理图像时，读入的图像处理为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        C
       
       
        ,
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[C,H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>格式，如果在训练时加入batch size，那么就有多个特征图，将batch size放在第一维，自然就是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        B
       
       
        ,
       
       
        C
       
       
        ,
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[B,C,H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.05017em">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>。这是pytorch的处理方式</p> 
<p>在网络提取图像特征层时，通过<strong>在卷积层之间添加通道注意力机制</strong>、<strong>空间注意力机制</strong>可以增强网络提取图像的能力。在编写代码时，考虑的是特征图间的attention机制，因此代码输入是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        B
       
       
        ,
       
       
        C
       
       
        ,
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[B,C,H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.05017em">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>的特征图，输出仍然是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        B
       
       
        ,
       
       
        C
       
       
        ,
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[B,C,H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.05017em">B</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>维的特征图。让我们接下来通过三篇论文来看这两种注意力机制是如何工作的。</p> 
<hr> 
<h2>
<a id="SENet_29"></a>二、SENet——通道注意力机制</h2> 
<h3>
<a id="1__31"></a>1. 论文介绍</h3> 
<h4>
<a id="SqueezeandExcitation_Networks_33"></a>论文名称：Squeeze-and-Excitation Networks</h4> 
<h4>
<a id="httpsarxivorgpdf170901507pdfhttpsarxivorgpdf170901507pdf_35"></a>论文链接：<a href="https://arxiv.org/pdf/1709.01507.pdf">https://arxiv.org/pdf/1709.01507.pdf</a>
</h4> 
<h4>
<a id="_httpsgithubcomhujiefrankSENethttpsgithubcomhujiefrankSENet_37"></a>论文代码： <a href="https://github.com/hujie-frank/SENet">https://github.com/hujie-frank/SENet</a>
</h4> 
<h4>
<a id="SEBlock_39"></a>SEBlock结构图：</h4> 
<p><img src="https://images2.imgbox.com/3e/61/lqMItSIa_o.png" alt="在这里插入图片描述"></p> 

 图2 SEBlock结构图
 
<blockquote> 
 <p>Abstract: The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. <strong>In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. </strong>We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251%, surpassing the winning entry of 2016 by a relative improvement of ∼25%. Models and code are available at https://github.com/hujie-frank/SENet.</p> 
</blockquote> 
<h4>
<a id="_50"></a>摘要重点：</h4> 
<p>卷积神经网络（CNN）的核心组成部分是卷积算子，它使网络能够通过融合每层局部感受野中的空间和通道信息来构建信息特征。之前的大量研究已经调查了这种关系的空间成分，并试图通过在CNN的特征层次中提高空间编码的质量来增强CNN。<strong>在这项工作中，我们将重点放在通道(channel-wise)关系上，并提出了一个新的名为SE模块的架构单元，它通过显式地建模通道之间的相互依赖性，自适应地重新校准通道特征响应。这些模块可以堆叠在一起形成SENet网络结构，并在多个数据集上非常有效地推广。</strong></p> 
<h4>
<a id="SEBlock_54"></a>SEBlock创新点：</h4> 
<ol>
<li>SEBlock会给每个通道一个权重，让不同通道对结果有不同的作用力。</li>
<li>这个SE模块能够非常方便地添加进目前主流的神经网络当中。</li>
</ol> 
<h3>
<a id="2__59"></a>2. 算法解读</h3> 
<p>图3展示了通道注意力机制的四个步骤，具体如下：</p> 
<p><img src="https://images2.imgbox.com/e2/b2/Zpps54jZ_o.png" alt="在这里插入图片描述"></p> 

 图3 SEBlock模块分析
 
<ol>
<li> <p><strong>从单张图像开始，提取图像特征，当前特征层U的特征图维度为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           [
          
          
           C
          
          
           ,
          
          
           H
          
          
           ,
          
          
           W
          
          
           ]
          
         
         
          {[C,H,W]}
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span></strong>。</p> </li>
<li> <p><strong>对特征图的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           [
          
          
           H
          
          
           ,
          
          
           W
          
          
           ]
          
         
         
          {[H,W]}
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>维度进行平均池化或最大池化</strong>，池化过后的特征图大小从<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          [
         
         
          C
         
         
          ,
         
         
          H
         
         
          ,
         
         
          W
         
         
          ]
         
        
        
         {[C,H,W]}
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>-&gt;<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          [
         
         
          C
         
         
          ,
         
         
          1
         
         
          ,
         
         
          1
         
         
          ]
         
        
        
         {[C,1,1]}
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span></span>。<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          [
         
         
          C
         
         
          ,
         
         
          1
         
         
          ,
         
         
          1
         
         
          ]
         
        
        
         {[C,1,1]}
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span></span>可理解为对于每一个通道C，都有一个数字和其一一对应。图4对应了步骤(2)的具体操作。</p> </li>
</ol> 
<p><img src="https://images2.imgbox.com/d0/40/lipCgvwY_o.png" alt="在这里插入图片描述"></p> 

 图4 平均池化(最大池化)操作，得到每个通道的权重，得到每个通道的权重
 
<ol start="3"><li>对<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         C
        
        
         ,
        
        
         1
        
        
         ,
        
        
         1
        
        
         ]
        
       
       
        {[C,1,1]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span></span>的特征可以理解为，从每个通道本身提取出来的权重，<strong>权重表示了每个通道对特征提取的影响力，全局池化后的向量通过MLP网络后，其意义为得到了每个通道的权重</strong>。图5对应了步骤(3)的具体操作。</li></ol> 
<p><img src="https://images2.imgbox.com/8d/01/mxErJZ2M_o.png" alt="在这里插入图片描述"></p> 

 图5 通道权重生成 

<ol start="4"><li>上述步骤，<strong>得到了每个通道C的权重<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          [
         
         
          C
         
         
          ,
         
         
          1
         
         
          ,
         
         
          1
         
         
          ]
         
        
        
         {[C,1,1]}
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span></span>，将权重作用于特征图U<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          [
         
         
          C
         
         
          ,
         
         
          H
         
         
          ,
         
         
          W
         
         
          ]
         
        
        
         {[C,H,W]}
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>，即每个通道各自乘以各自的权重</strong>。可以理解为，<em>当权重大时，该通道特征图的数值相应的增大，对最终输出的影响也会变大；当权重小时，该通道特征图的数值就会更小，对最终输出的影响也会变小</em>。图6对应了步骤(4)的具体操作。</li></ol> 
<p><img src="https://images2.imgbox.com/90/ff/EZtBxyP1_o.png" alt="在这里插入图片描述"></p> 

 图6 通道注意力——各通道乘以各自不同权重
 
<p>原论文中给出了通道注意力网络细节，这里展示出来，如图7所示。</p> 
<p><img src="https://images2.imgbox.com/dc/c9/lcw22uKG_o.png" alt="在这里插入图片描述"></p> 

 图7 SEBlock实现前（左）后（右）对比
 
<h3>
<a id="3_Pytorch_97"></a>3. Pytorch代码实现</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn


<span class="token keyword">class</span> <span class="token class-name">SEBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> mode<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> ratio<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>SEBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>avg_pooling <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>max_pooling <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> mode <span class="token operator">==</span> <span class="token string">"max"</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>global_pooling <span class="token operator">=</span> self<span class="token punctuation">.</span>max_pooling
        <span class="token keyword">elif</span> mode <span class="token operator">==</span> <span class="token string">"avg"</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>global_pooling <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pooling
        self<span class="token punctuation">.</span>fc_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features <span class="token operator">=</span> channels<span class="token punctuation">,</span> out_features <span class="token operator">=</span> channels <span class="token operator">//</span> ratio<span class="token punctuation">,</span> bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features <span class="token operator">=</span> channels <span class="token operator">//</span> ratio<span class="token punctuation">,</span> out_features <span class="token operator">=</span> channels<span class="token punctuation">,</span> bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
		self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
     
    
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>global_pooling<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_layers<span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> v

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> SEBlock<span class="token punctuation">(</span><span class="token string">"max"</span><span class="token punctuation">,</span> <span class="token number">54</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">)</span>
    feature_maps <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">54</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    model<span class="token punctuation">(</span>feature_maps<span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="4__134"></a>4. 个人理解</h3> 
<p>通道注意力机制为什么有效的原因：特征图在提取图像特征的过程当中，不可避免的就是会出现有些特征图层作用大，而有些特征图层作用小。因此由通道本身提取出的权重施加在特征图上，保证了在特征图提取特征的基础上，自适应地给定通道权重，让作用大的特征图对结果的影响更大一点。因此在最终结果上是比普通的卷积层要更有效提取特征一点。</p> 
<hr> 
<h2>
<a id="ECANetSENetMLP_140"></a>三、ECANet——通道注意力机制（一维卷积替换SENet中的MLP）</h2> 
<h3>
<a id="1__142"></a>1. 论文介绍</h3> 
<h4>
<a id="ECANet_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_144"></a>论文名称：ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</h4> 
<h4>
<a id="httpsarxivorgpdf191003151pdfhttpsarxivorgpdf191003151pdf_146"></a>论文链接：<a href="https://arxiv.org/pdf/1910.03151.pdf">https://arxiv.org/pdf/1910.03151.pdf</a>
</h4> 
<h4>
<a id="httpsgithubcomBangguWuECANethttpsgithubcomBangguWuECANet_148"></a>论文代码：<a href="https://github.com/BangguWu/ECANet%5B">https://github.com/BangguWu/ECANet</a>
</h4> 
<h4>
<a id="ECABlock_150"></a>ECABlock主要结构图</h4> 
<p><img src="https://images2.imgbox.com/95/dc/uLbSGaiQ_o.png" alt="在这里插入图片描述"></p> 

 图8 有效通道注意（ECA）模块示意图
 
<h3><a id="_157"></a></h3> 
<blockquote> 
 <p>Abstract: Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. <strong>To overcome the paradox of performance and complexity trade-off</strong>, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. <strong>By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local crosschannel interaction strategy without dimensionality reduction, which can be efficiently implemented via 1D convolution. </strong>Furthermore, we develop a method to adaptively select kernel size of 1D convolution, determining coverage of local cross-channel interaction. The proposed ECA module is efficient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.</p> 
</blockquote> 
<h4>
<a id="_161"></a>摘要重点：</h4> 
<p>近年来，通道注意机制在改善深度卷积神经网络（CNN）性能方面显示出巨大的潜力。然而，大多数现有方法致力于开发更复杂的注意模块，以获得更好的性能，这不可避免地增加了模型的复杂性。<strong>为了克服性能和复杂性之间的矛盾</strong>，本文提出了一种高效的通道注意（ECA）模块，该模块只涉及少量参数，同时带来明显的性能增益。<strong>通过剖析SENet中的通道注意模块，我们实证地表明，避免维度缩减对于学习通道注意非常重要，适当的跨通道交互可以在显著降低模型复杂度的同时保持性能。因此，我们提出了一种无降维的局部交叉信道交互策略，该策略可以通过一维卷积有效地实现。</strong></p> 
<h4>
<a id="ECABlock_165"></a>ECABlock创新点</h4> 
<ol>
<li> <p><strong>针对SEBlock的步骤(3)，将MLP模块(FC-&gt;ReLU&gt;FC-&gt;Sigmoid)，转变为一维卷积的形式</strong>，有效减少了参数计算量（我们都知道在CNN网络中，往往连接层是参数量巨大的，因此将全连接层改为一维卷积的形式）</p> </li>
<li> <p><strong>一维卷积自带的功效就是非全连接，每一次卷积过程只和部分通道的作用</strong>，即实现了适当的跨通道交互而不是像全连接层一样全通道交互。</p> </li>
</ol> 
<h3>
<a id="2__171"></a>2. 论文解读</h3> 
<p><strong>给定通过平均池化(average pooling)获得的聚合特征<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         C
        
        
         ,
        
        
         1
        
        
         ,
        
        
         1
        
        
         ]
        
       
       
        {[C,1,1]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span></span>，ECA模块通过执行卷积核大小为k的一维卷积来生成通道权重，其中k通过通道维度C的映射自适应地确定。</strong></p> 
<p>图中与SEBlock不一样的地方仅在于SEBlock的步骤(3)，用一维卷积替换了全连接层，其中一维卷积核大小通过通道数C自适应确定。</p> 
<p>自适应确定卷积核大小公式：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        k
       
       
        =
       
       
        ∣
       
       
        
         
          
           l
          
          
           o
          
          
           
            g
           
           
            2
           
          
          
           C
          
          
           +
          
          
           b
          
         
         
          γ
         
        
       
       
        
         ∣
        
        
         
          o
         
         
          d
         
         
          d
         
        
       
      
      
       {k=|cfrac{log_2{C}+b}{gamma}|_{odd}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 2.47044em;vertical-align: -0.88044em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03148em">k</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mord">∣</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.59em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05556em">γ</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.03588em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07153em">C</span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mord mathdefault">b</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.88044em"><span class=""></span></span></span></span></span><span class=""></span></span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br> 其中k表示卷积核大小，C表示通道数，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        ∣
       
       
        
         ∣
        
        
         
          o
         
         
          d
         
         
          d
         
        
       
      
      
       {| |_{odd}}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">d</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></span>表示k只能取奇数，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        γ
       
      
      
       {gamma}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05556em">γ</span></span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        b
       
      
      
       {b}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">b</span></span></span></span></span></span>在论文中设置为2和1,用于改变通道数C和卷积核大小和之间的比例。</p> 
<p>(如何理解通道C自适应确定卷积核大小：当通道数多的时候，我需要卷积核k稍大一点，当通道数少的时候，我需要卷积核k稍微小一点，这样能充分融合部分通道间的交互)</p> 
<h3>
<a id="3_Pytorch_182"></a>3. Pytorch代码实现</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> math
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn


<span class="token keyword">class</span> <span class="token class-name">ECABlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> gamma <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> b <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>ECABlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        kernel_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span> <span class="token operator">/</span> gamma<span class="token punctuation">)</span><span class="token punctuation">)</span>
        kernel_size <span class="token operator">=</span> kernel_size <span class="token keyword">if</span> kernel_size <span class="token operator">%</span> <span class="token number">2</span> <span class="token keyword">else</span> kernel_size <span class="token operator">+</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size <span class="token operator">=</span> kernel_size<span class="token punctuation">,</span> padding <span class="token operator">=</span> <span class="token punctuation">(</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>v<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> v


<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    features_maps <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">54</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    model <span class="token operator">=</span> ECABlock<span class="token punctuation">(</span><span class="token number">54</span><span class="token punctuation">,</span> gamma <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> b <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
    model<span class="token punctuation">(</span>features_maps<span class="token punctuation">)</span>
</code></pre> 
<p>这里对比了两个论文的代码实现，可以看到，只是把MLP更换为了一维卷积。</p> 
<pre><code class="prism language-python"><span class="token comment"># SEBlock 采用全连接层方式</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> _<span class="token punctuation">,</span> _ <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
    v <span class="token operator">=</span> self<span class="token punctuation">.</span>global_pooling<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">)</span>
    v <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_layers<span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    v <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
    <span class="token keyword">return</span> x <span class="token operator">*</span> v

<span class="token comment"># ECABlock 采用一维卷积方式</span>
<span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
	v <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
	v <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>v<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
	v <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
	<span class="token keyword">return</span> x <span class="token operator">*</span> v
</code></pre> 
<h3>
<a id="4_231"></a>4.个人理解</h3> 
<p>ECABlock本身没有大的内容上的改变，只是替换了全连接层，减少了数据量而已（有时候做减法比做加法好）</p> 
<hr> 
<h2>
<a id="_CBAMBlockSEBlockECABlock_237"></a>四、 CBAMBlock——通道注意力机制和空间注意力机制混合使用（SEBlock或ECABlock后接空间注意力机制）</h2> 
<h3>
<a id="1__239"></a>1. 论文介绍</h3> 
<h4>
<a id="CBAM_Convolutional_Block_Attention_Module_241"></a>论文名称：CBAM: Convolutional Block Attention Module</h4> 
<h4>
<a id="httpsarxivorgpdf180706521v2pdfhttpsarxivorgpdf180706521v2pdf_243"></a>论文链接：<a href="https://arxiv.org/pdf/1807.06521v2.pdf">https://arxiv.org/pdf/1807.06521v2.pdf</a>
</h4> 
<h4>
<a id="httpsgithubcomluuuyiCBAMPyTorchhttpsgithubcomluuuyiCBAMPyTorch_245"></a>论文代码：<a href="https://github.com/luuuyi/CBAM.PyTorch">https://github.com/luuuyi/CBAM.PyTorch</a>(复现版本)</h4> 
<h4>
<a id="CBAMBlock_247"></a>CBAMBlock结构图</h4> 
<p><img src="https://images2.imgbox.com/ab/b9/C0huLmFc_o.png" alt="在这里插入图片描述"></p> 

 图9 CBAM(Convolutional Block Attention Module)模块结构
 
<h3><a id="_255"></a></h3> 
<blockquote> 
 <p>Abstract: We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. <strong>Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement</strong>. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS COCO detection, and VOC 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.</p> 
</blockquote> 
<h4>
<a id="_259"></a>摘要重点：</h4> 
<p>我们提出了卷积块注意模块（CBAM），一种简单而有效的前馈卷积神经网络注意模块。<strong>给定一个中间的特征图，我们的模块采用两个独立的注意力机制，通道注意力和空间注意力，然后将注意力机制得到的权重乘以输入特征图以进行自适应特征细化</strong>。因为CBAM是一个轻量级的通用模块，它可以无缝地集成到任何CNN架构中，开销可以忽略不计，并且可以与基础CNN一起进行端到端培训。我们通过在ImageNet-1K、MS COCO检测和VOC 2007检测数据集上的大量实验来验证我们的CBAM。我们的实验表明，各种模型在分类和检测性能上都有一致的改进，证明了CBAM的广泛适用性。代码和模型将公开提供。</p> 
<h4>
<a id="CBAM_263"></a>CBAM创新点</h4> 
<ol>
<li>在SENet或ECANet的基础上，<strong>在通道注意力模块后，接入空间注意力模块</strong>，实现了通道注意力和空间注意力的双机制</li>
<li>选择SENet还是ECANet主要取决于通道注意力的连接是MLP还是一维卷积</li>
<li>
<strong>注意力模块不再采用单一的最大池化或平均池化，而是采用最大池化和平均池化的相加或堆叠</strong>。通道注意力模块采用相加，空间注意力模块采用堆叠方式。</li>
</ol> 
<h3>
<a id="2__270"></a>2. 论文解读</h3> 
<p>在上述两篇论文中已经实现了通道注意力方法的全连接(SENet)或卷积(ECANet)实现，这一篇论文与上文的最大不同点就在于，加入了空间注意力机制。</p> 
<h4>
<a id="1__274"></a>1) 通道注意力机制</h4> 
<p><img src="https://images2.imgbox.com/d1/48/JRA30lgy_o.png" alt="在这里插入图片描述"></p> 

 图10 通道注意力模块实现方法
 
<ol>
<li>特征图分别经过MaxPool和AvgPool，形成两个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         C
        
        
         ,
        
        
         1
        
        
         ,
        
        
         1
        
        
         ]
        
       
       
        {[C,1,1]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span></span>的权重向量</li>
<li>两个权重向量分别经过同一个MLP网络（由于是同一个网络，因此也可看作是网络参数共享的MLP），映射成每个通道的权重</li>
<li>将映射后的权重相加，后接Sigmoid输出</li>
<li>将得到的通道权重<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         C
        
        
         ,
        
        
         1
        
        
         ,
        
        
         1
        
        
         ]
        
       
       
        {[C,1,1]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span></span></span>与原特征图<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         C
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[C,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>按通道相乘</li>
</ol> 
<p><strong>整体上和SENet基本一致，只是将单一的平均池化变为了同时采用最大池化和平均池化方法</strong>。之后若将MLP稍加修改，改成一维卷积，就成为了ECANet的变形版本</p> 
<h4>
<a id="2__288"></a>2) 空间注意力机制</h4> 
<p><img src="https://images2.imgbox.com/73/63/CaRDqKuC_o.png" alt="在这里插入图片描述"></p> 

 图11 空间注意力模块实现方法
 
<ol>
<li>特征图分别经过MaxPool和AvgPool，形成两个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         1
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[1,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>的权重向量，即按通道最大池化和平均池化。通道数从<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         C
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[C,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>变为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         1
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[1,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>，对同一特征点的所有通道池化。</li>
<li>得到的两张特征图进行堆叠，形成<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         2
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[2,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>的特征图空间权重</li>
<li>经过一层卷积层，特征图维度从<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         2
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[2,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>变为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         1
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[1,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>，这<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         1
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[1,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>的特征图表征了特征图上的每个点的重要程度，数值大的更重要</li>
<li>将得到的空间权重<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         1
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[1,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>与原特征图<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         C
        
        
         ,
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[C,H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>相乘，即特征图上<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         [
        
        
         H
        
        
         ,
        
        
         W
        
        
         ]
        
       
       
        {[H,W]}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>的每一个点都赋予了权重</li>
</ol> 
<p>我们可以看成大小为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>的特征图，在每一个点<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        x
       
       
        ,
       
       
        y
       
       
        )
       
       
        ,
       
       
        x
       
       
        ∈
       
       
        (
       
       
        0
       
       
        ,
       
       
        H
       
       
        )
       
       
        ,
       
       
        y
       
       
        ∈
       
       
        (
       
       
        0
       
       
        ,
       
       
        W
       
       
        )
       
      
      
       {(x,y),xin(0,H),yin(0,W)}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">∈</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">)</span></span></span></span></span></span>上，都有C个数值，数值表征了特征图该点的重要程度，通过感受野反推回原图像，即表示了该区域的重要程度。我们需要让网络自适应关注需要关注的地方（数值大的地方更易受到关注），空间注意力机制应运而生。</p> 
<h3>
<a id="3_Pytorch_303"></a>3. Pytorch代码实现</h3> 
<h4>
<a id="_305"></a>通道注意力机制——全连接层版本</h4> 
<pre><code class="prism language-python"><span class="token keyword">import</span> math
<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn


<span class="token keyword">class</span> <span class="token class-name">Channel_Attention_Module_FC</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> ratio<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Channel_Attention_Module_FC<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>avg_pooling <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>max_pooling <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc_layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features <span class="token operator">=</span> channels<span class="token punctuation">,</span> out_features <span class="token operator">=</span> channels <span class="token operator">//</span> ratio<span class="token punctuation">,</span> bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features <span class="token operator">=</span> channels <span class="token operator">//</span> ratio<span class="token punctuation">,</span> out_features <span class="token operator">=</span> channels<span class="token punctuation">,</span> bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> h<span class="token punctuation">,</span> w <span class="token operator">=</span> x<span class="token punctuation">.</span>shape
        avg_x <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pooling<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">)</span>
        max_x <span class="token operator">=</span> self<span class="token punctuation">.</span>max_pooling<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_layers<span class="token punctuation">(</span>avg_x<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>fc_layers<span class="token punctuation">(</span>max_x<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>v<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>b<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> v
</code></pre> 
<h4>
<a id="_335"></a>通道注意力机制——一维卷积版本</h4> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Channel_Attention_Module_Conv</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channels<span class="token punctuation">,</span> gamma <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> b <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Channel_Attention_Module_Conv<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        kernel_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token builtin">abs</span><span class="token punctuation">(</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span>channels<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span> <span class="token operator">/</span> gamma<span class="token punctuation">)</span><span class="token punctuation">)</span>
        kernel_size <span class="token operator">=</span> kernel_size <span class="token keyword">if</span> kernel_size <span class="token operator">%</span> <span class="token number">2</span> <span class="token keyword">else</span> kernel_size <span class="token operator">+</span> <span class="token number">1</span>
        self<span class="token punctuation">.</span>avg_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size <span class="token operator">=</span> kernel_size<span class="token punctuation">,</span> padding <span class="token operator">=</span> <span class="token punctuation">(</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>v<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> v
</code></pre> 
<h4>
<a id="_354"></a>空间注意力机制</h4> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">Spatial_Attention_Module</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> k<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>Spatial_Attention_Module<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>avg_pooling <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean
        self<span class="token punctuation">.</span>max_pooling <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">max</span>
        <span class="token comment"># In order to keep the size of the front and rear images consistent</span>
        <span class="token comment"># with calculate, k = 1 + 2p, k denote kernel_size, and p denote padding number</span>
        <span class="token comment"># so, when p = 1 -&gt; k = 3; p = 2 -&gt; k = 5; p = 3 -&gt; k = 7, it works. when p = 4 -&gt; k = 9, it is too big to use in network</span>
        <span class="token keyword">assert</span> k <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"kernel size = 1 + 2 * padding, so kernel size must be 3, 5, 7"</span>
        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size <span class="token operator">=</span> <span class="token punctuation">(</span>k<span class="token punctuation">,</span> k<span class="token punctuation">)</span><span class="token punctuation">,</span> stride <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> padding <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token punctuation">(</span>k <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>k <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                              bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># compress the C channel to 1 and keep the dimensions</span>
        avg_x <span class="token operator">=</span> self<span class="token punctuation">.</span>avg_pooling<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> keepdim <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
        max_x<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>max_pooling<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> keepdim <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>max_x<span class="token punctuation">,</span> avg_x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        v <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>v<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x <span class="token operator">*</span> v
</code></pre> 
<h4>
<a id="CBAM_379"></a>CBAM模块(空间注意力和通道注意力二者结合)</h4> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">CBAMBlock</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> channel_attention_mode<span class="token punctuation">:</span> <span class="token builtin">str</span><span class="token punctuation">,</span> spatial_attention_kernel_size<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">,</span> channels<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                 ratio<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> gamma<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span> b<span class="token punctuation">:</span> <span class="token builtin">int</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>CBAMBlock<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> channel_attention_mode <span class="token operator">==</span> <span class="token string">"FC"</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> channels <span class="token operator">!=</span> <span class="token boolean">None</span> <span class="token keyword">and</span> ratio <span class="token operator">!=</span> <span class="token boolean">None</span> <span class="token keyword">and</span> channel_attention_mode <span class="token operator">==</span> <span class="token string">"FC"</span><span class="token punctuation">,</span> 
                <span class="token string">"FC channel attention block need feature maps' channels, ratio"</span>
            self<span class="token punctuation">.</span>channel_attention_block <span class="token operator">=</span> Channel_Attention_Module_FC<span class="token punctuation">(</span>channels <span class="token operator">=</span> channels<span class="token punctuation">,</span> ratio <span class="token operator">=</span> ratio<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> channel_attention_mode <span class="token operator">==</span> <span class="token string">"Conv"</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> channels <span class="token operator">!=</span> <span class="token boolean">None</span> <span class="token keyword">and</span> gamma <span class="token operator">!=</span> <span class="token boolean">None</span> <span class="token keyword">and</span> b <span class="token operator">!=</span> <span class="token boolean">None</span> <span class="token keyword">and</span> channel_attention_mode <span class="token operator">==</span> <span class="token string">"Conv"</span><span class="token punctuation">,</span> 
                <span class="token string">"Conv channel attention block need feature maps' channels, gamma, b"</span>
            self<span class="token punctuation">.</span>channel_attention_block <span class="token operator">=</span> Channel_Attention_Module_Conv<span class="token punctuation">(</span>channels <span class="token operator">=</span> channels<span class="token punctuation">,</span> gamma <span class="token operator">=</span> gamma<span class="token punctuation">,</span> b <span class="token operator">=</span> b<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> channel_attention_mode <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">"FC"</span><span class="token punctuation">,</span> <span class="token string">"Conv"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> 
                <span class="token string">"channel attention block must be 'FC' or 'Conv'"</span>
        self<span class="token punctuation">.</span>spatial_attention_block <span class="token operator">=</span> Spatial_Attention_Module<span class="token punctuation">(</span>k <span class="token operator">=</span> spatial_attention_kernel_size<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>channel_attention_block<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>spatial_attention_block<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    feature_maps <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">54</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    model <span class="token operator">=</span> CBAMBlock<span class="token punctuation">(</span><span class="token string">"FC"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> channels <span class="token operator">=</span> <span class="token number">54</span><span class="token punctuation">,</span> ratio <span class="token operator">=</span> <span class="token number">9</span><span class="token punctuation">)</span>
    model<span class="token punctuation">(</span>feature_maps<span class="token punctuation">)</span>
    model <span class="token operator">=</span> CBAMBlock<span class="token punctuation">(</span><span class="token string">"Conv"</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> channels <span class="token operator">=</span> <span class="token number">54</span><span class="token punctuation">,</span> gamma <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span> b <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>
    model<span class="token punctuation">(</span>feature_maps<span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="4__411"></a>4. 个人理解</h3> 
<p>空间注意力机制与通道注意力机制有异曲同工之妙，都是通过提取权重，作用在原特征图上，只不过一个是在<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        H
       
       
        ,
       
       
        W
       
       
        ]
       
      
      
       {[H,W]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.08125em">H</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault" style="margin-right: 0.13889em">W</span><span class="mclose">]</span></span></span></span></span></span>维度上，一个是在<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        [
       
       
        C
       
       
        ]
       
      
      
       {[C]}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mopen">[</span><span class="mord mathdefault" style="margin-right: 0.07153em">C</span><span class="mclose">]</span></span></span></span></span></span>维度上，这样的方法在不增加过多的计算量的前提下能提点，不失为一个好的trick。<br> Attention is all you need!</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>