<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Word2Vec对新闻进行分类 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Word2Vec对新闻进行分类</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <h1><strong>词表征</strong></h1> 
<p><strong>·词表征就是如何用向量的方式来表示一个词的特征，让计算机能够对词进行处理，常用的两种词表征的方法：</strong></p> 
<p><strong>·词袋模型：一个词也可以理解为是一篇最简单的文档，所以它可以用词袋来表示他的特征，这个时候的词袋就是一个独热编码。</strong></p> 
<p><strong>独热编码举例：</strong></p> 
<p><img alt="" height="415" src="https://images2.imgbox.com/8b/26/g9m15Ir1_o.png" width="441"></p> 
<p> </p> 
<p><strong>·词向量模型：</strong></p> 
<p><strong>词向量：又叫词嵌入，这种方法可以解决词袋模型的稀核心思想是：每一个词映射到一个多维空间中，成为空间中的一个向量，一般这个多维空间的维数不会太高，在几百个的量级，这几百维的特征向量是稠密的，向量中的每一个成员都是非零的。</strong></p> 
<p><strong>由于词向量由几百个维度构成，所以也被称为分布式表征。词向量模型是通过对原始文本建模训练学习得到的。</strong></p> 
<p><strong>由于词向量把每一个词映射到了一个高维空间中，并用向量表示，响亮的生成是基于词与词之间的相关性得来，可以理解为相关的词在空间中的位置比较靠近，所以词向量有一个非常有趣的特征，那就是类比。</strong></p> 
<p><strong>·中心词：就是每一个待分析的词</strong></p> 
<p><strong>·邻居词：在文档语料中，出现在中心词，周围某个小窗口内的关联词</strong></p> 
<p><strong>·窗口大小c：就是指寻找邻居词的时候需要观察中心词的前后c个词</strong></p> 
<p><strong>举例说明：“我家/猫/是/我/养/的/第一/只/宠物”这句话中，如果把“猫”当成当前正在分析的中心词，如果窗口大小c=3，那么，“猫”的邻居词是：我家，是，我，养。</strong></p> 
<p><strong>词向量模型的核心原理就是用邻居词的概率分布来作为中心词的向量表示。</strong></p> 
<p><strong>1.基于邻居词共现矩阵分解法</strong></p> 
<p><img alt="" height="302" src="https://images2.imgbox.com/e3/67/2uJWlau5_o.png" width="528"></p> 
<p><strong> 2.神经网络训练：通过构建两种类型的预测模型，然后使用网络的隐藏层输出作为词向量表征，这两种预测模型是</strong></p> 
<p><strong>CBOW：利用中心词和邻居词预测中心词</strong></p> 
<p><strong>Skip-gram：利用中心词来预测邻居词</strong></p> 
<p><strong>不管是哪种类型的神经网络，它的本质都是希望发现中心词和邻居词之间的相关关系，词向量就是隐藏在这个相关关系中的隐特征。</strong></p> 
<p><img alt="" height="415" src="https://images2.imgbox.com/f2/59/ljex9Zc4_o.png" width="614"></p> 
<p> <strong>从上图的示例中可以看到：输入是中心词（或者是邻居词），输出是邻居词（或者中心词）。神经网络中间有一个隐藏层，他的神经元个数要显著小于词的个数（一般就只有几百个），通过预测模型的训练学习，我们会得到网络的连接权重，例如”drink“这个词会和隐藏层的所有神经元都有连接权重，依据这个权重就可以得到drink这个词的词向量，向量的长度就是隐藏层的神经元个数，向量的数值就是神经元之间的连接权重。再看图的右边，与drink连接权重较高的神经元，他的右边又连接了一些词，这些词可以理解为就是drink的邻居词，例如juice，milk之类的词。</strong></p> 
<p></p> 
<p><strong>词向量只是对词的特征表征，如果要对一篇文档进行特征表征，有以下几种方法</strong></p> 
<p><strong>·直接使用文档中所有词的词向量的平均值</strong></p> 
<p><strong>·使用文档中每个词的TF-IDF值做为权重，与每个词的词向量进行加权平均</strong></p> 
<p><strong>·根据文档中每个词的词向量对文档进行聚类，使用聚类后包含词最多的那个类的中心点作为文档特征向量</strong></p> 
<p><strong>·使用doc2vec模型，这是个类似word2vec的模型，不过他是直接对doc来建模</strong></p> 
<p></p> 
<h1><strong>以下为训练word2vec的代码及一些参数的讲解</strong></h1> 
<p><strong>创建输出目录 用来保存训练好的词向量</strong></p> 
<pre><code>output_dir='output_word2vec'
import os
if not os.path.exists(output_dir):
    os.mkdir(output_dir)</code></pre> 
<h2 id="1-导入数据">导入数据</h2> 
<pre><code>import numpy as np
import pandas as pd
</code></pre> 
<p> 查看训练数据</p> 
<pre><code>
train_data=pd.read_csv('sohu_train.txt',sep='t',header=None,dtype=np.str_,encoding='utf8',names=['频道','文章'])
train_data.info()</code></pre> 
<p>·<strong>输出结果</strong></p> 
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
RangeIndex: 12000 entries, 0 to 11999
Data columns (total 2 columns):
 #   Column  Non-Null Count  Dtype 
---  ------  --------------  ----- 
 0   频道      12000 non-null  object
 1   文章      12000 non-null  object
dtypes: object(2)
memory usage: 187.6+ KB</code></pre> 
<p>载入停用词</p> 
<pre><code>stopwords = set()
with open('stopwords.txt', 'r',encoding='utf8') as infile:
    for line in infile:
        line = line.rstrip('n')
        if line:
            stopwords.add(line.lower())</code></pre> 
<h2 id="2-分词">分词</h2> 
<pre><code>import jieba
article_words=[]
# 遍历每篇文章
for article in train_data[u'文章']:
    curr_words=[]
    # 遍历文章中的每个词
    for word in jieba.cut(article):
        # 去除停用词
        if word not in stopwords:
            curr_words.append(word)
    article_words.append(curr_words)</code></pre> 
<p><strong>·输出结果</strong></p> 
<pre><code>Building prefix dict from the default dictionary ...
Loading model from cache C:Users10248AppDataLocalTempjieba.cache
Loading model cost 0.530 seconds.
Prefix dict has been built successfully.</code></pre> 
<p>分词结果存储到文件</p> 
<pre><code>seg_word_file=os.path.join(output_dir,'seg_words.txt')
with open(seg_word_file,'wb') as outfile:
    for words in article_words:
        outfile.write(u' '.join(words).encode('utf8') + b'n')
print('分词结果保存到文件:{}'.format(seg_word_file))</code></pre> 
<p>·<strong>输出结果</strong></p> 
<pre><code>分词结果保存到文件:output_word2vecseg_words.txt</code></pre> 
<h2 id="3-训练word2vec模型">预训练word2vec模型</h2> 
<pre><code>from gensim.models import Word2Vec
from gensim.models.word2vec import LineSentence
from gensim.models import KeyedVectors</code></pre> 
<p>创建一个句子迭代器，一行为一个句子，词和词之间用空格分开<br> 这里我们把一篇文章当作一个句子</p> 
<pre><code>sentences=LineSentence(seg_word_file)</code></pre> 
<p>预训练word2vec模型<br> 参数说明：<br>  sentences: 包含句子的list，或迭代器<br> size: 词向量的维数，size越大需要越多的训练数据，同时能得到更好的模型<br> alpha: 初始学习速率，随着训练过程递减，最后降到 min_alpha<br> window: 上下文窗口大小，即预测当前这个词的时候最多使用距离为window大小的词<br> max_vocab_size: 词表大小，如果实际词的数量超过了这个值，过滤那些频率低的<br> workers: 并行度<br> iter: 训练轮数<br> min_count: 忽略出现次数小于该值的词</p> 
<pre><code>model=Word2Vec(sentences=sentences,min_count=20)</code></pre> 
<p>保存模型</p> 
<pre><code>model_file = os.path.join(output_dir, 'model.w2v')
model.save(model_file)</code></pre> 
<h1><strong>测试预训练模型</strong></h1> 
<p><strong>读取模型</strong></p> 
<pre><code>model_file = os.path.join(output_dir, 'model.w2v')
model2=Word2Vec.load(model_file)</code></pre> 
<h3 id="4.1-查找语义相近的词">查找语义相近的词</h3> 
<pre><code>def invest_similar(*args,**kwargs):
    res=model2.wv.most_similar(*args,**kwargs)
    print('n'.join(['{}:{}'.format(x[0],x[1]) for x in res]))

invest_similar(u'摄影', topn=5)</code></pre> 
<p><strong>·输出结果</strong></p> 
<pre><code>刘晓科:0.7663402557373047
刘建东:0.7626208066940308
评语:0.7005037069320679
作曲:0.6986984014511108
璇:0.67718505859375</code></pre> 
<p>女人 + 先生 - 男人 = 女士<br> 先生 - 女士 = 男人 - 女人，这个向量的方向就代表了性别! </p> 
<pre><code>invest_similar(positive=[u'女人', u'先生'], negative=[u'男人'], topn=1)</code></pre> 
<p>·<strong>输出结果</strong></p> 
<pre><code>蔡:0.6899486184120178</code></pre> 
<h3 id="4.2-计算两个词的相似度">计算两个词的相似度</h3> 
<pre><code>model2.wv.similarity('摄影','摄像')</code></pre> 
<p>·输出结果</p> 
<pre><code>0.6245521</code></pre> 
<h3 id="4.3-查询某个词的词向量">查询某个词的词向量</h3> 
<pre><code>model2.wv[u'摄影'].shape</code></pre> 
<p>·输出结果</p> 
<pre><code>(100,)</code></pre> 
<pre><code>model2.wv[u'摄影']</code></pre> 
<p>·输出结果</p> 
<pre><code>array([ 1.12402821e+00, -1.43426090e-01,  1.27216709e+00, -1.03221321e+00,
       -1.87992001e+00,  5.13237119e-01, -1.13613218e-01,  4.65803176e-01,
        2.23834977e-01, -5.68113267e-01, -1.13676023e+00,  6.05148017e-01,
        1.92878091e+00,  1.35197982e-01,  4.71386909e-01,  3.13203558e-02,
       -4.88490194e-01, -5.21153510e-01, -3.16076130e-01, -4.14293671e+00,
       -1.09550381e+00,  2.31205606e+00,  3.80034757e+00, -8.64517391e-01,
        8.61354887e-01, -4.89337295e-01, -3.63620043e-01,  2.25580406e+00,
       -9.07084405e-01,  7.68696427e-01,  8.44246987e-03, -4.67379779e-01,
        2.23277569e+00, -1.60536277e+00, -1.76252687e+00,  2.04124570e+00,
       -5.92672646e-01, -1.79212022e+00, -8.45354021e-01,  1.63020134e-01,
       -4.94004756e-01,  7.84639716e-02,  2.46292621e-01,  3.91405135e-01,
        2.69702244e+00,  1.12125501e-01, -3.00367903e-02,  3.96094732e-02,
       -7.09702730e-01,  2.72683471e-01,  1.63493916e-01,  3.45271856e-01,
       -7.32331157e-01, -1.10088050e+00,  7.25350261e-01,  1.89776182e-01,
       -1.67757552e-03, -1.81457877e+00, -2.36800209e-01,  5.88630319e-01,
       -1.17891036e-01,  1.70819044e+00, -2.11411715e-01,  4.82740730e-01,
        2.90950954e-01, -6.00913882e-01,  6.11816823e-01,  3.15804314e-03,
       -9.11727548e-01,  1.11618125e+00,  5.53577483e-01,  9.87007380e-01,
        1.19754769e-01, -4.53332961e-02,  1.14017117e+00,  5.29826954e-02,
       -6.54554486e-01, -1.82963490e+00,  1.63241223e-01, -6.50338531e-01,
        1.28191340e+00,  1.39220166e+00, -3.26665908e-01,  7.38676339e-02,
       -2.12200940e-01,  6.16843961e-02, -1.28452039e+00, -1.28339744e+00,
       -1.09384215e+00, -1.32426918e+00,  1.16123927e+00, -3.39918613e-01,
        1.30219662e+00,  3.30029815e-01,  1.47671258e+00,  4.75448519e-01,
        6.79319859e-01, -2.00764275e+00,  8.49902809e-01, -4.79526490e-01],
      dtype=float32)</code></pre> 
<h1>完整训练word2vec模型</h1> 
<p>创建输出目录</p> 
<pre><code>import os
output_dir = u'output_w2v'
if not os.path.exists(output_dir):
    os.mkdir(output_dir)</code></pre> 
<h2 id="1-加载数据">加载数据</h2> 
<pre><code>import numpy as np
import pandas as pd</code></pre> 
<p>查看训练数据</p> 
<pre><code>train_data = pd.read_csv('sohu_train.txt', sep='t', header=None, dtype=np.str_, encoding='utf8', names=[u'频道', u'文章'])
train_data.head()</code></pre> 
<p>载入停用词</p> 
<pre><code>stopwords = set()
with open('stopwords.txt', 'r',encoding='utf8') as infile:
    for line in infile:
        line = line.rstrip('n')
        if line:
            stopwords.add(line.lower())</code></pre> 
<h2 id="2-计算每个文章的词向量">计算每个文章的词向量</h2> 
<p>加载训练好的Word2Vec模型<br> 需要预训练的执行结果</p> 
<pre><code>from gensim.models import Word2Vec
w2v = Word2Vec.load('output_word2vec/model.w2v')</code></pre> 
<p>使用文章中所有词的平均词向量作为文章的向量</p> 
<pre><code>import jieba
def compute_doc_vec_single(article):
    vec = np.zeros((w2v.layer1_size,), dtype=np.float32)
    n = 0
    for word in jieba.cut(article):
        if word in w2v.wv:
            vec += w2v.wv[word]#求所有词向量的和
            n += 1#计算词的个数
    return vec / n#求平均值

def compute_doc_vec(articles):
    return np.row_stack([compute_doc_vec_single(x) for x in articles])

x = compute_doc_vec(train_data[u'文章'])</code></pre> 
<h2 id="3-训练分类器">训练分类器</h2> 
<p>编码目标变量</p> 
<pre><code>from sklearn.preprocessing import LabelEncoder
y_encoder = LabelEncoder()
y = y_encoder.fit_transform(train_data[u'频道'])</code></pre> 
<p>划分训练测试数据</p> 
<pre><code>from sklearn.model_selection import train_test_split
# 根据y分层抽样，测试数据占20%
train_idx, test_idx = train_test_split(range(len(y)), test_size=0.2, stratify=y)
train_x = x[train_idx, :]
train_y = y[train_idx]
test_x = x[test_idx, :]
test_y = y[test_idx]</code></pre> 
<p>训练逻辑回归模型</p> 
<p>常用参数说明<br> penalty: 正则项类型，l1还是l2<br> C: 正则项惩罚系数的倒数，越大则惩罚越小<br> fit_intercept: 是否拟合常数项<br> max_iter: 最大迭代次数<br> multi_class: 以何种方式训练多分类模型<br>     ovr = 对每个标签训练二分类模型<br>     multinomial = 直接训练多分类模型，仅当solver={newton-cg, sag, lbfgs}时支持<br> solver: 用哪种方法求解，可选有{liblinear, newton-cg, sag, lbfgs}<br>     小数据liblinear比较好，大数据量sag更快<br>     多分类问题，liblinear只支持ovr模式，其他支持ovr和multinomial<br>     liblinear支持l1正则，其他只支持l2正则</p> 
<pre><code>from sklearn.linear_model import LogisticRegression
model = LogisticRegression(multi_class='multinomial', solver='lbfgs')
model.fit(train_x, train_y)</code></pre> 
<h2 id="4-模型效果评估">模型效果评估</h2> 
<pre><code>from sklearn.metrics import confusion_matrix, precision_recall_fscore_support</code></pre> 
<p>在测试集上计算模型的表现</p> 
<pre><code>test_y_pred = model.predict(test_x)</code></pre> 
<p>计算混淆矩阵</p> 
<pre><code>pd.DataFrame(confusion_matrix(test_y, test_y_pred), columns=y_encoder.classes_, index=y_encoder.classes_)</code></pre> 
<p>·输出结果</p> 
<pre><code>	体育	健康	女人	娱乐	房地产	教育	文化	新闻	旅游	汽车	科技	财经
体育	184	0	2	0	0	3	1	4	6	0	0	0
健康	0	158	11	0	0	3	3	11	2	1	2	9
女人	1	5	149	13	0	0	16	4	6	2	4	0
娱乐	1	0	18	145	2	2	26	1	2	1	2	0
房地产	0	0	1	1	173	0	1	8	4	3	1	8
教育	0	2	5	1	1	169	5	9	3	0	3	2
文化	2	3	12	35	2	1	118	14	9	0	4	0
新闻	6	8	4	3	4	9	4	134	7	3	6	12
旅游	1	2	8	0	4	0	9	7	158	1	3	7
汽车	1	3	1	0	0	1	0	1	3	185	3	2
科技	0	5	3	0	2	4	1	5	2	3	162	13
财经	1	2	4	0	12	0	1	13	1	7	19	140</code></pre> 
<p>计算各项评价指标</p> 
<pre><code>def eval_model(y_true, y_pred, labels):
    # 计算每个分类的Precision, Recall, f1, support
    p, r, f1, s = precision_recall_fscore_support(y_true, y_pred)
    # 计算总体的平均Precision, Recall, f1, support
    tot_p = np.average(p, weights=s)
    tot_r = np.average(r, weights=s)
    tot_f1 = np.average(f1, weights=s)
    tot_s = np.sum(s)
    res1 = pd.DataFrame({
        u'Label': labels,
        u'Precision': p,
        u'Recall': r,
        u'F1': f1,
        u'Support': s
    })
    res2 = pd.DataFrame({
        u'Label': [u'总体'],
        u'Precision': [tot_p],
        u'Recall': [tot_r],
        u'F1': [tot_f1],
        u'Support': [tot_s]
    })
    res2.index = [999]
    res = pd.concat([res1, res2])
    return res[[u'Label', u'Precision', u'Recall', u'F1', u'Support']]

eval_model(test_y, test_y_pred, y_encoder.classes_)</code></pre> 
<p>·输出结果</p> 
<pre><code>0	体育	0.934010	0.92000	0.926952	200
1	健康	0.840426	0.79000	0.814433	200
2	女人	0.683486	0.74500	0.712919	200
3	娱乐	0.732323	0.72500	0.728643	200
4	房地产	0.865000	0.86500	0.865000	200
5	教育	0.880208	0.84500	0.862245	200
6	文化	0.637838	0.59000	0.612987	200
7	新闻	0.635071	0.67000	0.652068	200
8	旅游	0.778325	0.79000	0.784119	200
9	汽车	0.898058	0.92500	0.911330	200
10	科技	0.775120	0.81000	0.792176	200
11	财经	0.725389	0.70000	0.712468	200
999	总体	0.782105	0.78125	0.781278	2400</code></pre> 
<h2 id="5-模型保存">模型保存</h2> 
<pre><code># 保存模型到文件
import dill
import pickle
model_file = os.path.join(output_dir, u'model.pkl')
with open(model_file, 'wb') as outfile:
    pickle.dump({
        'y_encoder': y_encoder,
        'lr': model
    }, outfile)</code></pre> 
<h2 id="6-对新文档预测">对新文档预测</h2> 
<pre><code>from gensim.models import Word2Vec
import dill
import pickle
import jieba</code></pre> 
<p>把预测相关的逻辑封装在一个类中，使用这个类的实例来对新文档进行分类预测</p> 
<pre><code>class Predictor(object):
    
    def __init__(self, w2v_model_file, lr_model_file):
        self.w2v = Word2Vec.load(w2v_model_file)
        with open(lr_model_file, 'rb') as infile:
            self.model = pickle.load(infile)
    
    def predict(self, articles):
        x = self._compute_doc_vec(articles)
        y = self.model['lr'].predict(x)
        y_label = self.model['y_encoder'].inverse_transform(y)
        return y_label
    
    def _compute_doc_vec(self, articles):
        return np.row_stack([compute_doc_vec_single(x) for x in articles])

    def _compute_doc_vec_single(self, article):
        vec = np.zeros((w2v.layer1_size,), dtype=np.float32)
        n = 0
        for word in jieba.cut(article):
            if word in w2v:
                vec += w2v[word]
                n += 1
        return vec / n</code></pre> 
<p>加载新文档数据</p> 
<pre><code>new_data = pd.read_csv('sohu_test.txt', sep='t', header=None, dtype=np.str_, encoding='utf8', names=[u'频道', u'文章'])
new_data.head()</code></pre> 
<p>加载模型</p> 
<pre><code>predictor = Predictor('output_word2vec/model.w2v', model_file)</code></pre> 
<p>预测前10000篇的分类</p> 
<pre><code>new_y_pred = predictor.predict(new_data[u'文章'][:10000])</code></pre> 
<p>对比预测</p> 
<pre><code>pd.DataFrame({u'预测频道': new_y_pred, u'实际频道': new_data[u'频道'][:10000]})</code></pre> 
<p><img alt="" height="415" src="https://images2.imgbox.com/e6/70/0xfzm5Pd_o.png" width="338"></p> 
<p> </p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>