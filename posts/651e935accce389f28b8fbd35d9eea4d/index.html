<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【Pytorch深度学习50篇】·······第五篇：【YOLO】【2】-----数据标签的准备 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【Pytorch深度学习50篇】·······第五篇：【YOLO】【2】-----数据标签的准备</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p>兄弟们，朋友们，为期两周的驻场生活结束了，没准说不定啥时候有要去，所以抓紧把YOLO篇搞定，驻场可是太累了，早6晚9，这和早9晚6可是一个天上一个地下啊，好了，废话不多说，今天进入YOLO中最难理解的部分。</p> 
<h1>2.dataset 数据准备和创建标签</h1> 
<p>先上代码吧，我们一点一点来讲</p> 
<pre><code class="language-python">from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import transforms
import torch

import os
import cv2
import xml.etree.ElementTree as ET
import numpy as np
import random

import config
import utils as ut
import data_agumentation as augmentation


class Yolo_Dataset(Dataset):
    def __init__(self, img_file_path, anno_file_path,data_augmentation):
        self.img_path = [os.path.join(img_file_path, x) for x in os.listdir(img_file_path)]
        self.anno_path = [os.path.join(anno_file_path, x.replace('.' + x.split('.')[-1], '.xml')) for x in
                          os.listdir(img_file_path)]
        self.tranform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])
        self.data_augmentation = data_augmentation

    def __len__(self):
        return len(self.img_path)

    def __getitem__(self, index):
        # 图片resize后转换为tensor
        img = cv2.imread(self.img_path[index])
        ori_img_size = img.shape
        img = cv2.resize(img, (config.resize_image_size[1], config.resize_image_size[0]))
        img_tensor = self.tranform(img)

        # 图片对应的原始坐标转换成tensor 维度是[-1,5] [cls,xmin,ymin,xmax,ymax]
        ori_anno_data_tensor = torch.tensor(ut.read_xml(self.anno_path[index], config.class_name)).view(-1, 5)
        # 数据增强
        if self.data_augmentation == True:
            after_aug_img,after_aug_box = augmentation.draw_mask(img_tensor,ori_anno_data_tensor)
        # 转换成resize之后的坐标
        resize_anno_data_tensor = ut.convert(ori_anno_data_tensor, config.resize_image_size, ori_img_size)
        labels = ut.make_labels(config.resize_image_size, config.ANCHORS_GROUP, len(config.class_name),
                                resize_anno_data_tensor, config.ANCHORS_GROUP_AREA)

        return img_tensor, labels[32], labels[16], labels[8]


if __name__ == '__main__':
    img_file_path = r'D:DATASface_maskJPEGImages'
    anno_file_path = r'D:DATASface_maskAnnotations'
    yolo_dataset = Yolo_Dataset(img_file_path, anno_file_path)
    train_loader = DataLoader(yolo_dataset, batch_size=1, shuffle=True, num_workers=0)
    for img, label_32, label_16, label_8 in train_loader:
        print(img.shape, label_32.shape, label_16.shape, label_8.shape)
        break
</code></pre> 
<p>首先我们定义了一个Yolo_Dataset的类，初始化函数做的事就是得到所有训练图片的路径和所有标签（xml文件的路径）。</p> 
<p>我们来看看数据集的图片和标签</p> 
<p><img alt="" height="89" src="https://images2.imgbox.com/f9/49/2vse3MVz_o.png" width="711"></p> 
<p>Annotations里面就是标签文件了，也就是xml文件</p> 
<p>JPEGImages里面就是对应的图片了，上图看看</p> 
<p><img alt="" height="243" src="https://images2.imgbox.com/8c/90/sfnIotlj_o.png" width="713"><img alt="" height="325" src="https://images2.imgbox.com/ff/23/MSsxJAXw_o.png" width="986"> 我们再打开一个xml文件看看里面都写了什么信息</p> 
<p style="text-align:center"><img alt="" height="623" src="https://images2.imgbox.com/20/44/xDDuqDwx_o.png" width="621"></p> 
<p> 这个文件中其实对我们有用的信息是&lt;name&gt;&lt;xmin&gt;&lt;ymin&gt;&lt;xmax&gt;&lt;ymax&gt;,它代表的含义就是类别名称，和这个目标的在图片的坐标位置，&lt;xmin&gt;&lt;ymin&gt;是左上角左边，&lt;xmax&gt;&lt;ymax&gt;是右下角坐标，所以这个是一个矩形框。这就是我们需要从标签文件中得到的信息。至于我们是怎么去得到这个信息的，接下来我也会把代码公布出来，其实有很多种方法的。</p> 
<h2>2.1 __getitem__函数的讲解</h2> 
<pre><code class="language-python">    def __getitem__(self, index):
        # 图片resize后转换为tensor
        img = cv2.imread(self.img_path[index])
        ori_img_size = img.shape
        img = cv2.resize(img, (config.resize_image_size[1], config.resize_image_size[0]))
        img_tensor = self.tranform(img)

        # 图片对应的原始坐标转换成tensor 维度是[-1,5] [cls,xmin,ymin,xmax,ymax]
        ori_anno_data_tensor = torch.tensor(ut.read_xml(self.anno_path[index], config.class_name)).view(-1, 5)
        # 数据增强
        if self.data_augmentation == True:
            after_aug_img,after_aug_box = augmentation.draw_mask(img_tensor,ori_anno_data_tensor)
        # 转换成resize之后的坐标
        resize_anno_data_tensor = ut.convert(ori_anno_data_tensor, config.resize_image_size, ori_img_size)
        labels = ut.make_labels(config.resize_image_size, config.ANCHORS_GROUP, len(config.class_name),
                                resize_anno_data_tensor, config.ANCHORS_GROUP_AREA)

        return img_tensor, labels[32], labels[16], labels[8]</code></pre> 
<p> 首先就是读取图片，然后获取图片的H,W,C，然后将图片resize成你想要的大小，然后再把numpy个格式的数据变成pytorch需要的tensor。前四行代码就是在干这个事情。</p> 
<p>第5行代码，就是在读取xml文件来获取标签了</p> 
<pre><code class="language-python">ori_anno_data_tensor = torch.tensor(ut.read_xml(self.anno_path[index], config.class_name)).view(-1, 5)</code></pre> 
<p>所以我们来看看ut.read_xml这个函数，ut是我自定义的一个py文件，不必大惊小怪。</p> 
<pre><code class="language-python">import xml.etree.ElementTree as ET
import math
import torch


def read_xml(annotation_path, class_name):
    tree = ET.parse(annotation_path)
    root = tree.getroot()
    size = root.find('size')
    w = int(size.find('width').text)
    h = int(size.find('height').text)
    cls_box = []
    for obj in root.iter('object'):
        cls = obj.find('name').text
        cls_id = class_name.index(cls)
        xmlbox = obj.find('bndbox')
        b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),
             float(xmlbox.find('ymax').text))
        b1, b2, b3, b4 = b
        # 标注越界修正
        if b2 &gt; w:
            b2 = w
        if b4 &gt; h:
            b4 = h
        b = [cls_id, b1, b3, b2, b4]
        # (xmin,ymin,xmax,ymax)
        cls_box.extend(b)
    return cls_box</code></pre> 
<p>我这里导入了xml.etree.ElementTree这个库，这是用来读取xml文件，这个xml其实是VOC数据集的标注格式，相信大家也看出来了。这个函数执行完成后，我便可以得到要的信息[cls_id, b1, b3, b2, b4]，分别就是[类别id,xmin,ymin,xmax,ymax]，注意这里的类别id是id而不是类别名称。然后就是我们这部分数据再变成tensor。</p> 
<p>第6行、第7行代码执行的是样本增强的功能，目前这部分我还没弄好，可以先不增强，因为我的数据集有7000+的图片，还算比较多。</p> 
<p>第8行代码</p> 
<pre><code class="language-python">resize_anno_data_tensor = ut.convert(ori_anno_data_tensor, config.resize_image_size, ori_img_size)</code></pre> 
<p>又是我自定义的ut库，我们来看看我的convert在干什么，这个函数传入的参数分别是刚刚获取的类别坐标信息、resize的图片的高宽，原始图片的高宽。</p> 
<pre><code>def convert(ori_data, resize_img_size, ori_img_size):
    """
    :param ori_data: [-1,5] [cls,xmin,ymin,xmax,ymax]
    :param resize_img_size: [h,w] from config
    :param ori_img_size: [h,w]
    :return:
    """
    h_ratio = resize_img_size[0] / ori_img_size[0]
    w_ratio = resize_img_size[1] / ori_img_size[1]
    ori_data[:, 1] = ori_data[:, 1] * w_ratio
    ori_data[:, 2] = ori_data[:, 2] * h_ratio
    ori_data[:, 3] = ori_data[:, 3] * w_ratio
    ori_data[:, 4] = ori_data[:, 4] * h_ratio

    return ori_data</code></pre> 
<p>这部分的代码的做的是其实就是把原始坐标转换成resize之后的坐标，不好理解的话，我画一个图</p> 
<p><img alt="" height="350" src="https://images2.imgbox.com/1c/25/H4qqB8Bd_o.png" width="817"></p> 
<p>这个函数就是求出（？，？，？，？）里的‘？’到底等于多少，也就是以个坐标的转换，很简单有没有。</p> 
<p>好了到了最难理解的第9行的代码了</p> 
<pre><code class="language-python">labels = ut.make_labels(config.resize_image_size, config.ANCHORS_GROUP, len(config.class_name),
                                resize_anno_data_tensor, config.ANCHORS_GROUP_AREA)</code></pre> 
<p>ut这个我就不再多说了，传入的参数分别是resize的高宽，anchor，类别的数量，刚刚转换之后的坐标信息，anchor的面积。</p> 
<p>大家，休息休息，保持脑袋冷静。anchor这玩意出现了，这个玩意挺绕人的。</p> 
<p>mae_labels这个函数代码我先放出来，然后我们慢慢理解一下。</p> 
<pre><code class="language-python">def make_labels(resize_img_size, ANCHORS_GROUP, CLASS_NUM, boxes, ANCHORS_GROUP_AREA):
    labels = {}
    for feature_size, anchors in ANCHORS_GROUP.items():
        labels[feature_size] = torch.zeros(
            [int(resize_img_size[0] / feature_size), int(resize_img_size[1] / feature_size), 3, 5 + CLASS_NUM])

        for box in boxes:
            cls, xmin, ymin, xmax, ymax = box
            cx = (xmin + xmax) / 2
            cy = (ymin + ymax) / 2
            w = xmax - xmin
            h = ymax - ymin
            cx_offset, cx_index = math.modf(cx / feature_size)
            cy_offset, cy_index = math.modf(cy / feature_size)

            for i, anchor in enumerate(anchors):
                anchor_area = ANCHORS_GROUP_AREA[feature_size][i]
                p_w, p_h = w / anchor[0], h / anchor[1]
                conf = IOU(gt_box=[xmin, ymin, xmax, ymax],
                           anchor=[cx - (anchor[0] / 2), cy - (anchor[1] / 2), cx + (anchor[0] / 2),
                                   cy + (anchor[1] / 2)])
                labels[feature_size][int(cy_index), int(cx_index), i] = torch.tensor(
                    [conf, cx_offset, cy_offset, torch.log(p_w), torch.log(p_h), *one_hot(CLASS_NUM, int(cls))]
                )
    return labels</code></pre> 
<p>传入的参数中的anchor我也给大家看一看，是什么东西</p> 
<pre><code class="language-python">with open('./my_anchors.txt') as f:
    content = f.readlines()
    anchor_32 = []
    anchor_16 = []
    anchor_8 = []
    for index, i in enumerate(content):
        wh = i.split('n')[0]
        wh = list(map(int, wh.split(' ')))
        if index &lt; 3:
            anchor_8.append(wh)
        elif 3 &lt;= index and index &lt;= 5:
            anchor_16.append(wh)
        else:
            anchor_32.append(wh)

ANCHORS_GROUP = {
    32: anchor_32,  # [w,h]
    16: anchor_16,
    8: anchor_8}

ANCHORS_GROUP_AREA = {
    32: [x * y for x, y in ANCHORS_GROUP[32]],
    16: [x * y for x, y in ANCHORS_GROUP[16]],
    8: [x * y for x, y in ANCHORS_GROUP[8]],
}</code></pre> 
<p> my_anchors.txt长这个样子的，这个文件是怎么来的呢，是我用kmeans聚类得到的，kmeans这里我们先不说，免得节外生枝，反正就是得到了9行数据</p> 
<p><img alt="" height="239" src="https://images2.imgbox.com/1a/4a/PM2DG0ii_o.png" width="664"> </p> 
<p>每一行的第一个数是anchor的宽，第二数是anchor的高。</p> 
<p>这里你可能就有很多问题了，比如我之前很疑惑的几个问题：</p> 
<p>1.为什么有9行？</p> 
<p>2.ANCHORS_GROUP 这个字典中的32,16,8这三个键是什么意思？</p> 
<p>3.求面积干什么啊？</p> 
<p>要明白这个问题，我们就需要再来看看yolov3的这个模型结构了，上图</p> 
<p><img alt="" src="https://images2.imgbox.com/ed/f8/oBiC6Jmn_o.png"></p> 
<p>看看prediction这个部分，一共有3个featuremap，每个featuremap有3个长宽不一样的anchor，所以，我们就有9行数据，</p> 
<p>32,16,8分别是你下采样的倍数，下采样32倍得到的就是第一个featuremap，16倍的就是第二个featurmap，8倍的就是第三个featuremap。</p> 
<p>面积是为了求后面给自信度赋值用的。</p> 
<p>那我再提一个问题，我们可以看到每个featuremap最后一个channel维度都是255，请问这个255是怎么来的？</p> 
<p>好了，这个make_labels的函数我们下期再来更新</p> 
<p>至此，敬礼，salute！！！！</p> 
<p>老规矩，上咩咩图</p> 
<p> <img alt="" height="689" src="https://images2.imgbox.com/20/d1/eEu1xRtC_o.png" width="627"></p> 
<p> </p> 
<p> </p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>