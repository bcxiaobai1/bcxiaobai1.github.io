<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Python应用：什么是爬虫？ - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python应用：什么是爬虫？</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <p></p> 
<div class="toc"> 
 <h3>文章目录</h3> 
 <ul>
<li><a href="#_4">什么是爬虫</a></li>
<li><a href="#_14">虫之初，性本善？</a></li>
<li>
<ul>
<li><a href="#_27">出行</a></li>
<li><a href="#_67">社交</a></li>
<li><a href="#_96">电商</a></li>
<li><a href="#_125">搜索引擎</a></li>
<li><a href="#_135">政府部门</a></li>
<li><a href="#_151">总结</a></li>
</ul> 
  </li>
<li><a href="#_158">面向监狱编程</a></li>
<li><a href="#_182">爬虫的君子协议</a></li>
<li>
<ul>
<li><a href="#_186">什么是君子协议</a></li>
<li><a href="#_192">君子协议是怎么产生的？</a></li>
<li><a href="#_204">君子协议是什么内容？</a></li>
<li><a href="#robots_230">如何查看一个网站的robots协议</a></li>
<li><a href="#_278">违反君子协议的案例</a></li>
</ul> 
  </li>
<li><a href="#_304">参考文献</a></li>
</ul> 
</div> 
<p></p> 
<p><mark>2022年初的笔记了，"虫之初"一节基本摘抄来自参考文献1，推荐看一下参考文献1，写的很全且很有趣</mark>。</p> 
<h1>
<a id="_4"></a>什么是爬虫</h1> 
<p>什么是爬虫？</p> 
<p>爬虫就是一个探测机器，它的基本操作就是模拟人的行为去各个网站去溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。</p> 
<p>最大的爬虫，就是搜索引擎。</p> 
<p>你每天使用的百度，其实就是利用了这种爬虫技术：每天放出无数爬虫到各个网站，把他们的信息抓回来，然后化好淡妆排着小队等你来检索。</p> 
<h1>
<a id="_14"></a>虫之初，性本善？</h1> 
<p>是的没错，爬虫也分善恶。</p> 
<p>像谷歌这样的搜索引擎爬虫，每隔几天对全网的网页扫一遍，供大家查阅，各个被扫的网站大都很开心。这种就被定义为「善意爬虫」。</p> 
<p>但是，像抢票软件这样的爬虫，对着 12306 每秒钟恨不得撸几万次。铁总并不觉得很开心。这种就被定义为「恶意爬虫」。（注意，抢票的你觉得开心没用，被扫描的网站觉得不开心，它就是恶意的。）</p> 
<p><img src="https://images2.imgbox.com/23/dd/Kx1Oe0D6_o.png" alt="在这里插入图片描述"></p> 
<p>上图（来自参考文献1）显示的就是各行各业被爬虫骚扰的比例，注意，这张图显示的是全世界。</p> 
<h2>
<a id="_27"></a>出行</h2> 
<p>国内爬出行行业的爬虫中，有90%的流量都是冲着12306去的，毕竟铁路就这一家。</p> 
<p>你还记得当年 12306 上线王珞丹和<a href="https://www.zhihu.com/search?q=%E7%99%BD%E7%99%BE%E4%BD%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7B%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A453634446%7D">白百何</a>的「史上最坑图片验证码」么？</p> 
<p><img src="https://images2.imgbox.com/55/3a/a44Cwv1D_o.png" alt="在这里插入图片描述"></p> 
<p>这些东西不是为了故意难为老老实实卖票的人的，而恰恰是为了阻止爬虫（也就是抢票软件）的点击。刚才说了，爬虫只会简单地机械点击，它不认识白百何，所以很大一部分爬虫就被挡在了门外。</p> 
<p>当然，所谓道高一尺魔高一丈，并不是所有爬虫都会被白百何挡在门外。</p> 
<p>有一种东西叫做<strong>打码平台</strong>，平台雇用了很多人手，手工来识别验证码。抢票软件如果遇到了之前没见过的图片验证码，系统就会自动把这些验证码传回来，由他们手工做好标记，然后再把结果传回去。总共的过程用不了几秒时间。</p> 
<p>当然，这样的打码平台还有记忆功能。等回头时间长了，12306里的图片基本都能被标记完一遍了，防御手段被攻破，那12306在爬虫面前，自然是任君采撷。</p> 
<p>所以你知道每年过年之前，12306 被点成什么样了吗？公开数据是这么说的：「最高峰时 1 天内页面浏览量达 813.4 亿次，1 小时最高点击量 59.3 亿次，平均每秒 164.8 万次。」这还是加上验证码防护之后的数据。可想而知被拦截在外面的爬虫还有多少。</p> 
<p>同样的，航空的境遇也没好到哪儿去。</p> 
<p><img src="https://images2.imgbox.com/08/eb/W72qLQw3_o.png" alt="在这里插入图片描述"></p> 
<p>上图为航空类爬虫的分布比例。</p> 
<p>以亚洲航空为例。这是一家马来西亚的廉价航空公司，航线基本都是从中国各地飞往东南亚的旅游胜地，飞机上连矿泉水都得自费买，堪称贫民度假首选。</p> 
<p>亚航经常放出一些特别便宜的票。初衷是为了吸引游客，但是对黄牛党来说，这就是商机。</p> 
<p>据说，他们是这么玩的：</p> 
<p>技术宅黄牛党们利用爬虫，不断刷新亚航的票务接口，一旦出现便宜的票，不管三七二十一先拍下来再说。</p> 
<p>亚航有规定，你拍下来半小时（具体时间记不清了）不付款票就自动回到票池，继续卖。但是黄牛党们在爬虫脚本里写好了精确的时间，到了半小时，一毫秒都不多，他又把票拍下来，如此循环。直到有人从黄牛党这里定了这个票，黄牛党就接着利用程序，在亚航系统里放弃这张票，然后 0.00001 秒之后，就帮你用你的名字预定了这张票。</p> 
<h2>
<a id="_67"></a>社交</h2> 
<p>社交的爬虫重灾区，就是微博</p> 
<p><img src="https://images2.imgbox.com/74/f7/zjGjviAN_o.png" alt="在这里插入图片描述"></p> 
<p>以上是爬虫经常光顾的微博地址。</p> 
<p>这里的代码其实指向了微博的一个接口。它可以用来获取某个人的微博列表、微博的状态、索引等等等等。</p> 
<p>获得这些，能搞出什么骚操作呢？</p> 
<p>你想想看，如果我能随心所欲地指挥一帮机器人，打开某人的微博，然后刷到某一条，然后疯狂关注、点赞或者留言，这不就是标准的僵尸粉上班儿的流程么。。。</p> 
<p>僵尸粉只是爬虫的常规操作，还有更加花哨的场景，甚至能躺着挣钱：</p> 
<ol>
<li> <p>我是一个路人甲，我的微博没人关注，我用大量的爬虫，给自己做了十万人的僵尸粉，一群僵尸在我的微博下面点赞评论，不亦乐乎。</p> </li>
<li> <p>我去找一个保险公司，跟他说：你看我有这么多粉丝，你在我这投广告吧。我帮你发一条你们APP的注册链接，每有一个人通过我的链接注册了你们app，你就给我一毛钱。广告主说，不错，就这么办。</p> </li>
<li> <p>我发出注册链接，然后没人点。。。</p> </li>
<li> <p>不慌，我让十万爬虫继续前赴后继地点击注册链接，然后自动去完成注册动作。</p> </li>
<li> <p>我躺在床上，数着赚来的一万块钱。</p> </li>
</ol> 
<h2>
<a id="_96"></a>电商</h2> 
<p>电商在爬虫的骚扰排名中，排名第三。</p> 
<p>有几种东西叫作「比价平台」「聚合电商」和「返利平台」。他们大体都是一个原理：</p> 
<p>你搜索一样商品，这类聚合平台就会自动把各个电商的商品都放在你面前供你选择。如淘宝、京东。</p> 
<p>这就是爬虫的功劳。它们去各个电商的网站上把商品的图片和价格都扒下来，然后在自己这里展示。</p> 
<p>比如说我找了一个比价平台，叫做<a href="http://www.manmanbuy.com/">慢慢买</a>，然后搜索了iPhone12的比价：</p> 
<p><img src="https://images2.imgbox.com/97/37/GteWbx9P_o.png" alt="在这里插入图片描述"></p> 
<p>可能身为消费者的我们会感觉不错，不过对电商平台来讲，被放在一起比价，京东和天猫肯定是拒绝的。</p> 
<p>但是，机器爬虫模拟的是人的点击，<strong>电商很难阻止这类事情发生</strong>。他们甚至都不能向 12306 学习。你想想看，如果你每点开一个商品详情，淘宝都让你先分辨一次白百何和王珞丹，我不信你还有心情继续买下去。</p> 
<p>当然，电商对抗爬虫有另外的方法，那就是「web 应用防火墙」，简称 WAF。</p> 
<p>当然，这些聚合平台花了大力气和大资金来维持爬虫，不是做好事帮淘宝京东卖货的，他们也是有盈利手段的：</p> 
<ol>
<li>假设淘宝上有好几家店铺都卖iPhone，但是用户在我这里搜索的时候，我是有权利决定谁的店铺排在前面，谁的排在后面，就看谁给钱多呗。这一套，百度都玩烂了。另外，店铺跟淘宝可不是一致行动人，淘宝不希望自己的内容被聚合平台抓取，但是每个店铺可是很乐意多一个渠道帮他们卖货的；</li>
<li>页面独立广告；</li>
</ol> 
<h2>
<a id="_125"></a>搜索引擎</h2> 
<p>你可能了解，搜索引擎决定哪个网页排名靠前，（<strong>除了看给搜索引擎交了多少钱之外</strong>）主要一个指标就是看哪个搜索结果被人点击的次数更多。</p> 
<p>既然这样，那么我就派出爬虫，搜索某个特定的「关键词」，然后在结果里拼命地点击某个链接，那么这个网站在搜索引擎的权重里自然就会上升。这个过程就叫作 <strong>SEO（搜索引擎优化）</strong>。</p> 
<p>作为任何一个搜索引擎，都肯定不允许外人对于自己的搜索结果动手动脚，否则就会丧失公立性。它们会通过不定期调整算法来对抗 SEO。</p> 
<p>尤其是很多赌博、黄色网站，搜索引擎如果敢收广告费让他们排到前面，那就离倒闭不远了。所以黄赌毒网站只能利用黑色 SEO，强行把自己刷到前面。直到被搜索引擎发现，赶紧对它们「降权」处理。</p> 
<h2>
<a id="_135"></a>政府部门</h2> 
<p><img src="https://images2.imgbox.com/ce/a3/9ZROvlTv_o.png" alt="在这里插入图片描述"></p> 
<p>第二名，北京市预约挂号统一平台。这就是黄牛号贩子的问题。</p> 
<p>其他的，例如法院公告、信用中国、信用安徽，为什么爬虫要爬这些信息呢？</p> 
<p>因为有些信息，是只有政府部门才掌握的。</p> 
<p>比如，谁被告过，哪家公司曾经被行政处罚，哪个人曾经进入了失信名单。这些信息综合起来，可以用来做一个公司或者个人的信誉记录。</p> 
<h2>
<a id="_151"></a>总结</h2> 
<p>有人说技术有罪，有人说技术无罪。但是在《网络安全法》中，基本上没有「爬取网络公开信息被认定为违法」的条款。算是法律的灰色区域吧。<br> 之前看过一些技术老哥在网上发帖，只要有过来问他们会不会爬虫的，不多说，问就是不会。</p> 
<h1>
<a id="_158"></a>面向监狱编程</h1> 
<p>爬虫界一直有这么一句笑话，叫做“<strong>爬虫玩得好，牢饭吃得早</strong>”，或者有的爬虫教程干脆起名叫“<strong>快速入狱指南</strong>”。</p> 
<p>因为爬虫进监狱的案件数不胜数，前段时间听过一个18年的例子，一个爬虫项目的CTO和程序员都被抓了，为什么呢？因为他们公司有个业务，需要经常性的访问政府居住证网站，来查询房产地址、编码等情况，手动查询太慢了，于是公司的产品们讨论决定使用爬虫软件来做自动查询，18年3月，程序被部署上线了，然后4月份就出事了。</p> 
<p>2018年4月27日10:34-12:00左右，居住证网站的承建厂商发现自己系统宕机了，怀疑是遭到了人为攻击，但是由于日志缺失，无法定位到IP来源，所以只能作罢。结果5月2日的时候，系统再次遭到攻击，这次响应比较迅速，运维成功截获了IP地址，然后报案了。然后5.17日的时候，网警把这个公司的服务器IP给锁了，这才顺藤摸瓜把这个公司扯了出来。</p> 
<p>后来程序员说，是因为居住证网站后来加了验证码，但是公司的爬虫程序没有做更新，导致爬虫失控，频繁请求高达每秒183次，直接把对面网站攻击瘫痪了，导致所有居住证办理等对外服务都无法正常工作，影响了近100多个派出所和受理点的系统。</p> 
<p>最后，2018年8月，CTO和程序员被捕，法院认为，二人违反国家规定，对计算机信息系统进行干扰，造成为5万以上用户提供服务的计算机信息系统不能正常运行累计1小时以上，属于后果特别严重，应以破坏计算机信息系统罪追究其刑事责任。</p> 
<p>最终，负责并授权程序员开发涉案爬虫的CTO是主犯，被判有期徒刑三年，而开发爬虫的程序员系从犯，判处有期徒刑一年六个月。详细可以看一下参考文献6。</p> 
<p><img src="https://images2.imgbox.com/f4/f5/OqGFGs4u_o.png" alt="在这里插入图片描述"></p> 
<p>关于这件事，众说纷纭，很多人都说是政府网站开发技术太差，就是垃圾网站碰上了沙雕开发。对面网站没有基本的反爬虫防火墙，而这边的技术也是憨憨的暴力强爬。</p> 
<p><img src="https://images2.imgbox.com/ed/f1/Xvww4uVf_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="_182"></a>爬虫的君子协议</h1> 
<p>爬虫第一步，查看robots.txt</p> 
<h2>
<a id="_186"></a>什么是君子协议</h2> 
<p>搜索引擎的爬虫是善意的，它们检索你的网页信息是为了服务其他用户，为此它们还定义了robots.txt文件，作为君子协议。</p> 
<p>robots.txt其实是网站和搜索引擎之间的一种博弈产物，也叫作robots协议，是一种放在网站根目录下的文本文件。它被用来告诉搜索引擎的漫游器(即网络蜘蛛)，此网站下的哪些内容是不能被搜索引擎的漫游器获取的，哪些是可以获取的。</p> 
<h2>
<a id="_192"></a>君子协议是怎么产生的？</h2> 
<p>robots协议并不是某一个公司制定的，最早在20世纪90年代就已经出现了，那时候还没有Google。真实Robots协议的起源，是在互联网从业人员的公开邮件组里面讨论并且诞生的。即便是今天，互联网领域的相关问题也仍然是在一些专门的邮件组中讨论，并产生（当然，主要是在美国）。</p> 
<p>1994年6月30日，在经过搜索引擎人员以及被搜索引擎抓取的网站站长共同讨论后，正式发布了一份行业规范，即robots.txt协议。在此之前，相关人员一直在起草这份文档，并在世界互联网技术邮件组发布后，这一协议被几乎所有的搜索引擎采用，包括最早的<a href="https://baike.baidu.com/item/altavista">altavista</a>，infoseek，后来的<a href="https://baike.baidu.com/item/google">google</a>，<a href="https://baike.baidu.com/item/bing">bing</a>，以及中国的百度，<a href="https://baike.baidu.com/item/%E6%90%9C%E6%90%9C">搜搜</a>，<a href="https://baike.baidu.com/item/%E6%90%9C%E7%8B%97">搜狗</a>等公司也相继采用并严格遵循。</p> 
<p>robot，又称为spider，是搜索引擎自动获取网页信息的电脑程序的通称。robots的核心思想是要求<strong>爬虫程序不要去检索那些站长们不希望被直接搜索到的内容</strong>。</p> 
<p>自有搜索引擎之日起，Robots协议已是一种目前为止最有效的方式，用<strong>自律</strong>维持着网站与搜索引擎之间的平衡，让两者之间的利益不致过度倾斜。</p> 
<h2>
<a id="_204"></a>君子协议是什么内容？</h2> 
<p>robots.txt的内容格式：</p> 
<ul>
<li>
<strong>User-agent</strong>：定义爬虫的名称，比如说推特的叫做Twitterbot，百度的叫做Baiduspider，谷歌的叫做Googlebot。User-agent是*则表示针对的是所有爬虫。</li>
<li>Disallow：不允许爬虫访问的地址，地址的描述符合正则表达式的规则；</li>
<li>Allow：允许爬虫访问的地址</li>
</ul> 
<p>拿一个示例来看，以下节选自百度的robots协议：</p> 
<pre><code>User-agent: Baiduspider
Disallow: /baidu
Disallow: /s?
Disallow: /ulink?
Disallow: /link?
Disallow: /home/news/data/
Disallow: /bh
</code></pre> 
<p>Disallow: /baidu表示不允许百度的爬虫访问baidu目录下的所有目录和文件，这是个正则匹配的过程，可以简单的理解成：含有/baidu的URL均不能访问。</p> 
<h2>
<a id="robots_230"></a>如何查看一个网站的robots协议</h2> 
<p>在浏览器的网址搜索栏里，输入网站的根域名，然后再输入/robots.txt，比如百度的robots.txt网址为：https://www.baidu.com/robots.txt，<a href="https://cn.bing.com/robots.txt">必应</a></p> 
<p>其他以此类推。</p> 
<p>淘宝网这一类购物网站基本都禁了搜索引擎：</p> 
<p>比如说<a href="https://www.taobao.com/robots.txt">淘宝</a>：</p> 
<pre><code>User-agent: Baiduspider
Disallow: /

User-agent: baiduspider
Disallow: /
</code></pre> 
<p>直接禁止了百度的爬虫对其的爬取，但是没有禁止其他爬虫。</p> 
<p><a href="https://www.tmall.com/robots.txt">天猫</a>就比较狠了，它是都禁了：</p> 
<pre><code>User-agent: * 
Disallow: /
</code></pre> 
<p>所以有时候我们会在搜索结果中看到这么个东西：</p> 
<p><img src="https://images2.imgbox.com/4a/d9/S65fNXx3_o.png" alt="在这里插入图片描述"></p> 
<p>国外好多网站会这么写：</p> 
<pre><code>User-agent: Googlebot
Allow: /

User-agent: *
Disallow: /
</code></pre> 
<p>就是除了谷歌，其他都别想爬我</p> 
<h2>
<a id="_278"></a>违反君子协议的案例</h2> 
<p>这种案例有很多，我个人比较感兴趣的是国内的360搜索案。</p> 
<p>2012年8月，<a href="https://baike.baidu.com/item/360%E7%BB%BC%E5%90%88%E6%90%9C%E7%B4%A2/1305356">360综合搜索</a>被指违反robots协议。其不仅未经授权大量抓取百度、<a href="https://baike.baidu.com/item/google">google</a>内容，还记录国内知名网站的后台订单、优惠码等，甚至一些用户的电子邮箱、帐号、密码也被360通过浏览器悄然记录在案。更严重的是，360连企业内网信息都抓，导致了大量企业内网信息被泄露。2012年年底，百度工程师通过一个名为“鬼节捉鬼”的测试，证明了360浏览器存在私自上传“孤岛页面”等隐私内容到<a href="https://baike.baidu.com/item/360%E6%90%9C%E7%B4%A2/1281867">360搜索</a>的行为。</p> 
<p>后来，百度起诉奇虎360违反Robots协议，抓取并复制百度旗下<a href="https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E7%9F%A5%E9%81%93/92140">百度知道</a>、<a href="https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E7%99%BE%E7%A7%91/85895">百度百科</a>、<a href="https://baike.baidu.com/item/%E7%99%BE%E5%BA%A6%E8%B4%B4%E5%90%A7/95221">百度贴吧</a>等网站的内容，涉嫌侵权，已经构成了不正当竞争，并向奇虎索赔1亿元。此案于2013年10月16日上午在<a href="https://baike.baidu.com/item/%E5%8C%97%E4%BA%AC%E5%B8%82%E7%AC%AC%E4%B8%80%E4%B8%AD%E7%BA%A7%E4%BA%BA%E6%B0%91%E6%B3%95%E9%99%A2/1347599">北京市第一中级人民法院</a>开庭审理。更加有趣的是，就在开庭审理的当日，360起诉百度“强制跳转”，且已被北京市高级人民法院正式受理。这一案件中，360索赔金额高达4亿元，360诉称，百度恶意阻断360搜索引擎用户的访问，拦截360用户，强迫其到百度首页进行搜索，且该等技术手段仅歧视性地对待360搜索引擎用户，这些行为不仅严重影响用户体验，且已经构成了不正当竞争，给360造成重大损失。而百度方面回应则称，这是百度针对部分网站匿名访问和违规抓取百度内容、导致网民搜索体验不完整的行为而上线的一项保护措施。</p> 
<p>这两个案件扯得有点远，大约2年后才宣判，2014年8月7日，北京一中院做出一审判决，360赔偿百度70万元，但是驳回了百度公司的其他诉讼请求。另一个案子我在网上没找到结果。</p> 
<p>比较有趣的是，360方面认为，360搜索这些页面内容并不涉嫌侵犯百度的权益，实际上还为百度带来了大量的用户和流量，百度应该感谢360。</p> 
<p>robots协议完全就是一个自律条约，目前国内的现状是，基本只有大的搜索引擎会遵守，小门小户没人会在意这个。</p> 
<h1>
<a id="_304"></a>参考文献</h1> 
<ol>
<li>
<a href="https://www.zhihu.com/question/24098641?sort=created">通俗的讲，网络爬虫到底是什么？</a> 12306,1小时最高点击量高达59.3亿次可太恐怖了。</li>
<li><a href="https://baike.baidu.com/item/robots%E5%8D%8F%E8%AE%AE/2483797?fr=aladdin">百度百科-robots协议</a></li>
<li><a href="https://bj1zy.chinacourt.gov.cn/article/detail/2014/09/id/1446252.shtml"><strong>百度诉奇虎360违反Robots协议案一审宣判 360赔偿百度70万元</strong></a></li>
<li><a href="https://zhuanlan.zhihu.com/p/65463520">爬虫第一步：查看robots.txt</a></li>
<li><a href="https://www.zhihu.com/question/396958646/answer/2081752804">公司让爬Robots.txt声明了不允许爬的网站应该怎么办？</a></li>
<li><a href="https://www.zhihu.com/question/509748270/answer/2303293651">网络爬虫失控导致 CTO 和程序猿员工被判刑，技术从业者工作中如何规避业务风险保证自身合法权益？</a></li>
</ol>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>