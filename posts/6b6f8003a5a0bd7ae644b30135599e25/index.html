<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>PyTorch学习笔记 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch学习笔记</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>PyTorch学习笔记</h3>
 <ul><li>
<ul><li>
<ul>
<li><a href="#_1">基本术语</a></li>
<li><a href="#_5">算法利弊</a></li>
<li>
<ul>
<li><a href="#_7">梯度下降算法</a></li>
<li><a href="#_16">随机梯度下降算法</a></li>
<li><a href="#_20">鞍点</a></li>
</ul>
    </li>
<li><a href="#_27">画图函数</a></li>
<li><a href="#PyTorch_37">PyTorch入门</a></li>
<li>
<ul>
<li><a href="#_39">生成张量矩阵</a></li>
<li><a href="#________52">加法 * - /类似</a></li>
<li><a href="#_61">求均值</a></li>
<li><a href="#___67">判断相等 只是比较数据</a></li>
<li><a href="#_76">切片</a></li>
<li><a href="#___82">改变张量的形状 ,保证元素的总数量不变</a></li>
<li><a href="#torch__Tensor__NumPy__89">torch 的 Tensor 与 NumPy 数组的相互转换</a></li>
<li><a href="#Nympy_96">Nympy操作</a></li>
<li><a href="#pytorch__a_104">pytorch 对a进行转置</a></li>
<li><a href="#_111">求实际存储的相距位置</a></li>
<li><a href="#_123">查看存储的地址</a></li>
<li><a href="#storage_129">查看storage的数据</a></li>
<li><a href="#_135">转换成连续存储的</a></li>
<li><a href="#view__reshape__142">view 与 reshape 的区别</a></li>
</ul>
    </li>
<li><a href="#PyTorch_____Autograd_152">PyTorch Autograd自动求导</a></li>
<li><a href="#_176">求梯度例子</a></li>
<li>
<ul>
<li><a href="#_214">关于文本处理</a></li>
<li><a href="#_221">构建计算图</a></li>
<li><a href="#_226">构建模型的模板</a></li>
<li><a href="#pytroch_231">pytroch中的各种优化器</a></li>
<li><a href="#_235">相应的模型函数</a></li>
</ul>
    </li>
<li><a href="#_240">各种函数模型</a></li>
<li>
<ul>
<li><a href="#1Linear_Unit_244">1、Linear Unit</a></li>
<li><a href="#2Logistic_Regression_Unit_sigmoid_252">2、Logistic Regression Unit (sigmoid)</a></li>
<li><a href="#3Binary_Cross_262">3、Binary Cross</a></li>
<li><a href="#4SGD_271">4、SGD</a></li>
<li><a href="#5Softmax_284">5、Softmax</a></li>
<li><a href="#6Softmax__288">6、Softmax 计算公式</a></li>
<li><a href="#7NLLLOSS_292">7、NLLLOSS</a></li>
<li><a href="#8CrossEntropyLoss_306">8、CrossEntropyLoss</a></li>
<li><a href="#9Relu_314">9、Relu</a></li>
</ul>
    </li>
<li><a href="#_327">模型构建的四部</a></li>
<li><a href="#CNN_343">卷积神经网络CNN</a></li>
<li>
<ul>
<li><a href="#Conv2d___345">Conv2d 卷积层</a></li>
<li><a href="#MaxPool2d___362">MaxPool2d 池化层</a></li>
<li><a href="#1x1_373">1x1的卷积</a></li>
</ul>
    </li>
<li><a href="#RNN_380">循环神经网络RNN</a></li>
</ul>
  </li></ul>
 </li></ul>
</div>
<p></p> 
<h3>
<a id="_1"></a>基本术语</h3> 
<p>（会一直更新）<br> 数乘：矩阵对应元素相乘，乘出结果相加 ，得到的结果是一个数</p> 
<h3>
<a id="_5"></a>算法利弊</h3> 
<h4>
<a id="_7"></a>梯度下降算法</h4> 
<p>只能找到局部最优，不能找到全局最优<br> 但是神经网络中经常使用梯度下降法，因为神经网络中局部最优点很少</p> 
<p>（对每一个点求loss，求和后再更新w的值，因此可以并行计算）<br> <img src="https://images2.imgbox.com/65/14/XyV57rjS_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="_16"></a>随机梯度下降算法</h4> 
<p>对每一个点求loss后立刻更新w的值，不能并行计算，计算速度相对梯度下降来说比较慢。可以克服局部最优的缺陷</p> 
<h4>
<a id="_20"></a>鞍点</h4> 
<p>O点为鞍点，导数为0</p> 
<p><img src="https://images2.imgbox.com/f5/b0/0l425Hhm_o.png" alt="请添加图片描述"></p> 
<h3>
<a id="_27"></a>画图函数</h3> 
<pre><code class="prism language-python"><span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>w_list<span class="token punctuation">,</span>mse_list<span class="token punctuation">)</span>  <span class="token comment">#填入横纵坐标数据</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"loss"</span><span class="token punctuation">)</span>  <span class="token comment">#纵坐标名称</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"w"</span><span class="token punctuation">)</span>     <span class="token comment">#横坐标名称</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="PyTorch_37"></a>PyTorch入门</h3> 
<h4>
<a id="_39"></a>生成张量矩阵</h4> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>   <span class="token comment">#生成一个未初始化的5行3列矩阵</span>
torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token comment">#生成一个初始化的5行3列矩阵</span>
torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>   <span class="token comment">#生成一个全零的5行3列矩阵， 数据类型为long，也可设置为int</span>
x<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.5</span><span class="token punctuation">,</span><span class="token number">3.5</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment">#直接将数据封装为张量</span>
y<span class="token operator">=</span>torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>x<span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token comment">#复制张量x得到相同尺寸的新张量，但是数据随机初始化</span>
torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment"># 生成一个2*2  全为1的矩阵</span>
x<span class="token punctuation">.</span>new_ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span>  <span class="token comment">#生成一个5行3列全为1的矩阵</span>
x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">#或者使用x.shape()  得出张量的尺寸</span>
</code></pre> 
<h4>
<a id="________52"></a>加法 * - /类似</h4> 
<pre><code class="prism language-python">a<span class="token operator">+</span>b
torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>  out<span class="token operator">=</span>result<span class="token punctuation">)</span>  <span class="token comment">#将结果存到result中，并打印结果</span>
b<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>a<span class="token punctuation">)</span>   <span class="token comment"># a+b的结果直接赋给b ，并打印结果</span>
</code></pre> 
<h4>
<a id="_61"></a>求均值</h4> 
<pre><code class="prism language-python">x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="___67"></a>判断相等 只是比较数据</h4> 
<pre><code class="prism language-python"><span class="token comment"># 判断里面的每一个值是否相等</span>
x<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
<span class="token comment"># 判断所有是否相等</span>
x<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">all</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="_76"></a>切片</h4> 
<pre><code class="prism language-python">a<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span> <span class="token comment"># 行和列都可以进行切片</span>
</code></pre> 
<h4>
<a id="___82"></a>改变张量的形状 ,保证元素的总数量不变</h4> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">)</span>  
<span class="token comment"># 或者 torch.reshape()</span>
</code></pre> 
<h4>
<a id="torch__Tensor__NumPy__89"></a>torch 的 Tensor 与 NumPy 数组的相互转换</h4> 
<pre><code class="prism language-python">b<span class="token operator">=</span>a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 将Tensor 转换为Nympy数组  b和a共享内存</span>
b<span class="token operator">=</span>torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>a<span class="token punctuation">)</span>  <span class="token comment"># 将Nympy数组 转换为Nympy数组Tensor  b和a共享内存</span>
</code></pre> 
<h4>
<a id="Nympy_96"></a>Nympy操作</h4> 
<pre><code class="prism language-python">np<span class="token punctuation">.</span>add<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span>out<span class="token operator">=</span>a<span class="token punctuation">)</span>  <span class="token comment">#Nympy在自己基础上加一，与torch中 a.add_(1)相同</span>

！！ 所有在CPU上的Tensors<span class="token punctuation">,</span>除了CharTensor<span class="token punctuation">,</span>都可以转换为Numpy array并可以反向转换<span class="token punctuation">.</span>
</code></pre> 
<h4>
<a id="pytorch__a_104"></a>pytorch 对a进行转置</h4> 
<pre><code class="prism language-python"><span class="token comment"># 转置后 b和a仍然用同一个存储区</span>
b<span class="token operator">=</span>a<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="_111"></a>求实际存储的相距位置</h4> 
<pre><code class="prism language-python">a<span class="token punctuation">.</span>stride<span class="token punctuation">(</span><span class="token punctuation">)</span>    
<span class="token comment">#例子 </span>
<span class="token comment"># a=tensor([[0, 1, 2],</span>
<span class="token comment">#	    	[3, 4, 5],</span>
<span class="token comment">#	    	[6, 7, 8]])		</span>
<span class="token comment"># a.stride()  =(3,1)   </span>
<span class="token comment">#行数据相距3个，列数据相距1个  （在实际的存储结构中）</span>
</code></pre> 
<h4>
<a id="_123"></a>查看存储的地址</h4> 
<pre><code class="prism language-python">a<span class="token punctuation">.</span>data_ptr<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="storage_129"></a>查看storage的数据</h4> 
<pre><code class="prism language-python">a<span class="token punctuation">.</span>storage<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="_135"></a>转换成连续存储的</h4> 
<pre><code class="prism language-python"><span class="token comment"># 转置,并转换为符合连续性条件的tensor ,contiguous 会开辟一个新的存储空间</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>       
</code></pre> 
<h4>
<a id="view__reshape__142"></a>view 与 reshape 的区别</h4> 
<pre><code class="prism language-python"><span class="token comment"># reshape方法更强大，可以认为a.reshape = a.view() + a.contiguous().view()</span>
<span class="token comment">#满足tensor连续性条件时，a.reshape返回的结果与a.view()相同，否则返回的结果与a.contiguous().view()相同</span>

</code></pre> 
<h3>
<a id="PyTorch_____Autograd_152"></a>PyTorch Autograd自动求导</h3> 
<p><code>autograd</code>包为Tensors上的所有操作提供了自动求导机制</p> 
<pre><code class="prism language-python"><span class="token comment"># 注意 </span>
w<span class="token punctuation">.</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span>   <span class="token comment"># Tensor has to be set to True</span>


<span class="token comment"># 可以通过.detach()获得一个新的Tensor,拥有相同的内容但不需要自动求导.</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>  <span class="token comment"># true</span>
y<span class="token operator">=</span>x<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>  <span class="token comment"># false</span>


<span class="token comment"># 终止对计算机图的回溯</span>
<span class="token comment"># 方式一：</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 建议使用这种方式</span>
    <span class="token comment"># 操作</span>
<span class="token comment"># 方式二：</span>
	x<span class="token operator">=</span>x<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span> 

</code></pre> 
<h3>
<a id="_176"></a>求梯度例子</h3> 
<pre><code class="prism language-python"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>  y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>  z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>  out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>  out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>  out 
tensor<span class="token punctuation">(</span><span class="token number">27</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MeanBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>   <span class="token comment"># grad_fn 表示执行了哪些操作</span>

<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span>  x<span class="token punctuation">.</span>grad
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

<span class="token comment">## out对x求导</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 获得求导后的结果</span>
w<span class="token punctuation">.</span>grad

<span class="token comment">## 防止产生计算图</span>
w<span class="token punctuation">.</span>data 获得的还是Tensor，但是可以防止产生计算图
    <span class="token comment">#例如，权重更新要使用data，不能直接使用张量</span>
    w<span class="token punctuation">.</span>data<span class="token operator">=</span>w<span class="token punctuation">.</span>data<span class="token operator">-</span><span class="token number">0.01</span> <span class="token operator">*</span> w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data

<span class="token comment"># 获取Tensor的数值    </span>
w<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span> 获得的是数值 不是Tensor
    <span class="token comment"># 例如 a 是 tensor([-8.])</span>
        a<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment"># 得到 -8</span>

<span class="token comment"># 清空w的梯度值（把w的导数清零）</span>
w<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p><img src="https://images2.imgbox.com/bb/f3/R9LNV5Vv_o.jpg" alt="请添加图片描述"></p> 
<h4>
<a id="_214"></a>关于文本处理</h4> 
<p><img src="https://images2.imgbox.com/88/8c/Y5DpKfHW_o.png" alt="请添加图片描述"><img src="https://images2.imgbox.com/15/3f/64kePG4z_o.png" alt="请添加图片描述"></p> 
<p><img src="https://images2.imgbox.com/7e/96/nY5e2CGP_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="_221"></a>构建计算图</h4> 
<p><img src="https://images2.imgbox.com/8f/8e/RW6jolCP_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="_226"></a>构建模型的模板</h4> 
<p><img src="https://images2.imgbox.com/f4/73/Nb2tCaR8_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="pytroch_231"></a>pytroch中的各种优化器</h4> 
<p><img src="https://images2.imgbox.com/11/88/jCEG0N59_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="_235"></a>相应的模型函数</h4> 
<p><img src="https://images2.imgbox.com/69/de/jaXm4o8c_o.png" alt="请添加图片描述"></p> 
<h3>
<a id="_240"></a>各种函数模型</h3> 
<p>模型函数看上图 “相应的模型函数”</p> 
<h4>
<a id="1Linear_Unit_244"></a>1、Linear Unit</h4> 
<p>公式： y = x* w<sup>T</sup>+ b (w<sup>T</sup>表示w的转置)</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment">## 生成一个3行 2列的weight   ，weight的值是随机产生的位于 -1 ~ 1之间 </span>
</code></pre> 
<h4>
<a id="2Logistic_Regression_Unit_sigmoid_252"></a>2、Logistic Regression Unit (sigmoid)</h4> 
<p><img src="https://images2.imgbox.com/f3/6a/8xKs2LE5_o.png" alt="请添加图片描述"></p> 
<p><img src="https://images2.imgbox.com/77/31/SiFZe6u5_o.png" alt="请添加图片描述"></p> 
<pre><code class="prism language-python">y_pred<span class="token operator">=</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="3Binary_Cross_262"></a>3、Binary Cross</h4> 
<p><img src="https://images2.imgbox.com/63/a1/3fSYgU3u_o.png" alt="请添加图片描述"></p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>BCELoss<span class="token punctuation">(</span>x<span class="token punctuation">,</span>y<span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="4SGD_271"></a>4、SGD</h4> 
<p>Implements stochastic gradient descent (optionally with momentum).</p> 
<p>inorder to update w.data</p> 
<p><img src="https://images2.imgbox.com/3f/70/tFd1ICzq_o.png" alt="请添加图片描述"></p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="5Softmax_284"></a>5、Softmax</h4> 
<p><img src="https://images2.imgbox.com/b5/85/vIhg5oZm_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="6Softmax__288"></a>6、Softmax 计算公式</h4> 
<p><img src="https://images2.imgbox.com/ea/9e/St0GzFZd_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="7NLLLOSS_292"></a>7、NLLLOSS</h4> 
<pre><code class="prism language-python"><span class="token comment"># 结果只与输入的 数据有关</span>
<span class="token comment">## 将选取input_softmax中与 target对应的数据加负号 求和 取平均数</span>
output <span class="token operator">=</span> nn<span class="token punctuation">.</span>NLLLoss<span class="token punctuation">(</span>input_softmax<span class="token punctuation">,</span> target<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/13/74/WuDmEh2f_o.png" alt="请添加图片描述"></p> 
<p>where <em>x</em> is the input, y* is the target, w* is the weight, and <em>N</em> is the batch size. If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then</p> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-sJrGWtxa-1636725888431)(F:DesktopNLP编程学习编程imgimage-20211102191241343.png)]</p> 
<h4>
<a id="8CrossEntropyLoss_306"></a>8、CrossEntropyLoss</h4> 
<p><img src="https://images2.imgbox.com/26/d4/fyT86PKn_o.png" alt="请添加图片描述"></p> 
<p>where <em>x</em> is the input, <em>y</em> is the target, <em>w</em> is the weight, C* is the number of classes, and N* spans the minibatch dimension as well as d*1,…,<em>d**k</em> for the K-dimensional case. If <code>reduction</code> is not <code>'none'</code> (default <code>'mean'</code>), then</p> 
<h4>
<a id="9Relu_314"></a>9、Relu</h4> 
<p><strong>负数改为0，正数不变</strong></p> 
<p><img src="https://images2.imgbox.com/f5/00/lkffKBQS_o.png" alt="请添加图片描述"><br> <img src="https://images2.imgbox.com/36/99/6VNyOrMI_o.png" alt="请添加图片描述"></p> 
<ul>
<li>Input ：(∗), where *∗ means any number of dimensions.</li>
<li>Output ： (∗), same shape as the input.<br> <img src="https://images2.imgbox.com/56/32/xPAiw7eK_o.png" alt="请添加图片描述">
</li>
</ul> 
<h3>
<a id="_327"></a>模型构建的四部</h3> 
<ul>
<li> <p>Prepare dataset</p> </li>
<li> <p>Design model using Class</p> <p>inherit from nn.Module</p> </li>
<li> <p>Construct loss and optimizer</p> <p>using PyTorch API</p> </li>
<li> <p>Training cycle</p> <p>forward , backward , update</p> </li>
</ul> 
<h3>
<a id="CNN_343"></a>卷积神经网络CNN</h3> 
<h4>
<a id="Conv2d___345"></a>Conv2d 卷积层</h4> 
<pre><code class="prism language-python"> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>卷积核与input内数据直接点乘，相加求和</p> 
<p><img src="https://images2.imgbox.com/b4/08/TLiXxO2c_o.png" alt="请添加图片描述"></p> 
<pre><code class="prism language-python"><span class="token comment"># 输出测试结果，kernel_size=3 , （padding=0 填充 , strid=1 步长 ：都是默认值）</span>
<span class="token builtin">input</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
output<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
weight<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h4>
<a id="MaxPool2d___362"></a>MaxPool2d 池化层</h4> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>直接获得卷积核内的最大值，作为结果</p> 
<p><img src="https://images2.imgbox.com/c2/0a/zP1chShO_o.png" alt="请添加图片描述"></p> 
<h4>
<a id="1x1_373"></a>1x1的卷积</h4> 
<p>对应相乘、相加，不改变宽和高</p> 
<p><img src="https://images2.imgbox.com/be/6a/riqvegMW_o.png" alt="请添加图片描述"></p> 
<h3>
<a id="RNN_380"></a>循环神经网络RNN</h3> 
<p>RNNCell，只执行一次</p> 
<p><img src="https://images2.imgbox.com/2b/ec/rfseXvLA_o.png" alt="请添加图片描述"></p> 
<p>RNN会自动帮你做循环<br> <img src="https://images2.imgbox.com/f5/f5/7AQlZAuH_o.png" alt="请添加图片描述"></p> 
<p>RNN的输入输出形状</p> 
<p><img src="https://images2.imgbox.com/67/be/NxxNaIJn_o.png" alt="请添加图片描述"></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>