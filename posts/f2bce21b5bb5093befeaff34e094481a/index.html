<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Taskonomy 多任务学习 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Taskonomy 多任务学习</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p></p> 
<h1>TASKONOMY Dataset</h1> 
<p> taskonomy 官网<a href="http://taskonomy.stanford.edu/" title="Taskonomy">Taskonomy</a> </p> 
<p>     该数据集包括来自500多座建筑的450多万张来自室内场景的图像。涵盖了26个视觉领域的常见任务，包括但不限于：2D边缘检测、平面法线、拼图、自动上色、房间布局构建、物体分类、场景预测 。每一张图片有这26个任务的所有标签，数据集的总大小为11.16 TB。</p> 
<p>    对于非语义类标签，作者采用程序自动计算标签；对于语义类标签，作者采用知识提炼，用已有的模型产生相关语义标签。例如：使用resnet-151（Image-Net 中100 类） 产生 sementic objects任务的标签。最后经过人工检测，发现得到的语义标签错误率不超过7%。</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/cb/f3/hryAH49z_o.png"></p> 
<h1> Taskonomy tree：</h1> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/ee/a5/gg9m7aJS_o.jpg"></p> 
<h1>单张图像任务包括：</h1> 
<pre><code>Autoencoder             Curvatures          Colorization             Denoising-Autoencoder 
Depth                   Edge-2D             Edge-3D                  Euclidean-Distance 
Inpainting              Jigsaw-Puzzle       Keypoint-2D              Keypoint-3D 
Object-Classification   Reshading           Room-Layout              Scene-Classification 
Segmentation-2D         Segmentation-3D     Segmentation-Semantic    Surface-Normal       
Vanishing-Point</code></pre> 
<h1>多张图像任务包括：</h1> 
<pre><code>Pairwise-Nonfixated-Camera-Pose Pairwise-Fixated-Camera-Pose
Triplet-Fixated-Camera-Pose     Point-Matching</code></pre> 
<h1> Data structure：</h1> 
<pre><code>class_object/
    Object classification (Imagenet 1000) annotation distilled from ResNet-152
class_scene/
    Scene classification annotations distilled from PlaceNet
depth_euclidean/
    Euclidian distance images.
           Units of 1/512m with a max range of 128m.
depth_zbuffer/
   Z-buffer depth images.
       Units of 1/512m with a max range of 128m.
edge_occlusion/
    Occlusion (3D) edge images.
edge_texture/ 
    2D texture edge images.
keypoints2d/
    2D keypoint heatmaps.
keypoints3d/
    3D keypoint heatmaps.
nonfixated_matches/
    All (point', view') which have line-of-sight and a view of "point" within the camera frustum
normal/
    Surface normal images.
        127-centered
points/
    Metadata about each (point, view).
    For each image, we keep track of the optical center of the image.
    This is uniquely identified by the pair (point, view).
        Contains annotations for:
             Room layout
             Vanishing point
             Point matching
             Relative camera pose esimation (fixated)
             Egomotion
        And other low-dimensional geometry tasks. 
principal_curvature/
    Curvature images. 
        Principal curvatures are encoded in the first two channels.
        Zero curvature is encoded as the pixel value 127
reshading/
    Images of the mesh rendered with new lighting.
rgb/
    RGB images in 512x512 resolution.
rgb_large/
    RGB images in 1024x1024 resolution.
segment_semantic/
    The semantic segmentation classes are a subset of MS COCO dataset classes. The annotations are in the form of pixel-wise object labels and are distilled from [FCIS](https://arxiv.org/pdf/1611.07709.pdf), so they should be viewed as pseudo labels, as opposed to labels done individually by human annotators (a more accurate annotation set will be released in the near future). 
    The annotations have 18 unique labels, which include 16 object classes, a "background" class, and an "uncertain" class. "Background" means the FCIS classifiers were certain that those pixels belong to none of the 16 objects in the dictionary. "Uncertain" means the classifiers had too low confidence for those pixels to mark them as either an object or the background -- so they could belong to any class and they should be masked during learning to not contribute to the loss in a positive or negative way. 
    The classes "0" and "1" mark "uncertain" and "background" pixels, respectively. The rest of the classes are specified in [this file](https://github.com/StanfordVL/taskonomy/blob/master/taskbank/assets/web_assets/pseudosemantics/coco_selected_classes.txt). 
segment_unsup2d/
   Pixel-level unsupervised superpixel annotations based on RGB.
segment_unsup25d/
    Pixel-level unsupervised superpixel annotations based on RGB + Normals + Depth + Curvature.</code></pre> 
<h1>Decoder loss： </h1> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/9f/7e/4sXLggf6_o.png"></p> 
<h1>taskonomy 数据集下载：</h1> 
<p>（1）安装 omnidata</p> 
<pre><code>sudo apt-get install aria2
pip install omnidata-tools
</code></pre> 
<p>Taskonomy数据集是Omnidata starter数据集（Omnidata:14M图像，包括室内、室外和聚焦对象的场景）的子集。用下面命令只下载taskonomy子数据集：</p> 
<pre><code>omnitools.download all --components taskonomy --subset fullplus 
  --dest ./omnidata_starter_dataset/ 
  --connections_total 40 --agree
</code></pre> 
<p>taskonomy 数据集提供标准的训练/验证/测试拆分，以标准化未来的基准测试。考虑到完整数据集的大容量，提供了4个等级（tiny、medium、full、fullplus）标准的数据集拆分，大小越来越大，用户可以根据其存储和计算资源使用这些子数据集。Full+包含Full，Full包含Medium，Medium包含Tiny。下表显示了每个分区中的建筑数量。</p> 
<p>下载链接<a href="https://github.com/StanfordVL/taskonomy/raw/master/data/assets/splits_taskonomy.zip" title="https://github.com/StanfordVL/taskonomy/raw/master/data/assets/splits_taskonomy.zip">https://github.com/StanfordVL/taskonomy/raw/master/data/assets/splits_taskonomy.zip</a></p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/ac/64/Lyj25b4h_o.png"></p> 
<h1>下载taskonomy Tiny数据集</h1> 
<pre><code>omnitools.download all --components taskonomy --subset tiny 
  --dest ./omnidata_starter_dataset/ 
  --connections_total 40 --agree</code></pre> 
<h1>Taskonomy任务详解 </h1> 
<p><a href="https://github.com/StanfordVL/taskonomy/blob/master/taskbank/assets/web_assets/task_definitions.pdf" title="taskonomy/task_definitions.pdf at master · StanfordVL/taskonomy · GitHub">taskonomy/task_definitions.pdf at master · StanfordVL/taskonomy · GitHub</a></p> 
<p>1. Autoencoding 自动编码</p> 
<p>PCA是一种广泛使用的方法，通过查找一个低维度的潜在表示来理解数据。自动编码是PCA的一种非线性概括，它最初是在迁移学习中提出的：通过自动编码预先训练来提高下游性能。</p> 
<p>2. Colorization着色</p> 
<p>着色需要拍摄灰度图像并预测原始颜色。 这是一项无人监督的任务，但也是一种语义感知的任务。 例如，一旦确定了水果，预测水果的颜色就很简单。</p> 
<p>3. Context Encoding 上下文编码</p> 
<p>背景编码首先由Pathak等人介绍[26]并且是自动编码的一种形式，其中大部分输入被掩盖在模型中。 为了填充遮挡区域，模型必须推断场景几何和语义。 与着色类似，它是一种无监督但仍然是语义的任务。</p> 
<p>4. Content Prediction   内容预测</p> 
<p>Context Encoding的一个有辨别力的版本，Jigsaw [25]要求网络解密输入图像的置换平铺（permuted tiling）。</p> 
<p>5. Curvature Estimation 曲率估计</p> 
<p>基于曲率的特征非常适合识别，因为它们在刚性变换下是不变的。 曲率在视觉处理中非常重要 - 因此猕猴（Macaque）视觉皮层具有专用的曲率处理区域[41]。</p> 
<p>6. Denoising 降噪</p> 
<p>对于类似的输入进行去燥来获得相似的representations，但是通过自动编码学习的representations对输入中的扰动过于敏感。降噪【36】（自动编码）通过将微扰动输入映射到未扰动输入，来鼓励有限的不变性。</p> 
<p>7. Depth Estimation, Euclidean 欧几里得深度估计</p> 
<p>深度估计是一项重要任务，可用于检测与障碍物和感兴趣物品的接近程度。 It is also a useful intermediate step for agents to localize themselves in 3D space（在3D空间定位物体的有用一步）。 欧几里得深度是指从每个像素到相机光学中心的距离。</p> 
<p>8. Depth Estimation, Z-Buffer</p> 
<p>与欧几里德深度估计相反，研究人员通常使用Z-Buffer深度，其被定义为到相机平面的距离。 这不是人类通常感知深度的方式，应用它是因为这是标准公式，我们所有深度派生的任务都是从Z-Buffer派生的。</p> 
<p>9. Edge Detection（2D）边缘检测</p> 
<p>边缘检测在历史上是计算机视觉中的基本任务。 边缘通常用作中间representations或作为较大处理管道（a larger processing pipeline）中的特征。 我们包括没有非极大值抑制的Canny边缘检测器的输出（以使任务可以通过神经网络学习）。</p> 
<p>10. Edge Detection（3D）</p> 
<p>与2D边缘相反，我们将3D边缘定义为“遮挡边缘”，或者前景中的对象遮挡其后面的东西的边缘。 2D边缘响应纹理的变化，但3D边缘是仅依赖于3D几何体和对颜色、光照不变的特征。</p> 
<p>11. Keypoint Detection (2D)关键点检测</p> 
<p>关键点检测在计算机视觉中具有悠久的历史，并且对许多任务都很有用。 关键点算法通常由两部分组成，包括关键点检测器和一些局部补丁描述符，它们在多个图像中是不变的[20,3,28]。 2D关键点检测鼓励网络识别图像的本地重要区域，并且点匹配鼓励网络学习特征描述符。 在更大的视觉管道中识别关键点通常仍是第一步。 我们使用SURF [3]的输出（在非最大抑制之前）作为我们的ground-truth。</p> 
<p>12.  Keypoint Detection (3D)</p> 
<p>3D关键点类似于2D关键点，除了它们是从3D数据派生的，因此考虑了场景几何。它们通常对纹理等信息(but possibly distracting)没反应[44, 45, 21, 42, 14]。 我们使用NARF的输出[35]算法（在非最大抑制之前）作为我们的3D关键点ground-truth。</p> 
<p>13. Point Matching</p> 
<p>为点匹配训练的深度网络学习特征描述符，证明对downstream tasks有用。 点匹配应用于细粒度分类和物体识别三维重建和运动结构，宽基线匹配，SLAM 和视觉测距。</p> 
<p>14. Relative Camera Pose Estimation, Non-Fixated 相对相机姿态估计，非固定</p> 
<p>Held和Hein 著名的“小猫旋转木马（Kitten Carousel）”实验表明，采取行动对强烈的感知至关重要。  对于具有相同光学中心的两个不同视图，我们尝试预测它们之间的6-DOF相对相机姿态（偏航，俯仰，滚动，xyz平移）。</p> 
<p>15. Relative Camera Pose Estimation, Triplets (Egomotion)</p> 
<p>视频是计算机视觉中常见的研究对象，它们提供具有高冗余度的密集数据。 因此，我们包括具有固定中心点的输入三元组的相机姿态匹配。 通过三个图像，模型具有更高的匹配点的能力，以实现精确定位。</p> 
<p>16. Reshading<br> 推断场景几何的一种方法是使用固有图像分解I = A·S的“阴影形状”，其中S是通过光照和深度参数化的阴影函数。 这种分解被认为在人类视觉感知中很有用。 按如下方式定义Reshading：给定RGB图像，标签是the shading function S，其由在相机原点处具有单个点光而得到，并且S乘以恒定的固定反照率。</p> 
<p>17. Room Layout Estimation 房间布局估算</p> 
<p>估计和对齐3D边界框是一个中级任务，包括消失点估计这个子问题，并且具有机器人导航，场景重建[和增强现实的应用。 在LSUN房间布局挑战中使用了房间布局估计的变体，但是当存在相机滚动或者没有房间角落时，该formulation是不适合的。 相反，taskonomy提供了一个无论相机的姿势和视野如何都保持well-deﬁned的formulation。 该任务包括一些语义信息，例如“什么构成房间”，同时还包括场景几何。</p> 
<p>18. Segmentation, Unsupervised (2D)) 分割，无监督</p> 
<p>格式塔（Gestalt）心理学家提出了将分组作为一种机制的原则，通过这种机制，人类学会将世界视为一组连贯的对象[38]。 规范化切割[33]是将图像分割成感知相似组的一种方法，我们在字典中包含这个格式塔任务。</p> 
<p>19. Segmentation, Unsupervised (2.5D))</p> 
<p>Segmentation2.5D使用与2D相同的算法，但是标签是从RGB图像，对齐的深度图像和对齐的表面法线图像联合计算的。 因此，2.5D分割不仅适用于the world as it seems（在RGB图像中），而且适用于the world as it is（ground-truth 3D）。 2.5D分割包含关于场景几何的信息，该场景几何不直接存在于RGB图像中，但是人类容易推断。</p> 
<p>20. Surface Normal Estimation 表面法线估计</p> 
<p>表面法线估计被认为对空间认知至关重要。 例如，对象只能放置在具有向上法线的表面上。 即使对于运动，具有水平面法线的点表示它不容易穿过。 曲面法线直接从3D网格计算。</p> 
<p>21. Vanishing Point Estimation 消失点估计</p> 
<p>透视（perspective）的结果是，消失点提供了关于场景几何的有用信息并且得到了很好的研究。 消失点证明在曼哈顿世界特别有用，其中有三个主要的消失点对应于X，Y和Z轴。 这种假设通常在城市环境中得到满足。 对于每个模型，我们分析地找到这三个消失点并将它们作为标签包含在内。</p> 
<p>22. Semantic Learning through Knowledge distillation 通过知识蒸馏进行语义学习<br> 虽然taskonomy数据集不包含语义注释，但语义理解是现代计算机视觉的一个重要组成部分。 因此，taskonomy通过知识蒸馏添加伪语义注释。 从ImageNet 和MS-COCO 训练的最先进模型中提取知识，通过使用它们来注释taskonomy数据集，然后使用这些注释监督模型。<br>  </p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>