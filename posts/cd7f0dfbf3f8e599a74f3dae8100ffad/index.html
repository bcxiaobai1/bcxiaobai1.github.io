<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>深度学习面试问题与答案（2023） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">深度学习面试问题与答案（2023）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="1_0"></a>1）什么是深度学习？</h1> 
<blockquote> 
 <p>如果你正在参加深度学习面试，那么你肯定知道深度学习到底是什么。然而，这个问题的面试官希望你能够给出一个详细的答案，并且附上一个例子。深度学习涉及到处理大量的结构化或非结构化数据，并使用复杂的算法来训练神经网络。它执行复杂的操作来提取隐藏的模式和特征（例如，区分猫和狗的图像）。<br><br> 举个例子，深度学习可以用于计算机视觉中的图像分类任务。假设我们有一组图像，其中既包含猫的图片，也包含狗的图片。我们希望训练一个神经网络来识别这些图片中的猫和狗。首先，我们需要将这些图像转换成数字数据，并将它们输入到神经网络中进行训练。神经网络通过多次反复训练来提高自己的准确性，并通过不断地微调参数来提取图像中的特征，最终学会区分猫和狗的图像。这个过程中，深度学习算法可以使用卷积神经网络（Convolutional Neural Network）等技术来提高分类准确率。</p> 
</blockquote> 
<h1>
<a id="2_4"></a>2）人工智能、机器学习和深度学习之间的主要区别是什么？</h1> 
<blockquote> 
 <p>AI代表人工智能，它是一种使机器能够模仿人类行为的技术。<br><br> 机器学习是人工智能的一个子集，它使用统计方法使机器能够通过经验进行改进。<br><br> 深度学习是机器学习的一部分，它使得多层神经网络的计算成为可能。它利用神经网络来模拟类似于人类决策的过程。</p> 
</blockquote> 
<h1>
<a id="3_9"></a>3）区分有监督和无监督的学习。</h1> 
<blockquote> 
 <p>监督学习是一个同时提供输入和期望输出数据的系统。 输入和输出数据被标记，为以后的数据处理提供学习基础。<br> 无监督过程不需要显式标注信息，无需标注信息即可进行操作。 常见的无监督学习方法是聚类分析。 它用于探索性数据分析以发现隐藏的模式或数据分组。</p> 
</blockquote> 
<h1>
<a id="4_13"></a>4）深度学习有哪些应用？</h1> 
<blockquote> 
 <p>深度学习有多种应用：<br><br> 计算机视觉<br> 自然语言处理与模式识别<br> 图像识别与处理<br> 机器翻译<br> 情绪分析<br> 问答系统<br> 对象分类和检测<br> 自动手写生成<br> 自动文本生成。</p> 
</blockquote> 
<h1>
<a id="5_25"></a>5）你认为深度网络比浅层网络好吗？</h1> 
<blockquote> 
 <p>浅层和深层网络都足够好，并能够近似任何函数。但是对于相同的精度水平，深层网络在计算和参数数量方面可以更加高效。深层网络可以创建深层表示。在每一层，网络都学习输入的新的、更抽象的表示。</p> 
</blockquote> 
<h1>
<a id="6_28"></a>6）“过拟合”是什么意思？</h1> 
<blockquote> 
 <p>过拟合是深度学习中最常见的问题。它通常发生在深度学习算法过度拟合于特定的数据模式时。当特定算法非常适合数据并且表现出高方差和低偏差时，过拟合也会出现。</p> 
</blockquote> 
<h1>
<a id="7_31"></a>7）什么是反向传播？</h1> 
<blockquote> 
 <p><font color="#0000FF"><strong>这是深度学习面试中最常被问到的问题之一。反向传播是一种改善网络性能的技术。它反向传播误差并更新权重以减小误差。</strong></font><br><br> 反向传播是一种用于多层神经网络的训练算法。它将误差信息从网络末端传递到网络内所有的权重上，使得梯度的计算更加高效。<br> 反向传播可以分为以下几个步骤：<br><br> 前向传播：将训练数据通过网络进行正向传递以生成输出。<br> 使用目标值和输出值计算关于输出激活的误差导数。<br> 反向传播：计算关于前一层输出激活的误差导数，并持续向后传递至所有隐藏层。<br> 利用先前计算出的关于输出和所有隐藏层的导数来计算关于权重的误差导数。<br> 更新权重。</p> 
</blockquote> 
<h1>
<a id="8_41"></a>8）傅立叶变换在深度学习中的作用是什么？</h1> 
<blockquote> 
 <p>傅里叶变换软件包非常高效，可用于分析、维护和管理大型数据库。该软件具有高质量的特殊描绘功能。人们可以有效地利用它生成实时的数组数据，这对于处理所有类型的信号非常有帮助。</p> 
</blockquote> 
<h1>
<a id="9_44"></a>9）什么是深度学习框架或工具？</h1> 
<blockquote> 
 <p>深度学习框架或工具包括：<br><br> TensorFlow、Keras、Chainer、PyTorch、Theano &amp; Ecosystem、Caffe2、CNTK、DyNet、Gensim、DSSTNE、Gluon、Paddle、MxNet、BigDL。</p> 
</blockquote> 
<h1>
<a id="10_48"></a>10）深度学习的缺点是什么？</h1> 
<blockquote> 
 <p>以下是深度学习的一些缺点：<br><br> 深度学习模型需要更长的时间来执行模型。 在某些情况下，根据复杂性，执行单个模型甚至需要几天时间。<br> 深度学习模型对于小数据集效果不佳，深度学习中的网络需要大量数据才能很好地训练。<br> 深度学习概念有时实施起来可能很复杂，在许多情况下，很难实现高模型效率。</p> 
</blockquote> 
<h1>
<a id="11_54"></a>11）神经网络中“权重初始化”的含义是什么？</h1> 
<blockquote> 
 <p>在神经网络中，权重初始化是一个非常重要的因素。不良的权重初始化会阻止网络学习，而良好的权重初始化可以加速收敛并提高整体的误差表现。偏置可以初始化为零。设置权重的标准规则是使它们接近于零，但不要过小。</p> 
</blockquote> 
<h1>
<a id="12_Data_Normalization_57"></a>12）解释 Data Normalization。</h1> 
<blockquote> 
 <p>数据标准化（归一化）是一个重要的预处理步骤，用于重新调整数值范围以适应特定的范围。它可以确保在反向传播期间实现更好的收敛性。一般来说，数据归一化的核心是对每个数据点减去均值，然后除以其标准差。<br><br> 标准化和重构数据的过程称为“数据归一化”。它是一种预处理步骤，用于消除数据冗余。通常，数据以不同的格式出现，但包含相同的信息。在这些情况下，应该重新调整值，使其适合特定的范围，从而实现更好的收敛。</p> 
</blockquote> 
<h1>
<a id="13_61"></a>13）为什么零初始化不是一种好的权重初始化过程？</h1> 
<blockquote> 
 <p>如果网络中的权重集合被设置为零，则每个层中的所有神经元都将开始产生相同的输出和相同的反向传播梯度。因此，网络不能学习，因为神经元之间没有任何不对称性的来源。这就是为什么我们需要在权重初始化过程中添加随机性的原因。</p> 
</blockquote> 
<h1>
<a id="14_64"></a>14）深度学习中有哪些监督学习和无监督学习算法？</h1> 
<blockquote> 
 <p>监督学习：Artificial neural network、Convolution neural network、Recurrent neural network<br><br> 无监督学习：Self Organizing Maps、Deep belief networks (Boltzmann Machine)、Auto Encoders</p> 
</blockquote> 
<h1>
<a id="15_68"></a>15）激活函数有什么用？</h1> 
<blockquote> 
 <p>激活函数用于将非线性引入神经网络，使其能够学习更复杂的函数。如果没有激活函数，神经网络只能学习其输入数据的线性组合函数。<br><br> 激活函数将输入转换为输出。激活函数负责决定神经元是否应该被激活。它通过计算加权和并进一步添加偏差来做出决策。激活函数的基本目的是将非线性引入神经元的输出。</p> 
</blockquote> 
<h1>
<a id="16_72"></a>16）有多少种类型的激活函数可用？</h1> 
<blockquote> 
 <p>Binary Step<br> Sigmoid<br> Tanh<br> ReLU<br> Leaky ReLU<br> Softmax<br> Swish</p> 
</blockquote> 
<div align="center"> 
 <img src="https://images2.imgbox.com/a0/e3/l1P4iXTm_o.png" width="100%"> 
</div> 
<h1>
<a id="17_softmax__84"></a>17）什么是 softmax 函数？</h1> 
<blockquote> 
 <p>softmax 函数用于计算 n 个不同事件的概率分布。使用 softmax 的主要优点之一是输出概率的范围。范围将在0到1之间，所有概率之和将等于一。当 softmax 函数用于多分类模型时，它返回每个类的概率，目标类将具有较高的概率。</p> 
</blockquote> 
<h1>
<a id="18_87"></a>18）最常用的激活函数是什么？</h1> 
<blockquote> 
 <p>Relu 函数是最常用的激活函数。 它帮助我们解决梯度消失问题，它具有简单的数学表达式和快速的计算速度。</p> 
</blockquote> 
<h1>
<a id="19_Relu__90"></a>19）输出层可以用 Relu 函数吗？</h1> 
<blockquote> 
 <p>可以使用 ReLU 函数作为输出层的激活函数，但这通常仅适用于特定类型的问题，例如回归问题。在分类问题中，通常使用 Softmax 函数作为输出层的激活函数，因为它可以产生归一化的概率分布。在使用 ReLU 函数作为输出层激活函数时，通常需要在输出层添加一个线性层来映射激活函数的输出到所需的输出空间。</p> 
</blockquote> 
<h1>
<a id="20_AutoEncoder__93"></a>20）你对 AutoEncoder 的理解是什么？</h1> 
<blockquote> 
 <p>自编码器是一种人工神经网络。它可以在没有任何监督的情况下学习一组数据的表示。网络通过将输入复制到输出来自动学习；通常，内部表示的维度比输入向量小。因此，它们可以学习表示数据的有效方法。自编码器由两个部分组成：编码器试图将输入拟合到内部表示中，解码器将内部状态转换为输出。</p> 
</blockquote> 
<div align="center"> 
 <img src="https://images2.imgbox.com/c4/0e/g8yIB4Bd_o.png" width="80%"> 
</div> 
<h1>
<a id="21Dropout__99"></a>21）Dropout 是什么意思？</h1> 
<blockquote> 
 <p>Dropout 是一种低成本的正则化技术，用于减少神经网络中的过拟合。我们在每个训练步骤中随机丢弃一组节点。因此，我们为每个训练案例创建一个不同的模型，并且所有这些模型共享权重。这是一种模型平均的形式。</p> 
</blockquote> 
<h1>
<a id="22_102"></a>22）你对张量的理解是什么？</h1> 
<blockquote> 
 <p>张量（Tensor）就是在深度学习中表示数据的一种标准。它们是多维数组，允许我们表示具有更高维度的数据。一般来说，我们处理高维数据集，其中各个维度指的是数据集中不同的特征。</p> 
</blockquote> 
<h1>
<a id="23_105"></a>23）你对玻尔兹曼机的理解是什么？</h1> 
<blockquote> 
 <p>Boltzmann机（也称具有隐藏单元的随机 Hopfield 网络）是一种循环神经网络。在 Boltzmann 机中，节点带有一些偏差进行二元决策。Boltzmann 机可以串联在一起，创建更复杂的系统，如深度置信网络。Boltzmann 机可以用于优化问题的解决方案。关于玻尔兹曼机的一些要点：<br><br> 它使用循环结构。<br> 它由随机神经元组成，其中包括两种可能状态之一，即1或0。<br> 存在于其中的神经元处于自适应状态（自由状态）或固定状态（冻结状态）。<br> 如果我们应用模拟退火或离散 Hopfield 网络，则它将成为 Boltzmann Machine。</p> 
</blockquote> 
<h1>
<a id="24What_is_the_cost_function_112"></a>24）What is the cost function?</h1> 
<blockquote> 
 <p>代价函数描述了神经网络在给定训练样本和期望输出方面的表现好坏，它可能依赖于变量，例如权重和偏差。代价函数提供了神经网络的整体性能。在深度学习中，我们的优先任务是最小化代价函数，这就是我们喜欢使用梯度下降法的原因。</p> 
</blockquote> 
<h1>
<a id="25Explain_gradient_descent_115"></a>25）Explain gradient descent?</h1> 
<blockquote> 
 <p>梯度下降是一种优化算法，它通过沿着负梯度指定的最陡下降方向重复移动来最小化某个函数。它是一个迭代算法，每次迭代中，我们计算代价函数相对于每个参数的梯度，并通过以下公式更新函数的参数：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
      
       
        
         
          
           Θ
          
          
           
            n
           
           
            e
           
           
            w
           
          
         
         
          =
         
         
          Θ
         
         
          −
         
         
          α
         
         
          
           ∂
          
          
           
            ∂
           
           
            Θ
           
          
         
         
          J
         
         
          
           (
          
          
           Θ
          
          
           )
          
         
        
        
          varTheta _{new}=varTheta -alpha frac{partial}{partial varTheta}Jleft( varTheta right) 
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathit">Θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right: 0.0269em">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.7667em;vertical-align: -0.0833em"></span><span class="mord mathit">Θ</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 2.0574em;vertical-align: -0.686em"></span><span class="mord mathnormal" style="margin-right: 0.0037em">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3714em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord" style="margin-right: 0.0556em">∂</span><span class="mord mathit">Θ</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord" style="margin-right: 0.0556em">∂</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right: 0.0962em">J</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathit">Θ</span><span class="mclose delimcenter">)</span></span></span></span></span></span></span><br> 其中，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         Θ
        
       
       
        varTheta
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathit">Θ</span></span></span></span></span> 是参数向量，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         α
        
       
       
        alpha
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal" style="margin-right: 0.0037em">α</span></span></span></span></span> 是学习率，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         J
        
        
         
          (
         
         
          Θ
         
         
          )
         
        
       
       
        Jleft( varTheta right)
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0962em">J</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord mathit">Θ</span><span class="mclose delimcenter">)</span></span></span></span></span></span> 是代价函数。<br> 在机器学习中，梯度下降用于更新我们模型的参数。 这些参数代表线性回归中的系数和神经网络中的权重。</p> 
</blockquote> 
<h1>
<a id="26_123"></a>26）解释梯度下降的以下变体：随机、批量和小批量？</h1> 
<blockquote> 
 <p>随机梯度下降法：随机梯度下降法使用单个训练样本计算梯度并更新参数。<br> 批量梯度下降法：批量梯度下降法用于计算整个数据集的梯度，并在每次迭代中执行一次更新。<br> 小批量梯度下降法：小批量梯度下降法是随机梯度下降法的一种变体。它使用小批量的样本进行训练。小批量梯度下降法是最流行的优化算法之一。</p> 
</blockquote> 
<h1>
<a id="27_128"></a>27）小批量梯度下降的主要好处是什么？</h1> 
<blockquote> 
 <p>相比随机梯度下降法，它在计算效率上更高。<br> 它通过找到平坦的极小值来提高泛化能力。<br> 它通过使用小批量来提高收敛性。我们可以近似计算整个训练集的梯度，这可能有助于避免局部极小值。</p> 
</blockquote> 
<h1>
<a id="28_133"></a>28）你对卷积神经网络的理解是什么？</h1> 
<blockquote> 
 <p>卷积神经网络通常称为 CNN，是一种前馈神经网络。它在至少一层中使用卷积。卷积层包含一组滤波器（卷积核）。该滤波器在整个输入图像上滑动，计算滤波器权重和输入图像之间的点积。在训练的过程中，网络会自动学习可以检测特定特征的滤波器。</p> 
</blockquote> 
<h1>
<a id="29_136"></a>29）什么是循环神经网络？</h1> 
<blockquote> 
 <p>RNN 代表循环神经网络（Recurrent Neural Networks），它们是专门设计用于识别数据序列中的模式，例如手写、文本、口语、基因组和数字时间序列数据的人工神经网络。RNN 使用反向传播算法进行训练，因为它们具有内部存储器。RNN可以记住有关其接收到的输入的重要信息，从而使它们能够非常精确地预测接下来会发生什么。</p> 
</blockquote> 
<h1>
<a id="30_139"></a>30）在循环网络中训练时面临哪些问题？</h1> 
<blockquote> 
 <p>循环神经网络使用反向传播算法进行训练，但是该算法会在每个时间戳上应用。这通常被称为“透过时间的反向传播”（Back-propagation Through Time，BTT）。反向传播存在两个重要问题：<br><br> <font color="#0000FF"><strong>梯度消失：</strong></font>当我们执行反向传播时，梯度往往会变得越来越小，因为我们在网络中不断向后移动。因此，如果将较早层的神经元与较晚层的神经元进行比较，那么较早层的神经元学习得非常缓慢。较早层是更有价值的，因为它们负责学习和检测简单的模式，是网络的基本构建块。如果它们提供不正确或不准确的结果，那么我们如何指望后续层和整个网络表现良好并提供准确的结果。训练过程需要很长时间，而模型的预测准确性会降低。<br><br> <font color="#0000FF"><strong>梯度爆炸：</strong></font>当大量的误差梯度积累时，梯度爆炸是主要问题。它会在训练期间对神经网络模型权重进行非常大的更新。当更新小而受控制时，梯度下降过程最有效。当梯度的幅度累积时，可能会出现不稳定的网络。这可能导致结果预测不良，甚至是报告无用信息的模型。</p> 
</blockquote> 
<h1>
<a id="31_LSTM__144"></a>31）解释 LSTM 的重要性。</h1> 
<blockquote> 
 <p>LSTM 是长短时记忆网络的缩写，是一种用于深度学习的人工循环神经网络结构。LSTM 具有反馈连接，使其成为一台“通用计算机”。它不仅能处理单个数据点，还能处理整个数据序列。<br> LSTM 是一种特殊类型的 RNN，能够学习长期依赖关系。它使用一个记忆单元和一组门来控制信息的流入和流出。门由 sigmoid 神经网络层和点乘操作组成，使网络能够有选择地忘记或记住每个时间步的信息。这使得 LSTM 能够长时间保留信息，避免传统 RNN 中常见的梯度消失问题。<br> LSTM 广泛用于语音识别、语言翻译和图像字幕等任务，其中长期依赖关系对于实现高精度至关重要。</p> 
</blockquote> 
<h1>
<a id="32_149"></a>32）你对感知器的理解是什么？</h1> 
<blockquote> 
 <p>感知机是一种神经网络单元（人工神经元），可以进行某些计算以检测特征。它是用于二元分类器的监督学习算法。此算法用于使神经元逐个学习和处理训练集中的元素。有两种类型的感知机：<br><br> 单层感知机：单层感知机只能学习线性可分模式。<br> 多层感知机：具有两个或更多层的多层感知机或前馈神经网络具有更高的处理能力。</p> 
</blockquote> 
<div align="center"> 
 <img src="https://images2.imgbox.com/6a/a7/LkIOAplb_o.png" width="100%"> 
</div> 
<h1>
<a id="33_MLP_157"></a>33）什么是多层感知器 (MLP)？</h1> 
<blockquote> 
 <p>与神经网络类似，MLP 具有输入层、隐藏层和输出层。它具有与单层感知器相同的结构，但具有一个或多个隐藏层。单层感知器只能对具有二元输出（0,1）的线性可分类进行分类，但 MLP 可以对非线性类进行分类。<br><br> 除了输入层之外，其他层中的每个节点都使用非线性激活函数。这意味着输入层、传入的数据和激活函数都基于所有节点和权重相加产生输出。MLP 使用一种称为“反向传播”的监督学习方法。在反向传播中，神经网络利用代价函数计算误差。它将这个误差向后传播到它来自的地方（调整权重以更准确地训练模型）。</p> 
</blockquote> 
<h1>
<a id="34What_is_the_Boltzmann_Machine_161"></a>34）What is the Boltzmann Machine?</h1> 
<blockquote> 
 <p>最基本的深度学习模型之一是 Boltzmann 机，类似于多层感知器的简化版本。该模型具有一个可见的输入层和一个隐藏层，是一个只有两层的神经网络，会随机地决定神经元应该是开启还是关闭。节点在不同层之间相互连接，但同一层的两个节点不会相互连接。</p> 
</blockquote> 
<h1>
<a id="35_164"></a>35）前馈神经网络和循环神经网络有什么区别？</h1> 
<blockquote> 
 <p>在这个深度学习面试问题中，面试官希望你给出一个详细的答案。<br> 在前馈神经网络中，信号从输入到输出沿着一个方向传递。没有反馈回路；网络只考虑当前输入。它不能记忆先前的输入（例如 CNN）。<br> 在循环神经网络中，信号向两个方向传递，形成一个循环网络。它在生成一个层的输出时考虑了当前输入和先前接收到的输入，并且由于其内部记忆能力，可以记忆过去的数据。</p> 
</blockquote> 
<h1>
<a id="36_Softmax__ReLU__169"></a>36）什么是 Softmax 和 ReLU 函数？</h1> 
<blockquote> 
 <p>Softmax 是一种激活函数，它在零到一之间生成输出。它分割每个输出，使得所有输出之和等于一。Softmax 经常用于输出层。<br> ReLU 是最广泛使用的激活函数。如果 X 是正数，它会给出一个 X 的输出，否则为零。ReLU 经常用于隐藏层。</p> 
</blockquote> 
<h1>
<a id="37_173"></a>37）什么是超参数？</h1> 
<blockquote> 
 <p>这是另一个常见的深度学习面试问题。 使用神经网络，一旦数据格式正确，您通常会使用超参数。 超参数是在学习过程开始之前设置其值的参数。 它决定了网络的训练方式和网络的结构（如隐藏单元的数量、学习率、epochs等）。</p> 
</blockquote> 
<h1>
<a id="38_176"></a>38）如果学习率设置得太低或太高会发生什么？</h1> 
<blockquote> 
 <p>当学习率过低时，模型的训练进展会非常缓慢，因为我们对权重进行的更新非常少。在到达最小点之前，需要进行多次更新。<br><br> 如果学习率设置得太高，这会导致损失函数出现不良的发散行为，因为权重进行了激烈的更新。它可能无法收敛（模型无法给出良好的输出），甚至可能出现发散现象（数据对于网络来说过于混乱而无法训练）。</p> 
</blockquote> 
<h1>
<a id="39_Dropout__Batch_Normalization_180"></a>39）什么是 Dropout 和 Batch Normalization？</h1> 
<blockquote> 
 <p>Dropout 是一种随机删除网络的隐藏和可见节点的技术，以防止过度拟合数据（通常删除20％的节点）。这将使需要收敛网络的迭代次数增加一倍。如果它的值设置太高，那么模型可能会学习不足，从而导致模型效率降低。<br><br> 批量归一化是一种通过归一化每一层的输入来提高神经网络性能和稳定性的技术，使它们的平均输出激活为零，标准差为 1。</p> 
</blockquote> 
<h1>
<a id="40_184"></a>40）批量梯度下降和随机梯度下降有什么区别？</h1> 
<blockquote> 
 <p>批量梯度使用整个数据集计算梯度。收敛需要时间，因为数据量很大，权重更新慢。<br> 随机梯度使用单个样本计算梯度。它比批量梯度收敛得更快，因为它更频繁地更新权重。</p> 
</blockquote> 
<h1>
<a id="41_188"></a>41）什么是过拟合和欠拟合，以及如何对抗它们？</h1> 
<blockquote> 
 <p>过拟合是指模型过分关注训练数据中的细节和噪音，以至于会对模型在新信息上的执行产生不利影响。非线性模型更容易出现过拟合，因为它们在学习目标函数时具有更大的灵活性。例如，如果模型正在查看汽车和卡车，但仅能识别具有特定盒状形状的卡车，则可能无法注意到平板卡车，因为它只在训练中看到了特定类型的卡车。该模型在训练数据上表现良好，但在现实世界中却不行。<br><br> 欠拟合指的是一个既没有很好地训练数据，也不能推广到新信息的模型。这通常发生在训练模型的数据量不足或存在错误时。欠拟合既性能差，也准确率低。<br><br> 为了解决过拟合和欠拟合问题，可以重新采样数据以估计模型准确性（k折交叉验证），并使用验证数据集来评估模型。</p> 
</blockquote> 
<h1>
<a id="42CNN__193"></a>42）CNN 有哪些不同的层？</h1> 
<blockquote> 
 <p>在卷积神经网络（CNN）中，通常包含以下四个层：<br><br> 卷积层：执行卷积操作的层，创建多个较小的图像窗口来扫描数据。<br> ReLU层：为网络带来非线性，并将所有负像素转换为零。输出是一个经过修正的特征映射。<br> 池化层：池化是一种下采样操作，可减少特征映射的维度。<br> 全连接层：该层识别和分类图像中的对象。</p> 
</blockquote> 
<h1>
<a id="43_EpochBatch__Iteration__200"></a>43）深度学习中的 Epoch、Batch 和 Iteration 有什么区别？</h1> 
<blockquote> 
 <p>Epoch 代表对整个数据集的一次迭代（放入训练模型的所有内容）。<br> Batch 指的是当我们不能一次将整个数据集传递给神经网络时，我们将数据集分成几个批次。<br> 如果我们有 10,000 张图像作为数据，批量大小为 200。那么一个 epoch 应该运行 50 次 Iteration。</p> 
</blockquote> 
<h1>
<a id="44__205"></a>44）你对迁移学习的理解是什么？ 举几个常用的迁移学习模型。</h1> 
<blockquote> 
 <p>迁移学习是将学习从一个模型迁移到另一个模型而无需从头开始训练的过程。 它采用预训练模型的关键部分，并将它们应用于解决新的但相似的机器学习问题。一些流行的迁移学习模型是：<br><br> VGG-16<br> BERT<br> GTP-3<br> Inception V3<br> XCeption</p> 
</blockquote> 
<h1>
<a id="45_213"></a>45）自动编码器在深度学习中有哪些用途？</h1> 
<blockquote> 
 <p>自动编码器用于将黑白图像转换为彩色图像。<br> 自动编码器有助于提取数据中的特征和隐藏模式。<br> 它还用于降低数据的维度。<br> 它还可用于去除图像中的噪声。</p> 
</blockquote> 
<h1>
<a id="46What_is_the_Swish_Function_219"></a>46）What is the Swish Function?</h1> 
<blockquote> 
 <p>Swish 是 Google 提出的激活函数，是 ReLU 激活函数的替代方案。Swish函数的定义为：<br><br> Swish(x) = x * sigmoid(beta * x)<br><br> 其中 x 为函数的输入，beta 是一个常量参数，控制函数的形状。Swish 函数类似于 Sigmoid 函数，但它还包括输入值 x，从而使它具有更广泛的输出值范围。<br> Swish 函数在某些类型的深度学习任务（如图像分类和语言处理）中表现比 ReLU 函数更好。在某些情况下，它已被证明能够提供更好的精度和更快的收敛速度，尽管其性能取决于具体的数据集和网络架构。</p> 
</blockquote> 
<h1>
<a id="47_Leaky_ReLU__225"></a>47）你对 Leaky ReLU 激活函数的理解是什么？</h1> 
<blockquote> 
 <p>Leaky ReLU 是 ReLU 激活函数的高级版本。 一般来说，ReLU 函数定义梯度为 0 时，所有的输入值都小于零。 这会使神经元失活。 为了克服这个问题，使用了 Leaky ReLU 激活函数。 它的负值斜率非常小，而不是平坦的斜率。</p> 
</blockquote> 
<div align="center"> 
 <img src="https://images2.imgbox.com/77/04/PROTGbXg_o.png" width="100%"> 
</div> 
<h1>
<a id="48_231"></a>48）什么是深度学习中的数据增强？</h1> 
<blockquote> 
 <p>数据增强是通过增强训练数据集的大小和质量来创建新数据的过程，以确保可以使用它们构建更好的模型。 有不同的技术来增强数据，例如数值数据增强、图像增强、基于 GAN 的增强和文本增强。</p> 
</blockquote> 
<h1>
<a id="49_Adam__234"></a>49）解释 Adam 优化算法。</h1> 
<blockquote> 
 <p>自适应矩估计（Adam 优化）是随机梯度下降的扩展。当处理涉及大量数据或参数的复杂问题时，此算法非常有用。它需要更少的内存并且效率高。Adam 优化算法是两种梯度下降方法的组合：RMSProp 和 Momentum。</p> 
</blockquote> 
<h1>
<a id="50_237"></a>50）哪种策略可以防止模型过度拟合训练数据？</h1> 
<blockquote> 
 <p>Dropout<br> Pooling（它是 CNN 中执行下采样操作的层。）<br> Data augmentation<br> Early stopping</p> 
</blockquote> 
<h1>
<a id="51_243"></a>51）解释两种处理深度神经网络中梯度消失问题的方法。</h1> 
<blockquote> 
 <p>使用 ReLU 激活函数代替 sigmoid 函数。<br> 使用与 tanh 激活一起使用的 Xavier 初始化来初始化神经网络。</p> 
</blockquote> 
<h1>
<a id="52Loss_Function_or_Cost_Function_247"></a>52）Loss Function or Cost Function?</h1> 
<blockquote> 
 <p>在深度学习中，训练模型时通常需要定义一个损失函数（loss function）或代价函数（cost function），用来度量模型的输出与真实值之间的差距。虽然这两个术语经常被用来指代同一概念，但它们实际上有一些微妙的区别。<br><br> 损失函数（loss function）是一个用于度量单个样本预测的误差的函数。在训练期间，模型根据每个样本的损失函数计算梯度，并根据这些梯度更新其内部参数以最小化总体损失。在深度学习中，常见的损失函数包括均方误差（MSE）、交叉熵（cross-entropy）等。<br><br> 代价函数（cost function）是用于度量整个训练集上的平均误差的函数。通常，代价函数是将损失函数应用于整个训练集得到的平均值。在训练期间，模型的目标是最小化代价函数。例如，在梯度下降中，模型通过计算代价函数的梯度来更新其内部参数。<br><br> 因此，损失函数和代价函数的区别在于它们所针对的粒度不同。损失函数针对单个样本计算误差，而代价函数则针对整个训练集计算平均误差。在实践中，这两个术语经常交替使用，并且通常只需要定义其中一个函数即可进行训练。但是，在一些特殊情况下，定义不同的损失函数和代价函数可能会对模型的训练产生影响。</p> 
</blockquote> 
<h1>
<a id="53_253"></a>53）什么是深度学习中的计算图？</h1> 
<blockquote> 
 <p>计算图是一系列操作，用于将输入作为节点排列在图形结构中。它可以被认为是将数学计算实现为图形的一种方式。这有助于并行处理，并在计算能力方面提供高性能。</p> 
</blockquote> 
<h1>
<a id="54_256"></a>54）自编码器有哪几种类型？</h1> 
<blockquote> 
 <p>深度自动编码器<br> 卷积自动编码器<br> 稀疏自动编码器<br> 收缩自动编码器</p> 
</blockquote> 
<h1>
<a id="55_262"></a>55）什么是受限玻尔兹曼机？</h1> 
<blockquote> 
 <p>受限玻尔兹曼机，简称 RBM，是当今深度学习中广泛使用的无向图模型。它是一种用于执行以下操作的算法：<br><br> 降维<br> 回归<br> 分类<br> 协同过滤<br> 主题建模</p> 
</blockquote> 
<h1>
<a id="56CNN_valid_padding__same_padding__270"></a>56）CNN中 valid padding 和 same padding 分别是什么意思？</h1> 
<blockquote> 
 <p>在卷积神经网络（CNN）中，padding（填充）是指在输入数据周围添加额外的值，以便输出数据的尺寸与输入数据相同或更大。在 CNN 中，padding 通常用于控制卷积层输出的尺寸，这有助于避免在边缘处信息的丢失。<br> 在 CNN 中，有两种常见的填充方式：valid padding（也称为“valid convolution”）和same padding（也称为“same convolution”）。<br><br> valid padding：在此填充方式下，不对输入数据进行任何填充。这意味着卷积核的每个位置只会与输入数据中存在的数据进行卷积运算。在 valid padding 下，卷积层输出的尺寸会比输入数据的尺寸小，因为边缘的数据没有被考虑进去。<br><br> same padding：在此填充方式下，对输入数据进行填充，以便输出数据的尺寸与输入数据的尺寸相同。具体而言，假设卷积核的大小为 K，那么会向输入数据的周围添加 (K-1)/2 个零元素。在 same padding 下，卷积层输出的尺寸与输入数据的尺寸相同，因为所有的输入数据都被考虑进去了。<br> 需要注意的是，same padding 并不是唯一一种能够使输出尺寸与输入尺寸相同的填充方式。其他的填充方式还包括“full padding”和“causal padding”等。</p> 
</blockquote> 
<h1>
<a id="57Transformer__RNN__277"></a>57）在深度学习中，Transformer 架构比 RNN 有何优势？</h1> 
<blockquote> 
 <p>Transformer 架构和 RNN 都是深度学习中常用的序列建模方法，但它们的设计和优势不同。<br> 相比于 RNN，Transformer 架构具有以下优势：<br><br> 并行计算：RNN 是一种逐步计算的序列模型，需要按照序列顺序依次计算，无法并行处理。而 Transformer 则可以进行并行计算，因为它不需要按照顺序计算，可以同时计算所有位置的注意力权重。<br><br> 长期依赖：RNN 在处理长序列时可能会出现梯度消失或爆炸的问题，导致无法捕捉到长期依赖关系。而 Transformer 中的自注意力机制可以在不同的位置上捕捉到长期依赖关系，使其能够更好地处理长序列。<br><br> 全局信息：在 RNN 中，信息只能从序列的前面向后面传递，无法直接获取后面的全局信息。而在 Transformer 中，所有的位置都可以与所有其他位置进行交互，能够更好地获取全局信息。<br><br> 可解释性：由于 Transformer 的注意力机制可以对不同位置之间的相互作用进行建模，因此它可以更好地解释模型的预测过程，使得模型的预测结果更加可解释和可靠。<br><br> 适用性：相比于 RNN，Transformer 更适用于处理各种类型的序列数据，包括自然语言、图像、音频等，因为它不依赖于序列的顺序和时间轴。<br><br> 总之，Transformer 架构在序列建模中具有很多优势，特别是在处理长序列、全局信息和并行计算方面，能够更好地捕捉序列之间的依赖关系，因此被广泛应用于自然语言处理、图像处理等领域。</p> 
</blockquote> 
<h1>
<a id="58Deep_Learning_bagging__boosting__287"></a>58）Deep Learning中 bagging 和 boosting 是什么意思？</h1> 
<blockquote> 
 <p>Bagging 和 Boosting 是深度学习（以及机器学习）中的两种集成学习方法，旨在提高模型的性能和鲁棒性。它们都是通过组合多个弱学习器来构建强学习器的方法，但它们的实现方式和效果略有不同。<br><br> Bagging 是一种基于数据的集成学习方法，它通过随机抽取训练数据的子集，重复地训练多个弱学习器，并将它们的输出进行聚合（比如取平均值）来得到最终的预测结果。Bagging 可以降低模型的方差，减少过拟合的风险，从而提高模型的泛化能力。<br><br> Boosting 是一种基于模型的集成学习方法，它通过训练多个弱学习器，并根据前一个学习器的错误情况对下一个学习器进行调整，最终将这些弱学习器组合成一个强学习器。Boosting 可以降低模型的偏差，提高模型的准确性和泛化能力。<br><br> 两种方法的区别在于弱学习器的生成方式不同，Bagging 通过随机采样不同的训练数据生成不同的弱学习器，而 Boosting 通过对同一组训练数据反复迭代生成不同的弱学习器。另外，Bagging 的多个弱学习器之间是独立的，而 Boosting 的多个弱学习器是有序的，后一个学习器会根据前一个学习器的表现进行调整。<br><br> 在深度学习中，Bagging 和 Boosting 也有相应的实现方式。比如，在神经网络中，可以通过随机初始化不同的权重来生成不同的弱学习器，从而实现 Bagging。而 Boosting 则可以通过训练一组浅层网络，然后将它们组合成一个更深的网络来实现。<br><br> 需要注意的是，Bagging 和 Boosting 不是万能的，它们也有一些局限性。比如，如果数据集本身就很干净并且足够大，Bagging 的效果可能不如单个强学习器。而 Boosting 对噪声敏感，如果训练数据中存在噪声或异常值，可能会导致模型的性能下降。因此，在应用这些方法时，需要根据具体情况进行选择和调整。</p> 
</blockquote> 
<h1>
<a id="59_GAN_295"></a>59）什么是生成对抗网络 (GAN)？</h1> 
<blockquote> 
 <p>生成对抗网络 (Generative Adversarial Networks, GAN) 是一种深度学习模型，通过两个神经网络相互博弈的方式来生成新的数据。一个网络（生成器）试图从潜在空间（latent space）中随机生成数据样本，另一个网络（判别器）则试图从真实数据集中和生成器生成的数据中区分出哪些是真实的数据，哪些是生成的数据。通过这种博弈过程，生成器可以逐渐学习生成更加逼真的数据，判别器也可以逐渐学习更加准确地判断数据的真伪，最终达到生成逼真数据的目的。<br><br> 生成对抗网络 (GAN) 之所以受欢迎，是因为它能够在没有明确训练目标的情况下生成逼真的数据。相比于传统的监督学习方法，GAN 不需要大量标注数据，可以更加自由地生成各种类型的数据，包括图像、音频、视频等等。此外，GAN 也具有一定的创造性，可以在生成数据时添加一些噪音，从而生成具有多样性的数据。这些优点使得 GAN 在图像生成、视频生成等领域得到广泛的应用。</p> 
</blockquote> 
<h1>
<a id="60sparklessparklessparklesroseroserose_299"></a>60）✨✨✨未完待续，欢迎点赞、收藏、关注。???</h1>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>