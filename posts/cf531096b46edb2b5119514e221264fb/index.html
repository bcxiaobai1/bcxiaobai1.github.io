<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>一个算子在深度学习框架中的旅程 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">一个算子在深度学习框架中的旅程</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p style="text-align:center"><img alt="8c5be85f71ff46c25454e8d9f085425d.png" src="https://images2.imgbox.com/0c/e0/6lKVmISM_o.png"></p> 
 <p style="text-align:justify"><strong>撰文｜赵露阳</strong></p> 
 <p style="text-align:justify">算子即Operator，这里简称op。op是深度学习的基础操作，任意深度学习框架中都包含了数百个op，这些op用于各种类型的数值、tensor运算。</p> 
 <p style="text-align:justify">在深度学习中，通过nn.Module这样搭积木的方式搭建网络，而op就是更基础的，用于制作积木的配方和原材料。</p> 
 <p style="text-align:justify">譬如如下的一个demo网络：</p> 
 <pre class="has"><code class="language-go">import oneflow as torch                  
class TinyModel(torch.nn.Module):

    def __init__(self):
        super(TinyModel, self).__init__()

        self.linear1 = torch.nn.Linear(100, 200)
        self.activation = torch.nn.ReLU()
        self.linear2 = torch.nn.Linear(200, 10)
        self.softmax = torch.nn.Softmax()

    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        x = self.linear2(x)
        x = self.softmax(x)
        return xtinymodel = TinyModel()print('The model:')print(tinymodel)</code></pre> 
 <p>从结构来看，这个网络是由各种nn.Module如Linear、ReLU、Softmax搭建而成，但从本质上，这些nn.Module则是由一个个基础op拼接，从而完成功能的。这其中就包含了Matmul、Relu、Softmax等op。 在OneFlow中，对于一个已有op，是如何完成从Python层-&gt;C++层的调用、流转和执行过程？本文将以</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-ini">output = flow.relu(input)</code></pre> 
 <pre class="has"><code class="language-go">为例，梳理一个op从Python -&gt; C++执行的完整过程。</code></pre> 
 <p style="text-align:justify">首先，这里给出一个流程示意图：</p> 
 <img alt="f8d41740f153920f595b99c27a7f1189.png" src="https://images2.imgbox.com/95/34/nGL4T8zd_o.png" width="1200">
 <p style="text-align:justify">下面，将分别详细从源码角度跟踪其各个环节。</p> 
 <p style="text-align:center"><strong>1</strong></p> 
 <p style="text-align:center"><strong>Binding</strong></p> 
 <p style="text-align:justify">这里，binding是指Python和C++代码的绑定。通常，我们用Python搭建网络，训练模型，调用函数完成各种操作。实际上，这些函数通常在Python层只是一层wrapper，底层实现还是通过C++代码完成的，那么Python -&gt; C++是如何调用的？这就需要用到Python和C++的绑定。</p> 
 <p style="text-align:justify">在深度学习框架的实现中，<strong>即可以用Python原生的C API，也可以通过pybind11来完成函数绑定</strong>，在OneFlow中，二者均有使用，譬如：</p> 
 <ul>
<li> <p style="text-align:justify">oneflow/api/python/framework/tensor.cpp</p> </li>
<li> <p style="text-align:justify">oneflow/api/python/framework/tensor_functions.cpp</p> </li>
</ul>
 <p style="text-align:justify">中涉及到的 tensor.xxx 方法都是通过Python C API完成了函数绑定；</p> 
 <ul><li> <p style="text-align:justify">oneflow/core/functional/functional_api.yaml</p> </li></ul>
 <p style="text-align:justify">中定义的诸多 flow.xxx 方法则是通过pybind实现的绑定。这里关于Python C API和pybind不做过多介绍，具体用法可以参考相应文档：</p> 
 <ul>
<li> <p style="text-align:justify"><em>https://docs.python.org/zh-cn/3.8/c-api/index.html</em></p> </li>
<li> <p style="text-align:justify"><em>https://pybind11.readthedocs.io/en/stable/index.html</em></p> </li>
</ul>
 <p style="text-align:justify">下面我们回到flow.relu方法，我们在Python层调用的flow.relu实际是调用了在</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-markdown">python/oneflow/__init__.py</code></pre> 
 <p style="text-align:justify">中定义的oneflow._C.relu。 _C表示其实现位于底层C++。和PyTorch类似，我们也基于.yaml定义了一套接口导出及code gen的规则，譬如在 functional_api.yaml 中，我们可以看到Relu的导出接口的函数签名：</p> 
 <pre class="has"><code class="language-php">- name: "relu"
  signature: "Tensor (Tensor x, Bool inplace=False) =&gt; Relu"
  bind_python: True</code></pre> 
 <p style="text-align:justify">从yaml定义可以看出，flow._C.relu 接收两个参数，tensor和一个bool值，其绑定了C++的Relu方法，函数返回值也是tensor。实际上，在OneFlow编译时，会通过执行</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-typescript">tools/functional/generate_functional_api.py</code></pre> 
 <p style="text-align:justify">这个文件，对 functional_api.yaml 进行解析和代码生成，动态生成C++的.h和.cpp文件。</p> 
 <ul>
<li> <p style="text-align:justify">build/oneflow/core/functional/functional_api.yaml.h</p> </li>
<li> <p style="text-align:justify">build/oneflow/core/functional/functional_api.yaml.cpp</p> </li>
</ul>
 <p style="text-align:left">并在.cpp文件中调用相应的functor完成C++层面的函数调用。这里，还是以flow._C.relu为例，其对应的functor定义位于<em>oneflow/core/functional/impl/activation_functor.cpp</em>:</p> 
 <pre class="has"><code class="language-cpp">class ReluFunctor {
 public:
  ReluFunctor() { op_ = CHECK_JUST(one::OpBuilder("relu").Input("x", 1).Output("y", 1).Build()); }
  Maybe&lt;Tensor&gt; operator()(const std::shared_ptr&lt;Tensor&gt;&amp; x, bool inplace) const {
    ...
  }


 private:
  std::shared_ptr&lt;OpExpr&gt; op_;
};</code></pre> 
 <p style="text-align:justify">‍</p> 
 <p style="text-align:justify">ReluFunctor通过</p> 
 <pre class="has"><code class="language-cpp">ONEFLOW_FUNCTION_LIBRARY(m) {
  m.add_functor&lt;impl::ReluFunctor&gt;("Relu");
  ...
}</code></pre> 
 <p>‍</p> 
 <p style="text-align:justify">完成functor的注册，注册成functional接口后，在Python层flow._C.relu就完成了和“Relu”的绑定。同时，这个函数在C++中也可以通过functional::Relu直接调用。</p> 
 <h2 id="2"><strong>2</strong></h2> 
 <p style="text-align:center"><strong>Functor</strong></p> 
 <p style="text-align:justify">Functor不仅是Python -&gt; C++交互的核心，也是op调用、输入参数推导和检查的第一站。通常，各种op在functor层需要完成对输入tensor的shape、dtype、维度、元素个数等各种check，以及对op特有的逻辑进行解析和处理。Relu Functor代码如下：</p> 
 <pre class="has"><code class="language-cpp">class ReluFunctor {
 public:
  ReluFunctor() { op_ = CHECK_JUST(one::OpBuilder("relu").Input("x", 1).Output("y", 1).Build()); }
  Maybe&lt;Tensor&gt; operator()(const std::shared_ptr&lt;Tensor&gt;&amp; x, bool inplace) const {
    if (inplace) {
      JUST(CheckInplaceValid(x));
      std::shared_ptr&lt;TensorTuple&gt; outputs = std::make_shared&lt;TensorTuple&gt;(1);
      outputs-&gt;at(0) = x;
      JUST(OpInterpUtil::Dispatch(*op_, {x}, outputs.get(), AttrMap{}));
      return outputs-&gt;at(0);
    } else {
      return OpInterpUtil::Dispatch&lt;Tensor&gt;(*op_, {x});
    }
  }


 private:
  std::shared_ptr&lt;OpExpr&gt; op_;
};</code></pre> 
 <p style="text-align:justify">可以看见，ReluFunctor是比较简单的，其定义了一个私有变量</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-cpp">std::shared_ptr&lt;OpExpr&gt; op_;</code></pre> 
 <p style="text-align:justify">这个op_即需要执行的Relu op，通过OpBuilder进行构建；functor的operator()内部，根据是否inplace走到2个不同分支，并最终通过OpInterpUtil::Dispatch()将op、输入tensor和参数派发至Interpreter处理。</p> 
 <h2 id="3"><strong>3</strong></h2> 
 <h2 id="Dispatch"><strong>Dispatch</strong></h2> 
 <p style="text-align:justify">各种op在functor中完成check和逻辑处理后，大多需要通过OpInterpUtil::Dispatch() 进行派发，其目的地是Interpreter。在Interpreter中，将会对op进行更进一步的处理。在<em>oneflow/core/framework/op_interpreter/op_interpreter_util.h</em> 中，我们可以看见多种重载的Dispatch模板代码：</p> 
 <pre class="has"><code class="language-cs">class OpInterpUtil {
 public:
  template&lt;typename T&gt;
  static Maybe&lt;T&gt; Dispatch(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs, const AttrMap&amp; attrs) {
    return Dispatch&lt;T&gt;(op_expr, inputs, OpExprInterpContext(attrs));
  }


  template&lt;typename T&gt;
  static Maybe&lt;T&gt; Dispatch(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs) {
    return Dispatch&lt;T&gt;(op_expr, inputs, OpExprInterpContext(AttrMap{}));
  }


  template&lt;typename T&gt;
  static Maybe&lt;T&gt; Dispatch(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs,
                           const OpExprInterpContext&amp; ctx);


  static Maybe&lt;void&gt; Dispatch(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs,
                              TensorTuple* outputs, const AttrMap&amp; attrs) {
    return Dispatch(op_expr, inputs, outputs, OpExprInterpContext(attrs));
  }


  static Maybe&lt;void&gt; Dispatch(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs,
                              TensorTuple* outputs) {
    return Dispatch(op_expr, inputs, outputs, OpExprInterpContext(AttrMap{}));
  }


  static Maybe&lt;void&gt; Dispatch(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs,
                              TensorTuple* outputs, const OpExprInterpContext&amp; ctx);</code></pre> 
 <p style="text-align:justify">这些重载，是为了应对不同的输入、输出以及OpExprInterpContext的情况。譬如这个OpExprInterpContext是op在Interpreter中所需的上下文，可能携带op计算所需要的属性(如conv2d op所需要的kernel_size、padding等)、device、sbp、parallel等描述信息。这些重载的Dispatch最终都会走到：</p> 
 <pre class="has"><code class="language-cpp">/* static */ Maybe&lt;void&gt; OpInterpUtil::Dispatch(
    const OpExpr&amp; op_expr, 
    const TensorTuple&amp; inputs,             
    TensorTuple* outputs,
    const OpExprInterpContext&amp; ctx) {
  return JUST(GetInterpreter(inputs, ctx, op_expr))-&gt;Apply(op_expr, inputs, outputs, ctx);
}</code></pre> 
 <p style="text-align:justify">Dispatch至此，剩下的就要交给Interpreter了。</p> 
 <h2 id="4"><strong>4</strong></h2> 
 <h2 id="Interpreter"><strong>Interpreter</strong></h2> 
 <h2 id="Get%20Interpreter"><strong>Get Interpreter</strong></h2> 
 <h3></h3> 
 <p style="text-align:justify">这里先看看GetInterpreter，这里其实就是获取所需的Interpreter，来负责op接下来的执行。省略check相关的逻辑，主要代码如下：<em>oneflow/core/framework/op_interpreter/op_interpreter_util.cpp</em></p> 
 <pre class="has"><code class="language-cpp">Maybe&lt;AutogradInterpreter&gt; GetInterpreter(const TensorTuple&amp; inputs, const OpExprInterpContext&amp; ctx,
                                          const OpExpr&amp; op_expr) {
  static const auto&amp; g_lazy_interpreter = BuildLazyInterpreter();
  static const auto&amp; g_eager_consistent_interpreter = BuildEagerInterpreter(/*is_mirrored=*/false);
  static const auto&amp; g_eager_mirrored_interpreter = BuildEagerInterpreter(/*is_mirrored=*/true);
  if (!LazyMode::is_enabled()) {
    if (inputs.empty()) {
      if (ctx.parallel_desc.has_value()) {
        JUST(ctx.nd_sbp);
        CHECK_OR_RETURN(!ctx.device.has_value());
        return g_eager_consistent_interpreter;
      } else {
        CHECK_OR_RETURN(!ctx.nd_sbp.has_value());
        return g_eager_mirrored_interpreter;
      }
    } else {
      if (inputs.at(0)-&gt;is_consistent()) {
        ...
        return g_eager_consistent_interpreter;
      } else {
        ...
        return g_eager_mirrored_interpreter;
      }
    }
    UNIMPLEMENTED_THEN_RETURN();
  }
  return g_lazy_interpreter;
}</code></pre> 
 <p style="text-align:justify">通过上面的逻辑可以看出，Interpreter大体上分为Eager Interpteter和Lazy Interpreter；其中Eager Interpteter又根据Eager Mirrored和Eager Consistent有所区别。具体就是以下3种子类实现：</p> 
 <ul>
<li> <p style="text-align:justify"><strong>EagerMirroredInterpreter</strong></p> </li>
<li> <p style="text-align:justify"><strong>EagerConsistentInterpreter</strong></p> </li>
<li> <p style="text-align:justify"><strong>LazyInterpreter</strong></p> </li>
</ul>
 <p style="text-align:justify">普通的Eager mode下（无论是单卡还是DDP的情况）都会走到 <strong>EagerMirroredInterpreter </strong>的逻辑；在普通Eager Mode之外，为输入tensor设置了sbp、placement则会进入到<strong>EagerConsistentInterpreter</strong>的逻辑；在Lazy Mode时（使用nn.Graph），则会进入到<strong>LazyInterpreter</strong>。</p> 
 <p style="text-align:justify">下面，我们看下这3种Interpreter的构建：</p> 
 <pre class="has"><code class="language-cpp">std::shared_ptr&lt;AutogradInterpreter&gt; BuildEagerInterpreter(const bool&amp; is_mirrored) {
  std::shared_ptr&lt;OpExprInterpreter&gt; internal;
  if (is_mirrored) {
    internal = std::make_shared&lt;EagerMirroredInterpreter&gt;();
  } else {
    internal = std::make_shared&lt;EagerConsistentInterpreter&gt;();
  }
  return std::make_shared&lt;AutogradInterpreter&gt;(internal);
}


std::shared_ptr&lt;AutogradInterpreter&gt; BuildLazyInterpreter() {
  auto internal = std::make_shared&lt;LazyInterpreter&gt;();
  return std::make_shared&lt;AutogradInterpreter&gt;(internal);
}</code></pre> 
 <p style="text-align:justify">可见，这3种Interpreter构建完成后，都会以私有变量internal的形式，参与AutogradInterpreter的构建，并最终返回<strong>AutogradInterpreter</strong>。</p> 
 <pre class="has"><code class="language-cpp">class AutogradInterpreter {
 public:
  AutogradInterpreter() = delete;
  AutogradInterpreter(const std::shared_ptr&lt;OpExprInterpreter&gt;&amp; internal) : internal_(internal) {}


  virtual ~AutogradInterpreter() = default;


  Maybe&lt;void&gt; Apply(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs, TensorTuple* outputs,
                    const AttrMap&amp; attrs) const {
    return Apply(op_expr, inputs, outputs, OpExprInterpContext(attrs));
  }


  Maybe&lt;void&gt; Apply(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs, TensorTuple* outputs) const {
    return Apply(op_expr, inputs, outputs, OpExprInterpContext(AttrMap{}));
  }


  Maybe&lt;void&gt; Apply(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs, TensorTuple* outputs,
                    const OpExprInterpContext&amp; ctx) const;


 private:
  std::shared_ptr&lt;OpExprInterpreter&gt; internal_;
};</code></pre> 
 <p style="text-align:justify"><strong>Apply()</strong></p> 
 <h3></h3> 
 <p style="text-align:justify">通过上面我们知道，<strong>EagerMirroredInterpreter</strong>、<strong>EagerConsistentInterpreter</strong>和<strong>LazyInterpreter</strong>都将为其包裹上<strong>AutogradInterpreter</strong>的壳，通过AutogradInterpreter触发Apply的调用。顾名思义，AutogradInterpreter的作用主要是和autograd相关，其主要为eager mode下前向的op节点插入对应的用于反向计算grad的节点。</p> 
 <p style="text-align:justify">我们看看这部分代码，关键部分的作用在注释里给出：</p> 
 <pre class="has"><code class="language-php">Maybe&lt;void&gt; AutogradInterpreter::Apply(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs,
                                       TensorTuple* outputs, const OpExprInterpContext&amp; ctx) const {
  // 判断是否需要计算梯度，如果处于GradMode的作用域切改op注册时没有禁用梯度
  // 则requires_grad的值根据输入tensor的requires_grad属性判断
  // any of input tensors requires_grad==True，则表示需要计算梯度
  bool requires_grad = false;
  if (autograd::GradMode::is_enabled() &amp;&amp; !JUST(op_expr.IsGradDisabled())) {
    requires_grad =
        std::any_of(inputs.begin(), inputs.end(),
                    [](const std::shared_ptr&lt;Tensor&gt;&amp; tensor) { return tensor-&gt;requires_grad(); });
  }
// 这一坨逻辑比较丑陋，是因为近期支持了oneflow系统中支持了stride&amp;&amp;view机制
// 而大部分op尚未注册stride推导、尚未支持non-contiguous的输入tensor
// 所以需要在这对这部分op的输入进行强制转换，将其变为contiguous的
// NOTE: if this op not support stride, then need to tensor-&gt;contiguous()
#define HANDLE_NON_CONTIGUOUS_INPUT(tensor_tuple_ptr)                                       
  TensorTuple tmp_inputs;                                                                   
  if (!LazyMode::is_enabled() &amp;&amp; !JUST(op_expr.SupportNonContiguous())) {                   
    tmp_inputs.resize(inputs.size());                                                       
    for (size_t i = 0; i &lt; inputs.size(); i++) { tmp_inputs[i] = inputs[i]-&gt;contiguous(); } 
    tensor_tuple_ptr = &amp;tmp_inputs;                                                         
  }


  const TensorTuple* inputs_ptr = &amp;inputs;
  HANDLE_NON_CONTIGUOUS_INPUT(inputs_ptr);


  // 这里是进行实际Interpreter执行的主要过程
  {
    autograd::AutoGradMode mode(false);
    JUST(internal_-&gt;Apply(op_expr, *inputs_ptr, outputs, ctx));
  }


  // 这里主要是为了eager mode下，且requires_grad==True的op，
  // 插入反向节点(AddNode)用于autograd，该节点包含反向梯度计算的方法(backward_fn)
  // Lazy mode will construct backward compute graph in passes, so disable autograd if lazy mode.
  std::shared_ptr&lt;OpExprGradClosure&gt; grad_closure(nullptr);
  if (requires_grad &amp;&amp; !LazyMode::is_enabled()) {
    grad_closure = JUST(op_expr.GetOrCreateOpGradClosure());
    auto backward_fn = std::make_shared&lt;BackwardFunction&gt;();
    backward_fn-&gt;body = [=](const TensorTuple&amp; out_grads, TensorTuple* in_grads,
                            bool create_graph) -&gt; Maybe&lt;void&gt; {
      autograd::AutoGradMode mode(create_graph);
      JUST(grad_closure-&gt;Apply(out_grads, in_grads));
      return Maybe&lt;void&gt;::Ok();
    };
    backward_fn-&gt;status = [=]() { return grad_closure-&gt;state()-&gt;SavedTensors().size() &gt; 0; };
    JUST(GetThreadLocalAutogradEngine()-&gt;AddNode(op_expr.op_type_name() + "_backward", backward_fn,
                                                 *inputs_ptr, outputs));
  }
  // Update outputs autograd meta
  // Note: if requires_grad is True, we will create a new autograd meta for each output
  // in `AddBackwardFuncPtr` to support inplace operation, so the update should after
  // `AddBackwardFuncPtr`
  for (auto&amp; output : *outputs) {
    output-&gt;set_is_leaf(inputs_ptr-&gt;size() == 0 || !requires_grad);
    ...
    if (!output-&gt;requires_grad()) {
      JUST(output-&gt;set_requires_grad(
          requires_grad &amp;&amp; IsSupportRequireGradDataType(output-&gt;dtype()-&gt;data_type())));
    }
  }
  // 捕获前向的inputs outputs，反向计算时可能用到
  if (requires_grad &amp;&amp; !LazyMode::is_enabled()) {
    // Capture inputs and outputs after `AddBackwardFuncPtr` because of that grad function
    // node has been attached to them.
    JUST(grad_closure-&gt;Capture(*inputs_ptr, *outputs, ctx));
  }
  return Maybe&lt;void&gt;::Ok();
}</code></pre> 
 <p style="text-align:justify">上面一坨逻辑有点多，让我们看一下重点，对于简单的Relu op，我们只需关注这部分代码：</p> 
 <pre class="has"><code class="language-cpp">// 这里是进行实际Interpreter执行的主要过程
  {
    autograd::AutoGradMode mode(false);
    JUST(internal_-&gt;Apply(op_expr, *inputs_ptr, outputs, ctx));
  }</code></pre> 
 <p style="text-align:justify">这里，还是以上面的flow.relu为例，由于是简单的Eager Mode，所以实际会走到EagerInterpreter的Apply方法：</p> 
 <pre class="has"><code class="language-cpp">Maybe&lt;void&gt; EagerInterpreter::Apply(const OpExpr&amp; op_expr, const TensorTuple&amp; inputs,
                                    TensorTuple* outputs, const OpExprInterpContext&amp; ctx) const {
#define APPLY_IF(op_type)                                              
  if (const auto* op = dynamic_cast&lt;const op_type##Expr*&gt;(&amp;op_expr)) { 
    return ApplyImpl(*op, inputs, outputs, ctx);                       
  }


  APPLY_IF(UserOp);
  APPLY_IF(VariableOp);
  APPLY_IF(CastToMirroredOp);
  APPLY_IF(CastFromMirroredOp);
  APPLY_IF(ConsistentToConsistentOp);
  APPLY_IF(CastToConsistentOp);
  APPLY_IF(CastFromConsistentOp);
  APPLY_IF(DistributeSplitOp);
  APPLY_IF(DistributeCloneOp);
  APPLY_IF(DistributeConcatOp);
  APPLY_IF(DistributeAddOp);
  APPLY_IF(FunctionOp);
  APPLY_IF(SelectTopNOp)
#undef APPLY_IF


  OF_UNIMPLEMENTED() &lt;&lt; "The type " &lt;&lt; op_expr.op_type_name()
                     &lt;&lt; " has not been supported in EagerInterpreter::Apply.";
}</code></pre> 
 <p>‍</p> 
 <p style="text-align:justify">这里，通过宏定义APPLY_IF，增加了对不同类型op的分支处理。对于大多数用户来说，用到的op都是UserOp类型，所以这里实际上会走到这个分支中：</p> 
 <pre class="has"><code class="language-cpp">if (const auto* op = dynamic_cast&lt;const UserOpExpr*&gt;(&amp;op_expr)) {
    return ApplyImpl(*op, inputs, outputs, ctx);
  }</code></pre> 
 <p style="text-align:justify">再看看<strong>EagerMirroredInterpreter::ApplyImpl</strong>，位于</p> 
 <p style="text-align:justify"><em>oneflow/core/framework/op_interpreter/eager_mirrored_op_interpreter.cpp</em><strong>：</strong></p> 
 <pre class="has"><code class="language-cpp">Maybe&lt;void&gt; EagerMirroredInterpreter::ApplyImpl(const UserOpExpr&amp; op_expr,
                                                const TensorTuple&amp; inputs, TensorTuple* outputs,
                                                const OpExprInterpContext&amp; ctx) const {
  return NaiveInterpret(op_expr, inputs, outputs, ctx);
}</code></pre> 
 <p style="text-align:justify">其最终实现是NaiveInterpret。</p> 
 <p style="text-align:justify"><strong>NaiveInterpret</strong></p> 
 <h3></h3> 
 <p style="text-align:justify">NaiveInterpret简单来说，主要用于做以下几件事：</p> 
 <ul>
<li> <p style="text-align:justify"><strong>check input tensor的device是否一致</strong></p> </li>
<li> <p style="text-align:justify"><strong>生成output tensor</strong></p> </li>
<li> <p style="text-align:justify"><strong>为output tensor推导和检查shape/stride/dtype</strong></p> </li>
<li> <p style="text-align:justify"><strong>构建op执行指令，并派发至vm</strong></p> </li>
</ul>
 <p style="text-align:justify">简化版的代码如下：</p> 
 <pre class="has"><code class="language-php">Maybe&lt;void&gt; NaiveInterpret(const UserOpExpr&amp; user_op_expr, const TensorTuple&amp; inputs,
                           const Symbol&lt;Device&gt;&amp; default_device, TensorTuple* outputs,
                           const OpExprInterpContext&amp; ctx) {
  const auto&amp; attrs = ctx.attrs;
  std::shared_ptr&lt;EagerBlobObjectList&gt; input_eager_blob_objects =
      std::make_shared&lt;EagerBlobObjectList&gt;(inputs.size());
  // check devices
  for (int i = 0; i &lt; inputs.size(); i++) {
    const auto&amp; input_device = JUST(inputs.at(i)-&gt;device());
    if (i &gt; 0) {
      CHECK_OR_RETURN(*default_device == *input_device)
          &lt;&lt; Error::RuntimeError()
          &lt;&lt; "Expected all tensors to be on the same device, but found at least two devices, "
          &lt;&lt; default_device-&gt;ToString() &lt;&lt; " (positional 0) and " &lt;&lt; input_device-&gt;ToString()
          &lt;&lt; " (positional " &lt;&lt; i &lt;&lt; ")!";
    }
    input_eager_blob_objects-&gt;at(i) = JUST(inputs.at(i)-&gt;eager_blob_object());
  }


  // make output tensors
  std::shared_ptr&lt;EagerBlobObjectList&gt; output_eager_blob_objects =
      std::make_shared&lt;EagerBlobObjectList&gt;(outputs-&gt;size());
  auto* output_tensor_metas = ThreadLocalDefaultOutputMutTensorMetas(outputs-&gt;size());
  for (int i = 0; i &lt; outputs-&gt;size(); i++) {
    if (!outputs-&gt;at(i)) {
      const auto&amp; tensor_impl = std::make_shared&lt;EagerMirroredTensorImpl&gt;();
      outputs-&gt;at(i) = std::make_shared&lt;MirroredTensor&gt;(tensor_impl);
      output_tensor_metas-&gt;at(i) = tensor_impl-&gt;mut_tensor_meta();
    } else {
      bool has_eager_blob_object = JUST(outputs-&gt;at(i)-&gt;has_eager_blob_object());
      CHECK_OR_RETURN(has_eager_blob_object);
      output_eager_blob_objects-&gt;at(i) = JUST(outputs-&gt;at(i)-&gt;eager_blob_object());
    }
  }
  Symbol&lt;Stream&gt; stream;
  bool need_check_mem_case = true;


  // Infer devices
  ...


  // Infer shapes strides dtype
  ...


  // 构建op执行指令，并派发至vm
  JUST(PhysicalRun([&amp;](InstructionsBuilder* builder) -&gt; Maybe&lt;void&gt; {
    return builder-&gt;LocalCallOpKernel(kernel, input_eager_blob_objects, output_eager_blob_objects,
                                      ctx, stream);
  }));
  return Maybe&lt;void&gt;::Ok();
}</code></pre> 
 <p style="text-align:justify">Interpreter的终点是虚拟机（vm）。vm部分，是OneFlow比较独特的设计，内容很多，这里暂不展开了：） 可以简单理解，派发至vm后，此op将进入一个任务执行的队列，将会等待其vm的调度、执行。</p> 
 <h2 id="5"><strong>5</strong></h2> 
 <h2 id="Compute"><strong>Compute</strong></h2> 
 <p style="text-align:justify">在Interpreter将op执行指令派发至vm后，经过调度逻辑处理后，将会在</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-go">oneflow/core/eager/opkernel_instruction_type.cpp</code></pre> 
 <p style="text-align:justify">被触发执行，核心代码如下：</p> 
 <pre class="has"><code class="language-php">static inline void OpKernelCompute(
    LocalCallOpKernelPhyInstrOperand* operand,
    DeviceCtx* device_ctx, user_op::OpKernelState* state,
    const user_op::OpKernelCache* cache) {


    auto* opkernel = operand-&gt;mut_opkernel();
    auto* compute_ctx =
        opkernel-&gt;UpdateComputeContext(operand-&gt;inputs().get(), operand-&gt;outputs().get(),
                                       operand-&gt;consistent_tensor_infer_result().get(), device_ctx);
    ...
    operand-&gt;user_opkernel()-&gt;Compute(compute_ctx, state, cache);
    opkernel-&gt;UpdateComputeContext(nullptr, nullptr, nullptr, nullptr);
}</code></pre> 
 <p style="text-align:justify">其中，</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-php">operand-&gt;user_opkernel()-&gt;Compute(compute_ctx, state, cache);</code></pre> 
 <p style="text-align:justify">将触发op kernel的实际执行。通常来说，op的kernel实现根据device的不同，会派发到不同的实现，其一般都位于：</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-go">oneflow/user/kernels/xxx_kernel.cpp</code></pre> 
 <p style="text-align:justify">或</p> 
 <pre>
</pre> 
 <pre class="has"><code class="language-go">oneflow/user/kernels/xxx_kernel.cu</code></pre> 
 <p style="text-align:justify">这里的Relu op相对比较特殊，是用primitive实现的（primitive也是oneflow中一种独特的设计，有着良好的抽象和可组合性），具体这个UnaryPrimitive就是elementwise unary的模板+UnaryFunctor的组合。其调用链如下：</p> 
 <img alt="c7f34126021fd211c0bfbd7b90f914c8.png" src="https://images2.imgbox.com/03/01/GCM4NCXF_o.png" width="1200">
 <h3></h3> 
 <h3 id="UnaryPrimitiveKernel"><strong>UnaryPrimitiveKernel</strong></h3> 
 <pre class="has"><code class="language-cpp">class UnaryPrimitiveKernel final : public user_op::OpKernel, public user_op::CudaGraphSupport {
 public:
  OF_DISALLOW_COPY_AND_MOVE(UnaryPrimitiveKernel);
  UnaryPrimitiveKernel() = default;
  ~UnaryPrimitiveKernel() = default;


  using PrimitiveFactoryFuncType = std::function&lt;std::unique_ptr&lt;ep::primitive::ElementwiseUnary&gt;(
      user_op::KernelComputeContext*)&gt;;


  UnaryPrimitiveKernel(const std::string&amp; output_name, const std::string&amp; input_name,
                       PrimitiveFactoryFuncType fn)
      : output_name_(output_name),
        input_name_(input_name),
        primitive_factory_func_(std::move(fn)) {}


 private:
  using user_op::OpKernel::Compute;
  void Compute(user_op::KernelComputeContext* ctx) const override {
    auto primitive = primitive_factory_func_(ctx);
    CHECK(primitive);


    const user_op::Tensor* input_tensor = ctx-&gt;Tensor4ArgNameAndIndex(input_name_, 0);
    ...
    const int64_t elem_cnt = input_shape.elem_cnt();


    if (elem_cnt != 0) {
      primitive-&gt;Launch(ctx-&gt;stream(), input_tensor-&gt;dptr(), output_tensor-&gt;mut_dptr(), elem_cnt);
    }
  }
  bool AlwaysComputeWhenAllOutputsEmpty() const override { return false; }


  std::string output_name_;
  std::string input_name_;
  PrimitiveFactoryFuncType primitive_factory_func_;
};</code></pre> 
 <h3 id="%E2%80%8D">‍</h3> 
 <h3></h3> 
 <h3 id="ep%3A%3Aprimitive%3A%3AElementwiseUnary"><strong>ep::primitive::ElementwiseUnary</strong></h3> 
 <pre class="has"><code class="language-cpp">template&lt;UnaryOp unary_op, typename Src, typename Dst&gt;
class ElementwiseUnaryImpl : public ElementwiseUnary {
 public:
  OF_DISALLOW_COPY_AND_MOVE(ElementwiseUnaryImpl);
  ElementwiseUnaryImpl(Scalar attr0, Scalar attr1) : attr0(attr0), attr1(attr1) {}
  ~ElementwiseUnaryImpl() override = default;


  void Launch(Stream* stream, const void* src_ptr, void* dst_ptr, size_t count) override {
    CpuStream* cpu_stream = stream-&gt;As&lt;CpuStream&gt;();


    Dst* dst = reinterpret_cast&lt;Dst*&gt;(dst_ptr);
    const Src* src = reinterpret_cast&lt;const Src*&gt;(src_ptr);
    auto functor = UnaryFunctor&lt;DeviceType::kCPU, unary_op, Dst, Src&gt;(attr0, attr1);
    cpu_stream-&gt;ParallelFor(0, count, [functor, src, dst](int64_t begin, int64_t end) {
      for (int64_t i = begin; i &lt; end; i++) { dst[i] = functor(src[i]); }
    });
  }


 protected:
  Scalar attr0, attr1;
};</code></pre> 
 <h3></h3> 
 <h3 id="UnaryFunctor"><strong>UnaryFunctor</strong></h3> 
 <h3 id="%E8%BF%99%E4%B8%AAUnaryFuntor%E6%A0%B9%E6%8D%AE%E4%B8%8D%E5%90%8C%E7%9A%84Unaray%20op%E7%B1%BB%E5%9E%8B%EF%BC%8C%E7%89%B9%E5%8C%96%E5%87%BA%E4%B8%8D%E5%90%8C%E7%9A%84%E5%85%B7%E4%BD%93functor%E5%AE%9E%E7%8E%B0%EF%BC%8C%E5%85%B7%E4%BD%93%E5%88%B0Relu%20op%EF%BC%8C%E5%85%B6%E5%AE%9E%E7%8E%B0%E4%BD%8D%E4%BA%8E">这个UnaryFuntor根据不同的Unaray op类型，特化出不同的具体functor实现，具体到Relu op，其实现位于</h3> 
 <p style="text-align:justify"><strong>oneflow/core/ep/common/primitive/unary_functor.h：</strong></p> 
 <pre class="has"><code class="language-cpp">template&lt;DeviceType device, typename Dst, typename Src&gt;
struct UnaryFunctor&lt;device, UnaryOp::kRelu, Dst, Src&gt; {
  UnaryFunctor(Scalar attr0, Scalar attr1) {}


  OF_DEVICE_FUNC Dst operator()(Src src) const {
    const Src zero_val = static_cast&lt;Src&gt;(0.0);
    if (src &lt;= zero_val) {
      return static_cast&lt;Dst&gt;(zero_val);
    } else {
      return static_cast&lt;Dst&gt;(src);
    }
  }
};</code></pre> 
 <p style="text-align:justify">至此，我们已经完成了一个op的Python -&gt; C++ 之旅。从细节上看，是相对复杂的，但从整体流程上看，其实是比较简单的，排除了binding，vm调度机制等细节，其主要过程其实就4个环节：<strong> Functor -&gt; Dispatch -&gt; Interpreter -&gt; Kernel Compute。</strong></p> 
 <p style="text-align:justify">实现/新增一个op，通常也不需要管中间的Dispatch以及Interpreter，我们只需重点关注和该op强相关的部分——Functor层面的参数、op逻辑检查，以及Kernel Compute部分的实际op运算。</p> 
 <p style="text-align:justify"><em>（参考代码：</em></p> 
 <p style="text-align:justify"><em>https://github.com/Oneflow-Inc/oneflow/commit/1dbdf8faed988fa7fd1a9034a4d79d5caf18512d）</em></p> 
 <p style="text-align:justify">其他人都在看</p> 
 <ul>
<li> <p><a href="">一个Tensor在深度学习框架中的执行过程</a></p> </li>
<li> <p><a href="">学习笔记：从Python到C++调用过程分析</a></p> </li>
<li> <p><a href="">学习笔记：从Functor到OpExprInterpreter</a></p> </li>
<li> <p><a href="">学习笔记：从OpExprInterpreter到OpKernel</a></p> </li>
<li> <p><a href="">李飞飞：我更像物理学家，而不是工程师</a></p> </li>
<li> <p><a href="">手把手推导分布式矩阵乘的最优并行策略</a></p> </li>
<li> <p><a href="">解读Pathways（二）：向前一步是OneFlow</a></p> </li>
</ul>
 <p style="text-align:justify"><strong>欢迎下载体验OneFlow v0.7.0：</strong><a class="has-card" href="https://github.com/Oneflow-Inc/oneflow/" title="GitHub - Oneflow-Inc/oneflow: OneFlow is a performance-centered and open-source deep learning framework."><span class="link-card-box"><span class="link-title">GitHub - Oneflow-Inc/oneflow: OneFlow is a performance-centered and open-source deep learning framework.</span><span class="link-desc">OneFlow is a performance-centered and open-source deep learning framework. - GitHub - Oneflow-Inc/oneflow: OneFlow is a performance-centered and open-source deep learning framework.</span><span class="link-link"><img alt="" class="link-link-icon" src="https://images2.imgbox.com/4a/29/UWIVwxu5_o.png">https://github.com/Oneflow-Inc/oneflow/</span></span></a></p> 
</div>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>