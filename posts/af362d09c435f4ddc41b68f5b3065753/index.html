<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Python爬虫入门教程！手把手教会你爬取网页数据 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python爬虫入门教程！手把手教会你爬取网页数据</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <p>其实在当今社会，网络上充斥着大量有用的数据，我们只需要耐心的观察，再加上一些技术手段，就可以获取到大量的有价值数据。这里的“技术手段”就是网络爬虫。今天就给大家分享一篇爬虫基础知识和入门教程：</p> 
<h1>什么是爬虫？</h1> 
<p>爬虫就是自动获取网页内容的程序，例如搜索引擎，Google，Baidu 等，每天都运行着庞大的爬虫系统，从全世界的网站中爬虫数据，供用户检索时使用。</p> 
<h1><strong>爬虫流程</strong></h1> 
<p>其实把网络爬虫抽象开来看，它无外乎包含如下几个步骤</p> 
<ul>
<li>模拟请求网页。模拟浏览器，打开目标网站。</li>
<li>获取数据。打开网站之后，就可以自动化的获取我们所需要的网站数据。</li>
<li>保存数据。拿到数据之后，需要持久化到本地文件或者数据库等存储设备中。</li>
</ul> 
<p>那么我们该如何使用 Python 来编写自己的爬虫程序呢，在这里我要重点介绍一个 Python 库：Requests。</p> 
<h1><strong>Requests 使用</strong></h1> 
<p>Requests 库是 Python 中发起 HTTP 请求的库，使用非常方便简单。</p> 
<p><strong>模拟发送 HTTP 请求</strong></p> 
<p>发送 GET 请求</p> 
<p>当我们用浏览器打开豆瓣首页时，其实发送的最原始的请求就是 GET 请求</p> 
<pre><code class="language-python">import requests
res = requests.get('http://www.douban.com')
print(res)
print(type(res))
&gt;&gt;&gt;
&lt;Response [200]&gt;
&lt;class 'requests.models.Response'&gt;</code></pre> 
<blockquote> 
 <p>可以看到，我们得到的是一个 Response 对象</p> 
</blockquote> 
<p>如果我们要获取网站返回的数据，可以使用 text 或者 content 属性来获取</p> 
<p>text：是以字符串的形式返回数据</p> 
<p>content：是以二进制的方式返回数据</p> 
<pre><code class="language-python">print(type(res.text))
print(res.text)
&gt;&gt;&gt;
&lt;class 'str'&gt; &lt;!DOCTYPE HTML&gt;
&lt;html lang="zh-cmn-Hans" class=""&gt;
&lt;head&gt;
&lt;meta charset="UTF-8"&gt;
&lt;meta name="google-site-verification" content="ok0wCgT20tBBgo9_zat2iAcimtN4Ftf5ccsh092Xeyw" /&gt;
&lt;meta name="description" content="提供图书、电影、音乐唱片的推荐、评论和价格比较，以及城市独特的文化生活。"&gt;
&lt;meta name="keywords" content="豆瓣,广播,登陆豆瓣"&gt;.....</code></pre> 
<p>发送 POST 请求</p> 
<p>对于 POST 请求，一般就是提交一个表单</p> 
<pre><code>r = requests.post('http://www.xxxx.com', data={"key": "value"})</code></pre> 
<p> </p> 
<blockquote> 
 <p>data 当中，就是需要传递的表单信息，是一个字典类型的数据。</p> 
</blockquote> 
<p><strong>header 增强</strong></p> 
<p>对于有些网站，会拒绝掉没有携带 header 的请求的，所以需要做一些 header 增强。比如：UA，Cookie，host 等等信息。</p> 
<pre><code class="language-python">header = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36",
         "Cookie": "your cookie"}</code></pre> 
<h1>Python爬虫入门教程！手把手教会你爬取网页数据</h1> 
<p>2020-10-25 14:56·<a href="https://www.toutiao.com/c/user/token/MS4wLjABAAAA1wP28vXxz_xaC3_z7SNwPPdJJ0eAnDwatvX5N9BOZK8/?source=tuwen_detail" title="数据分析不是个事儿">数据分析不是个事儿</a></p> 
<p>其实在当今社会，网络上充斥着大量有用的数据，我们只需要耐心的观察，再加上一些技术手段，就可以获取到大量的有价值数据。这里的“技术手段”就是网络爬虫。今天就给大家分享一篇爬虫基础知识和入门教程：</p> 
<h1>什么是爬虫？</h1> 
<p>爬虫就是自动获取网页内容的程序，例如搜索引擎，Google，Baidu 等，每天都运行着庞大的爬虫系统，从全世界的网站中爬虫数据，供用户检索时使用。</p> 
<h1><strong>爬虫流程</strong></h1> 
<p>其实把网络爬虫抽象开来看，它无外乎包含如下几个步骤</p> 
<ul>
<li>模拟请求网页。模拟浏览器，打开目标网站。</li>
<li>获取数据。打开网站之后，就可以自动化的获取我们所需要的网站数据。</li>
<li>保存数据。拿到数据之后，需要持久化到本地文件或者数据库等存储设备中。</li>
</ul> 
<p>那么我们该如何使用 Python 来编写自己的爬虫程序呢，在这里我要重点介绍一个 Python 库：Requests。</p> 
<h1><strong>Requests 使用</strong></h1> 
<p>Requests 库是 Python 中发起 HTTP 请求的库，使用非常方便简单。</p> 
<p><strong>模拟发送 HTTP 请求</strong></p> 
<p>发送 GET 请求</p> 
<p>当我们用浏览器打开豆瓣首页时，其实发送的最原始的请求就是 GET 请求</p> 
<pre><code>import requests
res = requests.get('http://www.douban.com')
print(res)
print(type(res))
&gt;&gt;&gt;
&lt;Response [200]&gt;
&lt;class 'requests.models.Response'&gt;
</code></pre> 
<blockquote> 
 <p>可以看到，我们得到的是一个 Response 对象</p> 
</blockquote> 
<p>如果我们要获取网站返回的数据，可以使用 text 或者 content 属性来获取</p> 
<p>text：是以字符串的形式返回数据</p> 
<p>content：是以二进制的方式返回数据</p> 
<pre><code>print(type(res.text))
print(res.text)
&gt;&gt;&gt;
&lt;class 'str'&gt; &lt;!DOCTYPE HTML&gt;
&lt;html lang="zh-cmn-Hans" class=""&gt;
&lt;head&gt;
&lt;meta charset="UTF-8"&gt;
&lt;meta name="google-site-verification" content="ok0wCgT20tBBgo9_zat2iAcimtN4Ftf5ccsh092Xeyw" /&gt;
&lt;meta name="description" content="提供图书、电影、音乐唱片的推荐、评论和价格比较，以及城市独特的文化生活。"&gt;
&lt;meta name="keywords" content="豆瓣,广播,登陆豆瓣"&gt;.....
</code></pre> 
<p>发送 POST 请求</p> 
<p>对于 POST 请求，一般就是提交一个表单</p> 
<pre><code>r = requests.post('http://www.xxxx.com', data={"key": "value"})
</code></pre> 
<blockquote> 
 <p>data 当中，就是需要传递的表单信息，是一个字典类型的数据。</p> 
</blockquote> 
<p><strong>header 增强</strong></p> 
<p>对于有些网站，会拒绝掉没有携带 header 的请求的，所以需要做一些 header 增强。比如：UA，Cookie，host 等等信息。</p> 
<pre><code>header = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36",
         "Cookie": "your cookie"}
res = requests.get('http://www.xxx.com', headers=header)
</code></pre> 
<p><strong>解析 HTML</strong></p> 
<p>现在我们已经获取到了网页返回的数据，即 HTML 代码，下面就需要解析 HTML，来提取其中有效的信息。</p> 
<p><strong>BeautifulSoup</strong></p> 
<p>BeautifulSoup 是 Python 的一个库，最主要的功能是从网页解析数据。</p> 
<pre><code class="language-python">from bs4 import BeautifulSoup  # 导入 BeautifulSoup 的方法
# 可以传入一段字符串，或者传入一个文件句柄。一般都会先用 requests 库获取网页内容，然后使用 soup 解析。
soup = BeautifulSoup(html_doc,'html.parser')  # 这里一定要指定解析器，可以使用默认的 html，也可以使用 lxml。
print(soup.prettify())  # 按照标准的缩进格式输出获取的 soup 内容。</code></pre> 
<p>BeautifulSoup 的一些简单用法 </p> 
<pre><code class="language-python">print(soup.title)  # 获取文档的 title
print(soup.title.name)  # 获取 title 的 name 属性
print(soup.title.string)  # 获取 title 的内容
print(soup.p)  # 获取文档中第一个 p 节点
print(soup.p['class'])  # 获取第一个 p 节点的 class 内容
print(soup.find_all('a'))  # 获取文档中所有的 a 节点，返回一个 list
print(soup.find_all('span', attrs={'style': "color:#ff0000"}))  # 获取文档中所有的 span 且 style 符合规则的节点，返回一个 list</code></pre> 
<p>具体的用法和效果，我会在后面的实战中详细说明。</p> 
<p><strong>XPath 定位</strong></p> 
<p>XPath 是 XML 的路径语言，是通过元素和属性进行导航定位的。几种常用的表达式</p> 
<p>表达式含义node选择 node 节点的所有子节点/从根节点选取//选取所有当前节点.当前节点..父节点@属性选取text()当前路径下的文本内容</p> 
<p>一些简单的例子</p> 
<pre><code class="language-python">xpath('node')  # 选取 node 节点的所有子节点
xpath('/div')  # 从根节点上选取 div 元素
xpath('//div')  # 选取所有 div 元素
xpath('./div')  # 选取当前节点下的 div 元素
xpath('//@id')  # 选取所有 id 属性的节点</code></pre> 
<p>  当然，XPath 非常强大，但是语法也相对复杂，不过我们可以通过 Chrome 的开发者工具来快速定位到元素的 xpath，如下图</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/07/dc/H10fTYfy_o.jpg"></p> 
<p>得到的 xpath 为</p> 
<pre><code>//*[@id="anony-nav"]/div[1]/ul/li[1]/a
</code></pre> 
<p>在实际的使用过程中，到底使用 BeautifulSoup 还是 XPath，完全取决于个人喜好，哪个用起来更加熟练方便，就使用哪个。</p> 
<h1>爬虫实战：爬取豆瓣海报</h1> 
<p>我们可以从豆瓣影人页，进入都影人对应的影人图片页面，比如以刘涛为例子，她的影人图片页面地址为</p> 
<blockquote> 
 <p>https://movie.douban.com/celebrity/1011562/photos/</p> 
</blockquote> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/66/66/EGkrvZEo_o.jpg"></p> 
<p>下面我们就来分析下这个网页</p> 
<h1><strong>目标网站页面分析</strong></h1> 
<p>注意：网络上的网站页面构成总是会变化的，所以这里你需要学会分析的方法，以此类推到其他网站。正所谓授人以鱼不如授人以渔，就是这个原因。</p> 
<h1><strong>Chrome 开发者工具</strong></h1> 
<p>Chrome 开发者工具（按 F12 打开），是分析网页的绝佳利器，一定要好好使用。</p> 
<p>我们在任意一张图片上右击鼠标，选择“检查”，可以看到同样打开了“开发者工具”，而且自动定位到了该图片所在的位置</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/40/0f/pg9MPB9l_o.png"></p> 
<p>可以清晰的看到，每张图片都是保存在 li 标签中的，图片的地址保存在 li 标签中的 img 中。</p> 
<p>知道了这些规律后，我们就可以通过 BeautifulSoup 或者 XPath 来解析 HTML 页面，从而获取其中的图片地址。</p> 
<h1><strong>代码编写</strong></h1> 
<p>我们只需要短短的几行代码，就能完成图片 url 的提取</p> 
<pre><code>import requests
from bs4 import BeautifulSoup 

url = 'https://movie.douban.com/celebrity/1011562/photos/'
res = requests.get(url).text
content = BeautifulSoup(res, "html.parser")
data = content.find_all('div', attrs={'class': 'cover'})
picture_list = []
for d in data:
    plist = d.find('img')['src']
    picture_list.append(plist)
print(picture_list)
&gt;&gt;&gt;
['https://img1.doubanio.com/view/photo/m/public/p2564834267.jpg', 'https://img1.doubanio.com/view/photo/m/public/p860687617.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2174001857.jpg', 'https://img1.doubanio.com/view/photo/m/public/p1563789129.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2363429946.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2382591759.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2363269182.jpg', 'https://img1.doubanio.com/view/photo/m/public/p1959495269.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2356638830.jpg', 'https://img3.doubanio.com/view/photo/m/public/p1959495471.jpg', 'https://img3.doubanio.com/view/photo/m/public/p1834379290.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2325385303.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2361707270.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2325385321.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2196488184.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2186019528.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2363270277.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2325240501.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2258657168.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2319710627.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2319710591.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2311434791.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2363270708.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2258657185.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2166193915.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2363265595.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2312085755.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2311434790.jpg', 'https://img3.doubanio.com/view/photo/m/public/p2276569205.jpg', 'https://img1.doubanio.com/view/photo/m/public/p2165332728.jpg']</code></pre> 
<p>可以看到，是非常干净的列表，里面存储了海报地址。<br> 但是这里也只是一页海报的数据，我们观察页面发现它有好多分页，如何处理分页呢。</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/eb/8f/133G15AS_o.png"></p> 
<h1><strong>分页处理</strong></h1> 
<p>我们点击第二页，看看浏览器 url 的变化</p> 
<blockquote> 
 <p>https://movie.douban.com/celebrity/1011562/photos/?type=C&amp;start=30&amp;sortby=like&amp;size=a&amp;subtype=a</p> 
</blockquote> 
<p>发现浏览器 url 增加了几个参数</p> 
<p>再点击第三页，继续观察 url</p> 
<blockquote> 
 <p>https://movie.douban.com/celebrity/1011562/photos/?type=C&amp;start=60&amp;sortby=like&amp;size=a&amp;subtype=a</p> 
</blockquote> 
<p>通过观察可知，这里的参数，只有 start 是变化的，即为变量，其余参数都可以按照常理来处理</p> 
<p>同时还可以知道，这个 start 参数应该是起到了类似于 page 的作用，start = 30 是第二页，start = 60 是第三页，依次类推，最后一页是 start = 420。</p> 
<p>于是我们处理分页的代码也呼之欲出了</p> 
<p>首先将上面处理 HTML 页面的代码封装成函数</p> 
<pre><code>def get_poster_url(res):
    content = BeautifulSoup(res, "html.parser")
    data = content.find_all('div', attrs={'class': 'cover'})
    picture_list = []
    for d in data:
        plist = d.find('img')['src']
        picture_list.append(plist)
    return picture_list</code></pre> 
<p>然后我们在另一个函数中处理分页和调用上面的函数</p> 
<pre><code>def fire():
    page = 0
    for i in range(0, 450, 30):
        print("开始爬取第 %s 页" % page)
        url = 'https://movie.douban.com/celebrity/1011562/photos/?type=C&amp;start={}&amp;sortby=like&amp;size=a&amp;subtype=a'.format(i)
        res = requests.get(url).text
        data = get_poster_url(res)
        page += 1
</code></pre> 
<p>此时，我们所有的海报数据都保存在了 data 变量中，现在就需要一个下载器来保存海报了</p> 
<pre><code>def download_picture(pic_l):
    if not os.path.exists(r'picture'):
        os.mkdir(r'picture')
    for i in pic_l:
        pic = requests.get(i)
        p_name = i.split('/')[7]
        with open('picture\' + p_name, 'wb') as f:
            f.write(pic.content)
</code></pre> 
<p>再增加下载器到 fire 函数，此时为了不是请求过于频繁而影响豆瓣网的正常访问，设置 sleep time 为1秒</p> 
<pre><code>def fire():
    page = 0
    for i in range(0, 450, 30):
        print("开始爬取第 %s 页" % page)
        url = 'https://movie.douban.com/celebrity/1011562/photos/?type=C&amp;start={}&amp;sortby=like&amp;size=a&amp;subtype=a'.format(i)
        res = requests.get(url).text
        data = get_poster_url(res)
        download_picture(data)
        page += 1
        time.sleep(1)
</code></pre> 
<p>下面就执行 fire 函数，等待程序运行完成后，当前目录下会生成一个 picture 的文件夹，里面保存了我们下载的所有海报</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/70/80/GpRUYRrv_o.jpg"></p> 
<p> </p> 
<h1>核心代码讲解</h1> 
<p>下面再来看下完整的代码</p> 
<pre><code>import requests
from bs4 import BeautifulSoup
import time
import osdef fire():
    page = 0
    for i in range(0, 450, 30):
        print("开始爬取第 %s 页" % page)
        url = 'https://movie.douban.com/celebrity/1011562/photos/?type=C&amp;start={}&amp;sortby=like&amp;size=a&amp;subtype=a'.format(i)
        res = requests.get(url).text
        data = get_poster_url(res)
        download_picture(data)
        page += 1
        time.sleep(1)def get_poster_url(res):
    content = BeautifulSoup(res, "html.parser")
    data = content.find_all('div', attrs={'class': 'cover'})
    picture_list = []
    for d in data:
        plist = d.find('img')['src']
        picture_list.append(plist)
    return picture_listdef download_picture(pic_l):
    if not os.path.exists(r'picture'):
        os.mkdir(r'picture')
    for i in pic_l:
        pic = requests.get(i)
        p_name = i.split('/')[7]
        with open('picture\' + p_name, 'wb') as f:
            f.write(pic.content)if __name__ == '__main__':
    fire()
</code></pre> 
<p><strong>fire 函数</strong></p> 
<p>这是一个主执行函数，使用 range 函数来处理分页。</p> 
<ul>
<li>range 函数可以快速的创建整数列表，在 for 循环时及其好用。函数中的0代表从0开始计数，450代表一直迭代到450，不包含450，30代表步长，即每次递增的数字间隔。range(0, 450, 30)，依次会输出：0，30，60，90 …</li>
<li>format 函数，是一种字符串格式化方式</li>
<li>time.sleep(1) 即为暂停1秒钟</li>
</ul> 
<p><strong>get_poster_url 函数</strong></p> 
<p>这个就是解析 HTML 的函数，使用的是 BeautifulSoup</p> 
<ul>
<li>通过 find_all 方法查找所有 class 为 “cover” 的 div 元素，返回的是一个列表</li>
<li>使用 for 循环，循环上一步拿到的列表，取出 src 的内容，append 到列表中</li>
<li>append 是列表的一个方法，可以在列表后面追加元素</li>
</ul> 
<p><strong>download_picture 函数</strong></p> 
<p>简易图片下载器</p> 
<ul>
<li>首先判断当前目录下是否存在 picture 文件夹，os.path.exists</li>
<li>os 库是非常常用用来操作系统相关的命令库，os.mkdir 就是创建文件夹</li>
<li>split 用于切割字符串，取出角标为7的元素，作为存储图片的名称</li>
<li>with 方法用来快速打开文件，打开的进程可以自行关闭文件句柄，而不再需要手动执行 f.close() 关闭文件</li>
</ul> 
<h1>总结</h1> 
<p>本节讲解了爬虫的基本流程以及需要用到的 Python 库和方法，并通过一个实际的例子完成了从分析网页，到数据存储的全过程。其实爬虫，无外乎模拟请求，解析数据，保存数据。</p> 
<p>当然有的时候，网站还会设置各种反爬机制，比如 cookie 校验，请求频度检查，非浏览器访问限制，JS 混淆等等，这个时候就需要用到反反爬技术了，比如抓取 cookie 放到 headers 中，使用代理 IP 访问，使用 Selenium 模拟浏览器等待方式。</p> 
<p>由于本课程不是专门的爬虫课，这些技能就留待你自己去探索挖掘啦。</p> 
<p style="text-align:center"><img alt="" src="https://images2.imgbox.com/c9/33/5LIvofuS_o.png"></p> 
<p> </p> 
<p><strong> 作为过来人，跟大家聊一聊我的自学心得，希望可以帮助大家少走弯路，少踩坑。</strong></p> 
<p><strong>更多Python、爬虫、人工智能配套视频教程+书籍可以+v 免费领取。</strong></p> 
<p> <img alt="" height="396" src="https://images2.imgbox.com/37/8c/pNj4Mx8j_o.jpg" width="396"></p> 
<p><strong>对方向选择、学习规划、学习路线、职业发展方面有问题的可以加群：809160367</strong></p> 
<p><img alt="" src="https://images2.imgbox.com/0b/d0/Zp1J4KwO_o.png"></p> 
<p> <img alt="" height="167" src="https://images2.imgbox.com/41/ba/sJPDfYoq_o.jpg" width="720"></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>