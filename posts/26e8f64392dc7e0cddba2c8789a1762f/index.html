<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Geoffrey Hinton：我的五十年深度学习生涯与研究心法 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Geoffrey Hinton：我的五十年深度学习生涯与研究心法</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <div id="js_content"> 
 <p><img alt="a64a9369f0acdffa74c338bdb052c14d.png" src="https://images2.imgbox.com/16/db/W7gYvkHI_o.png" width="1024"></p> 
 <p><strong>来源｜The Robot Brains Podcast</strong></p> 
 <p><strong>翻译｜沈佳丽、程浩源、许菡如、胡燕君、贾川</strong></p> 
 <p>他从未正式上过计算机课程，本科在剑桥大学读的是生理学和物理学，期间曾转向哲学，但最终拿到的却是心理学方向的学士学位；他曾因为一度厌学去做木匠，但遇挫后还是回到爱丁堡大学，并拿到“冷门专业”人工智能方向的博士学位；数学不好让他在做研究时倍感绝望，当了教授之后，对于不懂的神经科学和计算科学知识，他也总要请教自己手下的研究生。</p> 
 <p>学术道路看似踉踉跄跄，但Geoffrey Hinton却成了笑到最后的那个人，他被誉为“深度学习教父”，并且获得了计算机领域的最高荣誉“图灵奖”。</p> 
 <p>Hinton出身在英国的科学“豪门”，但他一生所经历的学术生涯和坎坷生活丰富而离奇。</p> 
 <p>他的父亲Howard Everest Hinton是英国昆虫学家，母亲Margaret是一名教师，他们都是共产主义者。他的叔叔是著名的经济学家Colin Clark，发明了“国民生产总值”这个经济术语，他的高祖父是著名的逻辑学家George Boole，他发明的布尔代数奠定了现代计算机科学的基础。</p> 
 <p>在厚重的科学家家族底蕴熏陶下，Hinton从小拥有独立思考能力以及坚韧品质，并且肩负着继承家族荣誉的担子。母亲给了他两种选择，“要么成为一名学者，要么做个失败者。”他没有理由选择躺平，即便读大学时几经折腾，但他还是完成了学业。</p> 
 <p>1973年，在英国爱丁堡大学，他师从Langer Higgins攻读人工智能博士学位，但那时几乎没人相信神经网络，导师也劝他放弃研究这项技术。周遭的质疑并不足以动摇他对神经网络的坚定信念，在随后的十年，他接连提出了反向传播算法、玻尔兹曼机，不过他还要再等数十年才会等到深度学习迎来大爆发，到时他的这些研究将广为人知。</p> 
 <p>博士毕业后，Hinton的生活也历经困苦。他和第一任妻子Ros（分子生物学家）前往美国，并在卡耐基梅隆大学获得教职，不过，由于对里根政府存有不满，以及在人工智能研究基本由美国国防部支持的情况下，他们于1987年前往加拿大，Hinton开始在多伦多大学计算机科学学院任教，并在加拿大高级研究所CIFAR开展机器和大脑学习项目的研究。</p> 
 <p>不幸的是，1994年，妻子Ros因卵巢癌逝世，Hinton只能独自抚养由他们收养的两个年幼的孩子，其中，儿子还患有注意力缺陷多动症（ADHD）和其他学习障碍。后来，他与现任妻子Jackie（艺术史学家）再婚，但类似的打击再度逼近，Jackie前几年也患上了癌症。</p> 
 <p>他本人也患有严重的腰椎疾病，这让他无法像正常人那样坐下，日常的大部分时间都必须站立工作，由此他也排斥坐飞机，因为起飞降落时都要求必须坐直，这也限制了他去外地做学术报告。</p> 
 <p><img alt="551d9f50be8f695fb595797a3ec074e2.png" src="https://images2.imgbox.com/07/a1/Ub8dTxlp_o.png" width="940"></p> 
 <p style="text-align:center"><em>（从左到右依次为Ilya Sutskever、Alex Krizhevsky和Geoffrey Hinton）</em></p> 
 <p>经过近半个世纪的技术坚守和生活磨砺，终于，2012年曙光乍现，他与学生Alex Krizhevsky、<a href=""><strong>Ilya Sutskever</strong></a>提出的AlexNet震动业界，就此重塑了计算机视觉领域，启动了新一轮深度学习的黄金时代。</p> 
 <p>也是在2012年底，他与这两位学生成立了三人组公司DNN-research，并将其以4400万美元的“天价”卖给了Google，他也从学者身份转变为Google副总裁、Engineering Fellow。</p> 
 <p>2019年，非计算机科班出身的AI教授Hinton，与Yoshua Bengio、Yann LeCun共同获得了图灵奖。</p> 
 <p>饱经风霜之后，这位已经74岁的“深度学习教父”依然奋战在AI研究一线，他不惮于其他学者发出的质疑，也会坦然承认那些没有实现的判断和预言。不管怎样，他仍然相信，在深度学习崛起十年之后，这一技术会继续释放它的能量，而他也在思索和寻找下一个突破点。</p> 
 <p><strong>那么，他对神经网络的坚定信念源自何处？在如今深度学习“撞墙了”的质疑声中，他如何看待下一阶段AI的发展？对于年轻一代的AI研究者们，他又有怎样的寄语？</strong></p> 
 <p>近期，在Pieter Abbeel主持的The Robot Brains Podcast节目中，Hinton非常坦诚地分享了自己的学术生涯，深度学习的未来和研究心得，以及拍卖DNN-research的内幕。以下是他的讲述，由OneFlow社区编译。</p> 
 <p style="text-align:center"><img alt="8113dc8bab7a42f4cf60ae027ef485ad.png" src="https://images2.imgbox.com/c8/8c/1bqzb3K8_o.png"></p> 
 <p style="text-align:center"><em>（8岁的Hinton）</em></p> 
 <p>对我影响最为深远的是童年时所接受的教育。我的家庭没有宗教信仰，父亲是一名共产主义者，但考虑到私立学校的科学教育比较好，7岁时，他坚持送我去一所昂贵的基督教私立学校读书，除了我，那里所有的孩子都相信上帝。</p> 
 <p>一回到家，家人就说宗教都是胡扯，当然，可能因为我有较强的自我意识，我自己也不信，意识到信仰上帝是错的，并且养成了质疑别人的习惯。当然，多年之后，他们也确实发现自己当初的信仰是错的，并意识到上帝也许并不是真的存在。</p> 
 <p>不过，如果我现在告诉你要有信仰，信仰很重要，可能听起来很讽刺，<strong>但我们确实需要对科学研究要有信仰，这样即使别人说你是错的，你也能在那条正确的道路上坚持走下去。</strong></p> 
 <p style="text-align:center"><strong>1</strong></p> 
 <p style="text-align:center"><strong>1970年代，研究神经网络“孤勇者”</strong></p> 
 <p>我的教育背景很丰富。在剑桥大学读大一那年，我是唯一一位同时修读物理学和生理学的学生，这为我后来的科研生涯奠定了一定的理工科基础。</p> 
 <p>不过，我数学不太好，只好放弃学物理，可是，我又很好奇生命的意义，于是转而去学哲学，取得一定成绩后，又开始修读心理学。</p> 
 <p>在剑桥的最后一年，我过得很艰难，也不开心，所以一考完试就退学，去做一名木匠。其实，相比于做其他事情，我更喜欢做一名木匠。</p> 
 <p>高中时，白天上完课后，我回家就会做一些木工活，那是我最开心的时刻。慢慢地，我就成为了一名木匠，但大概做了六个月左后，就发现木匠挣得钱少得难以维生，尽管木匠需要做的事情远比表面看到的要多。搞装修轻松得多，来钱快，所以在做木匠的同时，我也会兼职装修活计。除非你是一名高级木匠，否则做木匠赚的钱肯定不如做装修。</p> 
 <p>直到一天，我遇到了一名真正出色的木匠，才意识到自己不适合这行当。一家煤炭公司让这位木匠给阴暗潮湿的地下室做一扇门，鉴于环境特殊，他就将木料以反方向排列，以此来抵消木料因潮湿膨胀产生的变形，这是我此前从未想过的方式。他还可以用手锯将一块木料切成正方形。他向我讲解道：要是想将木料切成正方形，那么你必须将锯床和木料跟房间对齐。</p> 
 <p>当时我就感觉，跟他相比自己差得太远了，就想或许还是回学校研究人工智能吧。</p> 
 <p>后来，我就去爱丁堡大学攻读神经网络的博士，导师是著名的Christopher Longute-Higgins教授。30多岁时，他就弄清了硼氢化物的结构，差点因此获得诺贝尓奖，真的很厉害。直到现在，我仍然不清楚他研究的是什么，只知道是跟量子力学有关，这项研究的事实基础是“恒等算子的旋转不是360度，而是720度”。</p> 
 <p>他曾经对神经网络和全息图之间的关系很感兴趣，只是在我到爱丁堡大学后，他突然对神经网络失去了兴趣，主要是因为他读了Winograd（美国计算机科学家）的论文后被彻底说服了，认为神经网络没有发展前景，而是应该转做符号人工智能，那篇论文对他影响挺大的。</p> 
 <p>事实上，他并不赞同我的研究方向，想让我做一些更容易获奖的研究，但他的为人不错，仍然告诉我要坚定自己的方向，也从未阻止我去研究神经网络。</p> 
 <p><img alt="15ab4c0e244258f7b16f77f0ff0681f0.png" src="https://images2.imgbox.com/f0/8e/uqWA77ji_o.png" width="790"></p> 
 <p style="text-align:center"><em>（Marvin Minsky和Seymour Papert）</em></p> 
 <p>1970年代初，身边的所有人都质问我，Marvin Minsky和Seymour Papert都说神经网络前途渺茫，为什么还要坚持下去？说实话，我感觉很孤独。</p> 
 <p>1973年，我第一次给一个小组做演讲，内容就是关于如何用神经网络做真正的递归。在第一个项目中，我发现，如果你想让神经网络绘制图形，将图形分割成多个部分，并且这些图形的部分都能被类似的神经硬件绘制出来，那么储存整个图形的神经中枢就需要记住整体图形的位置、方向和大小。</p> 
 <p>如果正在绘制图形的神经网络突然停止运行了，你想使用另一个神经网络来继续绘制图形，那么就需要有地方来存储这个图形以及工作进度，然后可以继续绘制工作。现在的难点在于，如何使神经网络实现这些功能。显然，仅仅靠复制神经元是不行的，因此我想设计一个系统通过快速权重（fast weight）来实时适配并记录工作进度。如此一来，通过恢复相关状态（state），就可以继续完成任务。</p> 
 <p>因此，我创建了一套神经网络，通过重用相同的神经元和权重来执行递归调用（就像用于高级调用一样），以此来实现真正的递归。但是，我不擅长演讲，所以感觉可能并没有人理解我演讲的内容。</p> 
 <p>他们说，明明可以使用Lisp递归，为什么要在神经网络中进行递归。他们不知道的是，<strong>除非神经网络能够实现递归之类的功能，否则有一大堆事情无法解决。</strong>现在，这又成为了一个有趣的问题，所以我还要再等一年，直到这个问题成为一个拥有50年历史的古董，然后我写了一份关于快速权重的研究报告。</p> 
 <p>那时，也不是所有人都反对神经网络。如果再往前追溯到1950年代，如冯·诺依曼和图灵这样的研究者还是很相信神经网络，他们都对大脑的工作方式很感兴趣，特别是图灵，很相信神经网络的强化训练，这也让我对自己的研究方向很有信心。</p> 
 <p>可惜他们英年早逝，若是能多活几年，他们的智慧足以影响一个领域的发展，英国在这方面可能早已取得突破，说不定人工智能的现状也会大有不同。</p> 
 <p style="text-align:center"><strong>2</strong></p> 
 <h2></h2> 
 <p style="text-align:center"><strong>从纯粹的学者转变为Google员工</strong></p> 
 <p>去Google工作的主要原因是，我的儿子患有残疾，我得为他挣钱。</p> 
 <p>2012年，我觉得在Coursera上讲课能挣到很多钱，所以就开设了神经网络相关课程。早期的Coursera软件并不好用，加上我自己并不太擅长操做软件，因此我时常感到烦躁。</p> 
 <p>最初我与多伦多大学达成了一项协议，如果这些课程能赚到钱的话，那么大学会把到手的钱分一部分给讲课老师。虽然他们没有明确说具体的分成比例，但有人说是对半分，我也就欣然接受了。</p> 
 <p>在录课过程中，我曾要求过学校帮我录制视频，但他们却反问我，“你知道制作视频有多贵吗？”我当然知道，因为我自己一直在制作视频，校方还是没有提供任何支持。然而在我开课之后（当时我已经骑虎难下了），教务长在没有咨询我和其他任何人的情况下就单方面决定学校会拿走所有的钱，而我则一分钱也拿不到，这就完全违反了当初的协议。</p> 
 <p>他们让我好好录课，并说那本就是我教学工作的一部分，但那实际上并不属于我的教学范畴，而只是基于我之前做过的相关讲座的课程。因此，我在后续的教学工作中再也没有用过Coursera。那件事让我很生气，甚至开始考虑是否要从事其他的职业。</p> 
 <p>就在此时，突然有很多公司向我们抛出了橄榄枝，愿意赞助一大笔经费，或者支持我们创立一家公司，这说明还是有很多公司对于我们的研究内容很感兴趣。</p> 
 <p>鉴于州政府已经给过我们一笔研究经费，我们也不再想赚外快，还是把精力放在自己的研究上。但那次学校骗我赚钱的经历不禁让我萌生想多赚点钱的想法，所以后来把成立不久的DNN-research拍卖了。</p> 
 <p>这桩买卖发生在2012年12月的NIPS（神经信息处理系统大会）期间，会议在塔霍湖边的一个娱乐场所举办，地下室里灯光闪耀，一群光着膀子的赌徒在烟雾缭绕的房间里尽情高呼，“你赢了25000，这些都是你的”......与此同时，楼上进行拍卖一家公司。</p> 
 <p>当时就像在演电影，与社交媒体上看到的情形一模一样，真的很棒。我们之所以拍卖公司，是因为我们完全不知道自身的价值，所以我就咨询了一个知识产权方面的律师，他说，现在有两个办法：一是直接雇一名专业的谈判员去和那些大公司谈判，但这可能会遇到不愉快；二是发起一场竞拍。</p> 
 <p>据我所知，像我们这样的小公司进行拍卖在历史上还是第一次。最终我选择通过Gmail进行竞拍，因为那年夏天我一直在Google工作，我知道他们不会随意窃取用户的邮件，即使到现在，我还是这样认为的。但对于我们这一决定，微软表现出不满。</p> 
 <p>拍卖过程如下：参与竞拍的公司必须通过Gmail将他们的报价发给我们，然后我们再将其连同Gmail的时间戳发送给其他参与者。起拍价为50万美元，然后有人出价100万美元，看到竞价不断上涨时，我们真是太高兴了，同时也意识到我们的价值远比预想的要高。当竞价达到一定程度时（当时我们认为是天文数字了），我们更倾向于在Google工作，于是叫停了拍卖。</p> 
 <p>来Google工作是一个正确的选择，到现在我在这儿工作了九年。等我在这里工作满十年，他们应该会给我颁个奖，毕竟在这儿工作这么久的人屈指可数。</p> 
 <p>相比其他公司，人们都更喜欢在Google工作，我也一样。我喜欢这家公司的主要原因是Google Brain团队很棒。我更专注于研究如何构建大型学习系统和研究大脑的工作机制，Google Brain不仅有研究大型系统所需要的丰富资源，还能跟众多优秀人才交流学习。</p> 
 <p>我属于那种直性子，而Jeff Dean是一个聪明人，跟他相处很愉快。他想让我做一些基础研究，尝试提出新的算法，而这正是我喜欢做的事。我不擅长管理大型团队，相比之下，我更乐意将语言识别的精度提升一个百分点，为这个领域带来一场新的变革是我一直想做的事。</p> 
 <p style="text-align:center"><strong>3</strong></p> 
 <h2></h2> 
 <p style="text-align:center"><strong>深度学习的下一个大事件</strong></p> 
 <p>深度学习的发展取决于，在拥有海量数据和强大算力的大型网络中做随机梯度下降，基于此，一些想法得以更好地生根发芽，比如随机失活（dropout）和现在的很多研究，但这一切离不开强大算力、海量数据以及随机梯度下降。 </p> 
 <p><strong>经常有人说深度学习遇到了瓶颈，但事实上它一直在不断向前发展，我希望怀疑论者能将深度学习现在不能做的事写下来。</strong>五年后，我们会证明深度学习能做到这些事。</p> 
 <p>当然，这些任务必须经过严格定义。比如Hector Levesque（多伦多大学计算机系教授）是一个典型的AI人士，他本人非常优秀。Hector制定了一个标准，即Winograd句子，其中一个例子是，“奖杯不适合放在手提箱中，因为它太小了；奖杯不适合放在手提箱里，因为它太大了。”</p> 
 <p>如果你想把这两句翻译成法语，必须明白在第一种情况下，“它”指的是手提箱，而在第二种情况下，“它”指的是奖杯，因为它们在法语中是不同的性数（genders ），而且早期的神经网络机器翻译是随机的，所以当机器把上述句子翻译成法语时，机器无法正确识别性数。但这种情况一直在改进，至少Hector给神经元下了一个非常明确的定义，指出神经元可以做什么。虽然做的并不完美，但这样至少比随机翻译要好得多。我希望怀疑论者能提出更多类似的质疑。</p> 
 <p>我认为，深度学习这种非常成功的范式将继续保持繁荣：即根据一些目标函数的梯度来调整大量的实值参数，但我们很可能不会使用反向传播机制来获得梯度，而目标函数可能会更加局部和分散。</p> 
 <p><strong>我个人猜测，下一个AI大事件肯定是脉冲神经网络的学习算法。</strong>它能够解决是否进行脉冲的离散判定，以及何时进行脉冲的连续性决策，这样就可以利用脉冲时间来进行有趣的计算，这在非脉冲神经网络中其实很难做到。之前没能深入研究脉冲神经网络的学习算法，这是我研究生涯的一大遗憾。</p> 
 <p>我没打算研究AGI，也尽量避免定义什么是AGI，因为AGI愿景背后有各种各样的问题，而仅仅通过扩大带参数的神经元数量或神经连接还无法实现通用人工智能。</p> 
 <p>AGI设想了一个类似人类的智能机器人，它和人类一样聪明。我不认为智能一定会这样发展，而是希望它更多地以共生方式发展。我认为，也许我们会设计出智能计算机，但它们不会像人类一样拥有自主意识。如果它们的目的是用来杀死其他人，那它们可能必须得有自主意识，但希望我们不会往那个方向发展。</p> 
 <p style="text-align:center">4</p> 
 <h2></h2> 
 <p style="text-align:center"><strong>相信研究直觉，好奇心驱动</strong></p> 
 <p>每个人的思维方式都有所不同，我们不一定了解自己的思维过程。<strong>我喜欢按直觉行事，更倾向于在做研究时运用类比，我认为，人类推理的基本方式是基于在大向量中利用正确的特征来进行类比，我本人也是这样做研究的。</strong></p> 
 <p>我经常在电脑上对某一研究反复进行试验，来看看哪些有用，哪些没用。弄清事物的数学底层逻辑和进行基础研究确实很重要，进行一些论证也很有必要，但这些不是我想做的事。</p> 
 <p>做一个小测试：假如现在NIPS会议上有两场讲座，一场是关于用一种全新、聪明和优雅的方法来证明一项已知的结论；另一场则是关于一种新的、强大的学习算法，但算法背后的逻辑暂时无人知晓。</p> 
 <p>如果你必须在这两场讲座中选择一场去听讲座，你会做何选择？相比第二场讲座，第一场可能更容易被人们所接受，大家似乎更好奇证明已知事物的新方法，但我会去听第二场，<strong>毕竟在神经网络领域，几乎所有的进步都源于人们在进行数学推演时瞬间萌生的直觉，而非常规推理。</strong></p> 
 <p>那么你是否要相信自己的直觉？我有一个标准——要么你有敏锐的直觉，要么干脆没有。如果没有敏锐的直觉，那做什么都没关系；但如果有敏锐的直觉，那应该相信直觉，去做你认为对的事。</p> 
 <p>当然，敏锐的直觉源自你对世界的理解以及大量的辛劳付出。当你在同一件事上积累了大量经验，就会产生直觉。</p> 
 <p>我患有轻微的狂躁抑郁症，所以一般会游走在两种状况之间：适当的自我批评会让我非常有创造力，而极度自我批评会让我产生轻度抑郁。但我认为这样比仅有单一情绪的效率更高。当你感到烦躁时，你只要忽视那些显而易见的问题，并且确信一些有趣的、激动人心的东西正等你去发现，继续前进。当你面对问题感到措手不及时，一定要坚持下去，理清思路，仔细斟酌想法的好坏。</p> 
 <p>由于有这样的情绪交替，我经常会告诉大家，我弄清大脑的工作机制了，可过段时间，我又失望地发现之前的结论是错误的，但事情就应该是这样发展的，正如William Blake的那两句诗，“将快乐和忧伤编织，披在我神圣的心上”。</p> 
 <p>我认为科研工作的本质也是如此，如果你不会因为成功而感到兴奋，也不会因为失败而感到沮丧，那算不上真正意义上的研究者。</p> 
 <p>研究生涯里，尽管有时会觉得自己完全摸不着一些算法的门道，但我还从未真正感到迷茫和毫无希望。在我看来，无论最终结果如何，总有值得去做的事情。优秀的研究人员总是有很多想做的事情，只是苦于没有多余的时间。</p> 
 <p>在多伦多大学任教时，我发现计算机科学专业的本科生都很优秀，而很多辅修计算机科学的认知科学专业的本科生也表现得相当出色，这一部分同学并不擅长技术，但他们仍然把研究做得很好，他们热爱计算机科学，非常想弄清人类的认知如何形成，有着源源不断的兴趣。</p> 
 <p>像Blake Richards（蒙特利尔神经学研究所助理教授）这样的科学家，他们很清楚自己想解决什么问题，然后就只管朝着这个方向前行。现在，很多科学家都不知道自己到底想做什么。</p> 
 <p>回头看，<strong>我觉得年轻人要找到自己感兴趣的方向，而不是单纯地学些技术。</strong>在自身兴趣的驱动下，你会主动去掌握一些应有的知识来寻找你想要的答案，这比盲目地学习技术更重要。</p> 
 <p>现在想想，我年轻时就应该再多学一点数学知识，这样做线性代数就会容易很多。</p> 
 <p>数学时常让我感到绝望，导致很难读懂一些论文，尤其要弄懂那一大堆符号，真是一项莫大的挑战，所以我并没有读太多论文。关于神经科学方面的问题，一般我会向Terry Sejnowski（计算神经学教授）请教，计算机科学方面的问题，我会请研究生解释给我听。当我需要用数学来证明某项研究是否可行时，我也总能找到合适的方法。</p> 
 <p>通过做研究让这个世界变得更美好的想法很不错，但我更享受探索人类创造力上限的乐趣，我真的很想了解大脑的工作机制，我相信我们需要一些新的想法，比如通过脉冲神经网络的学习算法了解大脑的运作方式。</p> 
 <p>我认为，最棒的研究工作应该由一大群研究生来完成，并且给他们提供丰富的资源。科研工作需要年轻的活力，源源不断的动力，以及对研究的强烈兴趣。</p> 
 <p>你必须有好奇心的驱动才能做出最好的基础研究。只有这样，你才有动力去忽视那些明显的障碍，去预估自己会取得怎样的结果。如果是一般性研究，创造力就不是最重要的。</p> 
 <p><strong>如果能弄清一大批聪明人正在研究什么，然后你再去做不一样的研究，总是一个好主意。</strong>如果你已经在某个领域取得一定的进展，那就不需要其他新的想法，只需要将现有的研究深挖下去就可以成功。但如果你想研究一些新想法，比如构建大型硬件，那也非常不错，尽管前路可能有些曲折。 </p> 
 <p><em>（本文经授权后编译发布，原视频：</em></p> 
 <p><em>1.</em><em>https://www.youtube.com/watch?v=4Otcau-C_Yc</em></p> 
 <p><em>2.</em><em>https://www.youtube.com/watch?v=2EDP4v-9TUA</em><em>）</em></p> 
 <p style="text-align:justify">其他人都在看</p> 
 <ul>
<li> <p style="text-align:left"><a href="">OneFlow v0.7.0发布</a></p> </li>
<li> <p><a href="">图解OneFlow的学习率调整策略</a></p> </li>
<li> <p><a href="">天才制造者：独行侠、科技巨头和AI</a></p> </li>
<li> <p><a href="">手把手推导分布式矩阵乘的最优并行策略</a></p> </li>
<li> <p><a href="">深度学习崛起十年：“开挂”的OpenAI革新者</a></p> </li>
<li> <p><a href="">解读Pathways（二）：向前一步是OneFlow</a></p> </li>
</ul>
 <p style="text-align:justify"><strong>欢迎体验OneFlow v0.7.0：</strong><a class="has-card" href="https://github.com/Oneflow-Inc/oneflow/" title="GitHub - Oneflow-Inc/oneflow: OneFlow is a performance-centered and open-source deep learning framework."><span class="link-card-box"><span class="link-title">GitHub - Oneflow-Inc/oneflow: OneFlow is a performance-centered and open-source deep learning framework.</span><span class="link-desc">OneFlow is a performance-centered and open-source deep learning framework. - GitHub - Oneflow-Inc/oneflow: OneFlow is a performance-centered and open-source deep learning framework.</span><span class="link-link"><img alt="" class="link-link-icon" src="https://images2.imgbox.com/6a/83/3Rq3EWHk_o.png">https://github.com/Oneflow-Inc/oneflow/</span></span></a></p> 
</div>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>