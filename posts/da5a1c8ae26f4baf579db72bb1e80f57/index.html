<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【视觉SLAM】Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【视觉SLAM】Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <table>
<thead><tr>
<th>A. Rosinol, M. Abate, Y. Chang and L. Carlone, “Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping,” 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 1689-1696, doi: 10.1109/ICRA40945.2020.9196885.</th>
<th></th>
</tr></thead>
<tbody><tr>
<td>引用格式</td>
<td></td>
</tr></tbody>
</table>
<h1>
<a id="_6"></a>?摘要</h1> 
<blockquote> 
 <p>我们为实时度量-语义视觉-惯性同步定位和建图（SLAM） 提供了一个开源C++库。该库超越了现有的视觉和视觉惯性 SLAM 库（例如ORB-SLAM、VINS-Mono、OKVIS、ROVIO），实现了 3D网格重建和语义标记。Kimera在设计时考虑了模块化，并有四个关键组件：</p> 
</blockquote> 
<ol>
<li>用于快速准确状态估计的视觉惯性里程计（VIO）模块，</li>
<li>用于全局轨迹估计的强大姿态图优化器，</li>
<li>用于快速网格重建的轻量级3D网格划分器模块，</li>
<li>以及用于快速网格重建的密集3D度量语义重建模块。</li>
</ol> 
<blockquote> 
 <p>这些模块可以单独运行或组合运行，因此Kimera可以很容易地回退到最先进的VIO或完整的SLAM系统。Kimera在CPU上实时运行，并从语义标记的图像中生成3D度量语义网格，这可以通过现代深度学习方法获得。我们希望Kimera提供的灵活性，计算效率，健壮性和准确性将为未来的度量语义SLAM和感知研究奠定坚实的基础，并将允许多个领域（例如，VIO，SLAM，3D重建，分割）的研究人员对自己的工作进行基准测试和原型设计，而不必从头开始。</p> 
</blockquote> 
<h1>
<a id="_16"></a>?一、介绍</h1> 
<p>度量语义理解是同时估计场景的3D几何图形并将语义标签附加到对象和结构（例如，表格，墙壁）的能力。<code>几何信息</code>对于机器人<code>安全导航和操纵物体</code>至关重要，而语义信息为机器人理解和执行人类指令（例如：“给我一杯咖啡”，“从红门退出”）并为人类提供易于理解的环境模型提供了理想的抽象水平。</p> 
<p><img src="https://images2.imgbox.com/7e/21/hlV1jFo6_o.png" alt="在这里插入图片描述"><br> 尽管在几何重建（例如，SLAM [1]，运动结构[2]和多视图立体[3]）和基于深度学习的语义分割（例如，[4]-[10]）方面取得了前所未有的进展，但这两个领域的研究传统上都是孤立进行的。然而，最近人们对这些领域的交叉点的研究和应用越来越感兴趣[1]，[11]-[15]。</p> 
<p>这种日益增长的兴趣促使我们创建并发布了Kimera，这是一个用于度量语义定位和建图的库，它将<code>几何和语义理解</code>的最新技术结合到现代感知库中。与针对视觉惯性里程计（VIO）和SLAM的相关努力相反，我们将视觉惯性SLAM，网格重建和语义理解相结合。我们的努力还在几个方面补充了度量和语义理解之间边界的方法。首先，虽然现有的努力集中在RGB-D传感上，但<code>Kimera使用视觉（RGB）和惯性传感，这在更广泛的（室内和室外）环境中效果很好</code>。其次，虽然相关作品[16]–[18]需要GPU进行3D建图，但我们提供了一个快速，轻量级和可扩展的基于CPU的解决方案。最后，我们专注于鲁棒性：我们包括最先进的异常值拒绝方法，以确保Kimera在各种场景中稳健地执行，并且参数调整最少，从真实的基准测试数据集[19]到照片级真实模拟[20]，[21]。</p> 
<h1>
<a id="_25"></a>?二、相关工作</h1> 
<p><img src="https://images2.imgbox.com/a2/d2/P2HuKuNQ_o.png" alt="在这里插入图片描述"><br> （mono就是单目相机、stereo是双目相机，lidar是激光雷达，IMU是惯性测量单元，RGB-D是深度相机）</p> 
<p>我们请读者参考表 I，以便<code>与现有的 VIO 和视觉 SLAM 系统进行视觉比较</code>，并参考 [1] 以获取有关 SLAM 的更广泛评论。虽然度量语义理解[11]和[33]的早期工作是为离线处理而设计的，但近年来，由<code>SLAM++</code>[16]等开创性工作引发，人们对实时度量语义映射的兴趣激增。这些作品中的大多数（i）依赖于RGB-D相机，（ii）使用GPU处理，（iii）替代跟踪和建图（表I中的“交替”），以及（iv）使用基于体素的（例如，截断的有符号距离函数，TSDF）表面或对象表示。例子包括SemanticFusion[17]，郑等人的方法[15]，泰特诺等人[34]，以及李等人[35]，Fusion++[36]，Mask-fusion[29]，Co-fusion[37]和MID-Fusion[38]。最近的工作研究了基于CPU的方法，例如，Wald等人[39]，PanopticFusion[40]和Voxblox ++ [14];这些也依赖于 RGB-D 传感。一组稀疏的贡献涉及其他传感模式，包括单目摄像头（例如，CNN-SLAM [41]，VSO [42]， VITAMIN-E [43]，XIVO [32]）和lidar（例如，SemanticKitti [44]，SegMap [31]）。XIVO [32] 和Voxblox++ [14] 最接近我们的提案。XIVO [32]是一种基于EKF的视觉惯性方法，可生成基于对象的地图。Voxblox++ [14] 依靠 RGB-D 传感、车轮里程测量和使用 maplab [26] 的预构建地图来获得视觉惯性姿态估计值。与这些工作相反，Kimera（i）<strong>提供了基于高精度实时优化的VIO</strong>，（ii）<strong>使用强大而通用的姿势图优化器</strong>，以及（iii）<strong>提供轻量级网格重建。</strong></p> 
<h2>
<a id="_32"></a>?贡献</h2> 
<p>我们发布了Kimera，这是一个开源C++库，它使用视觉惯性传感来估计机器人的状态，并构建一个轻量级的度量语义网格环境模型。Kimera这个名字源于我们图书馆的混合性质，它统一了各个研究领域的最新工作，包括VIO，姿势图优化（PGO），网格重建和3D语义分割。Kimera包括四个关键模块：</p> 
<ul>
<li> <p><strong>Kimera-VIO</strong>：用于快速准确的IMU速率状态估计的VIO模块。基美拉-VIO的核心是采用基于GTSAM的VIO方法[45]，使用IMU预集成和无结构视觉因子[27]，并在EuRoC数据集上实现最佳性能[19];</p> </li>
<li> <p><strong>Kimera-RPGO</strong>：一种稳健（鲁棒）的位姿图优化（RPGO）方法，利用现代技术进行异常值拒绝[46]。Kimera-RPGO增加了一个健壮性层，避免了由于感知混叠而导致的SLAM故障，并使用户免于耗时的参数调整;</p> </li>
<li> <p><strong>Kimera-Mesher</strong>：一种计算快速每帧和多帧正则化3D网格以支持避障的模块。该网格划分器建立在作者和其他小组先前的算法的基础上 [43]， [47]–[49];</p> </li>
<li> <p><strong>Kimera-Semantics</strong>：一个语义模块，它使用体积法[28]构建一个更慢但更准确的全局3D网格，并使用2D像素语义分割对3D网格进行语义注释。</p> </li>
</ul> 
<p>Kimera<code>既可以使用离线数据集</code>，<code>也可以使用机器人操作系统（ROS）在线工作</code>[50]。它在CPU上实时运行，并提供有用的调试和可视化工具。此外，它是模块化的，允许替换每个模块或单独执行它们。例如，它可以回退到VIO解决方案，或者如果语义标签不可用，它可以简单地估计几何网格。</p> 
<h1>
<a id="Kimera_45"></a>?三、Kimera</h1> 
<p>图2显示了Kimera的建筑。Kimera 将立体帧和高速率惯性测量作为输入并返回 （i） 以 IMU 速率计算的高度准确的状态估计值，（ii） 全局一致的轨迹估计值，以及 （iii） 环境的多个网格，包括快速局部网格和全局语义注释网格。Kimera高度并行化，并<code>使用四个线程来容纳不同速率的输入和输出</code>（例如，IMU，帧，关键帧）。在这里，我们按线程描述体系结构，而每个模块的描述将在以下各节中给出。</p> 
<ul>
<li>第一个线程包括Kimera-VIO前端（第II-A节），它获取立体图像和IMU数据，并输出特征轨迹和预集成的IMU测量。前端还发布 IMU 速率状态估计值。</li>
<li>第二个线程包括 （i） 返回优化状态估计值的 Kimera-VIO 后端，以及 （ii） Kimera网格（第 II-C 节），用于计算每帧和多帧 3D 网格的低延迟（&lt; 20 毫秒）。这两个线程允许创建图 2（b） 中的每帧网格（也可以带有图 2（c）中的语义标签），以及图 2（d） 中的多帧网格。最后两个线程以较慢的速率运行，旨在支持低频功能，例如路径规划。</li>
<li>第三个线程包括 Kimera-RPGO（第 II-B 节），这是一个健壮的 PGO 实现，可检测循环闭包、拒绝异常值并估计全局一致的轨迹（图 2（a））。</li>
<li>最后一个线程包括Kimera语义（第 II-D 节），它使用密集的立体和 2D 语义标签，使用 Kimera-VIO 的姿势估计来获得精细的度量语义网格。</li>
</ul> 
<p><img src="https://images2.imgbox.com/b4/2f/q5RpZzuU_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="A_KimeraVIO_54"></a>?A. Kimera-VIO：视觉惯性里程计模块</h2> 
<p>Kimera-VIO 实现了 [27] 中介绍的基于关键帧的最大后验视觉惯性估计器。在我们的实现中，估计器可以执行完全平滑或固定滞后平滑，具体取决于指定的时间范围;我们通常使用后者来限制估计时间。我们还扩展了 [27] 以处理单目和立体声帧。Kimera-VIO包括一个（视觉和惯性）前端，负责处理原始传感器数据，以及一个后端，它融合处理的测量结果以获得传感器状态的估计值（即，姿态，速度和传感器偏差）。</p> 
<h3>
<a id="1_VIO_Frontend_57"></a>1) VIO Front-end</h3> 
<p>我们的IMU前端执行流形预集成[27]，从原始IMU数据中获得两个连续关键帧之间相对状态的紧凑预集成测量值。视觉前端检测Shi-Tomasi角[51]，使用Lukas-Kanade跟踪器[52]跨帧跟踪它们，找到左右立体声匹配，并执行几何验证。我们使用 5 点 RANSAC [53] 执行单声道（透明）验证，并使用 3 点 RANSAC [54] 进行立体声验证;该代码还提供了使用IMU旋转的选项，并分别使用2点[55]和1点RANSAC执行单声道和立体声验证。特征检测、立体匹配和几何验证在每个关键帧执行，而我们只跟踪中间帧的特征。</p> 
<h3>
<a id="2_VIO_Backend_60"></a>2) VIO Back-end</h3> 
<p>在每个关键帧上，预集成的IMU和视觉测量被添加到构成我们的VIO后端的固定滞后平滑器（因子图）中。我们使用预集成的IMU模型和[27]的无结构视觉模型。因子图在 GTSAM [57] 中使用 iSAM2 [56] 求解。在每次 iSAM2 迭代中，无结构视觉模型使用 DLT [58] 估计观测到的特征的 3D 位置，并分析地从 VIO 状态 [59] 中消除相应的 3D 点。在消除之前，将去除退化点（即相机后面的点或没有足够的视差进行三角测量的点）和异常值（即具有较大重投影误差的点），从而提供额外的鲁棒性层。最后，脱离平滑地平线的国家被GTSAM边缘化。</p> 
<h2>
<a id="B_KimeraRPGO_63"></a>?B. Kimera-RPGO：稳健的姿势图优化模块</h2> 
<p>Kimera-RPGO 负责 （i） 检测当前和过去关键帧之间的循环闭包，以及 （ii） 使用可靠的 PGO 计算全局一致的关键帧姿势。</p> 
<h3>
<a id="1_Loop_Closure_Detection_66"></a>1) Loop Closure Detection</h3> 
<p>闭环检测依赖于 DBoW2 库 [60]，并使用词袋表示来快速检测推定的环路。对于每个假定的环路闭包，我们使用单目和双目几何验证拒绝异常值环路闭包（如第 II-A 节所述），并将剩余的环路闭包传递给鲁棒 PGO 求解器。请注意，由于感知混叠，生成的循环闭包仍可能包含异常值（例如，建筑物不同楼层的两个相同房间）。</p> 
<h3>
<a id="2_Robust_PGO_69"></a>2) Robust PGO</h3> 
<p>该模块在GTSAM中实现，包括一种现代异常值抑制方法，增量一致性测量集最大化（PCM）[46]，我们将其定制为单个机器人和在线设置。我们分别存储里程线边缘（由Kimera-VIO产生）和环闭包（由环闭合检测产生）;每次执行 PGO 时，我们首先使用 PCM 的修改版本选择最大的一致循环闭包集，然后在姿势图上执行 GTSAM，包括里程法和一致循环闭包。</p> 
<p>PCM专为多机器人案例而设计，仅检查机器人间循环闭合是否一致。我们开发了一个C++的PCM实现，该实现（i）在环闭合上添加了里程测量一致性检查，并且（ii）增量更新了一致测量集以实现在线操作。里程测量检查验证每个环闭包（例如，l1在图2（a））中与里程计一致（图中为红色）：在没有噪声的情况下，沿着由里程计形成的循环和环l的姿态1必须组成标识。与 PCM 一样，我们将沿周期累积的误差与使用卡方检验的测量噪声不一致的异常值循环标记为异常值循环。如果在当前时间检测到的环 t 通过了里程检查，我们将测试它是否与 [46] 中的先前循环闭包成对一致（例如，检查循环 l1和 l2图2（a）中的图2（a）彼此一致）。当 PCM [46] 构建邻接矩阵 A ∈ R 时长×升从头开始跟踪成对一致的循环（其中L是检测到的循环闭包的数量），我们通过增量构建矩阵A来实现在线操作。每次检测到新循环时，我们都会向矩阵 A 中添加一行和列，并且仅针对以前的循环测试新循环。最后，我们使用 [61] 的快速最大集团实现来计算最大的一致循环闭包集。一组一致的测量值被添加到姿态图中（与里程法一起），并使用 <code>Gauss-Newton</code>进行优化。</p> 
<h2>
<a id="C_KimeraMesher_3D_Mesh_Reconstruction_74"></a>?C. Kimera-Mesher: 3D Mesh Reconstruction</h2> 
<p>Kimera-Mesher 可以快速生成两种类型的 3D 网格：（i） 每帧 3D 网格，以及 （ii） 跨越 VIO 固定滞后平滑器中的关键帧的多帧 3D 网格。</p> 
<h3>
<a id="1_Perframe_mesh_77"></a>1) Per-frame mesh</h3> 
<p>与 [47] 中一样，我们首先对当前关键帧中成功跟踪的 2D 特征（由 VIO 前端生成）执行 2D Delaunay 三角测量。然后，我们使用 VIO 后端的 3D 点估计值，反向投影 2D Delaunay 三角测量以生成 3D 网格（图 2（b））。虽然每帧网格旨在提供低延迟障碍物检测，但我们还提供了通过测量2D标签的网格对3D网格进行语义标记（图2（c））。</p> 
<h3>
<a id="2_Multiframe_mesh_80"></a>2) Multi-frame mesh</h3> 
<p>多帧网格将 VIO 后退地平线上收集的每帧网格融合成单个网格（图 2（d））。每帧和多帧 3D 网格都编码为顶点位置列表，以及顶点 ID 的三元组列表，用于描述三角形面。假设我们在时间 t − 1 时已经有了一个多帧网格，对于我们生成的每个新的每帧 3D 网格（在时间 t 处），我们循环遍历其顶点和三元组，并添加位于每帧网格中但在多帧网格中缺失的顶点和三元组。然后，我们循环遍历多帧网格顶点，并根据最新的 VIO 后端估计值更新其 3D 位置。最后，我们删除了与在VIO时间范围之外观察到的旧特征相对应的顶点和三元组。结果是一个最新的 3D 网格，该网格跨越当前 VIO 时间范围内的关键帧。如果在网格中检测到平面，则会将规则因子 [47] 添加到 VIO 后端，从而导致 VIO 和网格正则化之间的紧密耦合，有关更多详细信息，请参阅 [47]。</p> 
<h2>
<a id="D_KimeraSemantics_MetricSemantic_Segmentation_83"></a>?D. Kimera-Semantics: Metric-Semantic Segmentation</h2> 
<p>我们将 [28] 中引入的BA技术改编为 （i） 构建精确的全局 3D 网格（覆盖整个轨迹），以及 （ii） 对网格进行语义注释。</p> 
<h3>
<a id="1_Global_mesh_86"></a>1) Global mesh</h3> 
<p>我们的实现建立在Voxblox [28]的基础上，并使用基于体素（TSDF）的模型来过滤掉噪声并提取全局网格。在每个关键帧中，我们使用密集立体声（半全局匹配[62]）从当前立体声对中获取3D点云。然后，我们使用 Voxblox [28] 应用捆绑的光线投射，使用 [28] 中讨论的“快速”选项。在每个关键帧上重复此过程并生成一个 TSFD，使用行进立方体 [63] 从中提取网格。</p> 
<h3>
<a id="2_Semantic_annotation_89"></a>2) Semantic annotation</h3> 
<p>Kimera-Semantics使用2D语义标记图像（在每个关键帧处生成）对全局网格进行语义注释;2D语义标签可以使用现成的工具获得像素级2D语义分割，例如深度神经网络[7]–[9]，[64]–[69]或经典的基于MRF的方法[70]。为此，在BA过程中，我们还传播语义标签。使用2D语义分割，我们将标签附加到由密集立体产生的每个3D点上。然后，对于BA中的每个光线束，我们从BA中观察到的标签的频率构建标签概率向量。然后，我们仅在TSDF截断距离（即靠近表面）内沿射线传播此信息以进行备用计算。换句话说，我们节省了更新“空”标签的概率的计算工作。在沿光线遍历体素时，我们使用贝叶斯更新来更新每个体素的标签概率，类似于 [17]。捆绑语义光线投射后，每个体素都有一个标签概率向量，我们从中提取最可能的标签。度量语义网格最终使用行进立方体 [63] 提取。生成的网格比 II-C 部分的多帧网格精确得多，但计算速度较慢（≈ 0.1 秒，请参见第 III-D 节）。</p> 
<h2>
<a id="E_Debugging_Tools_92"></a>?E. Debugging Tools</h2> 
<p>虽然我们出于篇幅原因限制了讨论，但值得一提的是，Kimera 还提供了一套开源评估工具，用于 VIO、SLAM 和度量语义重建的调试、可视化和基准测试。Kimera包括一个持续集成服务器（Jenkins），它断言代码的质量（编译，单元测试），但也使用evo [71]自动评估EuRoC数据集上的Kimera-VIO和Kimera-RPGO。此外，我们还提供 Jupyter 笔记本来可视化中间 VIO 统计数据（例如，特征轨迹的质量、IMU 预集成错误），以及使用 Open3D [72] 自动评估 3D 重建的质量。</p> 
<h1>
<a id="_96"></a>✨四、实验评估</h1> 
<table>
<thead><tr>
<th>第 III-A 节显示 （i） Kimera 实现了最先进的状态估计性能，（ii） 我们稳健的 PGO 使用户无需进行耗时的参数调整。第 III-B 部分演示了 Kimera 在 EuRoC 上进行的 3D 网格重建，使用提供地面实况点云的场景子集。第 III-C 部分使用照片级真实型模拟器（参见视频附件）检查 Kimera 的 3D 度量语义重建，该模拟器提供真实 3D 语义。最后，第 III-D 部分重点介绍了 Kimera 的实时性能，并分析了每个模块的运行时。</th>
<th></th>
</tr></thead>
<tbody>
<tr>
<td></td>
<td></td>
</tr>
<tr>
<td><img src="https://images2.imgbox.com/5b/83/ynVYeDBH_o.png" alt="在这里插入图片描述"></td>
<td></td>
</tr>
</tbody>
</table>
<h2>
<a id="A__102"></a>?A. 位姿估计性能</h2> 
<p>表 II 使用 [77] 中的独立报告值和 [24] 中的独立报告值，将Kimera-VIO 的绝对平移误差 （ATE） 与最先进的开源 VIO 管道进行比较：OKVIS [73]、MSCKF [74]、ROVIO [75]、VINS-单声道 [24] 和 SVO-GTSAM [76].请注意，这些算法使用单目相机，而我们使用双目相机。在评估误差之前，我们使用 SE（3） 变换来对齐估计轨迹和真实轨迹。使用 Sim（3） 对齐， 如 [77] 所示， 会给 Kimera 带来更小的误差： 我们更喜欢 SE（3） 对齐， 因为它更适合 VIO， 由于 IMU，比例尺是可观测到的。我们根据这些技术是否使用固定滞后平滑、完全平滑和循环闭包来对它们进行分组。Kimera-VIO 和Kimera-RPGO 在整个频谱中实现了最佳性能。</p> 
<p>此外，Kimera-RPGO确保了强大的性能，并且对循环闭包参数调整不太敏感。表III显示了DBoW2中使用的不同环路闭合阈值α有和没有异常值抑制（PCM）的PGO精度。较小的α值会导致更多的循环闭包检测，但这些检测不太保守（更多异常值）。表三表明，通过使用PCM，Kimera-RPGO对α的选择相当不敏感。表II中的结果使用α = 0.001。</p> 
<h2>
<a id="B__107"></a>?B. 几何重建</h2> 
<p>我们使用<code>EuRoC V1和V2数据集</code>中提供的地面实况点云来评估Kimera生成的3D网格的质量。我们使用 [78， Sec. 4.3] 中的<code>准确性和完整性指标</code>根据实况评估每个网格：（i） 我们通过对均匀密度为 10 的网格进行采样来计算点云3点/米2，（ii）我们使用CloudCompare [80]用ICP[79]记录估计云和地面真实云，并且（iii）我们评估从真实点云到估计点云中其最近邻居的平均距离（精度），反之亦然（完整性）。图3（a）显示了估计的点云（对应于V1_01上的Kimera-Semantics的全球网格），由到地面实况云中最近点的距离（精度）进行颜色编码;图3（b）显示了真实点云，用颜色编码到估计云中最近点的距离（完整性）。<br> <img src="https://images2.imgbox.com/12/2d/G1Lxq7qB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/74/60/XVO3GuTR_o.png" alt="在这里插入图片描述"><br> 表四提供了Kimera网格生成的快速多帧网格与Kimera语义通过 TSDF 生成的慢速网格之间的定量比较。为了从Kimera网格中获取完整的网格，我们设置了一个大的VIO视界（即，我们执行完全平滑）。如图3（a）所示，<code>来自Kimera-Semantics的全局网格非常准确，数据集的平均误差为0.35 − 0.48m</code>。Kimera-Mesher <code>产生的网格噪声更大（误差增加高达 24%），但计算时间缩短了两个数量级</code>（参见第 III-D 节）。<br> <img src="https://images2.imgbox.com/0e/66/WnUqWu7G_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="C__114"></a>?C. 语义重建</h2> 
<p>为了评估来自Kimera-Semantics的度量语义重建的准确性，我们使用麻省理工学院林肯实验室提供的基于Unity的照片级真实感模拟器，<code>该模拟器为场景的几何和语义提供传感器流（在ROS中）和地面实况</code>，并且具有类似于[20]，[21]的接口。为了避免将结果偏向于特定的2D语义分割方法，我们使用基本事实2D语义分割，并向读者推荐[70]以获取潜在的替代方案。</p> 
<p>Kimera-Semantics根据VIO姿态估计值构建了3D网格，并使用密集立体和BA的组合。<code>我们通过运行三个不同的实验来评估这些组件中每个组件的影响。</code>首先，我们使用具有真实值 （GT） 姿势和真实深度图（在模拟中可用）的 Kimera-Semantics 来评估由于BA而导致的初始性能损失。其次，我们使用Kimera-VIO的姿势估计。最后，我们使用完整的Kimera语义管道，包括密集立体声。为了分析语义性能，我们计算了平均交并比（mIoU）[13]，以及正确标记的点（Acc）的整体部分[81]。我们还报告了ATE，以将结果与Kimera-VIO产生的漂移相关联。最后，我们评估了度量重建，将估计的网格与地面实况进行注册，并计算了点的 RMSE，如第 III-B 节所示。</p> 
<p><img src="https://images2.imgbox.com/ce/67/sdr4CT2V_o.png" alt="在这里插入图片描述"></p> 
<p>表V总结了我们的发现，并表明BA导致几何（3D网格上的&lt;8cm误差）和语义（精度&gt;94%）的性能略有下降。使用Kimera-VIO也会导致性能损失可以忽略不计，因为我们的VIO漂移很小（&lt;0.2%，32米长的轨迹为4厘米）。当然，性能下降最大的是由于使用了密集的立体声。密集立体声[62]很难解析无纹理区域（如墙壁）的深度，这些区域在模拟场景中很常见。图 4 显示了使用Kimera-VIO 和地面实况深度运行Kimera语义时的混淆矩阵（图 4（a）），与使用密集立体（图 4（b））的比较。混淆矩阵中的较大值显示在“墙/架”和“地板/墙”之间。这正是密集立体声遭受最大影响的地方。无纹理的墙壁难以重建，并且靠近架子和地板，导致几何和语义错误增加。</p> 
<p><img src="https://images2.imgbox.com/67/4b/rTJtPGTA_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="D__129"></a>?D. 时间安排</h2> 
<p>图5报告了Kimera模块的时序性能。IMU前端需要大约40μ秒进行预集成，因此可以以IMU速率（&gt;200Hz）生成状态估计值。视觉前端模块显示双峰分布，因为对于每个帧，我们只需执行特征跟踪（平均需要4.5ms），而在关键帧速率下，我们执行特征检测，立体匹配和几何验证，这些组合平均需要45ms。Kimera网格能够<code>在不到5ms的时间内生成每帧3D网格</code>，而构建多帧网格平均需要15ms。<code>后端在不到40ms的时间内求解因子图优化</code>。Kimera-RPGO和Kimera-Semantics在较慢的线程上运行，因为它们的输出不需要时间关键型操作（例如，控制，避障）。在我们在EuRoC上的实验中，Kimera-RPGO<code>平均需要55毫秒</code>，但一般来说，它的运行时间取决于姿势图的大小。最后，Kimera-Semantics（为清楚起见，图中未报告）<code>平均需要0.1秒来更新每个关键帧的全局度量语义网格</code>，融合720×480密集深度的图像，就像我们的模拟器生成的图像一样。<br> <img src="https://images2.imgbox.com/76/01/4xndygzd_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="_133"></a>?五、结论</h1> 
<p>Kimera是一个用于度量语义 SLAM 的开源C++库。它包括视觉惯性里程测量、稳健的姿态图优化、网格重建和 3D 语义标记的最新实现。它在CPU上实时运行，并提供一套持续集成和基准测试工具。<code>我们希望Kimera能够为未来机器人感知研究提供坚实的基础，并为社区的研究人员提供易于使用的基础设施。</code></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>