<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>为什么ResNet深度残差网络广受好评呢？ - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">为什么ResNet深度残差网络广受好评呢？</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <h2>一、简介</h2> 
<p>何凯明大神在CVPR 2016上发表的《Deep Residual Learning for Image Recognition 图像识别中的深度残差学习网络》深受工业界的欢迎，自提出以来已经成为工业界最受欢迎的卷积神经网络结构。在coco目标检测任务中提升28%的精度，并基于ResNet夺得ILSVRC的检测、定位，COCO 的检测和分割四大任务的冠军。接下来就一起来看这个广受好评的Resnet</p> 
<h2>二、论文解读</h2> 
<p>摘要原文：</p> 
<pre><code>Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions1 , where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.</code></pre> 
<p><span style="color:#fe2c24"><strong>摘要核心：</strong></span></p> 
<ul>
<li>1. 提出问题：深度卷积网络难训练</li>
<li>2. 本文方法：残差学习框架可以让深层网络更容易训练</li>
<li>3. 本文优点：ResNet易优化，并随着层数增加精度也能提升</li>
<li>4. 本文成果：ResNet比VGG深8倍，但是计算复杂度更低，在ILSVRC-2015获得3.57%的top-error 5. 本文其它工作：CIFAR-10上训练1000层的ResNet</li>
<li>6. 本文其它成果：在coco目标检测任务中提升28%的精度，并基于ResNet夺得ILSVRC的检测、定位，COCO 的检测和分割四大任务的冠军</li>
</ul>
<p><span style="color:#fe2c24"><strong> 为什么要使用这样一种残差结构呢？</strong></span></p> 
<p>我们往往认为越深的网络拟合能力越强，因此越深的网络训练误差应该越低，但实际相反。如下图所示。实际上的原因并非是因为过拟合导致的，而是网络优化困难。</p> 
<p class="img-center"><img alt="" height="309" src="https://images2.imgbox.com/ab/47/L2MDw9ZM_o.png" width="465"></p> 
<p><strong> 因为提出一种残差结构来解决网络退化的能力，于此同时也解决了由于网络的加深从而带来的一些梯度问题。</strong></p> 
<h2><strong>三、残差结构</strong></h2> 
<h2 style="text-align:center"><img alt="" height="283" src="https://images2.imgbox.com/29/ac/cQpUeQ0X_o.png" width="488"></h2> 
<p>如上图所示，当输入为x时，其学习到的特征记为H(x)，现在我们希望其可以学习到残差F(x)=H(x)-x，这样其实原始的学习特征是F(x)+x 。当残差为0时，此时堆积层仅仅做了<strong><span style="color:#fe2c24">恒等映射</span></strong>，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能。因此Residual learning：让网络层拟合H(x)-x， 而非H(x)</p> 
<p><span style="color:#a2e043"><strong>整个building block仍旧拟合H(x) ，注意区分building block与网络层的差异，两者不一定等价</strong></span></p> 
<p><strong><span style="color:#fe2c24"> 为什么要网络层拟合H(x)-x？</span></strong></p> 
<p><strong>答：提供building block更容易学到恒等映射（identity mapping）的可能</strong></p> 
<p><span style="color:#fe2c24"><strong>为什么要恒等映射？</strong></span></p> 
<p><strong>答：让深度网络不至于比浅层网络差</strong></p> 
<h2>三、恒等映射 </h2> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/59/c5/s0mRZZyJ_o.png"></p> 
<p><span style="color:#fe2c24">  左图代表一个浅层网络-18层 ，右图代表一个深层网络-34层，蓝色框我们可认为是额外增加层</span></p> 
<p>思考一下，如果蓝色框里的网络层能学习到<strong>恒等映射</strong>，34层网络至少能与18层网络有相同性能 那么我们如何让额外的网络层更容易的学习到恒等映射？</p> 
<p>答：skip connection == residual learning == shortcut connection</p> 
<p class="img-center"><img alt="" height="157" src="https://images2.imgbox.com/2c/ac/lhjkejdv_o.png" width="290"></p> 
<p> 上图所示，若F(x)=0 , 那么H(X) = X，网络实现恒等映射。</p> 
<p>我们求解H(x)梯度，梯度为xxx + 1。恒等映射使得梯度畅通无阻的从后向前传播，这使得ResNet结构可达到上千层。</p> 
<h2>四、Shortcut mapping </h2> 
<p>因为我们最后有一个相加的操作，x输入到卷积层后通道数有可能会发生改变，那么在相加的时候，原始x和经过layer后的x通道数不一致是无法完成相加，针对这种问题，作者提出了三种解决方案</p> 
<ul>
<li>(A) zero-padding shortcuts are used for increasing dimensions, and all shortcuts are parameter-free  <span style="color:#fe2c24">全零填充：维度增加的部分采用零来填充</span>
</li>
<li>(B) projection shortcuts are used for increasing dimensions, and other shortcuts are identity<span style="color:#fe2c24">网络层映射：当维度发生变化的时候，通过网络映射的方式（例如1*1卷积）至相同的维度</span>
</li>
<li>(C) all shortcuts are projections. <span style="color:#fe2c24">所有的shortcuts均通过网络映射</span>
</li>
</ul>
<p class="img-center"><img alt="" height="294" src="https://images2.imgbox.com/ed/92/D8qOq4O5_o.png" width="385"></p> 
<p> 根据文章给出的实验数据结果，我们可以发现三种方案A&lt;B&lt;C   整体B和C效果相差不大，最终采用了B方案。</p> 
<h2>五、ResNet结构</h2> 
<p><img alt="" height="597" src="https://images2.imgbox.com/24/05/HvY09m7Z_o.png" width="1200"></p> 
<p> ResNet结构 划分为6个stage ，其中中间4个阶段为残差结构堆叠 ，对于残差结构堆叠方式有两种</p> 
<ul>
<li>Basic：两个3*3卷积堆叠   上图黄框1</li>
<li>Bottleneck：利用1*1卷积减少计算量    上图黄框2</li>
</ul>
<p class="img-center"><img alt="" height="287" src="https://images2.imgbox.com/ac/0e/9bQMieg0_o.png" width="520"></p> 
<p><strong><span style="color:#a2e043">右边 Bottleneck： 第一个1*1下降1/4通道数  第二个1*1提升4倍通道数</span></strong></p> 
<h2><span style="color:#0d0016"><strong>六、论文总结</strong></span></h2> 
<p><strong> 引入</strong>shortcut connection，让网络信息有效传播，梯度反传顺畅，使得数千层卷积神经网络都可以收敛 。注意是引入了而不是原创。在本文中：shortcut connection == skip connection == identity  mapping</p> 
<p><strong><span style="color:#fe2c24"> 启发点：</span></strong></p> 
<ul>
<li>1. 大部分的梯度消失与爆炸问题，可通过良好初始化或者中间层的标准化来解决。</li>
<li>2. shortcut connection有很多种方式，本文主要用的是恒等映射，即什么也不操作的往后传播</li>
<li>3.借鉴VGG，本文模型设计原则：1.处理相同大小特征图，卷积核数量一样；2.特征图分辨率降低时，通道数翻倍</li>
<li>4. 训练时，我们可以考虑本文种用到的预热训练Warm up</li>
</ul>
<p>！！我们根据文章提供的超参数设置和训练tricks很容易就能达到文章中的实验结果，不像其他网络，我们的训练结果和论文中的实验结果达到相似十分困难。</p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>