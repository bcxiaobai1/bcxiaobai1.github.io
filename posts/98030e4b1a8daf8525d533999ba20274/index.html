<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>5、DataX（DataX简介、DataX架构原理、DataX部署、使用、同步MySQL数据到HDFS、同步HDFS数据到MySQL） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">5、DataX（DataX简介、DataX架构原理、DataX部署、使用、同步MySQL数据到HDFS、同步HDFS数据到MySQL）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <h3>
<a id="1DataX_0"></a>1、DataX简介</h3> 
<h5>
<a id="11_DataX_1"></a>1.1 DataX概述</h5> 
<pre><code>DataX 是阿里巴巴开源的一个异构数据源离线同步工具，致力于实现包括关系型数据库(MySQL、Oracle等)、HDFS、Hive、ODPS、HBase、FTP等各种异构数据源之间稳定高效的数据同步功能。
</code></pre> 
<p>源码地址：https://github.com/alibaba/DataX</p> 
<h5>
<a id="12_DataX_5"></a>1.2 DataX支持的数据源</h5> 
<p>DataX目前已经有了比较全面的插件体系，主流的RDBMS数据库、NOSQL、大数据计算系统都已经接入，目前支持数据如下图。<br> <img src="https://images2.imgbox.com/da/04/pwdCbTg9_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="2DataX_9"></a>2、DataX架构原理</h3> 
<h5>
<a id="21_DataX_10"></a>2.1 DataX设计理念</h5> 
<p>为了解决异构数据源同步问题，DataX将复杂的网状的同步链路变成了星型数据链路，DataX作为中间传输载体负责连接各种数据源。当需要接入一个新的数据源的时候，只需要将此数据源对接到DataX，便能跟已有的数据源做到无缝数据同步。<br> <img src="https://images2.imgbox.com/97/bd/VTljYjDo_o.png" alt="在这里插入图片描述"></p> 
<h5>
<a id="22_DataX_13"></a>2.2 DataX框架设计</h5> 
<p>DataX本身作为离线数据同步框架，采用Framework + plugin架构构建。将数据源读取和写入抽象成为Reader/Writer插件，纳入到整个同步框架中。<br> Reader：数据采集模块，负责采集数据源的数据，将数据发送给Framework。<br> Writer：数据写入模块，负责不断向Framework取数据，并将数据写入到目的端。<br> Framework：用于连接Reader和Writer，作为两者的数据传输通道，并处理缓存，流控，并发，数据转换等核心技术问题。</p> 
<h5>
<a id="23_DataX_19"></a>2.3 DataX运行流程</h5> 
<p>下面用一个DataX作业生命周期的时序图说明DataX的运行流程、核心概念以及每个概念之间的关系。<br> <img src="https://images2.imgbox.com/6e/d5/fAz0xII9_o.png" alt="在这里插入图片描述"></p> 
<h5>
<a id="24_DataX_22"></a>2.4 DataX调度决策思路</h5> 
<p>举例来说，用户提交了一个DataX作业，并且配置了总的并发度为20，目的是对一个有100张分表的mysql数据源进行同步。DataX的调度决策思路是：<br> 1）DataX Job根据分库分表切分策略，将同步工作分成100个Task。<br> 2）根据配置的总的并发度20，以及每个Task Group的并发度5，DataX计算共需要分配4个TaskGroup。<br> 3）4个TaskGroup平分100个Task，每一个TaskGroup负责运行25个Task。</p> 
<h5>
<a id="25_DataXSqoop_28"></a>2.5 DataX和Sqoop对比</h5> 
<p><img src="https://images2.imgbox.com/b6/c0/DPe2L15H_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="3DataX_30"></a>3、DataX部署</h3> 
<p>1、下载DataX安装包并上传到hadoop102的/opt/software<br> 下载地址：http://datax-opensource.oss-cn-hangzhou.aliyuncs.com/datax.tar.gz<br> 2、解压datax.tar.gz到/opt/module</p> 
<pre><code class="prism language-bash"> <span class="token function">tar</span> -zxvf datax.tar.gz -C /opt/module/
</code></pre> 
<p>3、自检，执行如下命令</p> 
<pre><code class="prism language-bash"> python /opt/module/datax/bin/datax.py /opt/module/datax/job/job.json
</code></pre> 
<p>4、出现如下内容，则表明安装成功<br> <img src="https://images2.imgbox.com/43/31/LnHjKVHo_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="4DataX_45"></a>4、DataX使用</h3> 
<h5>
<a id="41_DataX_46"></a>4.1 DataX使用概述</h5> 
<h6>
<a id="411_DataX_47"></a>4.1.1 DataX任务提交命令</h6> 
<p>Datax的使用十分简单，用户只需要根据自己同步数据的数据源和目的地选择相应的Reader和Writer，并将Reader和Writer的信息配置在一个json文件中，然后执行如下命令提交数据同步任务即可。</p> 
<pre><code class="prism language-bash"> python bin/datax.py path/to/your/job.json
</code></pre> 
<h6>
<a id="412_DataX_53"></a>4.1.2 DataX配置文件格式</h6> 
<p>可以使用如下命名查看DataX配置文件模板。</p> 
<pre><code class="prism language-bash">python bin/datax.py -r mysqlreader -w hdfswriter
</code></pre> 
<p>配置文件模板如下，json最外层是一个job，job包含setting和content两部分，其中setting用于对整个job进行配置，content用户配置数据源和目的地。<br> <img src="https://images2.imgbox.com/51/78/vB183wPT_o.png" alt="在这里插入图片描述"></p> 
<h5>
<a id="42_MySQLHDFS_61"></a>4.2 同步MySQL数据到HDFS案例</h5> 
<p>案例要求：同步gmall数据库中base_province表数据到HDFS的/base_province目录<br> 需求分析：要实现该功能，需选用MySQLReader和HDFSWriter，MySQLReader具有两种模式分别是TableMode和QuerySQLMode，前者使用table，column，where等属性声明需要同步的数据；后者使用一条SQL查询语句声明需要同步的数据。<br> 下面分别使用两种模式进行演示。</p> 
<h6>
<a id="421_MySQLReaderTableMode_65"></a>4.2.1 MySQLReader之TableMode</h6> 
<p>1、编写配置文件<br> （1）创建配置文件base_province.json</p> 
<pre><code class="prism language-bash"><span class="token function">vim</span> /opt/module/datax/job/base_province.json
</code></pre> 
<p>（2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "where": "id&gt;=3",
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "table": [
                                    "base_province"
                                ]
                            }
                        ],
                        "password": "000000",
                        "splitPk": "",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、配置文件说明<br> （1）Reader参数说明<br> <img src="https://images2.imgbox.com/f5/63/agSUCzgr_o.png" alt="在这里插入图片描述"><br> （2）Writer参数说明<br> <img src="https://images2.imgbox.com/89/3a/erx7VkfI_o.png" alt="在这里插入图片描述"><br> 注意事项：<br> HFDS Writer并未提供nullFormat参数：也就是用户并不能自定义null值写到HFDS文件中的存储格式。默认情况下，HFDS Writer会将null值存储为空字符串（‘’），而Hive默认的null值存储格式为N。所以后期将DataX同步的文件导入Hive表就会出现问题。<br> （3）Setting参数说明<br> <img src="https://images2.imgbox.com/f7/63/FtQx4cfD_o.png" alt="在这里插入图片描述"><br> 3、提交任务<br> （1）在HDFS创建/base_province目录<br> 使用DataX向HDFS同步数据时，需确保目标路径已存在</p> 
<pre><code class="prism language-bash">hadoop fs -mkdir /base_province
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash"> python bin/datax.py job/base_province.json 
</code></pre> 
<p>4、查看结果<br> （1）DataX打印日志<br> <img src="https://images2.imgbox.com/4c/3a/XFfSQejM_o.png" alt="在这里插入图片描述"><br> （2）查看HDFS文件</p> 
<pre><code class="prism language-bash">hadoop fs -cat /base_province/* <span class="token operator">|</span> zcat
</code></pre> 
<h6>
<a id="422_MySQLReaderQuerySQLMode_184"></a>4.2.2 MySQLReader之QuerySQLMode</h6> 
<p>1、编写配置文件<br> （1）修改配置文件base_province.json<br> （2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "querySql": [
                                    "select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id&gt;=3"
                                ]
                            }
                        ],
                        "password": "000000",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、配置文件说明<br> （1）Reader参数说明<br> <img src="https://images2.imgbox.com/66/5c/e2fa7ZtB_o.png" alt="在这里插入图片描述"><br> 3、提交任务<br> （1）清空历史数据</p> 
<pre><code class="prism language-bash"> hadoop fs -rm -r -f /base_province/*
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash">python bin/datax.py job/base_province.json
</code></pre> 
<p>4、查看结果<br> （1）DataX打印日志<br> <img src="https://images2.imgbox.com/3f/7f/Ctyaktfd_o.png" alt="在这里插入图片描述"><br> （2）查看HDFS文件</p> 
<pre><code class="prism language-bash">hadoop fs -cat /base_province/* <span class="token operator">|</span> zcat
</code></pre> 
<h6>
<a id="423_DataX_283"></a>4.2.3 DataX传参</h6> 
<p>通常情况下，离线数据同步任务需要每日定时重复执行，故HDFS上的目标路径通常会包含一层日期，以对每日同步的数据加以区分，也就是说每日同步数据的目标路径不是固定不变的，因此DataX配置文件中HDFS Writer的path参数的值应该是动态的。为实现这一效果，就需要使用DataX传参的功能。<br> DataX传参的用法如下，在JSON配置文件中使用${param}引用参数，在提交任务时使用-p"-Dparam=value"传入参数值，具体示例如下。<br> 1、编写配置文件<br> （1）修改配置文件base_province.json</p> 
<p>（2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "connection": [
                            {
                                "jdbcUrl": [
                                    "jdbc:mysql://hadoop102:3306/gmall"
                                ],
                                "querySql": [
                                    "select id,name,region_id,area_code,iso_code,iso_3166_2 from base_province where id&gt;=3"
                                ]
                            }
                        ],
                        "password": "000000",
                        "username": "root"
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "column": [
                            {
                                "name": "id",
                                "type": "bigint"
                            },
                            {
                                "name": "name",
                                "type": "string"
                            },
                            {
                                "name": "region_id",
                                "type": "string"
                            },
                            {
                                "name": "area_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_code",
                                "type": "string"
                            },
                            {
                                "name": "iso_3166_2",
                                "type": "string"
                            }
                        ],
                        "compress": "gzip",
                        "defaultFS": "hdfs://hadoop102:8020",
                        "fieldDelimiter": "t",
                        "fileName": "base_province",
                        "fileType": "text",
                        "path": "/base_province/${dt}",
                        "writeMode": "append"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、提交任务<br> （1）创建目标路径</p> 
<pre><code class="prism language-bash"> hadoop fs -mkdir /base_province/2020-06-14
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash"> python bin/datax.py -p<span class="token string">"-Ddt=2020-06-14"</span> job/base_province.json
</code></pre> 
<p>3、查看结果</p> 
<pre><code class="prism language-bash">hadoop fs -ls /base_province
</code></pre> 
<h5>
<a id="43_HDFSMySQL_379"></a>4.3 同步HDFS数据到MySQL案例</h5> 
<p>案例要求：同步HDFS上的/base_province目录下的数据到MySQL gmall 数据库下的test_province表。<br> 需求分析：要实现该功能，需选用HDFSReader和MySQLWriter。<br> 1、编写配置文件<br> （1）创建配置文件test_province.json<br> （2）配置文件内容如下</p> 
<pre><code>{
    "job": {
        "content": [
            {
                "reader": {
                    "name": "hdfsreader",
                    "parameter": {
                        "defaultFS": "hdfs://hadoop102:8020",
                        "path": "/base_province",
                        "column": [
                            "*"
                        ],
                        "fileType": "text",
                        "compress": "gzip",
                        "encoding": "UTF-8",
                        "nullFormat": "\N",
                        "fieldDelimiter": "t",
                    }
                },
                "writer": {
                    "name": "mysqlwriter",
                    "parameter": {
                        "username": "root",
                        "password": "000000",
                        "connection": [
                            {
                                "table": [
                                    "test_province"
                                ],
                                "jdbcUrl": "jdbc:mysql://hadoop102:3306/gmall?useUnicode=true&amp;characterEncoding=utf-8"
                            }
                        ],
                        "column": [
                            "id",
                            "name",
                            "region_id",
                            "area_code",
                            "iso_code",
                            "iso_3166_2"
                        ],
                        "writeMode": "replace"
                    }
                }
            }
        ],
        "setting": {
            "speed": {
                "channel": 1
            }
        }
    }
}
</code></pre> 
<p>2、配置文件说明<br> （1）Reader参数说明<br> <img src="https://images2.imgbox.com/a0/8c/UwrlcuEs_o.png" alt="在这里插入图片描述"><br> （2）Writer参数说明<br> <img src="https://images2.imgbox.com/9b/9f/S9fxUydm_o.png" alt="在这里插入图片描述"><br> 3、提交任务<br> （1）在MySQL中创建gmall.test_province表</p> 
<pre><code class="prism language-bash">DROP TABLE IF EXISTS <span class="token variable"><span class="token variable">`</span>test_province<span class="token variable">`</span></span><span class="token punctuation">;</span>
CREATE TABLE <span class="token variable"><span class="token variable">`</span>test_province<span class="token variable">`</span></span>  <span class="token punctuation">(</span>
  <span class="token variable"><span class="token variable">`</span><span class="token function">id</span><span class="token variable">`</span></span> bigint<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> NOT NULL,
  <span class="token variable"><span class="token variable">`</span>name<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>region_id<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>area_code<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>iso_code<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  <span class="token variable"><span class="token variable">`</span>iso_3166_2<span class="token variable">`</span></span> varchar<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">)</span> CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL,
  PRIMARY KEY <span class="token punctuation">(</span><span class="token variable"><span class="token variable">`</span><span class="token function">id</span><span class="token variable">`</span></span><span class="token punctuation">)</span>
<span class="token punctuation">)</span> ENGINE <span class="token operator">=</span> InnoDB CHARACTER SET <span class="token operator">=</span> utf8 COLLATE <span class="token operator">=</span> utf8_general_ci ROW_FORMAT <span class="token operator">=</span> Dynamic<span class="token punctuation">;</span>
</code></pre> 
<p>（2）进入DataX根目录<br> （3）执行如下命令</p> 
<pre><code class="prism language-bash"> python bin/datax.py job/test_province.json 
</code></pre> 
<p>4、查看结果<br> （1）DataX打印日志<br> （2）查看MySQL目标表数据<br> <img src="https://images2.imgbox.com/b9/09/urut5cYu_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="5DataX_470"></a>5、DataX优化</h3> 
<h5>
<a id="51__471"></a>5.1 速度控制</h5> 
<p>DataX3.0提供了包括通道(并发)、记录流、字节流三种流控模式，可以随意控制你的作业速度，让你的作业在数据库可以承受的范围内达到最佳的同步速度。<br> <img src="https://images2.imgbox.com/d8/11/1eodbR7b_o.png" alt="在这里插入图片描述"><br> 注意事项：<br> 1.若配置了总record限速，则必须配置单个channel的record限速<br> 2.若配置了总byte限速，则必须配置单个channe的byte限速<br> 3.若配置了总record限速和总byte限速，channel并发数参数就会失效。因为配置了总record限速和总byte限速之后，实际channel并发数是通过计算得到的：<br> 计算公式为:<br> min(总byte限速/单个channel的byte限速，总record限速/单个channel的record限速)</p> 
<h5>
<a id="52__480"></a>5.2 内存调整</h5> 
<p>当提升DataX Job内Channel并发数时，内存的占用会显著增加，因为DataX作为数据交换通道，在内存中会缓存较多的数据。例如Channel中会有一个Buffer，作为临时的数据交换的缓冲区，而在部分Reader和Writer的中，也会存在一些Buffer，为了防止OOM等错误，需调大JVM的堆内存。<br> 建议将内存设置为4G或者8G，这个也可以根据实际情况来调整。<br> 调整JVM xms xmx参数的两种方式：一种是直接更改datax.py脚本；另一种是在启动的时候，加上对应的参数，如下：<br> python datax/bin/datax.py --jvm=“-Xms8G -Xmx8G” /path/to/your/job.json</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>