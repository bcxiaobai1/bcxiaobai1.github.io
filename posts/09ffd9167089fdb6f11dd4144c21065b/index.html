<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>《机器学习实战》10.K-均值聚类算法 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《机器学习实战》10.K-均值聚类算法</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p id="main-toc"><strong>目录</strong></p> 
<p id="%E5%88%A9%E7%94%A8K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AF%B9%E6%9C%AA%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE%E5%88%86%E7%BB%84-toc" style="margin-left:0px"><a href="#%E5%88%A9%E7%94%A8K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AF%B9%E6%9C%AA%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE%E5%88%86%E7%BB%84">利用K-均值聚类算法对未标注数据分组</a></p> 
<p id="K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-toc" style="margin-left:40px"><a href="#K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95">K-均值聚类算法</a></p> 
<p id="2%20%E4%BD%BF%E7%94%A8%E5%90%8E%E5%A4%84%E7%90%86%E6%9D%A5%E6%8F%90%E9%AB%98%E8%81%9A%E7%B1%BB%E6%80%A7%E8%83%BD-toc" style="margin-left:40px"><a href="#2%20%E4%BD%BF%E7%94%A8%E5%90%8E%E5%A4%84%E7%90%86%E6%9D%A5%E6%8F%90%E9%AB%98%E8%81%9A%E7%B1%BB%E6%80%A7%E8%83%BD">2 使用后处理来提高聚类性能</a></p> 
<p id="3%20%E4%BA%8C%E5%88%86K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95-toc" style="margin-left:40px"><a href="#3%20%E4%BA%8C%E5%88%86K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95">3 二分K-均值算法</a></p> 
<p id="4%20%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%AF%B9%E5%9C%B0%E5%9B%BE%E4%B8%8A%E7%9A%84%E7%82%B9%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB-toc" style="margin-left:40px"><a href="#4%20%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%AF%B9%E5%9C%B0%E5%9B%BE%E4%B8%8A%E7%9A%84%E7%82%B9%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB">4 示例：对地图上的点进行聚类</a></p> 
<p id="4.1%20Yahoo%EF%BC%81PlaceFinder%20API-toc" style="margin-left:80px"><a href="#4.1%20Yahoo%EF%BC%81PlaceFinder%20API">4.1 Yahoo！PlaceFinder API</a></p> 
<p id="4.2%20%E5%AF%B9%E5%9C%B0%E7%90%86%E5%9D%90%E6%A0%87%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB-toc" style="margin-left:80px"><a href="#4.2%20%E5%AF%B9%E5%9C%B0%E7%90%86%E5%9D%90%E6%A0%87%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB">4.2 对地理坐标进行聚类</a></p> 
<p id="5%20%E6%9C%AC%E7%AB%A0%E5%B0%8F%E7%BB%93-toc" style="margin-left:40px"><a href="#5%20%E6%9C%AC%E7%AB%A0%E5%B0%8F%E7%BB%93">5 本章小结</a></p> 
<hr id="hr-toc">
<p>本章涉及到的<a class="link-info" href="https://pan.baidu.com/s/1ps7QyJkRSL_EVwCXi9Kl9g?pwd=qdxj" title="相关代码和数据">相关代码和数据</a></p> 
<h1 id="%E5%88%A9%E7%94%A8K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%AF%B9%E6%9C%AA%E6%A0%87%E6%B3%A8%E6%95%B0%E6%8D%AE%E5%88%86%E7%BB%84">利用K-均值聚类算法对未标注数据分组</h1> 
<p>本章内容：</p> 
<blockquote> 
 <p>①K-均值聚类算法</p> 
 <p>②对聚类得到的簇进行后处理</p> 
 <p>③二分K-均值聚类算法</p> 
 <p>④对地理位置进行聚类</p> 
</blockquote> 
<p>聚类是一种无监督学习，他将类似的对象归到同一个簇中。有点像全自动分类，聚类算法几乎可以应用于所有对象，簇内的对象越相似，聚类的效果就越好。</p> 
<p>K-means算法，可以发现k个不同的簇，且每个簇的中心采用簇中所含值的均值计算而成</p> 
<p><span style="background-color:#ed7976">簇识别：</span>簇识别给出聚类结果的含义，假定有一些数据，现在将相似的数据归到一起，簇识别会告诉我们在这些簇到底都是什么。聚类与分类最大的不同在于，分类的目标事先已知，而聚类则不一样，因为其产生的效果与分类相同，而只是类别没有预先定义，剧烈有时也被称为无监督分类</p> 
<h2 id="K-%E5%9D%87%E5%80%BC%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95">K-均值聚类算法</h2> 
<blockquote> 
 <p>优点：容易实现</p> 
 <p>缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢</p> 
 <p>适用数据类型：数值型数据</p> 
</blockquote> 
<p>K-均值是发现给定数据集的k个簇的算法。簇个数是由用户给定的，每一个簇通过其质心，即簇中所有点的中心来描述。</p> 
<p>K-均值算法的工作流程是这样的。首先，随机确定k个初始点作为质心，然后将数据集中的每个点分配到每个簇中，具体来讲，为每个点找到距其最近的质心，并将其分配到盖志新所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均。</p> 
<p>伪代码如下：</p> 
<blockquote> 
 <p>创建k个点作为起始质心（经常是随机选择）</p> 
 <p>当任意一个点的簇分配结果发生改变时</p> 
 <p>    对数据集中的每个数据点</p> 
 <p>        对每个质心</p> 
 <p>            计算质心与数据点之间的距离</p> 
 <p>        将数据分配到距其最近的簇</p> 
 <p>    对每个簇，计算簇中所有点的均值并将均值作为质心</p> 
</blockquote> 
<p>一般流程：</p> 
<blockquote> 
 <p>①收集数据：适用任意方法</p> 
 <p>②准备数据：需要数值型数据来计算距离，也可以将标称型数据映射为二值型数据再用于距离计算</p> 
 <p>③分析数据：适用任意方法</p> 
 <p>④训练算法：无监督学习没有训练过程</p> 
 <p>⑤测试算法：应用聚类算法，观察结果。可以使用量化的误差指标如误差平方和来评价算法的结果</p> 
 <p>⑥使用算法：可以用于所希望的任何应用。通常情况下，簇质心可以代表整个簇来做出决策。</p> 
</blockquote> 
<pre><code class="language-python">from numpy import *

def loadDataSet(fileName):
    dataMat=[]
    fr=open(fileName)
    for line in fr.readlines():
        curLine=line.strip().split('t')
        fltLine=list(map(float,curLine))
        dataMat.append(fltLine)
    return dataMat

# 计算欧氏距离
def distEclud(vecA,vecB):
    return sqrt(sum(power(vecA-vecB,2)))

# 构建一个包含k个随机质心的集合
def randCent(dataSet,k):
    n=shape(dataSet)[1]
    # 初始化矩阵：定义一个k*n的二维全是0的矩阵
    centroids=mat(zeros((k,n)))
    for j in range(n):
        minJ=min(dataSet[:,j])
        rangeJ=float(max(dataSet[:,j])-minJ)
        centroids[:,j]=minJ+rangeJ*random.rand(k,1)
    return centroids</code></pre> 
<p>测试以上函数</p> 
<pre><code class="language-python">datMat=mat(loadDataSet('testSet.txt'))
# 测试计算距离函数
print(distEclud(datMat[0],datMat[1]))
# 测试随机质心函数
randCent(datMat,2)</code></pre> 
<p>得到的输出结果为：</p> 
<p><img alt="" height="132" src="https://images2.imgbox.com/55/f1/fU0o2VbR_o.png" width="480"></p> 
<p>可以看出上述函数没有问题，接下来写K-means算法的函数以及对结果进行画图的函数</p> 
<pre><code class="language-python"># k-均值聚类算法
        # 数据集，簇的个数 距离计算方式 创建初始质心的方式
def kMeans(dataSet,k,distMeas=distEclud,createCent=randCent):
    # 确定数据集中点的个数
    m=shape(dataSet)[0]
    # 创建一个矩阵存放每个点的簇分配结果（簇索引值、误差：当前点到簇质心的距离），后面会使用误差来评价聚类的效果
    clusterAssment=mat(zeros((m,2)))

    centroids=createCent(dataSet,k)
    clusterChanged=True
    # 循环直到所有点簇的分配结果不再改变为止
    while clusterChanged:
        clusterChanged=False
        for i in range(m):
            minDist=inf
            minIndex=-1
            for j in range(k):
                distJI=distMeas(centroids[j,:],dataSet[i,:])
                if distJI&lt;minDist:
                    minDist=distJI
                    minIndex=j
            if clusterAssment[i,0]!=minIndex:
                clusterChanged=True
            clusterAssment[i,:]=minIndex
            minDist**2
        print(centroids)
        for cent in range(k):
            ptsInClust=dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]
            centroids[cent,:]=mean(ptsInClust,axis=0)
            # 质心    点分配结果
    # print(clusterAssment)
    return centroids,clusterAssment

import matplotlib.pyplot as plt
# 画图函数
def plotDataKMeans(centroids,clusterAssment):
    numClust,n=shape(centroids)
    fig=plt.figure()
    # 点的类型
    scatterMarkers=['s','o','^','8','p','d','v','h','&gt;','&lt;']
    axprops=dict(xticks=[],yticks=[])
    ax=fig.add_subplot(111)
    for i in range(numClust):
        # nonzero函数得到数组array中非零元素的位置
        ptsInCurrCluster = datMat[nonzero(clusterAssment[:,0].A==i)[0],:]
        markerStyle = scatterMarkers[i % len(scatterMarkers)]
        ax.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=90)
    print(centroids)
    ax.scatter(centroids[:,0].flatten().A[0], centroids[:,1].flatten().A[0], marker='+', s=300)
    plt.show()
</code></pre> 
<p> 测试结果</p> 
<pre><code class="language-python">myCenttroids,clustAssing=kMeans(datMat,4)
# (可视化)
plotDataKMeans(myCenttroids,clustAssing)</code></pre> 
<p>得到的输出结果为：</p> 
<p><img alt="" height="749" src="https://images2.imgbox.com/24/7d/fIgEtZ3A_o.png" width="464"></p> 
<p>到目前为止，关于聚类的一切都进展顺利，但是事情并不总是如此</p> 
<h2 id="2%20%E4%BD%BF%E7%94%A8%E5%90%8E%E5%A4%84%E7%90%86%E6%9D%A5%E6%8F%90%E9%AB%98%E8%81%9A%E7%B1%BB%E6%80%A7%E8%83%BD">2 使用后处理来提高聚类性能</h2> 
<p>前面提到，在k-均值聚类中簇的数目k是一个用户预先定义的饿参数，但是k的选择并不一定准确。</p> 
<p>考虑到上面的聚类结果，其实点的分配结果并没有那么准确。K-均值算法收敛单聚类效果交叉的原因时，<strong>K-均值算法手来能与局部最小值，而非全局最小值。</strong></p> 
<p>一种用于度量剧烈效果的指标是SSE（误差平方和）。SSE值越小表示数据集越接近于他们的质心，聚类效果也越好。因为误差取了平方，因此更加中重视那些远离中心的点。一种可以肯定可以降低SSE值的方法是增加簇的个数，但这违背了聚类的目标，聚类的目标是保持粗数目不变的情况下提高簇的质量。</p> 
<p>我们可以对生成的簇进行后处理，一种方法是将具有最大SSE值的簇划分为两个簇，具体实现时可以将最大粗包含的点过滤出来并在这些点上运行K-均值聚类算法，其中k设为2为了保持簇总数不变，可以将某两个簇进行合并。</p> 
<p>有两种可以量化的方法：合并最近的质心或者合并两个使得SSE增幅最小的质心。第一种思路通过计算所有质心之间的距离，然后合并距离最近的两个点来实现。第二种方法需要合并两个簇然后计算总SSE值。必须在所有可能的两个簇上重复上述处理过程，直到找到最佳的两个簇为止。</p> 
<h2 id="3%20%E4%BA%8C%E5%88%86K-%E5%9D%87%E5%80%BC%E7%AE%97%E6%B3%95">3 二分K-均值算法</h2> 
<p>为了歌赋上述问题，一种二分K-均值算法：<strong>该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择其中一个簇继续进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE的值。上述基于SSE的划分过程不断重复，直到得到用户的满意为止。</strong></p> 
<p>伪代码：</p> 
<blockquote> 
 <p>将所有点看成一个簇</p> 
 <p>当簇的数目小于k时</p> 
 <p>    对每一个簇</p> 
 <p>        计算总误差</p> 
 <p>        在给定的簇上面进行K-均值聚类（k=2）</p> 
 <p>        计算将该簇一分为二后的总误差</p> 
 <p>    选择使得误差最小的那个簇进行划分操作</p> 
</blockquote> 
<p>另一种做法是选择SSE最大的簇进行划分，直到簇数目达到用户指定的数目为止。</p> 
<pre><code class="language-python">def bikmeans(dataSet, k, distMeas=distEclud):
    """
    二分K-均值聚类算法
    :param dataSet:数据集
    :param k:质心数量
    :param distMeas:距离计算方法，默认欧氏距离distEclud()
    :return:
    """
    numSamples = dataSet.shape[0]  # 获得数据集的样本数
    
    # 创建一个初始簇
    clusterAssment = mat(zeros((numSamples, 2)))  # 初始化一个元素值0的(numSamples,2)矩阵
    centroid0 = mean(dataSet, axis=0).tolist()[0]  # 列表中的第一个列表元素：即全部数据每个属性
    centList = [centroid0]  # 当前聚类列表为将数据集聚为一类
    for j in range(numSamples):  # 遍历每个数据集样本
        clusterAssment[j, 1] = (distMeas(mat(centroid0), dataSet[j, :])) ** 2  # 计算当前聚为一类时各个数据点距离质心的平方距离

     # 循环，直至二分k-均值达到k类为止
    while len(centList) &lt; k:  
        lowestSSE = inf  # 将当前最小平方误差置为正无穷
        for i in range(len(centList)):  # 遍历当前每个聚类
            # 通过数组过滤筛选出属于第i类的数据集合
            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A == i)[0], :]
            # 对该类利用二分k-均值算法进行划分，返回划分后结果，及误差
            centroidMat, splitClusAss = kMeans(ptsInCurrCluster, 2, distMeas)
            sseSplit = sum(splitClusAss[:, 1])  # 计算该类划分后两个类的误差平方和
            # 计算数据集中不属于该类的数据的误差平方和
            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1])
            print("sseSplit,and notSplit:", sseSplit, sseNotSplit)
            if (sseSplit + sseNotSplit) &lt; lowestSSE:  # 划分第i类后总误差小于当前最小总误差
                bestCentToSplit = i  # 第i类作为本次划分类
                bestNewCents = centroidMat  # 第i类划分后得到的两个质心向量
                bestClustAss = splitClusAss.copy()  # 复制第i类中数据点的聚类结果即误差值
                lowestSSE = sseSplit + sseNotSplit  # 将划分第i类后的总误差作为当前最小误差
        # 数组过滤筛选出本次2-均值聚类划分后类编号为1数据点，将这些数据点类编号变为当前类个数+1，作为新的一个聚类
        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)
        # 同理，将划分数据集中类编号为0的数据点的类编号仍置为被划分的类编号，使类编号连续不出现空缺
        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit
        print("the bestCentToSplit is：", bestCentToSplit)
        print("the len of bestClustAss is :", len(bestClustAss))
        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0]  # 更新质心列表中的变化后的质心向量
        centList.append(bestNewCents[1, :].tolist()[0])  # 添加新的类的质心向量
        # 更新clusterAssment列表中参与2-均值聚类数据点变化后的分类编号，及数据该类的误差平方
        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss
    return mat(centList), clusterAssment</code></pre> 
<p>测试函数：</p> 
<pre><code class="language-python">datMat2=mat(loadDataSet("testSet.txt"))
# print(datMat2)
centList,myNewAssments=kMeans(datMat2,4)
plotDataKMeans(centList,myNewAssments)</code></pre> 
<p>得到的输出结果为：</p> 
<p><img alt="" height="436" src="https://images2.imgbox.com/9f/86/wwo2A2ax_o.png" width="279"></p> 
<p><img alt="" height="307" src="https://images2.imgbox.com/25/03/H4Qek211_o.png" width="450"></p> 
<h2 id="4%20%E7%A4%BA%E4%BE%8B%EF%BC%9A%E5%AF%B9%E5%9C%B0%E5%9B%BE%E4%B8%8A%E7%9A%84%E7%82%B9%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB">4 示例：对地图上的点进行聚类</h2> 
<p>示例：对于地理数据应用的二分K-均值算法</p> 
<blockquote> 
 <p>①收集数据：使用Yahoo！ PlaceFinder API收集数据，这里因为API已经不能用，我们直接读取txt文本</p> 
 <p>②准备数据：只保留经纬度信息</p> 
 <p>③分析数据：使用matplotlib来构建一个二维数据图，其中包含簇与地图</p> 
 <p>④训练算法：无监督学习不用训练算法</p> 
 <p>⑤测试算法：使用biKmeans函数</p> 
 <p>⑥使用算法：最后的输出是包含簇及簇中心的地图</p> 
</blockquote> 
<h3 id="4.1%20Yahoo%EF%BC%81PlaceFinder%20API">4.1 Yahoo！PlaceFinder API</h3> 
<p>书中所提到的API已经不能在使用，我们直接使用已经提供的places.txt文件</p> 
<h3 id="4.2%20%E5%AF%B9%E5%9C%B0%E7%90%86%E5%9D%90%E6%A0%87%E8%BF%9B%E8%A1%8C%E8%81%9A%E7%B1%BB">4.2 对地理坐标进行聚类</h3> 
<pre><code class="language-python"># 球面距离计算
def distSLC(vecA,vecB):
    a=sin(vecA[0,1]*pi/180)*sin(vecB[0,1]*pi/180)
    b=cos(vecA[0,1]*pi/180)*cos(vecB[0,1]*pi/180)*cos(pi*(vecB[0,0]-vecA[0,0])/180)
    # 反三角函数中的反余弦
    return arccos(a+b)*6371.0



import matplotlib.pyplot as plt
def clusterClubs(numClust=5):
    datList=[]
    # 打开文本文件
    for line in open('places.txt').readlines():
        lineArr=line.split('t')
        # 文本文件的第五列和第四列为坐标
        datList.append([float(lineArr[4]),float(lineArr[3])])
    datMat=mat(datList)
    # 进行聚类：距离计算公式利用球面计算公式
    myCentroids,clustAssing=bikmeans(datMat,numClust,distMeas=distSLC)

    # 画图
    fig=plt.figure()
    # rect=[0.1,0.1,0.8,0.8]
    rect=[1,1,1,1]
    scatterMarkers=['s','o','^','8','p','d','v','h','&gt;','&lt;']
    axprops=dict(xticks=[],yticks=[])
    ax0=fig.add_axes(rect,label='ax0',**axprops)
    imgP=plt.imread("Portland.png")
    ax0.imshow(imgP)
    ax1=fig.add_axes(rect,label='ax1',frameon=False)

    for i in range(numClust):
        # nonzero函数得到数组array中非零元素的位置
        ptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A==i)[0],:]
        markerStyle = scatterMarkers[i % len(scatterMarkers)]
        ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=50)
    # print(centroids)
    ax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0], marker='+', s=200)
    plt.show()
 </code></pre> 
<p> 测试函数</p> 
<pre><code class="language-python">clusterClubs()</code></pre> 
<p>得到的输出结果为：</p> 
<p><img alt="" height="670" src="https://images2.imgbox.com/7d/04/9Q4V667i_o.png" width="272"></p> 
<p> <img alt="" height="321" src="https://images2.imgbox.com/de/f3/57VByYCd_o.png" width="488"></p> 
<h2 id="5%20%E6%9C%AC%E7%AB%A0%E5%B0%8F%E7%BB%93">5 本章小结</h2> 
<p>聚类是一种无监督学习。所谓无监督就是事先并不知道要寻找的内容，即没有目标变量。聚类将数据点归到多个簇中，其中相似数据点处于同一簇，而不相似数据点处于不同簇中。聚类中可以使用多种不同方法来计算相似度</p> 
<p>一种广泛使用的聚类算法是K-均值算法。其中k是用户指定的要创建的簇的数目。<strong>K-均值聚类算法</strong>以k个随机质心开始，算法会计算每个点到质心的距离。每个点被分配到距其最近的簇质心，然后紧接着基于新分配到簇的点更新质心。以上过程重复数次，直到簇质心不再改变。这种简单的算法非常有效但是也容易受到初始簇质心的影响。</p> 
<p>为了获得更好的效果，可以使用另一种<strong>二分K-均值的聚类算法</strong>。二分K-均值算法首先将所有点作为一个簇，然后使用K-均值算法（k=2）对其划分。下一次迭代时，选择有最大误差的簇进行划分。该过程重复直到k个簇创建成功为止，效果更好。</p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>