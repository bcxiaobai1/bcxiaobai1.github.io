<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>GoogLeNet网络结构详解与代码复现 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">GoogLeNet网络结构详解与代码复现</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <div class="kdocs-document">
 <p> 参考内容来自up：<a class="kdocs-link" style="color:#0A6CFF" href="https://www.bilibili.com/video/BV1z7411T7ie/?spm_id_from=333.999.0.0&amp;vd_source=d4e1261efd87c5ce017b4129586b6763" target="_blank" rel="noopener noreferrer">5.1 GoogLeNet网络详解_哔哩哔哩_bilibili</a></p>
 <p>up的代码和ppt：<a class="kdocs-link" style="color:#0A6CFF" href="https://github.com/WZMIAOMIAO/deep-learning-for-image-processing" target="_blank" rel="noopener noreferrer">GitHub - WZMIAOMIAO/deep-learning-for-image-processing: deep learning for image processing including classification and object-detection etc.</a></p>
 <p></p>
 <h2 style="text-align:left"><span class="kdocs-bold" style="font-weight:bold">一、简介</span></h2>
 <p></p>
 <p>GoogLeNet在2014年由Google团队提出，斩获当年ImageNet竞 赛中Classification Task (分类任务)</p>
 <p> </p>
 <ul><li style="margin-left:1.4em;list-style-type:disc;text-indent:0"><p>原论文地址：<a class="kdocs-link" style="color:#0A6CFF" href="https://arxiv.org/abs/1409.4842" target="_blank" rel="noopener noreferrer">[1409.4842] Going Deeper with Convolutions (arxiv.org)</a></p></li></ul>
 <ul><li style="margin-left:1.4em;list-style-type:disc;text-indent:0"><p><span class="kdocs-color" style="color:#C21C13"><span class="kdocs-bold" style="font-weight:bold">GoogLeNet 的创新点：</span> </span></p></li></ul>
 <ul><li style="margin-left:2.8em;list-style-type:circle;text-indent:0"><p>引入了 Inception 结构（融合不同尺度的特征信息） </p></li></ul>
 <ul><li style="margin-left:2.8em;list-style-type:circle;text-indent:0"><p>使用1x1的卷积核进行降维以及映射处理 （虽然VGG网络中也有，但该论文介绍的更详细）</p></li></ul>
 <ul><li style="margin-left:2.8em;list-style-type:circle;text-indent:0"><p>添加两个辅助分类器帮助训练</p></li></ul>
 <ul><li style="margin-left:2.8em;list-style-type:circle;text-indent:0"><p>丢弃全连接层，使用平均池化层（大大减少模型参数，除去两个辅助分类器，网络大小只有vgg的1/20）</p></li></ul>
 <div class="kdocs-line-container">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:1045px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:53.68421%;height:0">
    <img src="https://images2.imgbox.com/06/01/EABncI6k_o.png" style="margin-left:;width:1045px;margin-top:-53.68421%;height:auto">
   </div>
  </div>
 </div>
 <p></p>
 <p></p>
 <h2 style="text-align:left"><span class="kdocs-bold" style="font-weight:bold">二、详解</span></h2>
 <p></p>
 <h3 style="text-align:left">
<span class="kdocs-bold" style="font-weight:bold">1.</span> <span class="kdocs-bold" style="font-weight:bold">inception 结构</span>
</h3>
 <p>传统的CNN结构如AlexNet、VggNet 网络都是串联的结构，即将一系列的卷积层和池化层进行串联得到的结构。</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:582px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:58.591064%;height:0">
    <img src="https://images2.imgbox.com/69/f4/WOwCyLet_o.png" style="margin-left:;width:582px;margin-top:-58.591064%;height:auto">
   </div>
  </div>
 </div>
 <p></p>
 <p style="text-align:left"><span class="kdocs-fontSize" style="font-size:13pt"><span class="kdocs-bold" style="font-weight:bold">inception原始结构</span></span></p>
 <p>GoogLeNet 提出了一种并联结构，下图是论文中提出的inception原始结构，将特征矩阵<span class="kdocs-bold" style="font-weight:bold">同时输入到多个分支</span>进行处理，并将输出的特征矩阵<span class="kdocs-bold" style="font-weight:bold">按深度进行拼接</span>，得到最终输出特征矩阵</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:543px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:56.353592%;height:0">
    <img src="https://images2.imgbox.com/aa/1e/xbT1uMop_o.png" style="margin-left:;width:543px;margin-top:-56.353592%;height:auto">
   </div>
  </div>
 </div>
 <p><span class="kdocs-color" style="color:#C21C13">注：每个分支所得特征矩阵的高和宽必须相同（通过调整stride和padding），以保证输出特征能在深度上进行拼接</span></p>
 <p></p>
 <p><span class="kdocs-bold" style="font-weight:bold">1×1卷积核的降维功能</span></p>
 <p>同样是对一个深度为512的特征矩阵使用64个大小为5x5的卷积核进行卷积：</p>
 <p style="text-indent:1.4em">不使用1x1卷积核进行降维的 话一共需要819200个参数，</p>
 <p style="text-indent:1.4em">如果使用1x1卷积核进行降维一共需要50688个参数，明显少了很多</p>
 <div class="kdocs-line-container">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:1256px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:47.770702%;height:0">
    <img src="https://images2.imgbox.com/8f/73/TKU6mKYy_o.png" style="margin-left:;width:1256px;margin-top:-47.770702%;height:auto">
   </div>
  </div>
 </div>
 <p><span class="kdocs-color" style="color:#C21C13">注：CNN参数个数 = 卷积核尺寸×卷积核深度 × 卷积核组数 = 卷积核尺寸 × 输入特征矩阵深度 × 输出特征矩阵深度</span></p>
 <p></p>
 <h3 style="text-align:left">
<span class="kdocs-bold" style="font-weight:bold">2.</span> <span class="kdocs-bold" style="font-weight:bold">辅助分类器（Auxiliary Classifier）</span>
</h3>
 <p>原文中关于辅助分类器的描述：</p>
 <blockquote class="kdocs-blockquote">
  •An average pooling layer with 5×5 filter size and stride 3, resulting in an4×4×512 output for the
  <span class="kdocs-color" style="background-color:#FBF5B3;color:#C21C13"> (4a), </span>and 4×4×528 for the
  <span class="kdocs-color" style="background-color:#FBF5B3;color:#C21C13"> (4d)</span> stage.
  <span class="kdocs-italic" style="font-style:italic">（两个辅助分类器分别来自</span>
  <span class="kdocs-fontSize" style="font-size:13pt"><span class="kdocs-italic" style="font-style:italic">inception4a和inception4d</span></span>
  <span class="kdocs-italic" style="font-style:italic">）</span>
  <br>•A 1×1 convolution with 128 filters for dimension reduction and rectified linearactivation. 
  <br>•A fully connected layer with 1024 units and rectified linear activation. 
  <br>•A dropout layer with 70% ratio of dropped outputs. 
  <br>•A linear layer with softmax loss as the classifier (predicting thesame 1000 classes as the main classifier, but removed at inference time). 
 </blockquote>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:431px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:98.37587%;height:0">
    <img src="https://images2.imgbox.com/a0/4a/nQkrt53j_o.png" style="margin-left:;width:431px;margin-top:-98.37587%;height:auto">
   </div>
  </div>
 </div>
 <p style="text-align:left"><span class="kdocs-fontSize" style="font-size:13pt"><span class="kdocs-color" style="color:#C21C13"><span class="kdocs-bold" style="font-weight:bold">辅助分类器的两个分支有什么用呢？</span></span></span></p>
 <p style="text-align:left"><span class="kdocs-fontSize" style="font-size:13pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">作用一：</span>可以把他看做inception网络中的一个小细节，<span class="kdocs-bold" style="font-weight:bold">它确保了即便是隐藏单元和中间层也参与了特征计算</span>，他们也能预测图片的类别，他在inception网络中起到一种调整的效果，并且能防止网络发生过拟合。</span></p>
 <p style="text-align:left"><span class="kdocs-fontSize" style="font-size:13pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">作用二：</span>给定深度相对较大的网络，有效传播梯度反向通过所有层的能力是一个问题。通过将辅助分类器添加到这些中间层，可以期望较低阶段分类器的判别力。在训练期间，它们的损失以折扣权重（辅助分类器损失的权重是0.3）加到网络的整个损失上。</span></p>
 <p style="text-align:left"><span class="kdocs-fontSize" style="font-size:13pt">Inception V1的参数量=5607184，约为560w</span></p>
 <p></p>
 <p></p>
 <p>AlexNet 和 VGG 都只有1个输出层，<span class="kdocs-bold" style="font-weight:bold">GoogLeNet 有3个输出层，其中的两个是辅助分类层</span>。</p>
 <p>如下图所示，网络主干右边的 两个分支 就是 辅助分类器，其结构一模一样。</p>
 <p>在训练模型时，将两个辅助分类器的损失乘以权重（论文中是0.3）加到网络的整体损失上，再进行反向传播。</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:407px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:306.63388%;height:0">
    <img src="https://images2.imgbox.com/e1/53/14NwRCFq_o.png" style="margin-left:;width:407px;margin-top:-306.63388%;height:auto">
   </div>
  </div>
 </div>
 <p></p>
 <p></p>
 <h3>
<span class="kdocs-bold" style="font-weight:bold">3.</span> <span class="kdocs-bold" style="font-weight:bold">GoogLeNet 参数理解</span>
</h3>
 <p></p>
 <p>下面是原论文中给出的网络参数列表，配合上图查看</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:676px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:55.029583%;height:0">
    <img src="https://images2.imgbox.com/18/e7/kYd5Rs52_o.png" style="margin-left:;width:676px;margin-top:-55.029583%;height:auto">
   </div>
  </div>
 </div>
 <p style="text-align:left">计算公式：</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:384px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:10.15625%;height:0">
    <img src="https://images2.imgbox.com/91/d7/h7qH0gWa_o.png" style="margin-left:;width:384px;margin-top:-10.15625%;height:auto">
   </div>
  </div>
 </div>
 <p>对于Inception模块，所需要使用到参数有<span class="kdocs-fontSize" style="font-size:9pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">#1x1 </span></span>, <span class="kdocs-fontSize" style="font-size:9pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">#3x3reduce</span></span>, <span class="kdocs-fontSize" style="font-size:9pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">#3x3</span></span>, <span class="kdocs-fontSize" style="font-size:9pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">#5x5reduce</span></span>, <span class="kdocs-fontSize" style="font-size:9pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">#5x5</span></span>, <span class="kdocs-fontSize" style="font-size:9pt"><span class="kdocs-color" style="background-color:#F7C7D3;color:#C21C13">poolproj</span></span>，这6个参数，分别对应着所使用的卷积核个数。</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:523px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:52.390057%;height:0">
    <img src="https://images2.imgbox.com/cb/a4/aBYomCNg_o.png" style="margin-left:;width:523px;margin-top:-52.390057%;height:auto">
   </div>
  </div>
 </div>
 <h3>4. 使用参数比较</h3>
 <p>辅助分类器在训练过程才使用</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:566px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:50.706715%;height:0">
    <img src="https://images2.imgbox.com/cd/99/gdl37zNO_o.png" style="margin-left:;width:566px;margin-top:-50.706715%;height:auto">
   </div>
  </div>
 </div>
 <p>使用VGG网络多的原因：</p>
 <ul><li style="margin-left:2.8em;list-style-type:circle;text-indent:0"><p>VGG网络搭建比较方便</p></li></ul>
 <ul><li style="margin-left:2.8em;list-style-type:circle;text-indent:0"><p>GoogLeNet有两个辅助分类器，搭建和修改比较麻烦</p></li></ul>
 <p>只针对分类而言，GoogLeNet明显更优秀。</p>
 <p></p>
 <h2 style="text-align:left"><span class="kdocs-bold" style="font-weight:bold">三、网络搭建</span></h2>
 <p></p>
 <h3>
<span class="kdocs-bold" style="font-weight:bold">1.</span> <span class="kdocs-bold" style="font-weight:bold">model.py</span>
</h3>
 <p>相比于 AlexNet 和 VggNet 只有卷积层和全连接层这两种结构，GoogLeNet多了 inception 和 辅助分类器（Auxiliary Classifier），而 inception 和 辅助分类器 也是由多个卷积层和全连接层组合的，因此在定义模型时可以将 卷积、inception 、辅助分类器定义成不同的类，调用时更加方便。</p>
 <pre class="kdocs-plaintext"><code class="language-plaintext">import torch.nn as nn
import torch
import torch.nn.functional as F


class GoogLeNet(nn.Module):
    def __init__(self, num_classes=1000, aux_logits=True, init_weights=False):
        super(GoogLeNet, self).__init__()
        self.aux_logits = aux_logits

        self.conv1 = BasicConv2d(3, 64, kernel_size=7, stride=2, padding=3)
        self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.conv2 = BasicConv2d(64, 64, kernel_size=1)
        self.conv3 = BasicConv2d(64, 192, kernel_size=3, padding=1)
        self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.inception3a = Inception(192, 64, 96, 128, 16, 32, 32)
        self.inception3b = Inception(256, 128, 128, 192, 32, 96, 64)
        self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.inception4a = Inception(480, 192, 96, 208, 16, 48, 64)
        self.inception4b = Inception(512, 160, 112, 224, 24, 64, 64)
        self.inception4c = Inception(512, 128, 128, 256, 24, 64, 64)
        self.inception4d = Inception(512, 112, 144, 288, 32, 64, 64)
        self.inception4e = Inception(528, 256, 160, 320, 32, 128, 128)
        self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True)

        self.inception5a = Inception(832, 256, 160, 320, 32, 128, 128)
        self.inception5b = Inception(832, 384, 192, 384, 48, 128, 128)

        if self.aux_logits:
            self.aux1 = InceptionAux(512, num_classes)
            self.aux2 = InceptionAux(528, num_classes)

        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))     #自适应平均池化下采样操作好处：不论输入的特征矩阵的高和宽时多少，都能得到指定的大小
        self.dropout = nn.Dropout(0.4)
        self.fc = nn.Linear(1024, num_classes)
        if init_weights:
            self._initialize_weights()

    def forward(self, x):
        # N x 3 x 224 x 224
        x = self.conv1(x)
        # N x 64 x 112 x 112
        x = self.maxpool1(x)
        # N x 64 x 56 x 56
        x = self.conv2(x)
        # N x 64 x 56 x 56
        x = self.conv3(x)
        # N x 192 x 56 x 56
        x = self.maxpool2(x)

        # N x 192 x 28 x 28
        x = self.inception3a(x)
        # N x 256 x 28 x 28
        x = self.inception3b(x)
        # N x 480 x 28 x 28
        x = self.maxpool3(x)
        # N x 480 x 14 x 14
        x = self.inception4a(x)
        # N x 512 x 14 x 14
        if self.training and self.aux_logits:    # eval model lose this layer
            aux1 = self.aux1(x)

        x = self.inception4b(x)
        # N x 512 x 14 x 14
        x = self.inception4c(x)
        # N x 512 x 14 x 14
        x = self.inception4d(x)
        # N x 528 x 14 x 14
        if self.training and self.aux_logits:    # eval model lose this layer
            aux2 = self.aux2(x)

        x = self.inception4e(x)
        # N x 832 x 14 x 14
        x = self.maxpool4(x)
        # N x 832 x 7 x 7
        x = self.inception5a(x)
        # N x 832 x 7 x 7
        x = self.inception5b(x)
        # N x 1024 x 7 x 7

        x = self.avgpool(x)
        # N x 1024 x 1 x 1
        x = torch.flatten(x, 1)
        # N x 1024
        x = self.dropout(x)
        x = self.fc(x)
        # N x 1000 (num_classes)
        if self.training and self.aux_logits:   # eval model lose this layer
            return x, aux2, aux1
        return x

    def _initialize_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)


class Inception(nn.Module):     #定义Inception结构
    def __init__(self, in_channels, ch1x1, ch3x3red, ch3x3, ch5x5red, ch5x5, pool_proj):
        super(Inception, self).__init__()

        self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)

        self.branch2 = nn.Sequential(
            BasicConv2d(in_channels, ch3x3red, kernel_size=1),
            BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1)   # 保证输出大小等于输入大小
        )

        self.branch3 = nn.Sequential(
            BasicConv2d(in_channels, ch5x5red, kernel_size=1),
            # 在官方的实现中，其实是3x3的kernel并不是5x5，这里我也懒得改了，具体可以参考下面的issue
            # Please see https://github.com/pytorch/vision/issues/906 for details.
            BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2)   # 保证输出大小等于输入大小
        )

        self.branch4 = nn.Sequential(
            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),
            BasicConv2d(in_channels, pool_proj, kernel_size=1)
        )

    def forward(self, x):
        branch1 = self.branch1(x)
        branch2 = self.branch2(x)
        branch3 = self.branch3(x)
        branch4 = self.branch4(x)

        outputs = [branch1, branch2, branch3, branch4]
        return torch.cat(outputs, 1)        #在深度方向拼接


class InceptionAux(nn.Module):      #定义辅助分类器结构
    def __init__(self, in_channels, num_classes):
        super(InceptionAux, self).__init__()
        self.averagePool = nn.AvgPool2d(kernel_size=5, stride=3)
        self.conv = BasicConv2d(in_channels, 128, kernel_size=1)  # output[batch, 128, 4, 4]

        self.fc1 = nn.Linear(2048, 1024)
        self.fc2 = nn.Linear(1024, num_classes)

    def forward(self, x):
        # aux1: N x 512 x 14 x 14, aux2: N x 528 x 14 x 14
        x = self.averagePool(x)
        # aux1: N x 512 x 4 x 4, aux2: N x 528 x 4 x 4
        x = self.conv(x)
        # N x 128 x 4 x 4
        x = torch.flatten(x, 1)
        x = F.dropout(x, 0.5, training=self.training)
        # N x 2048
        x = F.relu(self.fc1(x), inplace=True)
        x = F.dropout(x, 0.5, training=self.training)
        # N x 1024
        x = self.fc2(x)
        # N x num_classes
        return x


class BasicConv2d(nn.Module):       #包含卷积和激活函数的卷积模板
    def __init__(self, in_channels, out_channels, **kwargs):
        super(BasicConv2d, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)
        self.relu = nn.ReLU(inplace=True)

    def forward(self, x):
        x = self.conv(x)
        x = self.relu(x)
        return x</code></pre>
 <p></p>
 <h3 style="text-align:left">
<span class="kdocs-bold" style="font-weight:bold">2.</span> <span class="kdocs-bold" style="font-weight:bold">train.py</span>
</h3>
 <p style="text-align:null">训练部分跟AlexNet和VGG类似，有两点需要注意：</p>
 <ol start="1"><li style="margin-left:1.4em;list-style-type:decimal;text-indent:0"><p>实例化网络时的参数</p></li></ol>
 <pre class="kdocs-plaintext"><code class="language-plaintext">net = GoogLeNet(num_classes=5, aux_logits=True, init_weights=True)</code></pre>
 <ol start="2"><li style="margin-left:1.4em;list-style-type:decimal;text-indent:0"><p>GoogLeNet的网络输出 loss 有三个部分，分别是主干输出loss、两个辅助分类器输出loss（权重0.3）</p></li></ol>
 <pre class="kdocs-plaintext"><code class="language-plaintext">logits, aux_logits2, aux_logits1 = net(images.to(device))
loss0 = loss_function(logits, labels.to(device))
loss1 = loss_function(aux_logits1, labels.to(device))
loss2 = loss_function(aux_logits2, labels.to(device))
loss = loss0 + loss1 * 0.3 + loss2 * 0.3
loss.backward()
optimizer.step()</code></pre>
 <pre class="kdocs-plaintext"><code class="language-plaintext">import os
import sys
import json

import torch
import torch.nn as nn
from torchvision import transforms, datasets
import torch.optim as optim
from tqdm import tqdm

from model import GoogLeNet


def main():
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print("using {} device.".format(device))

    data_transform = {
        "train": transforms.Compose([transforms.RandomResizedCrop(224),
                                     transforms.RandomHorizontalFlip(),
                                     transforms.ToTensor(),
                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),
        "val": transforms.Compose([transforms.Resize((224, 224)),
                                   transforms.ToTensor(),
                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])}

    data_root = os.path.abspath(os.path.join(os.getcwd(), "../.."))  # get data root path
    image_path = os.path.join(data_root, "data_set", "flower_data")  # flower data set path
    assert os.path.exists(image_path), "{} path does not exist.".format(image_path)
    train_dataset = datasets.ImageFolder(root=os.path.join(image_path, "train"),
                                         transform=data_transform["train"])
    train_num = len(train_dataset)

    # {'daisy':0, 'dandelion':1, 'roses':2, 'sunflower':3, 'tulips':4}
    flower_list = train_dataset.class_to_idx
    cla_dict = dict((val, key) for key, val in flower_list.items())
    # write dict into json file
    json_str = json.dumps(cla_dict, indent=4)
    with open('class_indices.json', 'w') as json_file:
        json_file.write(json_str)

    batch_size = 32
    nw = min([os.cpu_count(), batch_size if batch_size &gt; 1 else 0, 8])  # number of workers
    print('Using {} dataloader workers every process'.format(nw))

    train_loader = torch.utils.data.DataLoader(train_dataset,
                                               batch_size=batch_size, shuffle=True,
                                               num_workers=nw)

    validate_dataset = datasets.ImageFolder(root=os.path.join(image_path, "val"),
                                            transform=data_transform["val"])
    val_num = len(validate_dataset)
    validate_loader = torch.utils.data.DataLoader(validate_dataset,
                                                  batch_size=batch_size, shuffle=False,
                                                  num_workers=nw)

    print("using {} images for training, {} images for validation.".format(train_num,
                                                                           val_num))

    # test_data_iter = iter(validate_loader)
    # test_image, test_label = test_data_iter.next()

    net = GoogLeNet(num_classes=5, aux_logits=True, init_weights=True)
    # 如果要使用官方的预训练权重，注意是将权重载入官方的模型，不是我们自己实现的模型
    # 官方的模型中使用了bn层以及改了一些参数，不能混用
    # import torchvision
    # net = torchvision.models.googlenet(num_classes=5)
    # model_dict = net.state_dict()
    # # 预训练权重下载地址: https://download.pytorch.org/models/googlenet-1378be20.pth
    # pretrain_model = torch.load("googlenet.pth")
    # del_list = ["aux1.fc2.weight", "aux1.fc2.bias",
    #             "aux2.fc2.weight", "aux2.fc2.bias",
    #             "fc.weight", "fc.bias"]
    # pretrain_dict = {k: v for k, v in pretrain_model.items() if k not in del_list}
    # model_dict.update(pretrain_dict)
    # net.load_state_dict(model_dict)
    net.to(device)
    loss_function = nn.CrossEntropyLoss()
    optimizer = optim.Adam(net.parameters(), lr=0.0003)

    epochs = 30
    best_acc = 0.0
    save_path = './googleNet.pth'
    train_steps = len(train_loader)
    for epoch in range(epochs):
        # train
        net.train()
        running_loss = 0.0
        train_bar = tqdm(train_loader, file=sys.stdout)
        for step, data in enumerate(train_bar):
            images, labels = data
            optimizer.zero_grad()
            logits, aux_logits2, aux_logits1 = net(images.to(device))
            loss0 = loss_function(logits, labels.to(device))
            loss1 = loss_function(aux_logits1, labels.to(device))
            loss2 = loss_function(aux_logits2, labels.to(device))
            loss = loss0 + loss1 * 0.3 + loss2 * 0.3
            loss.backward()
            optimizer.step()

            # print statistics
            running_loss += loss.item()

            train_bar.desc = "train epoch[{}/{}] loss:{:.3f}".format(epoch + 1,
                                                                     epochs,
                                                                     loss)

        # validate
        net.eval()
        acc = 0.0  # accumulate accurate number / epoch
        with torch.no_grad():
            val_bar = tqdm(validate_loader, file=sys.stdout)
            for val_data in val_bar:
                val_images, val_labels = val_data
                outputs = net(val_images.to(device))  # eval model only have last output layer
                predict_y = torch.max(outputs, dim=1)[1]
                acc += torch.eq(predict_y, val_labels.to(device)).sum().item()

        val_accurate = acc / val_num
        print('[epoch %d] train_loss: %.3f  val_accuracy: %.3f' %
              (epoch + 1, running_loss / train_steps, val_accurate))

        if val_accurate &gt; best_acc:
            best_acc = val_accurate
            torch.save(net.state_dict(), save_path)

    print('Finished Training')


if __name__ == '__main__':
    main()
</code></pre>
 <p></p>
 <h3 style="text-align:left">
<span class="kdocs-bold" style="font-weight:bold">3.</span> <span class="kdocs-bold" style="font-weight:bold">predict.py</span>
</h3>
 <p>预测部分跟AlexNet和VGG类似，需要注意在<span class="kdocs-bold" style="font-weight:bold">实例化模型时不需要辅助分类器</span></p>
 <pre class="kdocs-plaintext"><code class="language-plaintext"># create model
model = GoogLeNet(num_classes=5, aux_logits=False)

# load model weights
model_weight_path = "./googleNet.pth"</code></pre>
 <p>但是在加载训练好的模型参数时，由于其中是包含有辅助分类器的，需要设置<span class="kdocs-fontSize" style="font-size:9pt">strict=False</span></p>
 <pre class="kdocs-plaintext"><code class="language-plaintext">missing_keys, unexpected_keys = model.load_state_dict(torch.load(model_weight_path), strict=False)</code></pre>
 <pre class="kdocs-plaintext"><code class="language-plaintext">import os
import json

import torch
from PIL import Image
from torchvision import transforms
import matplotlib.pyplot as plt

from model import GoogLeNet


def main():
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

    data_transform = transforms.Compose(
        [transforms.Resize((224, 224)),
         transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    # load image
    img_path = "../tulip.jpg"
    assert os.path.exists(img_path), "file: '{}' dose not exist.".format(img_path)
    img = Image.open(img_path)
    plt.imshow(img)
    # [N, C, H, W]
    img = data_transform(img)
    # expand batch dimension
    img = torch.unsqueeze(img, dim=0)

    # read class_indict
    json_path = './class_indices.json'
    assert os.path.exists(json_path), "file: '{}' dose not exist.".format(json_path)

    with open(json_path, "r") as f:
        class_indict = json.load(f)

    # create model
    model = GoogLeNet(num_classes=5, aux_logits=False).to(device)

    # load model weights
    weights_path = "./googleNet.pth"
    assert os.path.exists(weights_path), "file: '{}' dose not exist.".format(weights_path)
    missing_keys, unexpected_keys = model.load_state_dict(torch.load(weights_path, map_location=device),
                                                          strict=False)

    model.eval()
    with torch.no_grad():
        # predict class
        output = torch.squeeze(model(img.to(device))).cpu()
        predict = torch.softmax(output, dim=0)
        predict_cla = torch.argmax(predict).numpy()

    print_res = "class: {}   prob: {:.3}".format(class_indict[str(predict_cla)],
                                                 predict[predict_cla].numpy())
    plt.title(print_res)
    for i in range(len(predict)):
        print("class: {:10}   prob: {:.3}".format(class_indict[str(i)],
                                                  predict[i].numpy()))
    plt.show()


if __name__ == '__main__':
    main()</code></pre>
 <p>训练结果：</p>
 <p>由于数据少少，每类只有十多张，30个epoch下来最高只有0.48</p>
 <div class="kdocs-line-container" style="justify-content:center">
  <div class="kdocs-img" style="flex-direction:column;max-width:100%;width:524px;justify-content:center;align-items:center;height:auto">
   <div class="kdocs-img" style="padding-top:75.0%;height:0">
    <img src="https://images2.imgbox.com/06/ad/JZ2CLm8D_o.png" style="margin-left:;width:524px;margin-top:-75.0%;height:auto">
   </div>
  </div>
 </div>
 <p></p>
</div>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>