<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Pytorch神经网络处理.mat格式Mnist数据 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch神经网络处理.mat格式Mnist数据</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <h2><span style="color:#0d0016"><strong>数据源提供的为.mat格式的数据，希望能够采用python的pytorch深度学习框架对其进行处理</strong></span></h2> 
<h2><span style="color:#0d0016"><strong>此博客同时也写了一种对常规数据集进行读取的框架</strong></span></h2> 
<h2></h2> 
<h2><u>以下为函数各个模块以及相应的说明（详细）</u></h2> 
<h2><strong>data_preprocess模块函数  </strong></h2> 
<pre><code class="language-python">def dp():
      path = './mnist_lite.mat'  # 定义路径
      matr = io.loadmat(path)  # 关键步骤 采用io的loadmat包将数据读入缓存

      train_x = matr['train_x']  # 四个标签类型的数据为.mat中存储的形式
      train_y = matr['train_y']
      test_x = matr['test_x']
      test_y = matr['test_y']
'''
    原本数据shape为(784,6000), transpose 对于二维函数是将其转置，即6000个数据，784个信息点
'''
      train_x = torch.tensor(np.transpose(train_x))         
      train_x = train_x.to(torch.float32)  
      train_y = torch.tensor(np.transpose(train_y))  
      train_y = train_y.to(torch.float32)
      test_x = torch.tensor(np.transpose(test_x))
      test_x = test_x.to(torch.float32)
      test_y = torch.tensor(np.transpose(test_y))
      test_y = test_y.to(torch.float32)
      print(' train_x.shape:t',train_x.shape,'n',
            'train_y.shape:t',train_y.shape,'n',
            'test_x.shape:t',test_x.shape,'n',
            'test_y.shape:t',test_y.shape)

      train_x = train_x.reshape(6000,1,28,28)  # 将图像reshape为28*28的图像
      test_x = test_x.reshape(1000,1,28,28)

      return train_x, train_y, test_x, test_y</code></pre> 
<h2><strong>神经网络定义部分 Network模块 </strong></h2> 
<pre><code class="language-python">
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.m = nn.MaxPool2d(2, stride=2)
        self.fc = nn.Linear(28 * 28 * 2, 10)
        # self.initialize_weights()

    def forward(self, x):
        x = F.relu(self.conv1(x))
        # print("1",x.shape)
        x = self.m(x)
        # print("2",x.shape)
        x = F.relu(self.conv2(x))
        # print("3",x.shape)
        x = self.m(x)
        # print("4",x.shape)
        x = F.relu(self.conv3(x))
        # print("5",x.shape)
        x = torch.flatten(x, 1)  # flatten all dimensions except the batch dimension
        # print("6",x.shape)
        x = self.fc(x)
        # print("7",x.shape)
        # x = F.softmax(x, dim = 0)

        return x


net = Net()</code></pre> 
<blockquote> 
 <h3><strong>对于神经网络的定义一些心得：</strong></h3> 
 <p><strong>关于nn.conv2d()中的参数：</strong></p> 
 <p>        第一层的输入图片样本in_channel的输入取决于图片的类型，RGB即为3，本项目的MNIST手写数据集为1通道数据所以为1，out_channel的大小取决于卷积核的数量，此时out_channel的大小为下一次卷积核的in_channel的大小</p> 
 <p>        对于pytorch，没有 padding = same 的设置，因此需要根据卷积核的大小，如果需要设置padding = same的情况下，如kernelsize设置为3时，padding设置为1，等价于 padding = same 的设置</p> 
 <p></p> 
 <p><strong>关于nn.MaxPool2d()中的参数：</strong></p> 
 <p><strong>        </strong>kernel_size的设置是移动窗口的大小，这里stride的大小默认值是与窗口大小相同的</p> 
 <p>        每过一个最大池化层，信息点减少一半</p> 
 <p></p> 
 <p><strong>关于torch.flatten()函数:</strong></p> 
 <p>        在全连接层前采用flatten将所有信息点进行展开</p> 
 <p></p> 
 <p><strong>特别注意：</strong></p> 
 <p>        在后面采用的Loss标准如果采用了交叉熵判定标准的话，是已经有了softmax函数了，因此在网络最后的Sequential中不需要额外添加softmax函数</p> 
</blockquote> 
<h2> <strong>训练部分train_process</strong>
</h2> 
<pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as data
import scipy.io as io
import numpy as np
import torch.optim as optim
import matplotlib.pyplot as plt
import data_preprocess
import Network

net = Network.Net()  # 网络定义在Network.py文件中
Epoch = 10
BATCH_SIZE = 128  # 批训练的数据个数
# 先转换成 torch 能识别的 Datasettorch_dataset = data.TensorDataset(train_x, train_y)

# 把 dataset 放入 DataLoader

train_x, train_y, test_x, test_y = data_preprocess.dp()
print(train_x.shape,train_y.shape)
torch_dataset = data.TensorDataset(train_x, train_y)  # 生成TensorDataset

loader = data.DataLoader(
    dataset = torch_dataset,      # torch TensorDataset format
    batch_size = BATCH_SIZE,      # mini batch size
    shuffle = True               # 要不要打乱数据 (打乱比较好)
)

# loss 画图预备
loss_record = []
bc_record = []
step_ = 0

loss_record_mean = []
epoch_record = []
step__ = 0

# Accuracy 画图预备
accuracy_record = []
_bc_record = []
_step_ = 0

accuracy_record_mean = []
_epoch_record = []
_step__ = 0


optimizer = optim.SGD(net.parameters(), lr=0.05, momentum=0.9)  
criterion = nn.CrossEntropyLoss()
optimizer.zero_grad()

for epoch in range(Epoch):
    print('............','Epoch:',epoch,'............')
    loss_sum = 0
    acc_sum = 0
    num = 0
    for step, (batch_x, batch_y) in enumerate(loader): #每一步loader释放一小批数据用来学习
        train_predict_y = net(batch_x)
        loss = criterion(train_predict_y.float(), batch_y.float())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        # print('parameter updating')

        output = torch.argmax(train_predict_y, dim=1, keepdim=False)  


        # print(output)  
        label = torch.argmax(batch_y, dim=1, keepdim=False)

'''
        这里判断Accuracy的思路为比较预测输出的最大值的索引即第几类与groud_truth是否相等
        因为数组形式相同，因此取最大索引值的方式相同
'''
        # print(label)
        # print('..........output...............',output)
        # print('..........label...............',label)
        acc = output[output == label].shape[0] / output.shape[0]  # 计算正确率
        # print(output[output==label])
        acc_sum = acc_sum + acc

        loss_sum = loss_sum + loss
        num = num + 1

        # loss for batch
        loss_record.append(loss.detach())  # y 坐标
        bc_record.append(step_)    # 每次加了大约 47 次
        step_ = step_ + 1

        # acc for batch
        accuracy_record.append(acc)
        _bc_record.append(_step_)
        _step_ = _step_ + 1

    print('Loss:', (loss_sum / num).detach())
    print("Accuracy:", acc_sum / num)

    loss_record_mean.append((loss_sum / num).detach())  # 每一次epoch训练结束之后的 Loss 取的是一个均值  如果取点的话就是一共eoch个点
    epoch_record.append(step__)
    step__ = step__ + 47

    accuracy_record_mean.append((acc_sum / num))   # 每一次epoch训练结束之后的 Accuracy 取的是一个均值
    _epoch_record.append(_step__)
    _step__ = step__ + 47 # 之前一个batch循环训练完后除以内层batch训练的次数

# 测试集的正确率进行检验


# 画图

fig_list = torch.Tensor(loss_record).numpy()
fig_mean = torch.Tensor(loss_record_mean).numpy()</code></pre> 
<blockquote> 
 <p>对于训练网络部分的一些心得：</p> 
 <p>        首先需要将输入数据X和预测输出数据y_hat组合为TensorDataset，才能将数据存储入pytorch数据迭代器的Dataloader中</p> 
 <p>        使用argmax提取出train_y 和 labels的最大值所在的索引进行判别，以此判定出accuracy的大小</p> 
 <p>        将画图的张量采用.detach()函数，返回的张量的requires_grad为false</p> 
 <p>关于画图：</p> 
 <p>        在每一个批结束后进行点的绘制，Loss和Accuracy在每一小次迭代后都进行值的存储</p> 
</blockquote> 
<blockquote> 
 <p>最终运行情况：</p> 
 <p><img alt="" height="765" src="https://images2.imgbox.com/1c/b0/iuvVkbXm_o.png" width="376"></p> 
 <p> </p> 
 <p><img alt="" height="672" src="https://images2.imgbox.com/53/77/wUgGKtCf_o.png" width="783"></p> 
 <p> <img alt="" height="661" src="https://images2.imgbox.com/6e/5d/6m2gNdNU_o.png" width="778"></p> 
 <p></p> 
</blockquote>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>