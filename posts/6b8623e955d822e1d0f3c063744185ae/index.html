<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>机器学习模型3——支持向量机SVM - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">机器学习模型3——支持向量机SVM</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <h1>
<a id="_0"></a>前置知识</h1> 
<p>拉格朗日乘子法</p> 
<h1>
<a id="SVM_2"></a>支持向量机SVM</h1> 
<p>SVM：SVM全称是supported vector machine（⽀持向量机），即寻找到⼀个超平⾯使样本分成两类，并且间隔最⼤。<br> SVM能够执⾏<strong>线性或⾮线性分类、回归</strong>，甚⾄是<strong>异常值检测</strong>任务。它是机器学习领域最受欢迎的模型之⼀。<br> SVM特别适⽤于<strong>中⼩型复杂数据集的分类。</strong><br> SVM是⼀种<strong>⼆类分类模型</strong>。<br> 它的基本模型是在特征空间中寻找间隔最⼤化的分离超平⾯的线性分类器。</p> 
<ul>
<li>1）当训练样本线性可分时，通过硬间隔最⼤化，学习⼀个线性分类器，即线性可分⽀持向量机；</li>
<li>2）当训练数据近似线性可分时，引⼊松弛变量，通过软间隔最⼤化，学习⼀个线性分类器，即线性⽀持向量 机；</li>
<li>3）当训练数据线性不可分时，通过使⽤核技巧及软间隔最⼤化，学习⾮线性⽀持向量机。</li>
<li>
</li>
</ul> 
<p>主要内容：<br> <img src="https://images2.imgbox.com/e3/33/AwipyCj2_o.png" alt="在这里插入图片描述"><br> 所有点到所有分割面的最小距离（垂直距离）的最大值。<br> 软间隔和硬间隔的区别在于：前者允许分割后的集合存在异类，即某些样本不满足约束；后者是完全分割，所有样本都必须划分正确，不允许有划分后的集合存在异类，可能导致过拟合。</p> 
<h1>
<a id="_16"></a>线性可分支持向量机（硬间隔）</h1> 
<p><img src="https://images2.imgbox.com/d0/10/areQp8AE_o.png" alt="在这里插入图片描述"><br> 样本空间中任意点x到超平⾯（w,b）的距离可写成：<br> <img src="https://images2.imgbox.com/29/e4/3cTRUmJJ_o.png" alt="在这里插入图片描述"><br> 假设超平⾯（w, b）能将训练样本正确分类，即对于(x , y ) ∈ D，<br> 若y = +1（正倒点），则有w x + b &gt; 0;<br> 若y = −1（负倒点），则有w x + b &lt; 0;<br> 令<br> <img src="https://images2.imgbox.com/ad/82/8PHXVuMm_o.png" alt="在这里插入图片描述"><br> 如图所示，<br> <img src="https://images2.imgbox.com/be/93/HGdBgF20_o.png" alt="在这里插入图片描述"></p> 
<p>距离超平⾯最近的⼏个训练样本点使上式等号成⽴，他们被称为“⽀持向量"，<br> 两个异类⽀持向量到超平⾯的距离之和为:<br> <img src="https://images2.imgbox.com/ad/d4/uiumqOp1_o.png" alt="在这里插入图片描述"><br> 欲找到具有最⼤间隔的划分超平⾯，也就是要找到能满⾜式<br> <img src="https://images2.imgbox.com/a7/bb/l7wx7FhW_o.png" alt="在这里插入图片描述"></p> 
<p>中约束的参数w和b，使得γ最⼤。<br> 也就是说：</p> 
<p><img src="https://images2.imgbox.com/bd/f8/KDboGXkq_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="wb_38"></a>拉格朗日乘子法求解w和b</h2> 
<p>第一步：做拉格朗日函数。<br> <img src="https://images2.imgbox.com/9e/02/KNKRgDF3_o.png" alt="在这里插入图片描述"></p> 
<p>第二步：求参数得偏导，并令之为0。<br> <img src="https://images2.imgbox.com/ff/eb/HGMauODr_o.png" alt="在这里插入图片描述"><br> 但是目标函数还是不能求解，我们将求得的w和b，代⼊原⽬标函数的w和b中，得到的原函数的对偶函数。</p> 
<h2>
<a id="_45"></a>对偶问题</h2> 
<p>实际上就是将目标函数的极小<s>极大</s> 值问题，转换成新函数的极大<s>极小</s> 值问题：<br> <img src="https://images2.imgbox.com/5a/25/39bLD5QD_o.png" alt="在这里插入图片描述"></p> 
<p>对偶函数：<br> <img src="https://images2.imgbox.com/fb/50/faM5qxRC_o.png" alt="在这里插入图片描述"><br> 于是我们就开始求解如下的式子：<br> <img src="https://images2.imgbox.com/4f/11/VKadSjS9_o.png" alt="在这里插入图片描述"><br> 只需要对上式求出极⼤值α，然后将α代⼊w求偏导的那个公式从⽽求出w.<br> 将w代⼊超平⾯的表达式，计算b值；<br> 现在的w,b就是我们要寻找的最优超平⾯的参数，代入可求得超平面。<br> 这一部分的求解式子就不写了，理解就好，理解就好。</p> 
<h2>
<a id="_58"></a>下面是截取的一个列子：</h2> 
<p><img src="https://images2.imgbox.com/7d/f8/Sijojjom_o.png" alt="在这里插入图片描述"><img src="https://images2.imgbox.com/03/a5/MclYjssy_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/28/91/Dgx2HLgh_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="_61"></a>线性支持向量机（软间隔）</h1> 
<p><img src="https://images2.imgbox.com/91/b5/TYFjYnjK_o.png" alt="在这里插入图片描述">w可以看作，正则项（惩罚项），<br> 取平方，可以看作SVM自带L2正则项，防止过拟合<br> <img src="https://images2.imgbox.com/ad/da/Y6Uy0Bbt_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="_65"></a>损失函数</h2> 
<p>损失函数相当于松弛因子，用来替代线性支持向量机目标函数式子中的松弛因子。<br> <img src="https://images2.imgbox.com/f0/a9/wwz9wuxg_o.png" alt="在这里插入图片描述">对于损失函数的分析如下：<br> <img src="https://images2.imgbox.com/18/52/svA1WXWe_o.png" alt="在这里插入图片描述"><br> 绿⾊：0/1损失</p> 
<ul>
<li>当正例的点落在y=0这个超平⾯的下边，说明是分类正确，⽆论距离超平⾯所远多近，误差都是0.</li>
<li>当这个正例的样本点落在y=0的上⽅的时候，说明分类错误，⽆论距离多远多近，误差都为1.</li>
</ul> 
<p>蓝⾊：SVM Hinge损失函数</p> 
<ul>
<li>当⼀个正例的点落在y=1的直线上，距离超平⾯⻓度1，那么1-ξ=1，ξ=0，也就是说误差为0；</li>
<li>当它落在距离超平⾯0.5的地⽅，1-ξ=0.5，ξ=0.5，也就是说误差为0.5；</li>
<li>当它落在y=0上的时候，距离为0，1-ξ=0，ξ=1，误差为1；</li>
<li>当这个点落在了y=0的上⽅，被误分到了负例中，距离算出来应该是负的，⽐如-0.5，那么1-ξ=-0.5，ξ=-1.5.误 差为1.5.</li>
<li>以此类推，画在⼆维坐标上就是上图中蓝⾊那根线了。</li>
</ul> 
<p>红⾊：Logistic损失函数</p> 
<ul>
<li>损失函数的公式为：ln(1 + exp(-yi) )</li>
<li>当y = 0时，损失等于ln2,这样真丑，所以我们给这个损失函数除以ln2.</li>
<li>这样到y = 0时，损失为1，即损失函数过（0，1）点</li>
</ul> 
<h1>
<a id="_85"></a>非线性支持向量机</h1> 
<p>其实就是高维空间的支持向量机<br> <img src="https://images2.imgbox.com/da/bc/Ysy0tz5h_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="_88"></a>常见核函数</h2> 
<p><img src="https://images2.imgbox.com/75/48/7IExuriJ_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="_90"></a>⼀般有如下指导规则：</h2> 
<p>1） 如果Feature的数量很⼤，甚⾄和样本数量差不多时，往往线性可分，这时选⽤LR或者线性核Linear；<br> 2） 如果Feature的数量很⼩，样本数量正常，不算多也不算少，这时选⽤RBF核；<br> 3） 如果Feature的数量很⼩，⽽样本的数量很⼤，这时⼿动添加⼀些Feature，使得线性可分，然后选⽤LR或 者线性核Linear；<br> 4） 多项式核⼀般很少使⽤，效率不⾼，结果也不优于RBF；<br> 5） Linear核参数少，速度快；RBF核参数多，分类结果⾮常依赖于参数，需要交叉验证或⽹格搜索最佳参 数，⽐较耗时；<br> 6）<strong>应⽤最⼴的应该就是RBF核</strong>，⽆论是⼩样本还是⼤样本，⾼维还是低维等情况，RBF核函数均适⽤。</p> 
<h1>
<a id="_97"></a>支持向量机回归</h1> 
<p>SVM回归是让尽可能多的实例位于预测线上，同时限制间隔违例（也就是不在预测线距上的实例）。 线距的宽度由超参数ε控制。</p> 
<h1>
<a id="SVM_99"></a>SVM优缺点</h1> 
<p>SVM的优点：<br> 在⾼维空间中⾮常⾼效；<br> 即使在数据维度⽐样本数量⼤的情况下仍然有效；<br> 在决策函数（称为⽀持向量）中使⽤训练集的⼦集,因此它也是⾼效利⽤内存的；<br> 通⽤性：不同的核函数与特定的决策函数⼀⼀对应；<br> SVM的缺点：<br> 如果特征数量⽐样本数量⼤得多，在选择核函数时要避免过拟合；<br> 对缺失数据敏感;<br> 对于核函数的⾼维映射解释</p> 
<h1>
<a id="APIhttpsscikitlearnorgstablemodulesclasseshtmlmodulesklearnsvm_109"></a><a href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.svm">支持向量机API</a>
</h1> 
<pre><code class="prism language-python">svm<span class="token punctuation">.</span>LinearSVC<span class="token punctuation">(</span><span class="token punctuation">[</span>penalty<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> dual<span class="token punctuation">,</span> tol<span class="token punctuation">,</span> C<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Linear Support Vector Classification<span class="token punctuation">.</span>
svm<span class="token punctuation">.</span>LinearSVR<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">,</span> epsilon<span class="token punctuation">,</span> tol<span class="token punctuation">,</span> C<span class="token punctuation">,</span> loss<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Linear Support Vector Regression<span class="token punctuation">.</span>

svm<span class="token punctuation">.</span>NuSVC<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">,</span> nu<span class="token punctuation">,</span> kernel<span class="token punctuation">,</span> degree<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Nu<span class="token operator">-</span>Support Vector Classification<span class="token punctuation">.</span>
svm<span class="token punctuation">.</span>NuSVR<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">,</span> nu<span class="token punctuation">,</span> C<span class="token punctuation">,</span> kernel<span class="token punctuation">,</span> degree<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Nu Support Vector Regression<span class="token punctuation">.</span>

svm<span class="token punctuation">.</span>OneClassSVM<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">,</span> kernel<span class="token punctuation">,</span> degree<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Unsupervised Outlier Detection<span class="token punctuation">.</span>

svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">,</span> C<span class="token punctuation">,</span> kernel<span class="token punctuation">,</span> degree<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>C<span class="token operator">-</span>Support Vector Classification<span class="token punctuation">.</span>
svm<span class="token punctuation">.</span>SVR<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">,</span> kernel<span class="token punctuation">,</span> degree<span class="token punctuation">,</span> gamma<span class="token punctuation">,</span> coef0<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Epsilon<span class="token operator">-</span>Support Vector Regression<span class="token punctuation">.</span>

svm<span class="token punctuation">.</span>l1_min_c<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">[</span><span class="token punctuation">,</span> loss<span class="token punctuation">,</span> fit_intercept<span class="token punctuation">,</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Return the lowest bound <span class="token keyword">for</span> C such that <span class="token keyword">for</span> C <span class="token keyword">in</span> <span class="token punctuation">(</span>l1_min_C<span class="token punctuation">,</span> infinity<span class="token punctuation">)</span> the model <span class="token keyword">is</span> guaranteed <span class="token keyword">not</span> to be empty<span class="token punctuation">.</span>
</code></pre> 
<h2>
<a id="_124"></a>支持向量机的分类器</h2> 
<h3>
<a id="LinearSVC_125"></a>LinearSVC线性支持向量机</h3> 
<pre><code class="prism language-python"> <span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>svm<span class="token punctuation">.</span>LinearSVC<span class="token punctuation">(</span>penalty<span class="token operator">=</span><span class="token string">'l2'</span><span class="token punctuation">,</span> loss<span class="token operator">=</span><span class="token string">'squared_hinge'</span><span class="token punctuation">,</span> <span class="token operator">*</span><span class="token punctuation">,</span> dual<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> C<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> multi_class<span class="token operator">=</span><span class="token string">'ovr'</span><span class="token punctuation">,</span> fit_intercept<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> intercept_scaling<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> class_weight<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
</code></pre> 
<p>参数</p> 
<ul>
<li> <p>penalty：{‘l1’, ‘l2’}, default=’l2’</p> </li>
<li> <p>loss：{‘hinge’, ‘squared_hinge’}, default=’squared_hinge’</p> </li>
<li> <p>dual： bool, default=True<br> 是否转化为对偶问题求解。当n_samples&gt;n_features时，首选dual=False。</p> </li>
<li> <p>tol：float, default=1e-4<br> 停止标准公差。</p> </li>
<li> <p>C：float, default=1.0<br> 正则化系数。正则化的强度与C成反比。必须严格为正。⽤来控制损失函数的惩罚系数，类似于线性回归中的正则化系数。 C越⼤，相当于惩罚松弛变量，希望松弛变量接近0，即对误分类的惩罚增⼤，趋向于对训练集全分对的情 况，这样会出现训练集测试时准确率很⾼，但泛化能⼒弱，容易导致过拟合。 C值⼩，对误分类的惩罚减⼩，容错能⼒增强，泛化能⼒较强，但也可能⽋拟合。</p> </li>
<li> <p>multi_class：{‘ovr’, ‘crammer_singer’}, default=’ovr’<br> 如果y包含两个以上的类，则确定多类策略。“ovr”训练n_classes-one与rest分类器，而“cramersinger”优化所有类的联合目标。虽然从理论角度来看，cramer_singer很有趣，因为它是一致的，但在实践中很少使用，因为它很少导致更好的精度，而且计算成本更高。如果选择“cramer_singer”，则选项损失（loss）、惩罚（penalty）和对偶（dual）将被忽略。</p> </li>
<li> <p>fit_intercept：bool, default=True<br> 是否计算此模型的截距。如果设置为false，则计算中将不使用截距（即数据预计已居中）。</p> </li>
<li> <p>intercept_scaling：float, default=1</p> <pre><code>  When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a “synthetic” feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased.
</code></pre> </li>
<li> <p>class_weight：dict or ‘balanced’, default=None<br> Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)).</p> </li>
<li> <p>verbose：int, default=0<br> Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context.</p> </li>
<li> <p>random_state：int, RandomState instance or None, default=None<br> Controls the pseudo random number generation for shuffling the data for the dual coordinate descent (if dual=True). When dual=False the underlying implementation of LinearSVC is not random and random_state has no effect on the results. Pass an int for reproducible output across multiple function calls. See Glossary.</p> </li>
<li> <p>max_iter：int, default=1000<br> 要运行的最大迭代次数。</p> </li>
</ul> 
<p>属性：</p> 
<ul>
<li>coef_：ndarray of shape (1, n_features) if n_classes == 2 else (n_classes, n_features)<br> Weights assigned to the features (coefficients in the primal problem).<br> coef_ is a readonly property derived from raw_coef_ that follows the internal memory layout of liblinear.</li>
<li>intercept_： ndarray of shape (1,) if n_classes == 2 else (n_classes,)<br> Constants in decision function.</li>
<li>classes_：ndarray of shape (n_classes,)<br> The unique classes labels.</li>
<li>n_features_in_：int<br> Number of features seen during fit.New in version 0.24.</li>
<li>feature_names_in_：ndarray of shape (n_features_in_,)<br> Names of features seen during fit. Defined only when X has feature names that are all strings.<br> n_iter_： int<br> Maximum number of iterations run across all classes.<br> 很多都是和决策树或者线性回归的参数和属性是一样的，就不展开了。<br> <img src="https://images2.imgbox.com/cd/0c/rXwVi0kN_o.png" alt="在这里插入图片描述">
</li>
</ul> 
<h3>
<a id="SVC_172"></a>SVC非线性支持向量机（核函数）</h3> 
<pre><code class="prism language-python"> <span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>svm<span class="token punctuation">.</span>SVC<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">,</span> C<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span> degree<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token string">'scale'</span><span class="token punctuation">,</span> coef0<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> shrinking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> probability<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> cache_size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> class_weight<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> decision_function_shape<span class="token operator">=</span><span class="token string">'ovr'</span><span class="token punctuation">,</span> break_ties<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<ul>
<li>kernel: 算法中采⽤的核函数类型，核函数是⽤来将⾮线性问题转化为线性问题的⼀种⽅法。 参数选择有RBF, Linear, Poly, Sigmoid或者⾃定义⼀个核函数。 <strong>默认的是"RBF"</strong>，即径向基核，也就是⾼斯核函数； ⽽Linear指的是线性核函数， Poly指的是多项式核， Sigmoid指的是双曲正切函数tanh核；。</li>
<li>degree: 当指定kernel为’poly’时，表示选择的多项式的最⾼次数，默认为三次多项式；<br> 若指定kernel不是’poly’，则忽略，即该参数只对’poly’有⽤。 多项式核函数是将低维的输⼊空间映射到⾼维的特征空间。</li>
<li>coef0: 核函数常数值(y=kx+b中的b值)， 只有‘poly’和‘sigmoid’核函数有，默认值是0。</li>
</ul> 
<h3>
<a id="NuSVC_181"></a>NuSVC</h3> 
<p>Similar to SVC but uses a parameter to control the number of support vectors. The implementation is based on libsvm.</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">sklearn</span><span class="token punctuation">.</span>svm<span class="token punctuation">.</span>NuSVC<span class="token punctuation">(</span><span class="token operator">*</span><span class="token punctuation">,</span> nu<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">,</span> kernel<span class="token operator">=</span><span class="token string">'rbf'</span><span class="token punctuation">,</span> degree<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> gamma<span class="token operator">=</span><span class="token string">'scale'</span><span class="token punctuation">,</span> coef0<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> shrinking<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> probability<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> cache_size<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">,</span> class_weight<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> max_iter<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> decision_function_shape<span class="token operator">=</span><span class="token string">'ovr'</span><span class="token punctuation">,</span> break_ties<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>
</code></pre> 
<p>nu： 训练误差部分的上限和⽀持向量部分的下限，取值在（0，1）之间，默认是0.5</p> 
<h1>
<a id="_187"></a>参考</h1> 
<p>1.黑马机器学习<br> 2.https://www.bilibili.com/video/BV1Ca411M7KA/?p=8&amp;spm_id_from=333.880.my_history.page.click&amp;vd_source=c35b16b24807a6dbe33f5473659062ac<br> 3.《机器学习》周志华</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>