<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>大数据应用技术期末复习 | Spark Scala版本 | 八个章节总共89个选择题汇总 (附带答案) - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">大数据应用技术期末复习 | Spark Scala版本 | 八个章节总共89个选择题汇总 (附带答案)</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-dracula">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#_89_1">选择题总结 (89个题)</a></li>
<li><a href="#1__10_3">第1章 大数据技术概述 (10个题)</a></li>
<li><a href="#2_Scala__20_80">第2章 Scala 语言基础 (20个题)</a></li>
<li><a href="#3_Spark_10_234">第3章 Spark的设计与运行原理 (10个题)</a></li>
<li><a href="#4_Spark_10_313">第4章 Spark环境搭建和使用方法 (10个题)</a></li>
<li><a href="#5_RDD_10_388">第5章 RDD编程 (10个题)</a></li>
<li><a href="#6_Spark_SQL_10_475">第6章 Spark SQL (10个题)</a></li>
<li><a href="#7_Spark_Streaming_11_554">第7章 Spark Streaming (11个题)</a></li>
<li><a href="#8_Spark_MLlib_8_642">第8章 Spark MLlib (8个题)</a></li>
</ul>
</div>
<p></p> 
<h1>
<a id="_89_1"></a>选择题总结 (89个题)</h1> 
<h1>
<a id="1__10_3"></a>第1章 大数据技术概述 (10个题)</h1> 
<hr> 
<p><em><strong>‎1.1 大数据技术及其代表性的软件种类很多，不同的技术有其不同应用场景，都对应着不同的大数据计算模式，请问软件产品Pregel主要应用于以下哪种计算模式？</strong></em> <code>B.图计算</code></p> 
<pre><code>A.流计算
B.图计算
C.查询分析计算
D.批处理计算
</code></pre> 
<p><em><strong>1.2 Hadoop生态系统中用于构建数据仓库并允许用户输入SQL语句进行查询的功能组件是?</strong></em> <code>C. Hive</code></p> 
<pre><code>A.Flume
B.Pregel
C.Hive
D.Spark
</code></pre> 
<p><em><strong>1.3 ‏Hadoop的生态系统组件之一Sqoop的功能是?</strong></em> <code>D.交换数据</code></p> 
<pre><code>A.负责集群资源调度管理的组件
B.用来存储非结构化和半结构化的松散数据
C.提供高可靠性、高可用、分布式的海量日志采集
D.用来在Hadoop和关系数据库之间的交换数据，改进数据的互操作性
</code></pre> 
<p><em><strong>1.4 以下哪一项不是Hadoop的缺点？</strong></em> <code>B.分布存储到多台机器</code></p> 
<pre><code>A.计算延迟高
B.数据文件被分布存储到多台机器上
C.磁盘I/O开销大
D.计算表达能力有限
</code></pre> 
<p><em><strong>1.5 用户在使用HDFS时，仍然可以像普通文件系统那样用文件名去访问文件，以下哪个选项是正确的访问方式？</strong></em> <code>D. 三短一长选最长(</code></p> 
<pre><code>‎A.把文件名发送给名称节点，根据文件名直接在名称节点上获取数据
B.把文件名发送给数据节点，根据文件名直接在数据节点上获取数据
C.以上说法都不对
D.把文件名发送给名称节点，根据文件名在名称节点上找到数据块的实际存储信息，客户端再到数据节点上获取数据
</code></pre> 
<p><em><strong>1.6 目前学术界和业界比较认可的关于大数据的四个特点是?</strong></em> <code>ABCD</code></p> 
<pre><code>‍A.数据类型多
B.价值密度低
C.数据量大
D.处理速度快
</code></pre> 
<p><em><strong>1.7 Hadoop两大核心组成部分是什么？</strong></em> <code>CD</code></p> 
<pre><code>​A.资源调度管理框架YARN
B.分布式协作服务Zookeeper
C.分布式计算框架MapReduce
D.分布式文件系统HDFS
</code></pre> 
<p><em><strong>1.8.‏YARN是负责集群资源调度管理的组件。不同的计算框架统一运行在YARN框架之上，具有哪些优点：</strong></em> <code>ABCD</code></p> 
<pre><code>‏A.计算资源按需伸缩
B.大大降低了运维成本
C.不同负载应用混搭，集群利用率高
D.共享底层存储，避免数据跨集群迁移
</code></pre> 
<p><em><strong>1.9 关于Hadoop生态系统中HBase与其它部分的关系，以下说法正确的有：</strong></em> <code>ABCD</code></p> 
<pre><code>‍A.HBase利用MapReduce来处理HBase中的海量数据，实现高性能计算
B.利用Pig和Hive为HBase提供了高层语言支持
C.使用HDFS作为高可靠的底层存储，利用廉价集群提供海量数据存储能力
D.使用Sqoop为HBase提供了高效便捷的RDBMS数据导入功能
</code></pre> 
<p><em><strong>1.10.Spark的设计遵循“一个软件栈满足不同应用场景”的理念，逐渐形成了一套完整的生态系统，可以支持以下哪些操作计算：</strong></em> <code>ABCD</code></p> 
<pre><code>‏A.流式计算（Spark Streaming）
B.SQL即席查询（Spark SQL）
C.图计算（GraphX）
D.机器学习（MLlib）
</code></pre> 
<h1>
<a id="2_Scala__20_80"></a>第2章 Scala 语言基础 (20个题)</h1> 
<hr> 
<p><em><strong>2.1 下面输出与其他不一致的是？</strong></em> <code>D</code></p> 
<pre><code>A.print("Hello Worldn")
B.println("Hello World")
C.printf("Hello %s", "Worldn")
D.val w = "World" ; println("Hello $w")
</code></pre> 
<p><em><strong>2.2 有关操作符优先级的描述不正确的是？</strong></em> <code>A</code></p> 
<pre><code>A.+的优先级高于！
B.%的优先级高于+
C.&gt;的优先级高于&amp;
D.*=的优先级低于+
</code></pre> 
<p><em><strong>2.3 对集合(Set)进行操作"Set(2, 0, 1) + 1 + 1 - 1"之后的结果为？</strong></em> <code>C</code></p> 
<pre><code>A.以上均不正确
B.Set(2, 0, 1, 1)
C.Set(2, 0)
D.Set(2, 0, 1)
</code></pre> 
<p><em><strong>2.4 以下关于闭包描述错误的是？</strong></em> <code>D</code></p> 
<pre><code>A.对于def mulBy(factor: Double) = (x: Double) =&gt; factor * x; val triple = mulBy(3);,函数triple是一个闭包
B.闭包是一个函数，其返回值依赖于声明在函数包部的一个或多个变量
C.通常来讲，可以将闭包看作是可以访问一个函数里面局部变量的另一个函数
D.对于def mulBy(factor: Double) = (x: Double) =&gt; 3 * x; val triple = mulBy(3);,函数triple是一个闭包
</code></pre> 
<p><em><strong>2.5 对于以下代码描述有误的是？</strong></em> <code>C</code></p> 
<pre><code>val data = Map(1 -&gt; "One", 2 -&gt; "Two")
‏val res = for((k, v) &lt;- data; if(k &gt; 1)) yield v
</code></pre> 
<pre><code>A.其中的if(k &gt; 1)是一个守卫表达式
B.运行后res的结果为List("Two")
C.运行后res的结果为List("One", "Two")
D.对映射data中的每一个(键，值)对，k被绑定对键，而v则被绑定到值
</code></pre> 
<p><em><strong>2.6‍ Scala中，下面的哪个类定义是不正确的？</strong></em> <code>B</code></p> 
<pre><code>A.class Counter{def counter = “counter”}
B.class Counter{var counter:String}
C.class Counter{def counter () {}}
D.class Counter{val counter = “counter”}
</code></pre> 
<p><em><strong>2.7 以下关于类和单例对象的对比说法正确的是？</strong></em> <code>A</code></p> 
<pre><code>A.单例对象不可以带参数，而类可以
B.单例对象不可以定义方法，而类可以
C.单例对象不可以定义私有属性，而类可以
D.单例对象不可以继承，而类可以
</code></pre> 
<p><em><strong>2.8 Scala语言中，关于List的定义，不正确的是？</strong></em> <code>B</code></p> 
<pre><code>A.val list = List(1,2,3)
B.val list = List [String]('A','B','C')
C.val list = List [Int](1,2,3)
D.val list = List [String]()
</code></pre> 
<p><em><strong>2.9‏ 对于Map(“book” -&gt; 5, “pen” -&gt; 2).map(m =&gt; m._1 -&gt; m._2 * 2)的结果，下面哪个是正确的？</strong></em> <code>A</code></p> 
<pre><code>‍A.Map("book" -&gt; 10, "pen" -&gt; 4)
B.Map("bookbook" -&gt; 10, "penpen" -&gt; 4)
C.Map("book" -&gt; 5, "pen" -&gt; 2 ,"book" -&gt; 5, "pen" -&gt; 2)
D.Map("bookbook" -&gt; 5, "penpen" -&gt; 2)
</code></pre> 
<p><em><strong>2.10‌ 表达式for(i &lt;- 1 to 3; j &lt;- 1 to 3; if i != j ) {print((10 * i + j));print(" ")}输出结果正确的是？</strong></em> <code>D</code></p> 
<pre><code>A.11 12 21 22 31 32
B.11 13 21 23 31 33
C.11 12 13 21 22 23 31 32 33
D.12 13 21 23 31 32
</code></pre> 
<p><em><strong>2.11 ‎以下哪些选项属于Scala的基本特性?</strong></em> <code>ABCD</code></p> 
<pre><code>A.是一门类Java的多范式语言
B.是一门函数式语言，支持高阶函数，允许嵌套多层函数，并支持柯里化（Currying）
C.运行于Java虚拟机（JVM）之上，并且兼容现有的Java程序
D.是一门纯粹的面向对象的语言
</code></pre> 
<p><em><strong>2.12 关于主构造器，以下说法正确的是？</strong></em> <code>ABD</code></p> 
<pre><code>‏A.主构造器的参数可以直接放在类名后
B.主构造器中可以使用默认参数
C.主构造器在每个类都可以定义多个
D.主构造器会执行类定义中的所有语句
</code></pre> 
<p><em><strong>2.13 Scala里的函数是“头等公民”，以下哪些说法是正确的？</strong></em> <code>ACD</code></p> 
<pre><code>A.将函数赋值给变量
B.以上说法都不正确
C.将函数作为其他函数的返回值
D.将函数作为参数传递给其他函数
</code></pre> 
<p><em><strong>2.14 以下关于特质的说法正确的是？</strong></em> <code>ABC</code></p> 
<pre><code>A.类可以实现任意数量的特质
B.特质可以要求实现它们的类具备特定的字段、方法或超类
C.当将多个特质叠加在一起时，顺序很重要，其方法先被执行的特质排在更后面
D.与Java接口(Interface)相同，Scala特质不可以提供方法和字段的实现
</code></pre> 
<p><em><strong>2.15 对于元组val t = (1, 3.14, “Fred”)说法正确的是？</strong></em> <code>BCD</code></p> 
<pre><code>A.t_1 等于 1
B.t._0无法访问，会抛出异常
C.t 的类型为 Tuple3[Int, Double, java.lang.String]
D.val (first, second, _) = t // second 等于 3.14
</code></pre> 
<p><em><strong>2.16 Scala 中，类和它的伴生对象说法正确的是？</strong></em> <code>BC</code></p> 
<pre><code>A.类和它的伴生对象可以有不同的名称
B.类和它的伴生对象定义在同一个文件中
C.类和它的伴生对象可以互相访问私有特性
D.类有静态方法，伴生对象没有静态方法
</code></pre> 
<p><em><strong>2.17 关于数组val a = Array(1,2,3)下列说法正确的是？</strong></em> <code>ABC</code></p> 
<pre><code>‍A.val b = for(elem &lt;- a if elem % 2 == 0) yield 2 * elem // b 等于 Array(4)
B.val b = for(elem &lt;- a) yield 2 * elem // b 等于 Array(2,4,6)
C.val b = a.map(_*2) // b 等于 Array(2,4,6)
D.val b = 2 * a // b 等于 Array(2,4,6)
</code></pre> 
<p><em><strong>2.18‎ 以下关于Scala各种数据结构的说法正确的是?</strong></em> <code>ABC</code></p> 
<pre><code>A.集合(Set)是不重复元素的容器
B.列表(List)一旦被定义,其值就不能改变
C.迭代器(Iterator)是一种提供了按顺序访问容器元素的数据结构
D.映射(Map)是一系列键值对的容器,在一个映射中,键是唯一的,值也是唯一的
</code></pre> 
<p><em><strong>2.19 ‎val books = List(“Hadoop”,”Hive”,”Mapreduce”),以下哪些操作能将字符串全部变成大写？</strong></em> <code>BCD</code></p> 
<pre><code>‎A.for (book &lt;-books; c&lt;-book) yield c.toUpperCase
B.books.map(s =&gt; s.toUpperCase)
C.for (book&lt;-books) yield book.toUpperCase
D.books.map(_.toUpperCase)
</code></pre> 
<p><em><strong>2.20 在Scala中，关于Nothing，null，Null，Option，Some，None的说法正确的是？</strong></em> <code>ABCD</code></p> 
<pre><code>‍A.Null是所有引用类型的子类，其唯一的实例是null
B.null表示一个空对象，可以赋值给任何引用类型
C.类Option是一个抽象类，有一个具体子类Some 和一个对象None，分别表示有值和无值的情况
D.Nothing 是所有其他类型的子类，没有实例，主要用于异常处理函数的返回类型
</code></pre> 
<h1>
<a id="3_Spark_10_234"></a>第3章 Spark的设计与运行原理 (10个题)</h1> 
<hr> 
<p><em><strong>3.1 ‎以下是Spark的主要特点的有?</strong></em> <code>ABCD</code></p> 
<pre><code>A.运行速度快
B.容易使用，简洁的API设计有助于用户轻松构建并行程序
C.通用性，Spark提供了完整而强大的技术栈
D.运行模式多样
</code></pre> 
<p><em><strong>3.2 Spark的运行架构包括哪些？</strong></em> <code>ABCD</code></p> 
<pre><code>A.集群资源管理器（Cluster Manager）
B.执行进程（Executor）
C.Worker Node
D.任务控制节点Driver Program
</code></pre> 
<p><em><strong>3.‎3 关于RDD之间的依赖分为窄依赖和宽依赖，以下说法正确的是？</strong></em> <code>AC</code></p> 
<pre><code>A.存在一个父RDD的一个分区对应一个子RDD的多个分区，则为宽依赖
B.存在一个父RDD的多个分区对应一个子RDD的一个分区，则为宽依赖
C.存在一个父RDD的一个分区只被一个子RDD的一个分区所使用，则为窄依赖
D.存在一个父RDD的一个分区被一个子RDD的多个分区所使用，则为窄依赖
</code></pre> 
<p><em><strong>3.4 Spark可以采用几种不同的部署方式，以下正确的部署方式有？</strong></em> <code>ABCD</code></p> 
<pre><code>A.Local
B.Standalone
C.Spark on Mesos
D.Spark on YARN
</code></pre> 
<p><em><strong>3.5 ​目前的大数据处理典型应用场景可分为哪几个类型?</strong></em> <code>ABD</code></p> 
<pre><code>A.复杂的批量数据处理
B.基于历史数据的交互式查询
C.大数据的分布式计算
D.基于实时数据流的数据处理
</code></pre> 
<p><em><strong>3.6 以下哪个不是Spark的组件?</strong></em> <code>D</code></p> 
<pre><code>A.Spark Streaming
B.MLlib
C.GraphX
D.Flink
</code></pre> 
<p><em><strong>3.7 下面哪个不是 RDD 的特点 ?</strong></em> <code>C</code></p> 
<pre><code>A.可分区
B.可序列化
C.可修改
D.可持久化
</code></pre> 
<p><em><strong>3.8.Task是Executor上的工作单元，运行于下面哪个组件上？</strong></em> <code>C</code></p> 
<pre><code>A.Driver Program
B.Spark Master
C.Worker Node
D.Cluster Manager
</code></pre> 
<p><em><strong>3.9 下面哪个操作肯定是宽依赖？</strong></em> <code>C</code></p> 
<pre><code>A.map
B.filter
C.reduceByKey
D.union
</code></pre> 
<p><em><strong>3.10 以下选项中哪些是Spark的优点？</strong></em> <code>AC</code></p> 
<pre><code>‍A.具有高效的容错性
B.利用进程模型
C.可以将中间结果持久化到内存
D.表达能力有限
</code></pre> 
<h1>
<a id="4_Spark_10_313"></a>第4章 Spark环境搭建和使用方法 (10个题)</h1> 
<hr> 
<p><em><strong>4.1​ Spark部署模式有哪几种?</strong></em> <code>ABCD</code></p> 
<pre><code>A.Local模式（单机模式）
B.Standalone模式 
C.YARN模式
D.Mesos模式
</code></pre> 
<p><em><strong>4.2‏ 关于Hadoop和Spark的相互关系，以下说法正确的是？</strong></em> <code>ABCD</code></p> 
<pre><code>A.Hadoop和Spark可以相互协作
B.Hadoop负责数据的存储和管理
C.Spark负责数据的计算
D.Spark要操作Hadoop中的数据，需要先启动HDFS
</code></pre> 
<p><em><strong>4.3 判断HDFS是否启动成功，可以通过哪个命令？</strong></em> <code>C</code></p> 
<pre><code>‎A.hdfs
B.spark
C.jps
D.start-dfs
</code></pre> 
<p><em><strong>4.4 ‏HDFS若启动成功，系统会列出以下哪些进程？</strong></em> <code>ACD</code></p> 
<pre><code>A.NameNode
B.HDFS
C.DataNode
D.SecondaryNameNode
</code></pre> 
<p><em><strong>4.5 spark-shell在启动时，采用<code>local[*]</code>时，它的含义是？</strong></em> <code>B</code></p> 
<pre><code>A.使用任意个线程来本地化运行Spark
B.使用与逻辑CPU个数相同数量的线程来本地化运行Spark
C.使用与逻辑CPU个数相同数量的进程来本地化运行Spark
D.使用单个线程来本地化运行Spark
</code></pre> 
<p><em><strong>4.6‎ spark-shell在启动时，采用yarn-client模式时，以下说法正确的是？</strong></em> <code>AC</code></p> 
<pre><code>A.当用户提交了作业之后，不能关掉Client
B.当用户提交了作业之后，就可以关掉Client
C.该模式适合运行交互类型的作业
D.该模式不适合运行交互类型的作业
</code></pre> 
<p><em><strong>4.7 spark-shell在启动时，采用yarn-cluster模式时，以下说法正确的是？</strong></em> <code>BD</code></p> 
<pre><code>A.当用户提交了作业之后，不能关掉Client
B.当用户提交了作业之后，就可以关掉Client
C.该模式适合运行交互类型的作业
D.该模式不适合运行交互类型的作业
</code></pre> 
<p><em><strong>4.8‍ 开发Spark独立应用程序的基本步骤通常有哪些?</strong></em> <code>ABCD</code></p> 
<pre><code>‌A.安装编译打包工具，如sbt，Maven
B.编写Spark应用程序代码
C.编译打包
D.通过spark-submit运行程序
</code></pre> 
<p><em><strong>4.9 下面描述正确的是：</strong></em> <code>C</code></p> 
<pre><code>A.Hadoop和Spark不能部署在同一个集群中
B.Hadoop只包含了存储组件，不包含计算组件
C.Spark是一个分布式计算框架，可以和Hadoop组合使用
D.Spark和Hadoop是竞争关系，二者不能组合使用
</code></pre> 
<p><em><strong>4.10‍ 集群上运行Spark应用程序的方法步骤有哪些?</strong></em> <code>ABCD</code></p> 
<pre><code>A.启动Hadoop集群
B.启动Spark的Master节点和所有Slave节点
C.在集群中运行应用程序JAR包
D.查看集群信息以获得应用程序运行的相关信息
</code></pre> 
<h1>
<a id="5_RDD_10_388"></a>第5章 RDD编程 (10个题)</h1> 
<hr> 
<p><em><strong>5.1 以下操作中，哪个不是Spark RDD编程中的操作</strong></em> <code>A</code></p> 
<pre><code>‍A.getLastOne()
B.filter()
C.reduceByKey(func)
D.reduce()
</code></pre> 
<p><em><strong>5.2下述语句执行的结果是</strong></em> <code>A</code></p> 
<pre><code>‏val rdd=sc.parallelize(Array(1,2,3,4,5))
rdd.take(3)
</code></pre> 
<pre><code>A.Array(1,2,3)
B.Array(2,3,4)
C.3
D.6
</code></pre> 
<p><em><strong>5.3‍ 有一个键值对RDD，名称为pairRDD，它包含4个元素，分别是(“Hadoop”,1)、(“Spark”,1)、(“Hive”,1)和(“Spark”,1),则pairRDD.reduceByKey((a,b)=&gt;a+b)执行结果得到的RDD，它里面包含的元素是</strong></em> <code>A</code></p> 
<pre><code>A.(“Hadoop”,1),(“Spark”,2),(“Hive”,1)
B.(“Hadoop”,2),(“Spark”,1),(“Hive”,1)
C.(“Hadoop”,2),(“Spark”,2),(“Hive”,2)
D.(“Hadoop”,1),(“Spark”,2),(“Hive”,2)
</code></pre> 
<p><em><strong>5.4 ‌下述语句的执行结果wordCountsWithGroup中包含的元素是</strong></em> <code>A</code></p> 
<pre><code>val  words = Array("one", "two", "two", "three", "three", "three") 
‌val  wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, 1))
‌val  wordCountsWithGroup = wordPairsRDD. groupByKey().map(t =&gt; (t._1, t._2.sum))
</code></pre> 
<pre><code>A.(“one”,1),(“two”,2),(“three”,3)
B.(“one”,1),(“two”,2),(“three”,1)
C.(“one”,3),(“two”,2),(“three”,1)
D.(“one”,1),(“two”,1),(“three”,1)
</code></pre> 
<p><em><strong>5.5 有一个键值对RDD，名称为pairRDD，包含4个元素，分别是(“Hadoop”,1)、(“Spark”,1)、(“Hive”,1)和(“Spark”,1)，则pairRDD.mapValues(x =&gt; x+1)操作得到的RDD中所包含的元素是</strong></em> <code>C</code></p> 
<pre><code>A.1,1,1,1
B.2,2,2,2
C.("Hadoop",2)、("Spark",2)、("Hive",2)和("Spark",2)
D. ("Hadoop",1)、("Spark",1)、("Hive",1)和("Spark",1)
</code></pre> 
<p><em><strong>5.6 RDD操作包括哪两种类型</strong></em> <code>AC</code></p> 
<pre><code>A.行动（Action）
B.分组（GroupBy）
C.转换（Transformation）
D.连接（Join）
</code></pre> 
<p><em><strong>5.7 ‏以下操作中，哪些是转换（Transformation）操作</strong></em> <code>AB</code></p> 
<pre><code>A.filter()
B.reduceByKey(func)
C.first()
D.count()
</code></pre> 
<p><em><strong>5.8 以下操作中，哪些是行动（Action）操作</strong></em> <code>AB</code></p> 
<pre><code>A.reduce()
B.collect()
C.groupByKey()
D.map()
</code></pre> 
<p><em><strong>5.9 ‏以下关于RDD的持久化的描述，正确的是</strong></em> <code>ABCD</code></p> 
<pre><code>A.persist(MEMORY_ONLY)：表示将RDD作为反序列化的对象存储于JVM中，如果内存不足，就要按照LRU原则替换缓存中的内容
B.通过持久化（缓存）机制可以避免重复计算的开销
C.persist(MEMORY_AND_DISK)：表示将RDD作为反序列化的对象存储在JVM中，如果内存不足，超出的分区将会被存放在硬盘上
D.使用cache()方法时，会调用persist(MEMORY_ONLY)
</code></pre> 
<p><em><strong>5.10 ‎关于RDD分区的作用，下面描述正确的是</strong></em> <code>BC</code></p> 
<pre><code>A.增加时间开销
B.增加并行度
C.减少通信开销
D.减少并行度
</code></pre> 
<h1>
<a id="6_Spark_SQL_10_475"></a>第6章 Spark SQL (10个题)</h1> 
<hr> 
<p><em><strong>6.1 关于Shark，下面描述正确的是：</strong></em> <code>C</code></p> 
<pre><code>A.Shark提供了类似Pig的功能
B.Shark把SQL语句转换成MapReduce作业
C.Shark重用了Hive中的HiveQL解析、逻辑执行计划翻译、执行计划优化等逻辑
D.Shark的性能比Hive差很多
</code></pre> 
<p><em><strong>6.2‎ 下面关于Spark SQL架构的描述错误的是：</strong></em> <code>D</code></p> 
<pre><code>A.在Shark原有的架构上重写了逻辑执行计划的优化部分，解决了Shark存在的问题
B.Spark SQL在Hive兼容层面仅依赖HiveQL解析和Hive元数据
C.Spark SQL执行计划生成和优化都由Catalyst（函数式关系查询优化框架）负责
D.Spark SQL执行计划生成和优化需要依赖Hive来完成
</code></pre> 
<p><em><strong>6.3 要把一个DataFrame保存到people.json文件中，下面语句哪个是正确的：</strong></em> <code>A</code></p> 
<pre><code>A.df.write.json("people.json")
B. df.json("people.json")
C.df.write.format("csv").save("people.json")
D.df.write.csv("people.json")
</code></pre> 
<p><em><strong>6.4 以下操作中，哪个不是DataFrame的常用操作：</strong></em> <code>D</code></p> 
<pre><code>A.printSchema()
B.select()
C.filter()
D.sendto()
</code></pre> 
<p><em><strong>6.5‍ Shark的设计导致了两个问题：</strong></em> <code>AC</code></p> 
<pre><code>A.执行计划优化完全依赖于Hive，不方便添加新的优化策略
B.执行计划优化不依赖于Hive，方便添加新的优化策略
C.Spark是线程级并行，而MapReduce是进程级并行，因此，Spark在兼容Hive的实现上存在线程安全问题，导致Shark不得不使用另外一套独立维护的、打了补丁的Hive源码分支
D.Spark是进程级并行，而MapReduce是线程级并行，因此，Spark在兼容Hive的实现上存在线程安全问题，导致Shark不得不使用另外一套独立维护的、打了补丁的Hive源码分支
</code></pre> 
<p><em><strong>6.6 ‏下面关于为什么推出Spark SQL的原因的描述正确的是：</strong></em> <code>AB</code></p> 
<pre><code>‍A.Spark SQL可以提供DataFrame API，可以对内部和外部各种数据源执行各种关系操作
B.可以支持大量的数据源和数据分析算法，组合使用Spark SQL和Spark MLlib，可以融合传统关系数据库的结构化数据管理能力和机器学习算法的数据处理能力
C.Spark SQL无法对各种不同的数据源进行整合
D.Spark SQL无法融合结构化数据管理能力和机器学习算法的数据处理能力
</code></pre> 
<p><em><strong>6.7 下面关于DataFrame的描述正确的是：</strong></em> <code>ABCD</code></p> 
<pre><code>A.DataFrame的推出，让Spark具备了处理大规模结构化数据的能力
B.DataFrame比原有的RDD转化方式更加简单易用，而且获得了更高的计算性能
C.Spark能够轻松实现从MySQL到DataFrame的转化，并且支持SQL查询
D.DataFrame是一种以RDD为基础的分布式数据集，提供了详细的结构信息
</code></pre> 
<p><em><strong>6.8‌ 要读取people.json文件生成DataFrame，可以使用下面哪些命令：</strong></em> <code>AC</code></p> 
<pre><code>A.spark.read.json("people.json")
B.spark.read.parquet("people.json")
C.spark.read.format("json").load("people.json")
D.spark.read.format("csv").load("people.json")
</code></pre> 
<p><em><strong>6.9 从RDD转换得到DataFrame包含两种典型方法，分别是：</strong></em> <code>AB</code></p> 
<pre><code>A.利用反射机制推断RDD模式
B.使用编程方式定义RDD模式
C.利用投影机制推断RDD模式
D.利用互联机制推断RDD模式
</code></pre> 
<p><em><strong>6.10 使用编程方式定义RDD模式时，主要包括哪三个步骤：</strong></em> <code>ABD</code></p> 
<pre><code>‍A.制作“表头”
B.制作“表中的记录”
C.制作映射表
D.把“表头”和“表中的记录”拼装在一起
</code></pre> 
<h1>
<a id="7_Spark_Streaming_11_554"></a>第7章 Spark Streaming (11个题)</h1> 
<hr> 
<p><em><strong>7.1 以下流计算框架中，哪个不是开源的：</strong></em> <code>A</code></p> 
<pre><code>A.IBM StreamBase
B.Twitter Storm
C.Yahoo! S4
D.Spark Streaming
</code></pre> 
<p><em><strong>7.2 ‎下面关于Spark Streaming的描述错误的是：</strong></em> <code>D</code></p> 
<pre><code>A.Spark Streaming的基本原理是将实时输入数据流以时间片为单位进行拆分，然后采用Spark引擎以类似批处理的方式处理每个时间片数据
B.Spark Streaming最主要的抽象是DStream（Discretized Stream，离散化数据流），表示连续不断的数据流
C.Spark Streaming可整合多种输入数据源，如Kafka、Flume、HDFS，甚至是普通的TCP套接字
D.Spark Streaming的数据抽象是DataFrame
</code></pre> 
<p><em><strong>7.3 ​下面关于Spark Streaming和Storm的描述正确的是：</strong></em> <code>A</code></p> 
<pre><code>A.Spark Streaming无法实现毫秒级的流计算，而Storm可以实现毫秒级响应
B.Spark Streaming可以实现毫秒级的流计算，而Storm无法实现毫秒级响应
C.Spark Streaming和Storm都可以实现毫秒级的流计算
D.Spark Streaming和Storm都无法实现毫秒级的流计算
</code></pre> 
<p><em><strong>7.4 ‏下面描述错误的是：</strong></em> <code>D</code></p> 
<pre><code>A.在RDD编程中需要生成一个SparkContext对象
B.在Spark SQL编程中需要生成一个SparkSession对象
C.运行一个Spark Streaming程序，就需要首先生成一个StreamingContext对象
D.在Spark SQL编程中需要生成一个StreamingContext对象
</code></pre> 
<p><em><strong>7.5 下面不属于Spark Streaming基本输入源的是：</strong></em> <code>D</code></p> 
<pre><code>A.文件流
B.套接字流
C.RDD队列流
D.双向数据流
</code></pre> 
<p><em><strong>7.6 以下关于流数据特征的描述，哪些是正确的：</strong></em> <code>ABCD</code></p> 
<pre><code>‍A.数据快速持续到达，潜在大小也许是无穷无尽的
B.数据来源众多，格式复杂
C.数据量大，但是不十分关注存储，一旦流数据中的某个元素经过处理，要么被丢弃，要么被归档存储
D.数据顺序颠倒，或者不完整，系统无法控制将要处理的新到达的数据元素的顺序
</code></pre> 
<p><em><strong>7.7 流计算处理流程一般包括哪三个阶段：</strong></em> <code>ABD</code></p> 
<pre><code>A.数据实时采集
B.数据实时计算
C.数据汇总分析
D.实时查询服务
</code></pre> 
<p><em><strong>7.8 ‎以下产品哪些属于日志采集组件：</strong></em> <code>AC</code></p> 
<pre><code>A.Scribe
B.GraphX
C.Flume
D.MySQL
</code></pre> 
<p><em><strong>7.9 流处理系统与传统的数据处理系统的不同之处在于：</strong></em> <code>ABC</code></p> 
<pre><code>A.流处理系统处理的是实时的数据，而传统的数据处理系统处理的是预先存储好的静态数据
B.用户通过流处理系统获取的是实时结果，而通过传统的数据处理系统获取的是过去某一时刻的结果
C.流处理系统无需用户主动发出查询，实时查询服务可以主动将实时结果推送给用户
D.流处理系统处理的是历史的数据，而传统的数据处理系统处理的是实时的数据
</code></pre> 
<p><em><strong>7.10‌ 编写Spark Streaming程序的基本步骤包括：</strong></em> <code>ABCD</code></p> 
<pre><code>A.通过创建输入DStream（Input Dstream）来定义输入源
B.通过对DStream应用转换操作和输出操作来定义流计算
C.调用StreamingContext对象的start()方法来开始接收数据和处理流程
D.调用StreamingContext对象的awaitTermination()方法来等待流计算进程结束
</code></pre> 
<p><em><strong>7.11 DStream有状态转换操作包括哪两种：</strong></em> <code>CD</code></p> 
<pre><code>A.update操作
B.reduceByKey操作
C.滑动窗口转换操作
D.updateStateByKey操作
</code></pre> 
<h1>
<a id="8_Spark_MLlib_8_642"></a>第8章 Spark MLlib (8个题)</h1> 
<hr> 
<p><em><strong>8.1 下面论述中错误的是：</strong></em> <code>A</code></p> 
<pre><code>A.机器学习和人工智能是不存在关联关系的两个独立领域
B.机器学习强调三个关键词：算法、经验、性能
C.推荐系统、金融反欺诈、语音识别、自然语言处理和机器翻译、模式识别、智能控制等领域，都用到了机器学习的知识
D.机器学习可以看作是一门人工智能的科学，该领域的主要研究对象是人工智能
</code></pre> 
<p><em><strong>8.2‌ 下面关于机器学习处理过程的描述，错误的是：</strong></em> <code>D</code></p> 
<pre><code>‌A.在数据的基础上，通过算法构建出模型并对模型进行评估
B.评估的性能如果达到要求，就用该模型来测试其他的数据
C.评估的性能如果达不到要求，就要调整算法来重新建立模型，再次进行评估
D.通过算法构建出的模型不需要评估就可以用于其他数据的测试
</code></pre> 
<p><em><strong>8.3 ​下面关于机器学习流水线(PipeLine)的描述，错误的是：</strong></em> <code>D</code></p> 
<pre><code>A.流水线将多个工作流阶段（转换器和评估器）连接在一起，形成机器学习的工作流，并获得结果输出
B.要构建一个机器学习流水线，首先需要定义流水线中的各个PipelineStage
C.PipelineStage称为工作流阶段，包括转换器和评估器，比如指标提取和转换模型训练等
D.流水线构建好以后，就是一个转换器（Transformer）
</code></pre> 
<p><em><strong>8.4 下面关于评估器（Estimator）的描述错误的是：</strong></em> <code>C</code></p> 
<pre><code>A.评估器是学习算法或在训练数据上的训练方法的概念抽象
B.在机器学习流水线里，评估器通常是被用来操作 DataFrame数据并生成一个转换器
C.评估器实现了方法transfrom()，它接受一个DataFrame并产生一个转换器
D.评估器实现了方法fit()，它接受一个DataFrame并产生一个转换器
</code></pre> 
<p><em><strong>8.5 下面关于转换器（Transformer）的描述错误的是：</strong></em> <code>B</code></p> 
<pre><code>A.转换器是一种可以将一个DataFrame转换为另一个DataFrame的算法
B.技术上，转换器实现了一个方法fit()，它通过附加一个或多个列，将一个DataFrame转换为另一个DataFrame
C.一个模型就是一个转换器，它把一个不包含预测标签的测试数据集DataFrame打上标签，转化成另一个包含预测标签的 DataFrame
D.技术上，转换器实现了一个方法transform()，它通过附加一个或多个列，将一个DataFrame转换为另一个DataFrame
</code></pre> 
<p><em><strong>8.6 下面的论述中，正确的是：</strong></em> <code>AB</code></p> 
<pre><code>A.传统的机器学习算法，由于技术和单机存储的限制，大多只能在少量数据上使用
B.利用MapReduce框架在全量数据上进行机器学习，这在一定程度上解决了统计随机性的问题，提高了机器学习的精度
C.MapReduce可以高效支持迭代计算
D.Spark无法高效支持迭代计算
</code></pre> 
<p><em><strong>8.7 下面关于Spark MLlib库的描述正确的是：</strong></em> <code>AC</code></p> 
<pre><code>‍A.MLlib库从1.2版本以后分为两个包：spark.mllib和spark.ml
B.spark.mllib包含基于DataFrame的原始算法API
C.spark.mllib包含基于RDD的原始算法API
D.spark.ml则提供了基于RDD的、高层次的API
</code></pre> 
<p><em><strong>8.8下面论述中正确的是：</strong></em> <code>ABC</code></p> 
<pre><code>A.DataFrame可容纳各种数据类型，与RDD数据集相比，它包含了模式（schema）信息，类似于传统数据库中的二维表格
B.流水线用DataFrame来存储源数据
C.转换器（Transformer）是一种可以将一个DataFrame转换为另一个DataFrame的算法
D.评估器（Estimator）是一种可以将一个DataFrame转换为另一个DataFrame的算法
</code></pre>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>