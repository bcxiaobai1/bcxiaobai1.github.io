<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Pytorch学习笔记 （参考官方教程） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Pytorch学习笔记 （参考官方教程）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <blockquote> 
 <p>参考： <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">pytorch官网教程</a></p> 
</blockquote> 
<p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#Quick_Start_4">一、快速开始（Quick Start）</a></li>
<li>
<ul>
<li><a href="#Working_with_data_5">数据处理（Working with data）</a></li>
<li><a href="#Creating_Models_61">创建模型（Creating Models）</a></li>
<li><a href="#Optimizing_the_Model_Parameters_92">优化模型参数（Optimizing the Model Parameters）</a></li>
<li><a href="#Saving_Models_154">保存模型（Saving Models）</a></li>
<li><a href="#Loading_Models_163">加载模型（Loading Models）</a></li>
</ul>
  </li>
<li><a href="#Tensors_195">二、张量（Tensors）</a></li>
<li>
<ul>
<li><a href="#Initializing_a_Tensor_208">初始化张量（Initializing a Tensor）</a></li>
<li><a href="#Attributes_of_a_Tensor_281">张量的属性（Attributes of a Tensor）</a></li>
<li><a href="#NumpyBridge_with_NumPy_409">用Numpy桥接（Bridge with NumPy）</a></li>
<li>
<ul>
<li><a href="#Tensor__NumPy_Tensor_to_NumPy_array_413">Tensor → NumPy (Tensor to NumPy array)</a></li>
<li><a href="#NumPyTensor_NumPy_array_to_Tensor_442">NumPy→Tensor (NumPy array to Tensor)</a></li>
</ul>
  </li>
</ul>
  </li>
<li><a href="#DATASETS__DATALOADERS_463">三、数据集和数据加载器（DATASETS &amp; DATALOADERS）</a></li>
<li>
<ul>
<li><a href="#Loading_a_Dataset_473">加载数据集（Loading a Dataset）</a></li>
<li><a href="#Iterating_and_Visualizing_the_Dataset_506">迭代和可视化数据集（Iterating and Visualizing the Dataset）</a></li>
<li><a href="#Creating_a_Custom_Dataset_for_your_files_538">创建自定义数据集（Creating a Custom Dataset for your files）</a></li>
<li><a href="#DataLoaders_616">使用DataLoaders为训练准备数据</a></li>
<li><a href="#Dataloader_629">遍历Dataloader</a></li>
</ul>
  </li>
<li><a href="#Transfroms_651">四、变换(Transfroms)</a></li>
<li>
<ul>
<li><a href="#ToTensor_673">ToTensor()</a></li>
<li><a href="#Lambda_Transforms_677">Lambda Transforms</a></li>
</ul>
  </li>
<li><a href="#_685">五、建立神经网络</a></li>
<li>
<ul>
<li><a href="#Get_Device_for_Training_701">获取训练设备（Get Device for Training）</a></li>
<li><a href="#Define_the_Class_710">定义类（Define the Class）</a></li>
<li><a href="#Model_Layers_769">模型层（Model Layers）</a></li>
<li>
<ul>
<li><a href="#nnFlatten_782">nn.Flatten</a></li>
<li><a href="#nnLinear_796">nn.Linear</a></li>
<li><a href="#nnReLU_810">nn.ReLU</a></li>
<li><a href="#nnSequential_822">nn.Sequential</a></li>
<li><a href="#nnSoftmax_837">nn.Softmax</a></li>
</ul>
   </li>
<li><a href="#Model_Parameters_846">模型参数（**Model Parameters**）</a></li>
</ul>
  </li>
<li><a href="#Autograd_889">六、自动求导（Autograd）</a></li>
<li>
<ul>
<li><a href="#Tensors_Functions_and_Computational_graph_908">张量、函数和计算图（Tensors, Functions and Computational graph）</a></li>
<li><a href="#Computing_Gradients_931">计算梯度（Computing Gradients）</a></li>
<li><a href="#Disabling_Gradient_Tracking_954">禁用梯度跟踪（Disabling Gradient Tracking）</a></li>
<li><a href="#_989">更多关于计算图</a></li>
<li><a href="#_Tensor_Gradients_and_Jacobian_Products_1006">可选阅读: 张量梯度和雅可比矩阵乘积(Tensor Gradients and Jacobian Products)</a></li>
</ul>
  </li>
<li><a href="#OPTIMIZING_MODEL_PARAMETERS_1054">七、优化模型参数（OPTIMIZING MODEL PARAMETERS）</a></li>
<li>
<ul>
<li><a href="#Prerequisite_Code_1058">前提代码（Prerequisite Code）</a></li>
<li><a href="#Hyperparameters_1106">超参数（Hyperparameters）</a></li>
<li><a href="#Optimization_Loop_1122">循环优化（Optimization Loop）</a></li>
<li><a href="#Loss_Function_1131">损失函数（Loss Function）</a></li>
<li><a href="#Optimizer_1144">优化器（Optimizer）</a></li>
<li><a href="#Full_Implementation_1160">完整实现（Full Implementation）</a></li>
</ul>
  </li>
<li><a href="#_1211">八、保存和加载模型</a></li>
<li>
<ul>
<li><a href="#Saving_and_Loading_Model_Weights_1220">加载和保存模型的权重（Saving and Loading Model Weights）</a></li>
<li><a href="#Saving_and_Loading_Models_with_Shapes_1239">通过形状来保存和加载模型（Saving and Loading Models with Shapes）</a></li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<h1>
<a id="Quick_Start_4"></a>一、快速开始（Quick Start）</h1> 
<h2>
<a id="Working_with_data_5"></a>数据处理（Working with data）</h2> 
<p>PyTorch有两个处理数据的基本体：<code>torch.utils.data.DataLoader</code> and <code>torch.utils.data.Dataset</code></p> 
<p>其中<code>Dataset</code> 存储样本及其对应的标签，<code>DataLoader</code>在数据集周围包装一个可迭代对象。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor
</code></pre> 
<p>PyTorch提供了特定领域的库，如<code>TorchText</code>、<code>TorchVision</code>和<code>TorchAudio</code>，所有这些库都包含数据集。在本教程中，我们将使用一个<code>TorchVision</code>数据集。</p> 
<p><code>torchvision.datassets</code>模块包含许多现实世界视觉数据的Dataset对象，如CIFAR, COCO。在本教程中，我们使用FashionMNIST数据集。每个TorchVision <code>Dataset</code>包含两个参数:<code>transform</code>和<code>target_transform</code>，分别用于修改样本和标签。</p> 
<pre><code class="prism language-python"><span class="token comment"># Download training data from open datasets.</span>
training_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">"data"</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

<span class="token comment"># Download test data from open datasets.</span>
test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">"data"</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>我们将<code>Dataset</code>作为参数传递给<code>DataLoader</code>。它在数据集上封装了一个可迭代对象，并支持自动批处理、采样、变换和多进程数据加载。这里我们定义的批处理大小为64，也就是说，数据加载器迭代器中的每个元素将返回一批64个特性和标签。</p> 
<pre><code class="prism language-python">batch_size <span class="token operator">=</span> <span class="token number">64</span>

<span class="token comment"># Create data loaders.</span>
train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>training_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>

<span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> test_dataloader<span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Shape of X [N, C, H, W]: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>X<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Shape of y: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>y<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string"> </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>y<span class="token punctuation">.</span>dtype<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
    <span class="token keyword">break</span>
</code></pre> 
<pre><code class="prism language-python">Shape of X <span class="token punctuation">[</span>N<span class="token punctuation">,</span> C<span class="token punctuation">,</span> H<span class="token punctuation">,</span> W<span class="token punctuation">]</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Shape of y<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>int64
</code></pre> 
<h2>
<a id="Creating_Models_61"></a>创建模型（Creating Models）</h2> 
<p>要在PyTorch中定义一个神经网络，我们需要创建一个继承自<code>nn.Module</code>的类。我们在<code>__init__</code>函数中定义网络的层，并在<code>forward</code>函数中指定数据将如何通过网络。为了加速神经网络中的操作，如果可用的话，我们将它移到GPU上。</p> 
<pre><code class="prism language-python"><span class="token comment"># Get cpu or gpu device for training.</span>
device <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Using </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>device<span class="token punctuation">}</span></span><span class="token string"> device"</span></span><span class="token punctuation">)</span>

<span class="token comment"># Define model</span>
<span class="token keyword">class</span> <span class="token class-name">NeuralNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_relu_stack <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_relu_stack<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits

model <span class="token operator">=</span> NeuralNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Optimizing_the_Model_Parameters_92"></a>优化模型参数（Optimizing the Model Parameters）</h2> 
<p>为了训练一个模型，我们需要一个损失函数和一个优化器。</p> 
<pre><code class="prism language-python">loss_fn <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-3</span><span class="token punctuation">)</span>
</code></pre> 
<p>在单个训练循环中，模型对训练数据集(分批输入)进行预测，并反向传播预测误差以调整模型参数。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> batch<span class="token punctuation">,</span> <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>

        <span class="token comment"># Compute prediction error</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

        <span class="token comment"># Backpropagation</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> batch <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            loss<span class="token punctuation">,</span> current <span class="token operator">=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batch <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">:</span><span class="token format-spec">&gt;7f</span><span class="token punctuation">}</span></span><span class="token string">  [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>current<span class="token punctuation">:</span><span class="token format-spec">&gt;5d</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>size<span class="token punctuation">:</span><span class="token format-spec">&gt;5d</span><span class="token punctuation">}</span></span><span class="token string">]"</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>我们还根据测试数据集检查模型的性能，以确保它正在学习。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">test</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">)</span><span class="token punctuation">:</span>
    size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>
    num_batches <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span>
    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    test_loss<span class="token punctuation">,</span> correct <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
            X<span class="token punctuation">,</span> y <span class="token operator">=</span> X<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
            pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            test_loss <span class="token operator">+=</span> loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            correct <span class="token operator">+=</span> <span class="token punctuation">(</span>pred<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    test_loss <span class="token operator">/=</span> num_batches
    correct <span class="token operator">/=</span> size
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Test Error: n Accuracy: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token punctuation">(</span><span class="token number">100</span><span class="token operator">*</span>correct<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">&gt;0.1f</span><span class="token punctuation">}</span></span><span class="token string">%, Avg loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>test_loss<span class="token punctuation">:</span><span class="token format-spec">&gt;8f</span><span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>训练过程在几个迭代(阶段)中进行。在每个时期，模型学习参数，以做出更好的预测。我们打印了模型在每个时代的精度和损失;我们希望看到精度随着时间的推移而增加，而损失则随着时间的推移而减少。</p> 
<pre><code class="prism language-python">epochs <span class="token operator">=</span> <span class="token number">5</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>t<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">n-------------------------------"</span></span><span class="token punctuation">)</span>
    train<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span>
    test<span class="token punctuation">(</span>test_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done!"</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Saving_Models_154"></a>保存模型（Saving Models）</h2> 
<p>保存模型的常用方法是序列化内部状态字典(包含模型参数)。</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"model.pth"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Saved PyTorch Model State to model.pth"</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Loading_Models_163"></a>加载模型（Loading Models）</h2> 
<p>加载模型的过程包括重新创建模型结构并将状态字典加载到其中。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> NeuralNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">"model.pth"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>这个模型现在可以用来做预测</p> 
<pre><code class="prism language-python">classes <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token string">"T-shirt/top"</span><span class="token punctuation">,</span>
    <span class="token string">"Trouser"</span><span class="token punctuation">,</span>
    <span class="token string">"Pullover"</span><span class="token punctuation">,</span>
    <span class="token string">"Dress"</span><span class="token punctuation">,</span>
    <span class="token string">"Coat"</span><span class="token punctuation">,</span>
    <span class="token string">"Sandal"</span><span class="token punctuation">,</span>
    <span class="token string">"Shirt"</span><span class="token punctuation">,</span>
    <span class="token string">"Sneaker"</span><span class="token punctuation">,</span>
    <span class="token string">"Bag"</span><span class="token punctuation">,</span>
    <span class="token string">"Ankle boot"</span><span class="token punctuation">,</span>
<span class="token punctuation">]</span>

model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
x<span class="token punctuation">,</span> y <span class="token operator">=</span> test_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> test_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    predicted<span class="token punctuation">,</span> actual <span class="token operator">=</span> classes<span class="token punctuation">[</span>pred<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> classes<span class="token punctuation">[</span>y<span class="token punctuation">]</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Predicted: "</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>predicted<span class="token punctuation">}</span></span><span class="token string">", Actual: "</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>actual<span class="token punctuation">}</span></span><span class="token string">"'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h1>
<a id="Tensors_195"></a>二、张量（Tensors）</h1> 
<p>张量(tensor)一种特殊的数据结构，非常类似于数组和矩阵。在PyTorch中，我们使用张量对模型的输入和输出以及模型的参数进行编码。</p> 
<p>张量与NumPy的ndarray相似，不同之处在于张量可以在gpu或其他硬件加速器上运行。事实上，张量和NumPy数组通常可以共享相同的底层内存，消除了复制数据的需要。张量也为自动微分进行了优化(我们将在后面的自动求导部分看到更多关于它的内容)。</p> 
<p>导包：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
</code></pre> 
<h2>
<a id="Initializing_a_Tensor_208"></a>初始化张量（Initializing a Tensor）</h2> 
<p>张量可以用各种方式初始化。看看下面的例子:</p> 
<ul><li>直接从<strong>数据</strong>中初始化（<strong>Directly from data</strong>）</li></ul> 
<p><code>torch.tensor(data)</code>:张量可以直接从数据中创建。自动推断数据类型。</p> 
<pre><code class="prism language-python">data <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span>
x_data <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre> 
<ul><li>从<strong>Numpy数组</strong>中初始化（<strong>From a NumPy array</strong>）</li></ul> 
<p><code>torch.from_numpy(np_array)</code>: 张量可以从NumPy数组中创建(反之亦然)。</p> 
<pre><code class="prism language-python">np_array <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
x_np <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>np_array<span class="token punctuation">)</span>
</code></pre> 
<ul><li>从另外一个<strong>张量</strong>中初始化（<strong>From another tensor）</strong>
</li></ul> 
<p><code>torch.ones_like(x_data)</code>、<code>torch.rand_like(x_data, dtype=torch.float)</code>新的张量保留参数张量的属性(形状、数据类型)，除非显式重写。</p> 
<pre><code class="prism language-python">x_ones <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>x_data<span class="token punctuation">)</span> <span class="token comment"># retains the properties of x_data</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Ones Tensor: n </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>x_ones<span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>

x_rand <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>x_data<span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span> <span class="token comment"># overrides the datatype of x_data</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Random Tensor: n </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>x_rand<span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">Ones Tensor<span class="token punctuation">:</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

Random Tensor<span class="token punctuation">:</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.8397</span><span class="token punctuation">,</span> <span class="token number">0.3300</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.3356</span><span class="token punctuation">,</span> <span class="token number">0.7086</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li>从随机值或者常量值初始化（<strong>With random or constant values</strong>）</li></ul> 
<p><code>torch.rand(shape)</code>、<code>torch.ones(shape)</code>、<code>torch.zeros(shape)</code>Shape是张量维度的元组。在下面的函数中，它决定了输出张量的维数。</p> 
<pre><code class="prism language-python">shape <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token punctuation">)</span>
rand_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
ones_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
zeros_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Random Tensor: n </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>rand_tensor<span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Ones Tensor: n </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>ones_tensor<span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Zeros Tensor: n </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>zeros_tensor<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">Random Tensor<span class="token punctuation">:</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.7453</span><span class="token punctuation">,</span> <span class="token number">0.7993</span><span class="token punctuation">,</span> <span class="token number">0.8484</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.3592</span><span class="token punctuation">,</span> <span class="token number">0.3243</span><span class="token punctuation">,</span> <span class="token number">0.7226</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

Ones Tensor<span class="token punctuation">:</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

Zeros Tensor<span class="token punctuation">:</span>
 tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Attributes_of_a_Tensor_281"></a>张量的属性（Attributes of a Tensor）</h2> 
<p>张量属性描述了它们的形状(<code>tensor.shape</code>)、数据类型(<code>tensor.dtype</code>)和存储它们的设备（<code>tensor.device</code>）。</p> 
<pre><code class="prism language-python">tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Shape of tensor: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tensor<span class="token punctuation">.</span>shape<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Datatype of tensor: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tensor<span class="token punctuation">.</span>dtype<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Device tensor is stored on: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tensor<span class="token punctuation">.</span>device<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">Shape of tensor<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Datatype of tensor<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>float32
Device tensor <span class="token keyword">is</span> stored on<span class="token punctuation">:</span> cpu
</code></pre> 
<p>超过100张量操作，包括算术，线性代数，矩阵操作(转置，索引，切片)，采样和更多的综合描述在</p> 
<p><a href="https://pytorch.org/docs/stable/torch.html">torch - PyTorch 1.13 documentation</a></p> 
<p>这些操作都可以在GPU上运行(速度通常比CPU快)。如果你正在使用Colab，分配一个GPU到运行时&gt;改变运行时类型&gt; GPU。</p> 
<p>默认情况下，在CPU上创建张量。我们需要使用<code>.to</code>方法显式地将张量移动到GPU(在检查GPU可用性之后)。请记住，跨设备复制大张量在时间和内存方面是非常昂贵的!</p> 
<pre><code class="prism language-python"><span class="token comment"># We move our tensor to the GPU if available</span>
<span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    tensor <span class="token operator">=</span> tensor<span class="token punctuation">.</span>to<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>
</code></pre> 
<p>尝试列表中的一些操作。如果你熟悉NumPy API，你会发现张量API很容易使用。</p> 
<ul><li><strong>标准numpy类索引和切片(Standard numpy-like indexing and slicing)</strong></li></ul> 
<pre><code class="prism language-python">tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"First row: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tensor<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"First column: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token format-spec">, 0]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Last column: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tensor<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
tensor<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">First row<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
First column<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Last column<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>连接张量:</strong> 你可以使用<code>torch.cat</code>将张量序列沿着给定的维数连接起来。参见<code>torch.stack</code>，另一个张量连接操作，它与torch.cat略有不同。</p> 
<pre><code class="prism language-python">t1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>t1<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<ul><li><strong>算术运算（Arithmetic operations）</strong></li></ul> 
<pre><code class="prism language-python"><span class="token comment"># This computes the matrix multiplication between two tensors. y1, y2, y3 will have the same value</span>
y1 <span class="token operator">=</span> tensor @ tensor<span class="token punctuation">.</span>T
y2 <span class="token operator">=</span> tensor<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor<span class="token punctuation">.</span>T<span class="token punctuation">)</span>

y3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>y1<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">.</span>T<span class="token punctuation">,</span> out<span class="token operator">=</span>y3<span class="token punctuation">)</span>

<span class="token comment"># This computes the element-wise product. z1, z2, z3 will have the same value</span>
z1 <span class="token operator">=</span> tensor <span class="token operator">*</span> tensor
z2 <span class="token operator">=</span> tensor<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>

z3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand_like<span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> tensor<span class="token punctuation">,</span> out<span class="token operator">=</span>z3<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p><strong>单元素张量（Single-element tensors）</strong>：如果你有一个单元素张量，例如通过将一个张量的所有值聚合到一个值，你可以使用<code>item()</code>将它转换为一个Python数值:</p> 
<pre><code class="prism language-python">agg <span class="token operator">=</span> tensor<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
agg_item <span class="token operator">=</span> agg<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token punctuation">(</span>agg_item<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token number">12.0</span> <span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'float'</span><span class="token operator">&gt;</span>
</code></pre> 
<p><strong>就地操作（In-place operations) :</strong> 将结果存储到操作数中的操作称为<code>in-place</code>。他们被一个<code>_</code>后缀所标记。例如：<code>x.copy_(y)</code> ，<code>x.t_()</code>,会改变x的值。</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>tensor<span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>
tensor<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">5.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">,</span> <span class="token number">6.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>就地操作节省了一些内存，但在计算导数时可能会出现问题，因为会立即丢失历史记录。因此，不鼓励使用它们。</p> 
<h2>
<a id="NumpyBridge_with_NumPy_409"></a>用Numpy桥接（Bridge with NumPy）</h2> 
<p>NumPy数组和在cpu上的张量可以共享它们的底层内存位置，更改其中一个将更改另一个。</p> 
<h3>
<a id="Tensor__NumPy_Tensor_to_NumPy_array_413"></a>Tensor → NumPy (Tensor to NumPy array)</h3> 
<p><code>t. numpy()</code> : 把张量转换成numpy 数组。</p> 
<pre><code class="prism language-python">t <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"t: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>t<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
n <span class="token operator">=</span> t<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"n: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>n<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">t<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
n<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">1.</span> <span class="token number">1.</span> <span class="token number">1.</span> <span class="token number">1.</span> <span class="token number">1.</span><span class="token punctuation">]</span>
</code></pre> 
<p>张量的变化反映在NumPy数组中：</p> 
<pre><code class="prism language-python">t<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"t: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>t<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"n: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>n<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">t<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
n<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">2.</span> <span class="token number">2.</span> <span class="token number">2.</span> <span class="token number">2.</span> <span class="token number">2.</span><span class="token punctuation">]</span>
</code></pre> 
<h3>
<a id="NumPyTensor_NumPy_array_to_Tensor_442"></a>NumPy→Tensor (NumPy array to Tensor)</h3> 
<p><code>torch.from_numpy(n)</code>: 把numpy数组转换成张量。</p> 
<pre><code class="prism language-python">n <span class="token operator">=</span> np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>
t <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>n<span class="token punctuation">)</span>
</code></pre> 
<p>NumPy数组的变化反映在张量中：</p> 
<pre><code class="prism language-python">np<span class="token punctuation">.</span>add<span class="token punctuation">(</span>n<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> out<span class="token operator">=</span>n<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"t: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>t<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"n: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>n<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">t<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float64<span class="token punctuation">)</span>
n<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">2.</span> <span class="token number">2.</span> <span class="token number">2.</span> <span class="token number">2.</span> <span class="token number">2.</span><span class="token punctuation">]</span>
</code></pre> 
<h1>
<a id="DATASETS__DATALOADERS_463"></a>三、数据集和数据加载器（DATASETS &amp; DATALOADERS）</h1> 
<p>处理数据样本的代码可能会变得混乱且难以维护;理想情况下，我们希望数据集代码与模型训练代码分离，以获得更好的可读性和模块化。</p> 
<p>Pytorch 提供了两个预加载数据的基本体：<strong><code>torch.utils.data.DataLoader</code></strong> 和**<code>torch.utils.data.Dataset</code>**</p> 
<p><code>Dataset</code> 存储数据样本以及他们的响应的标签，<code>DataLoader</code>在数据集周围包装了一个可迭代对象，以方便访问数据样本。</p> 
<p>PyTorch域库提供了许多预加载的数据集(如FashionMNIST)，它们是<code>torch.utils.data.Dataset</code>的子类，并实现了基于特定数据的函数。它们可以用于模型的原型和基准测试。</p> 
<h2>
<a id="Loading_a_Dataset_473"></a>加载数据集（Loading a Dataset）</h2> 
<p>下面是一个如何从TorchVision加载Fashion-MNIST数据集的例子。Fashion-MNIST是Zalando文章图像的一个数据集，包含6万个训练示例和1万个测试示例。每个示例包括一个28×28灰度图像和来自10个类中的一个相关标签。</p> 
<p>我们用以下参数加载FashionMNIST数据集:</p> 
<ul>
<li>
<code>root</code>是存储训练/测试数据的路径</li>
<li>
<code>train</code>指定训练或测试数据集</li>
<li>
<code>download=True</code>如果根目录下不可用，则从互联网上下载数据。</li>
<li>
<code>Transform</code>和<code>target_transform</code>指定了特征变换和标签变换</li>
</ul> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> Dataset
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

training_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">"data"</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>

test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">"data"</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Iterating_and_Visualizing_the_Dataset_506"></a>迭代和可视化数据集（Iterating and Visualizing the Dataset）</h2> 
<p>我们可以手动索引数据集，就像一个列表: <code>training_data[index]</code>。我们使用<code>matplotlib</code>来可视化训练数据中的一些样本。</p> 
<pre><code class="prism language-python">labels_map <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
    <span class="token number">0</span><span class="token punctuation">:</span> <span class="token string">"T-Shirt"</span><span class="token punctuation">,</span>
    <span class="token number">1</span><span class="token punctuation">:</span> <span class="token string">"Trouser"</span><span class="token punctuation">,</span>
    <span class="token number">2</span><span class="token punctuation">:</span> <span class="token string">"Pullover"</span><span class="token punctuation">,</span>
    <span class="token number">3</span><span class="token punctuation">:</span> <span class="token string">"Dress"</span><span class="token punctuation">,</span>
    <span class="token number">4</span><span class="token punctuation">:</span> <span class="token string">"Coat"</span><span class="token punctuation">,</span>
    <span class="token number">5</span><span class="token punctuation">:</span> <span class="token string">"Sandal"</span><span class="token punctuation">,</span>
    <span class="token number">6</span><span class="token punctuation">:</span> <span class="token string">"Shirt"</span><span class="token punctuation">,</span>
    <span class="token number">7</span><span class="token punctuation">:</span> <span class="token string">"Sneaker"</span><span class="token punctuation">,</span>
    <span class="token number">8</span><span class="token punctuation">:</span> <span class="token string">"Bag"</span><span class="token punctuation">,</span>
    <span class="token number">9</span><span class="token punctuation">:</span> <span class="token string">"Ankle Boot"</span><span class="token punctuation">,</span>
<span class="token punctuation">}</span>
figure <span class="token operator">=</span> plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
cols<span class="token punctuation">,</span> rows <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span>
<span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> cols <span class="token operator">*</span> rows <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    sample_idx <span class="token operator">=</span> torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>training_data<span class="token punctuation">)</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
    img<span class="token punctuation">,</span> label <span class="token operator">=</span> training_data<span class="token punctuation">[</span>sample_idx<span class="token punctuation">]</span>
    figure<span class="token punctuation">.</span>add_subplot<span class="token punctuation">(</span>rows<span class="token punctuation">,</span> cols<span class="token punctuation">,</span> i<span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span>labels_map<span class="token punctuation">[</span>label<span class="token punctuation">]</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token string">"off"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"gray"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/d6/6b/p8QtA5Ua_o.png" alt="在这里插入图片描述" width="400"></p> 
<h2>
<a id="Creating_a_Custom_Dataset_for_your_files_538"></a>创建自定义数据集（Creating a Custom Dataset for your files）</h2> 
<p>自定义Dataset类必须实现三个函数: <code>__init__</code>、<code>__len__</code>和<code>__getitem__</code>。看看这个实现： FashionMNIST图像存储在<code>img_dir</code>目录中，它们的标签单独存储在CSV文件<code>annotations_file</code>中。</p> 
<p>在下一节中，我们将详细分析每个函数中发生的事情。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>io <span class="token keyword">import</span> read_image

<span class="token keyword">class</span> <span class="token class-name">CustomImageDataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> annotations_file<span class="token punctuation">,</span> img_dir<span class="token punctuation">,</span> transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> target_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>img_labels <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>annotations_file<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>img_dir <span class="token operator">=</span> img_dir
        self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform
        self<span class="token punctuation">.</span>target_transform <span class="token operator">=</span> target_transform

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_labels<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
        img_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_dir<span class="token punctuation">,</span> self<span class="token punctuation">.</span>img_labels<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        image <span class="token operator">=</span> read_image<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>
        label <span class="token operator">=</span> self<span class="token punctuation">.</span>img_labels<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">:</span>
            image <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>target_transform<span class="token punctuation">:</span>
            label <span class="token operator">=</span> self<span class="token punctuation">.</span>target_transform<span class="token punctuation">(</span>label<span class="token punctuation">)</span>
        <span class="token keyword">return</span> image<span class="token punctuation">,</span> label
</code></pre> 
<ul><li><code>__init__</code></li></ul> 
<p>在实例化Dataset对象时，<code>__init__</code>函数只运行一次。我们初始化包含图像、注释文件和两个转换的目录(下一节将详细介绍)。</p> 
<p>csv文件如下所示:</p> 
<pre><code class="prism language-python">tshirt1<span class="token punctuation">.</span>jpg<span class="token punctuation">,</span> <span class="token number">0</span>
tshirt2<span class="token punctuation">.</span>jpg<span class="token punctuation">,</span> <span class="token number">0</span>
<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>
ankleboot999<span class="token punctuation">.</span>jpg<span class="token punctuation">,</span> <span class="token number">9</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> annotations_file<span class="token punctuation">,</span> img_dir<span class="token punctuation">,</span> transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> target_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>img_labels <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>annotations_file<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>img_dir <span class="token operator">=</span> img_dir
    self<span class="token punctuation">.</span>transform <span class="token operator">=</span> transform
    self<span class="token punctuation">.</span>target_transform <span class="token operator">=</span> target_transform
</code></pre> 
<ul><li><code>__len__</code></li></ul> 
<p><code>__len__</code>函数返回数据集中样本的数量。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_labels<span class="token punctuation">)</span>
</code></pre> 
<ul><li><code>__getitem__</code></li></ul> 
<p><code>__getitem__</code>函数从给定索引<code>idx</code>处的数据集中加载并返回一个样本。基于索引，它识别图像在磁盘上的位置，使用<code>read_image</code>将其转换为一个张量，从csv数据中检索相应的标签<code>self.Img_labels</code>，调用它们transform函数(如果适用)，并返回一个元组<code>(张量图像, 对应的标签)</code>。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    img_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>self<span class="token punctuation">.</span>img_dir<span class="token punctuation">,</span> self<span class="token punctuation">.</span>img_labels<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
    image <span class="token operator">=</span> read_image<span class="token punctuation">(</span>img_path<span class="token punctuation">)</span>
    label <span class="token operator">=</span> self<span class="token punctuation">.</span>img_labels<span class="token punctuation">.</span>iloc<span class="token punctuation">[</span>idx<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">:</span>
        image <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>image<span class="token punctuation">)</span>
    <span class="token keyword">if</span> self<span class="token punctuation">.</span>target_transform<span class="token punctuation">:</span>
        label <span class="token operator">=</span> self<span class="token punctuation">.</span>target_transform<span class="token punctuation">(</span>label<span class="token punctuation">)</span>
    <span class="token keyword">return</span> image<span class="token punctuation">,</span> label
</code></pre> 
<h2>
<a id="DataLoaders_616"></a>使用DataLoaders为训练准备数据</h2> 
<p><code>Dataset</code>检索我们的数据集的特征，并每次标记一个样本。在训练模型时，我们通常希望以“小批”的方式传递样本，在每个epoch重新调整数据以减少模型过拟合，并使用Python的<code>multiprocessing</code>来加速数据检索。</p> 
<p>DataLoader是一个可迭代对象，它在一个简单的API中为我们抽象了这种复杂性。</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>training_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Dataloader_629"></a>遍历Dataloader</h2> 
<p>我们已经将该数据集加载到DataLoader中，并可以根据需要迭代该数据集。下面的每个迭代都返回一批<code>train_features</code>和<code>train_labels</code>(分别包含<code>batch_size=64</code>个特征和标签)。因为我们指定了<code>shuffle=True</code>，所以在遍历所有批之后，数据会被打乱。</p> 
<pre><code class="prism language-python"><span class="token comment"># Display image and label.</span>
train_features<span class="token punctuation">,</span> train_labels <span class="token operator">=</span> <span class="token builtin">next</span><span class="token punctuation">(</span><span class="token builtin">iter</span><span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Feature batch shape: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>train_features<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Labels batch shape: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>train_labels<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
img <span class="token operator">=</span> train_features<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
label <span class="token operator">=</span> train_labels<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
plt<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>img<span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"gray"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Label: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>label<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">Feature batch shape<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Labels batch shape<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
Label<span class="token punctuation">:</span> <span class="token number">9</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/67/03/oC4P4QgM_o.png" alt="在这里插入图片描述" width="300"></p> 
<h1>
<a id="Transfroms_651"></a>四、变换(Transfroms)</h1> 
<p>数据并不总是以训练机器学习算法所需的最终处理形式出现。我们使用<code>Transfrom</code>来执行数据的一些操作，并使其适合于训练。</p> 
<p>所有TorchVision数据集都有两个参数——<code>transform</code>用于修改特征，<code>target_transform</code>用于修改标签——它们接受包含转换逻辑的可调用对象。<code>torchvision.transforms</code>模块提供了几个开箱即用的常用转换。</p> 
<p>FashionMNIST特征为PIL Image格式，标签为整数。在训练中，我们需要将<strong>特征</strong>作为<strong>归一化张量</strong>，将<strong>标签</strong>作为<strong>独热编码</strong>（ont-hot）张量。为了进行这些转换，我们使用<code>ToTensor</code>和<code>Lambda</code>。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor<span class="token punctuation">,</span> Lambda

ds <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">"data"</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    target_transform<span class="token operator">=</span>Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> y<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="ToTensor_673"></a>ToTensor()</h2> 
<p>ToTensor将PIL图像或NumPy <code>ndarray</code>转换为<code>FloatTensor</code>。并在范围内缩放图像的像素强度值在范围<code>[0.,1.]</code></p> 
<h2>
<a id="Lambda_Transforms_677"></a>Lambda Transforms</h2> 
<p>Lambda转换应用任何用户定义的Lambda函数。在这里，我们定义一个函数将整数转换为一个单次编码张量。它首先创建一个大小为10(数据集中标签的数量)的零张量，并调用<code>scatter_</code>，在标签y给出的索引上赋值为1</p> 
<pre><code class="prism language-python">target_transform <span class="token operator">=</span> Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> y<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>
    <span class="token number">10</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span>scatter_<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> index<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token punctuation">,</span> value<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h1>
<a id="_685"></a>五、建立神经网络</h1> 
<p>神经网络由对数据执行操作的层/模块组成。</p> 
<p>神经网络由对数据执行操作的**层/模块（layers/modules）**组成。<code>torch.nn</code>命名空间提供了构建自己的神经网络所需的所有构建块。PyTorch中的每个模块都继承了<code>nn.Module</code>。神经网络本身就是一个由其他模块(层)组成的模块。这种嵌套结构允许轻松地构建和管理复杂的体系结构。</p> 
<p>在下面的小节中，我们将构建一个神经网络来对FashionMNIST数据集中的图像进行分类。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> os
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets<span class="token punctuation">,</span> transforms
</code></pre> 
<h2>
<a id="Get_Device_for_Training_701"></a>获取训练设备（Get Device for Training）</h2> 
<p>我们希望能够在硬件加速器上训练我们的模型，如GPU，如果它是可用的。让我们检查一下<code>torch.cuda</code>是否可用的，否则我们继续使用CPU。</p> 
<pre><code class="prism language-python">device <span class="token operator">=</span> <span class="token string">"cuda"</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">"cpu"</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Using </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>device<span class="token punctuation">}</span></span><span class="token string"> device"</span></span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Define_the_Class_710"></a>定义类（Define the Class）</h2> 
<p>我们通过子类化<code>nn.Module</code>来定义我们的神经网络。并初始化<code>__init__</code>中的神经网络层。每一个<code>nn.Moudule</code>子类在<code>forward</code>方法中实现对输入数据的操作。</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">NeuralNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>NeuralNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_relu_stack <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_relu_stack<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits
</code></pre> 
<p>我们创建一个NeuralNetwork实例，把它移到设备上，打印出它的结构。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> NeuralNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">NeuralNetwork<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span>linear_relu_stack<span class="token punctuation">)</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>
    <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>
</code></pre> 
<p>为了使用模型，我们将输入数据传递给它。这将执行模型的<code>forward</code>，以及一些background operation。不要直接调用<code>model.forward()</code> !</p> 
<p>在输入上调用模型将返回一个二维张量，<code>dim=0</code>对应10行，每一行是一个类的预测值；<code>dim=1</code>对应于每个输出的单个值。我们通过将它传递给<code>nn.Softmax</code>一个实例模型来获得预测概率。</p> 
<pre><code class="prism language-python">X <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> device<span class="token operator">=</span>device<span class="token punctuation">)</span>
logits <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
pred_probab <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">(</span>logits<span class="token punctuation">)</span>
y_pred <span class="token operator">=</span> pred_probab<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Predicted class: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>y_pred<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">Predicted <span class="token keyword">class</span><span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Model_Layers_769"></a>模型层（Model Layers）</h2> 
<p>让我们分解FashionMNIST模型中的层。为了说明这一点，我们将使用3张大小为28x28的图片组成的小批样本，看看在网络中传递它时会发生什么。</p> 
<pre><code class="prism language-python">input_image <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>input_image<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="nnFlatten_782"></a>nn.Flatten</h3> 
<p>我们初始化<code>nn.Flatten</code> 层将每个2D 28x28图像转换为一个784像素值的连续数组(保持小批尺寸(dim=0))。</p> 
<pre><code class="prism language-python">flatten <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
flat_image <span class="token operator">=</span> flatten<span class="token punctuation">(</span>input_image<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>flat_image<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="nnLinear_796"></a>nn.Linear</h3> 
<p>线性层是一个模块，它使用存储的权值和偏差对输入应用线性转换。</p> 
<pre><code class="prism language-python">layer1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>
hidden1 <span class="token operator">=</span> layer1<span class="token punctuation">(</span>flat_image<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>hidden1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="nnReLU_810"></a>nn.ReLU</h3> 
<p>非线性激活在模型的输入和输出之间创建了复杂的映射。它们被应用在线性变换后引入非线性，帮助神经网络学习各种各样的现象。</p> 
<p>在这个模型中，在我们的线性层之间使用<code>nn.ReLU</code>作为激活函数，但还有其他非线性的激活函数。</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Before ReLU: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>hidden1<span class="token punctuation">}</span></span><span class="token string">nn"</span></span><span class="token punctuation">)</span>
hidden1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>hidden1<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"After ReLU: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>hidden1<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="nnSequential_822"></a>nn.Sequential</h3> 
<p>nn.Sequential 是模型的一个有序容器，数据按照定义的相同顺序在所有模块中传递。您可以使用顺序容器将一个快速网络组合在一起，比如<code>seq_modules</code>。</p> 
<pre><code class="prism language-python">seq_modules <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    flatten<span class="token punctuation">,</span>
    layer1<span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
input_image <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">,</span><span class="token number">28</span><span class="token punctuation">)</span>
logits <span class="token operator">=</span> seq_modules<span class="token punctuation">(</span>input_image<span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="nnSoftmax_837"></a>nn.Softmax</h3> 
<p>对数被缩放到值[0,1]，表示模型对每个类的预测概率。<code>dim</code>参数指示数值之和必须为1的维度。</p> 
<pre><code class="prism language-python">softmax <span class="token operator">=</span> nn<span class="token punctuation">.</span>Softmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
pred_probab <span class="token operator">=</span> softmax<span class="token punctuation">(</span>logits<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Model_Parameters_846"></a>模型参数（<strong>Model Parameters</strong>）</h2> 
<p>神经网络中的许多层都是参数化的，即具有相关的权重和偏差，并在训练过程中进行优化。子类化<code>nn.Module</code>自动跟踪模型对象中定义的所有字段，并使用模型的<code>parameters()</code>或<code>named_parameters()</code>方法使所有参数可访问。</p> 
<p>在本例中，我们迭代每个参数，并打印其大小和其值的预览。</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Model structure: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>model<span class="token punctuation">}</span></span><span class="token string">nn"</span></span><span class="token punctuation">)</span>

<span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Layer: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>name<span class="token punctuation">}</span></span><span class="token string"> | Size: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string"> | Values : </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>param<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token format-spec">2]</span><span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">Model structure<span class="token punctuation">:</span> NeuralNetwork<span class="token punctuation">(</span>
  <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
  <span class="token punctuation">(</span>linear_relu_stack<span class="token punctuation">)</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>
    <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">784</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
  <span class="token punctuation">)</span>
<span class="token punctuation">)</span>

Layer<span class="token punctuation">:</span> linear_relu_stack<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>weight <span class="token operator">|</span> Size<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">|</span> Values <span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.0125</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0280</span><span class="token punctuation">,</span>  <span class="token number">0.0184</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0006</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0179</span><span class="token punctuation">,</span>  <span class="token number">0.0052</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.0332</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0088</span><span class="token punctuation">,</span>  <span class="token number">0.0086</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0284</span><span class="token punctuation">,</span>  <span class="token number">0.0238</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0240</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SliceBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>

Layer<span class="token punctuation">:</span> linear_relu_stack<span class="token punctuation">.</span><span class="token number">0</span><span class="token punctuation">.</span>bias <span class="token operator">|</span> Size<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">|</span> Values <span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.0045</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0279</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SliceBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>

Layer<span class="token punctuation">:</span> linear_relu_stack<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>weight <span class="token operator">|</span> Size<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">|</span> Values <span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.0392</span><span class="token punctuation">,</span>  <span class="token number">0.0210</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0063</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0.0264</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0129</span><span class="token punctuation">,</span>  <span class="token number">0.0172</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0415</span><span class="token punctuation">,</span>  <span class="token number">0.0196</span><span class="token punctuation">,</span>  <span class="token number">0.0027</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0.0205</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0392</span><span class="token punctuation">,</span>  <span class="token number">0.0401</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SliceBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>

Layer<span class="token punctuation">:</span> linear_relu_stack<span class="token punctuation">.</span><span class="token number">2</span><span class="token punctuation">.</span>bias <span class="token operator">|</span> Size<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">|</span> Values <span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0169</span><span class="token punctuation">,</span>  <span class="token number">0.0353</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SliceBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>

Layer<span class="token punctuation">:</span> linear_relu_stack<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>weight <span class="token operator">|</span> Size<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">|</span> Values <span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0427</span><span class="token punctuation">,</span>  <span class="token number">0.0441</span><span class="token punctuation">,</span>  <span class="token number">0.0027</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">0.0201</span><span class="token punctuation">,</span>  <span class="token number">0.0362</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0359</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span> <span class="token number">0.0062</span><span class="token punctuation">,</span>  <span class="token number">0.0343</span><span class="token punctuation">,</span>  <span class="token number">0.0296</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0399</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0287</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0368</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
       device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SliceBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>

Layer<span class="token punctuation">:</span> linear_relu_stack<span class="token punctuation">.</span><span class="token number">4</span><span class="token punctuation">.</span>bias <span class="token operator">|</span> Size<span class="token punctuation">:</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">|</span> Values <span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0219</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0387</span><span class="token punctuation">]</span><span class="token punctuation">,</span> device<span class="token operator">=</span><span class="token string">'cuda:0'</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SliceBackward0<span class="token operator">&gt;</span><span class="token punctuation">)</span>
</code></pre> 
<h1>
<a id="Autograd_889"></a>六、自动求导（Autograd）</h1> 
<p>在训练神经网络时，最常用的算法是<strong>反向传播 (back propagation)</strong>。该算法根据损失函数相对于给定参数的<strong>梯度(gradient)</strong> 调整参数(模型权值)。</p> 
<p>为了计算这些梯度，PyTorch有一个内置的微分引擎叫做<code>torch.autograd</code>。它支持任何计算图的<strong>梯度自动计算</strong>。</p> 
<p>考虑最简单的单层神经网络，输入<code>x</code>，参数<code>w</code>和<code>b</code>，以及一些损失函数。它可以在PyTorch中以以下方式定义:</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch

x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span>  <span class="token comment"># input tensor</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># expected output</span>
w <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
z <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token operator">+</span>b
loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>binary_cross_entropy_with_logits<span class="token punctuation">(</span>z<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Tensors_Functions_and_Computational_graph_908"></a>张量、函数和计算图（Tensors, Functions and Computational graph）</h2> 
<p>这段代码定义了以下计算图:</p> 
<p><img src="https://images2.imgbox.com/d7/03/PSveOeMb_o.png" alt="在这里插入图片描述" width="500"></p> 
<p>在这个网络中，<code>w</code>和<code>b</code>是我们需要优化的参数。因此，我们需要能够计算损失函数相对于这些变量的梯度。为了做到这一点，我们设置这些张量的<code>requires_grad</code>属性。</p> 
<p>您可以在创建张量时设置<code>requires_grad</code>的值，或者稍后使用<code>x.requires_grad_(True)</code>方法。</p> 
<p>应用于张量构造计算图的函数，实际上是函数类的对象。这个对象知道如何在正向方向上计算函数，也知道如何在反向传播步骤中计算其导数。对反向传播函数的引用存储在张量的<code>grad_fn</code>属性中。</p> 
<pre><code class="prism language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Gradient function for z = </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>z<span class="token punctuation">.</span>grad_fn<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Gradient function for loss = </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">.</span>grad_fn<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">Gradient function <span class="token keyword">for</span> z <span class="token operator">=</span> <span class="token operator">&lt;</span>AddBackward0 <span class="token builtin">object</span> at <span class="token number">0x7f623391cfd0</span><span class="token operator">&gt;</span>
Gradient function <span class="token keyword">for</span> loss <span class="token operator">=</span> <span class="token operator">&lt;</span>BinaryCrossEntropyWithLogitsBackward0 <span class="token builtin">object</span> at <span class="token number">0x7f623391cdf0</span><span class="token operator">&gt;</span>
</code></pre> 
<h2>
<a id="Computing_Gradients_931"></a>计算梯度（Computing Gradients）</h2> 
<p>为了优化神经网络中参数的权重，我们需要计算损失函数对参数的导数。也就是说，我们需要固定<code>x</code>和<code>y</code>的值，得到<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          ∂
         
         
          l
         
         
          o
         
         
          s
         
         
          s
         
        
        
         
          ∂
         
         
          w
         
        
       
      
      
       frac{partial l o s s}{partial w}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.22511em;vertical-align: -0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.880108em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mathdefault mtight" style="margin-right: 0.02691em">w</span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mathdefault mtight" style="margin-right: 0.01968em">l</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         
          ∂
         
         
          l
         
         
          o
         
         
          s
         
         
          s
         
        
        
         
          ∂
         
         
          b
         
        
       
      
      
       frac{partial l o s s}{partial b}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.22511em;vertical-align: -0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.880108em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mathdefault mtight">b</span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mathdefault mtight" style="margin-right: 0.01968em">l</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">s</span><span class="mord mathdefault mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span> 。为了计算这些导数，我们调用<code>loss.backward()</code>然后从<code>w.grad</code>和<code>b.grad</code>中得到导数值。</p> 
<pre><code class="prism language-python">loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0048</span><span class="token punctuation">,</span> <span class="token number">0.0694</span><span class="token punctuation">,</span> <span class="token number">0.0747</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0048</span><span class="token punctuation">,</span> <span class="token number">0.0694</span><span class="token punctuation">,</span> <span class="token number">0.0747</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0048</span><span class="token punctuation">,</span> <span class="token number">0.0694</span><span class="token punctuation">,</span> <span class="token number">0.0747</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0048</span><span class="token punctuation">,</span> <span class="token number">0.0694</span><span class="token punctuation">,</span> <span class="token number">0.0747</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.0048</span><span class="token punctuation">,</span> <span class="token number">0.0694</span><span class="token punctuation">,</span> <span class="token number">0.0747</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0048</span><span class="token punctuation">,</span> <span class="token number">0.0694</span><span class="token punctuation">,</span> <span class="token number">0.0747</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>我们只能获得计算图的叶节点的<code>grad</code>属性，它们的<code>requires_grad</code>属性设置为<code>True</code>。对于图中的所有其他节点，梯度将不可用。</p> 
<p>由于性能原因，我们只能在给定的图上反向(<code>backward</code>)执行一次梯度计算。如果我们需要在同一个图上执行多个<code>反向调用</code>，我们需要将<code>retain_graph=True</code>传递给<code>反向调用</code>。</p> 
<h2>
<a id="Disabling_Gradient_Tracking_954"></a>禁用梯度跟踪（Disabling Gradient Tracking）</h2> 
<p>默认情况下，所有<code>requires_grad=True</code>的张量都在跟踪它们的计算历史并支持梯度计算。然而，在某些情况下，我们不需要这样做，例如，当我们已经训练了模型，只想将其应用于一些输入数据时，即我们只想通过网络进行<code>前向计算</code>。我们可以在计算代码周围加上<code>torch.no_grad()</code>代码块来停止跟踪计算:</p> 
<pre><code class="prism language-python">z <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token operator">+</span>b
<span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>

<span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    z <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token operator">+</span>b
<span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token boolean">True</span>
<span class="token boolean">False</span>
</code></pre> 
<p>另一种实现相同结果的方法是在张量上使用<code>detach()</code>方法:</p> 
<pre><code class="prism language-python">z <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>x<span class="token punctuation">,</span> w<span class="token punctuation">)</span><span class="token operator">+</span>b
z_det <span class="token operator">=</span> z<span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>z_det<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token boolean">False</span>
</code></pre> 
<p>禁用梯度跟踪可能有以下原因:</p> 
<ul>
<li>将神经网络中的一些参数标记为冻结参数(<code>frozen parameters</code>)。这是对预训练网络进行微调以加快计算的一个非常常见的场景。</li>
<li>为了在只进行前向传递时加快计算，因为对不跟踪梯度的张量的计算将更加高效。</li>
</ul> 
<h2>
<a id="_989"></a>更多关于计算图</h2> 
<p>从概念上讲，<code>autograd</code>在一个由<code>Function</code>对象组成的有向无环图(DAG)中保存<strong>数据</strong>(张量)和所有执行的<strong>操作</strong>(以及产生的新张量)的记录。在这个DAG中，叶是输入张量，根是输出张量。通过从根到叶跟踪这个图，您可以使用<strong>链式法则</strong>（<code>chain rule</code>）自动计算梯度。</p> 
<p><strong>Forward</strong>: 在向前传递中，<code>autograd</code>会同时做两件事:</p> 
<ul>
<li>运行请求的操作来计算得到的张量</li>
<li>维持DAG中操作的梯度函数。</li>
</ul> 
<p><strong>Backward</strong>: 当在DAG根节点上调用<code>backward()</code>时，向后传递开始。Autograd 会开始做如下事：</p> 
<ul>
<li>从每个<code>.grad_fn</code>计算梯度</li>
<li>将它们累积到各自张量的<code>.grad</code>属性中</li>
<li>使用链式法则一直传播到叶张量</li>
</ul> 
<p>PyTorch中的DAG是动态的，需要注意的是，图形是从头创建的; 在每次<code>.backward()</code>调用之后，<code>autograd</code>开始填充一个新的图。这正是允许您在模型中使用控制流语句的原因; 如果需要，您可以在每次迭代中更改形状、大小和操作。</p> 
<h2>
<a id="_Tensor_Gradients_and_Jacobian_Products_1006"></a>可选阅读: 张量梯度和雅可比矩阵乘积(Tensor Gradients and Jacobian Products)</h2> 
<p>在很多情况下，我们有一个标量损失函数，我们需要计算关于一些参数的梯度。然而，也有输出函数是任意张量的情况。在本例中，PyTorch允许您计算所谓的雅可比矩阵乘积，而不是实际的梯度。</p> 
<p>对于一个向量函数 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         y
        
        
         ⃗
        
       
       
        =
       
       
        f
       
       
        (
       
       
        
         x
        
        
         ⃗
        
       
       
        )
       
      
      
       vec{y}=f(vec{x})
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.90844em;vertical-align: -0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.714em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="overlay" style="height: 0.714em;width: 0.471em">
            
             
            </span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.10764em">f</span><span class="mopen">(</span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="overlay" style="height: 0.714em;width: 0.471em">
            
             
            </span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> , 其中<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         ⃗
        
       
       
        =
       
       
        
         ⟨
        
        
         
          x
         
         
          1
         
        
        
         ,
        
        
         …
        
        
         ,
        
        
         
          x
         
         
          n
         
        
        
         ⟩
        
       
      
      
       vec{x}=leftlangle x_1, ldots, x_nrightrangle
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.714em;vertical-align: 0em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="overlay" style="height: 0.714em;width: 0.471em">
            
             
            </span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="minner"><span class="mopen delimcenter">⟨</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">⟩</span></span></span></span></span></span> 和 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         y
        
        
         ⃗
        
       
       
        =
       
       
        
         ⟨
        
        
         
          y
         
         
          1
         
        
        
         ,
        
        
         …
        
        
         ,
        
        
         
          y
         
         
          m
         
        
        
         ⟩
        
       
      
      
       vec{y}=leftlangle y_1, ldots, y_mrightrangle
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.90844em;vertical-align: -0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.714em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="overlay" style="height: 0.714em;width: 0.471em">
            
             
            </span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="minner"><span class="mopen delimcenter">⟨</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.03588em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: -0.03588em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">⟩</span></span></span></span></span></span>, <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         y
        
        
         ⃗
        
       
      
      
       vec{y}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.90844em;vertical-align: -0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.714em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="overlay" style="height: 0.714em;width: 0.471em">
            
             
            </span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em"><span class=""></span></span></span></span></span></span></span></span></span> 对<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         x
        
        
         ⃗
        
       
      
      
       vec{x}
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.714em;vertical-align: 0em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.714em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathdefault">x</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="overlay" style="height: 0.714em;width: 0.471em">
            
             
            </span></span></span></span></span></span></span></span></span></span></span> 的梯度如下面的雅克比矩阵<br> <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        J
       
       
        =
       
       
        
         (
        
        
         
          
           
            
             
              
               ∂
              
              
               
                y
               
               
                1
               
              
             
             
              
               ∂
              
              
               
                x
               
               
                1
               
              
             
            
           
          
          
           
            
             ⋯
            
           
          
          
           
            
             
              
               ∂
              
              
               
                y
               
               
                1
               
              
             
             
              
               ∂
              
              
               
                x
               
               
                n
               
              
             
            
           
          
         
         
          
           
            
             ⋮
            
            
             
            
           
          
          
           
            
             ⋱
            
           
          
          
           
            
             ⋮
            
            
             
            
           
          
         
         
          
           
            
             
              
               ∂
              
              
               
                y
               
               
                m
               
              
             
             
              
               ∂
              
              
               
                x
               
               
                1
               
              
             
            
           
          
          
           
            
             ⋯
            
           
          
          
           
            
             
              
               ∂
              
              
               
                y
               
               
                m
               
              
             
             
              
               ∂
              
              
               
                x
               
               
                n
               
              
             
            
           
          
         
        
        
         )
        
       
      
      
       J=left(begin{array}{ccc} frac{partial y_1}{partial x_1} &amp; cdots &amp; frac{partial y_1}{partial x_n} \ vdots &amp; ddots &amp; vdots \ frac{partial y_m}{partial x_1} &amp; cdots &amp; frac{partial y_m}{partial x_n} end{array}right)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.09618em">J</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 4.61463em;vertical-align: -2.05732em"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.35002em"><span class=""><span class="pstrut" style="height: 3.155em"></span><span class="delimsizinginner delim-size4"><span class="">⎝</span></span></span><span class=""><span class="pstrut" style="height: 3.155em"></span><span class="delimsizinginner delim-size4"><span class="">⎜</span></span></span><span class=""><span class="pstrut" style="height: 3.155em"></span><span class="delimsizinginner delim-size4"><span class="">⎛</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.85003em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.55732em"><span class=""><span class="pstrut" style="height: 3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.932216em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.317314em"><span class="" style="margin-left: 0em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.317314em"><span class="" style="margin-left: -0.03588em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4451em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span class=""><span class="pstrut" style="height: 3.6875em"></span><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width: 0em;border-top-width: 1.5em"></span></span></span></span><span class=""><span class="pstrut" style="height: 3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.932216em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.317314em"><span class="" style="margin-left: 0em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.164543em"><span class="" style="margin-left: -0.03588em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4451em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.05732em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.55732em"><span class=""><span class="pstrut" style="height: 3.5em"></span><span class="mord"><span class="minner">⋯</span></span></span><span class=""><span class="pstrut" style="height: 3.5em"></span><span class="mord"><span class="minner">⋱</span></span></span><span class=""><span class="pstrut" style="height: 3.5em"></span><span class="mord"><span class="minner">⋯</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.05732em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span><span class="arraycolsep" style="width: 0.5em"></span><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.55732em"><span class=""><span class="pstrut" style="height: 3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.932216em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.164543em"><span class="" style="margin-left: 0em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.317314em"><span class="" style="margin-left: -0.03588em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4451em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span><span class=""><span class="pstrut" style="height: 3.6875em"></span><span class="mord"><span class="mord"><span class="mord">⋮</span><span class="mord rule" style="border-right-width: 0em;border-top-width: 1.5em"></span></span></span></span><span class=""><span class="pstrut" style="height: 3.6875em"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.932216em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.164543em"><span class="" style="margin-left: 0em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right: 0.05556em">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.164543em"><span class="" style="margin-left: -0.03588em;margin-right: 0.0714286em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.143em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.4451em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 2.05732em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 0.5em"></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.35002em"><span class=""><span class="pstrut" style="height: 3.155em"></span><span class="delimsizinginner delim-size4"><span class="">⎠</span></span></span><span class=""><span class="pstrut" style="height: 3.155em"></span><span class="delimsizinginner delim-size4"><span class="">⎟</span></span></span><span class=""><span class="pstrut" style="height: 3.155em"></span><span class="delimsizinginner delim-size4"><span class="">⎞</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.85003em"><span class=""></span></span></span></span></span></span></span></span></span></span></span></p> 
<p>对于给定的向量<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        v
       
       
        =
       
       
        
         (
        
        
         
          v
         
         
          1
         
        
        
         …
        
        
         
          v
         
         
          m
         
        
        
         )
        
       
      
      
       v=left(v_1 ldots v_mright)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">v</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="minner"><span class="mopen delimcenter">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.03588em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: -0.03588em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose delimcenter">)</span></span></span></span></span></span>, Pytorch 允许你计算<strong>Jacobian Product <span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          v
         
         
          T
         
        
       
       
        v^T
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.841331em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">v</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span></span></span></span></span> ⋅<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         J
        
       
       
        J
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.09618em">J</span></span></span></span></span></strong> 而不是直接计算雅克比矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        J
       
      
      
       J
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.09618em">J</span></span></span></span></span>。这是通过反向调用<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        v
       
      
      
       v
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">v</span></span></span></span></span>作为参数来实现的。<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        v
       
      
      
       v
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">v</span></span></span></span></span>的大小应该和原始张量的大小相同，我们想要针对它计算乘积:</p> 
<pre><code class="prism language-python">inp <span class="token operator">=</span> torch<span class="token punctuation">.</span>eye<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
out <span class="token operator">=</span> <span class="token punctuation">(</span>inp<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"First calln</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>inp<span class="token punctuation">.</span>grad<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"nSecond calln</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>inp<span class="token punctuation">.</span>grad<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
inp<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones_like<span class="token punctuation">(</span>out<span class="token punctuation">)</span><span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"nCall after zeroing gradientsn</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>inp<span class="token punctuation">.</span>grad<span class="token punctuation">}</span></span><span class="token string">"</span></span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">First call
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

Second call
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">8.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">8.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">8.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">8.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

Call after zeroing gradients
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>注意，当我们用相同的参数第二次反向调用时，梯度的值是不同的。这是因为在进行反向传播时，PyTorch会累积梯度，即计算的梯度值被添加到计算图的所有叶节点的grad属性中。如果你想计算合适的梯度，你需要把梯度属性归零。在现实训练中，优化器帮助我们做到这一点。</p> 
<p>以前我们调用的是不带参数的<code>backward()</code>函数。这本质上相当于调用<code>backward(torch.tensor(1.0))</code>，这是在标量值函数的情况下计算梯度的一种有用的方法，比如在神经网络训练期间的损失。</p> 
<h1>
<a id="OPTIMIZING_MODEL_PARAMETERS_1054"></a>七、优化模型参数（OPTIMIZING MODEL PARAMETERS）</h1> 
<p>现在我们有了一个模型和数据，是时候通过优化数据上的参数来训练、验证和测试我们的模型了。训练模型是一个迭代的过程; 在每次迭代(称为<code>epoch</code>)中，模型对输出进行猜测，计算猜测中的误差(<code>loss</code>)，收集误差对其参数的导数(正如我们在上一节中看到的)，并使用梯度下降（<code>gradient descent</code>）优化这些参数。</p> 
<h2>
<a id="Prerequisite_Code_1058"></a>前提代码（Prerequisite Code）</h2> 
<p>我们从前一节的数据集和数据加载器（<strong><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.html">Datasets &amp; DataLoaders</a></strong>）和构建模型加载代码（<strong><a href="https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html">Build Model</a>）</strong>。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader
<span class="token keyword">from</span> torchvision <span class="token keyword">import</span> datasets
<span class="token keyword">from</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">import</span> ToTensor

training_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">"data"</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>

test_data <span class="token operator">=</span> datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>
    root<span class="token operator">=</span><span class="token string">"data"</span><span class="token punctuation">,</span>
    train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
    transform<span class="token operator">=</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>

train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>training_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span>
test_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">)</span>

<span class="token keyword">class</span> <span class="token class-name">NeuralNetwork</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>NeuralNetwork<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>linear_relu_stack <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">28</span><span class="token operator">*</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_relu_stack<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> logits

model <span class="token operator">=</span> NeuralNetwork<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Hyperparameters_1106"></a>超参数（Hyperparameters）</h2> 
<p>超参数是可调整的参数，可以让您控制模型优化过程。不同的超参数值会影响模型的训练和收敛速度</p> 
<p>我们为训练定义了以下超参数:</p> 
<ul>
<li>
<strong>Number of Epochs</strong> : 遍历数据集的次数</li>
<li>
<strong>Batch Size</strong> ：在更新参数之前通过网络传播的数据样本的数量</li>
<li>
<strong>Learning Rate：</strong> 在每个批处理/历中要更新多少模型参数。较小的值产生较慢的学习速度，而较大的值可能导致训练过程中不可预测的行为。</li>
</ul> 
<pre><code class="prism language-python">learning_rate <span class="token operator">=</span> <span class="token number">1e-3</span>
batch_size <span class="token operator">=</span> <span class="token number">64</span>
epochs <span class="token operator">=</span> <span class="token number">5</span>
</code></pre> 
<h2>
<a id="Optimization_Loop_1122"></a>循环优化（Optimization Loop）</h2> 
<p>一旦我们设置了超参数，我们就可以用优化循环（optimization loop）训练和优化我们的模型。优化循环的每次迭代称为一个epoch。</p> 
<p>每个epoch由两个部分组成：</p> 
<ul>
<li>**The Train Loop：**遍历训练数据集并尝试收敛到最优参数。</li>
<li>**The Validation/Test Loop ：**遍历测试数据集以检查模型性能是否正在改进。</li>
</ul> 
<h2>
<a id="Loss_Function_1131"></a>损失函数（Loss Function）</h2> 
<p>当面对一些训练数据时，我们未经训练的网络很可能不会给出正确的答案。损失函数衡量的是得到的结果与目标值的不相似程度，是我们在训练过程中想要最小化的损失函数。为了计算损失，我们使用给定数据样本的输入进行预测，并将其与真实的数据标签值进行比较。</p> 
<p>常见的损失函数包括: <code>nn.MSELoss</code> (Mean Square Error: 均方差) 对于回归任务； <code>nn.NLLLoss</code>(Negative Log Likelihood: 负对数似然) 对于分类任务。<code>nn.CrosspyLoss</code> 结合了<code>nn.LogSoftmax</code> 和 <code>nn.NLLLoss</code></p> 
<p>我们将模型的输出logits传递给<code>nn.CrossEntropyLoss</code>，它将规范化日志并计算预测误差。</p> 
<pre><code class="prism language-python"><span class="token comment"># Initialize the loss function</span>
loss_fn <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="Optimizer_1144"></a>优化器（Optimizer）</h2> 
<p>优化是在每个训练步骤中调整模型参数以减少模型误差的过程。优化算法定义了这个过程是如何执行的(在本例中，我们使用随机梯度下降)。所有优化逻辑都封装在<code>optimizer</code>对象中。这里，我们使用SGD优化器;此外，PyTorch中还有许多不同的优化器，如ADAM和RMSProp，它们可以更好地处理不同类型的模型和数据。</p> 
<p>我们通过注册需要训练的模型参数，并传入学习率超参数来初始化优化器。</p> 
<pre><code class="prism language-python">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
</code></pre> 
<p>在训练循环中，优化分为三个步骤:</p> 
<ul>
<li>调用<code>optimizer.zero_grad()</code>重置模型参数的梯度。默认情况下，梯度是累加的;为了防止重复计算，我们在每次迭代时显式地将它们归零。</li>
<li>通过调用<code>loss.backward()</code>反向传播预测损失。PyTorch存储每个参数的损耗梯度。</li>
<li>有了梯度之后，我们调用<code>optimizer.step()</code>来根据在向后传递中收集的梯度调整参数。</li>
</ul> 
<h2>
<a id="Full_Implementation_1160"></a>完整实现（Full Implementation）</h2> 
<p>我们定义遍历优化代码的<code>train_loop</code>和根据测试数据评估模型性能的<code>test_loop</code>。</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train_loop</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span><span class="token punctuation">:</span>
    size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>
    <span class="token keyword">for</span> batch<span class="token punctuation">,</span> <span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># Compute prediction and loss</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

        <span class="token comment"># Backpropagation</span>
        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> batch <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            loss<span class="token punctuation">,</span> current <span class="token operator">=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batch <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>loss<span class="token punctuation">:</span><span class="token format-spec">&gt;7f</span><span class="token punctuation">}</span></span><span class="token string">  [</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>current<span class="token punctuation">:</span><span class="token format-spec">&gt;5d</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>size<span class="token punctuation">:</span><span class="token format-spec">&gt;5d</span><span class="token punctuation">}</span></span><span class="token string">]"</span></span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">test_loop</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">)</span><span class="token punctuation">:</span>
    size <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span>
    num_batches <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span>
    test_loss<span class="token punctuation">,</span> correct <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span>

    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> dataloader<span class="token punctuation">:</span>
            pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            test_loss <span class="token operator">+=</span> loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>
            correct <span class="token operator">+=</span> <span class="token punctuation">(</span>pred<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>

    test_loss <span class="token operator">/=</span> num_batches
    correct <span class="token operator">/=</span> size
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Test Error: n Accuracy: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token punctuation">(</span><span class="token number">100</span><span class="token operator">*</span>correct<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token format-spec">&gt;0.1f</span><span class="token punctuation">}</span></span><span class="token string">%, Avg loss: </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>test_loss<span class="token punctuation">:</span><span class="token format-spec">&gt;8f</span><span class="token punctuation">}</span></span><span class="token string"> n"</span></span><span class="token punctuation">)</span>
</code></pre> 
<p>我们初始化损失函数和优化器，并将其传递给<code>train_loop</code>和<code>test_loop</code>。您可以随意增加epoch的数量，以跟踪模型不断改进的性能。</p> 
<pre><code class="prism language-python">loss_fn <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>

epochs <span class="token operator">=</span> <span class="token number">10</span>
<span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f"Epoch </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>t<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">n-------------------------------"</span></span><span class="token punctuation">)</span>
    train_loop<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">,</span> optimizer<span class="token punctuation">)</span>
    test_loop<span class="token punctuation">(</span>test_dataloader<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_fn<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Done!"</span><span class="token punctuation">)</span>
</code></pre> 
<h1>
<a id="_1211"></a>八、保存和加载模型</h1> 
<p>在本节中，我们将看看如何通过保存、加载和运行模型预测来持久保存模型状态。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> torch
<span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models
</code></pre> 
<h2>
<a id="Saving_and_Loading_Model_Weights_1220"></a>加载和保存模型的权重（Saving and Loading Model Weights）</h2> 
<p>PyTorch模型将学习到的参数存储在一个名为<code>state_dict</code>的内部状态字典中。这些可以通过方法保存:</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'model_weights.pth'</span><span class="token punctuation">)</span>
</code></pre> 
<p>要加载模型权重，您需要首先创建同一个模型的实例，然后使用<code>load_state_dict()</code>方法加载参数。</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># we do not specify pretrained=True, i.e. do not load default weights</span>
model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model_weights.pth'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>确保在推断之前调用<code>model.eval()</code>方法，将退出层和批处理归一化层设置为求值模式。如果不这样做，将产生不一致的推断结果.</p> 
<h2>
<a id="Saving_and_Loading_Models_with_Shapes_1239"></a>通过形状来保存和加载模型（Saving and Loading Models with Shapes）</h2> 
<p>当加载模型权重时，我们需要首先实例化模型类，因为类定义了网络的结构。我们可能想要将这个类的结构和模型一起保存，在这种情况下，我们可以将<code>model</code>(而不是<code>model.state_dict()</code>)传递给保存函数:</p> 
<pre><code class="prism language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> <span class="token string">'model.pth'</span><span class="token punctuation">)</span>
</code></pre> 
<p>然后我们可以像这样加载模型:</p> 
<pre><code class="prism language-python">model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">'model.pth'</span><span class="token punctuation">)</span>
</code></pre>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>