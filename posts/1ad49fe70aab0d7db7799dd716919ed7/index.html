<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【手把手带你学习神经机器翻译--模型篇】 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【手把手带你学习神经机器翻译--模型篇】</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#1_4">1、深度神经网络</a></li>
<li>
<ul>
<li><a href="#11_5">1.1、全连接神经网络</a></li>
<li><a href="#12_14">1.2、循环神经网络</a></li>
<li>
<ul><li><a href="#121RNN_27">1.2.1、RNN模型代码</a></li></ul>
   </li>
<li><a href="#13_312">1.3、卷积神经网络</a></li>
<li><a href="#14_323">1.4、基于循环神经网络的深度神经序列模型</a></li>
<li>
<ul><li><a href="#141_Lstm_332">1.4.1 Lstm模型实现代码</a></li></ul>
   </li>
<li><a href="#15_1092">1.5、基于卷积神经网络的深度神经网络模型</a></li>
<li>
<ul><li><a href="#151CNNfconv_1098">1.5.1、CNN（fconv）模型代码</a></li></ul>
  </li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<hr> 
<h1>
<a id="1_4"></a>1、深度神经网络</h1> 
<h2>
<a id="11_5"></a>1.1、全连接神经网络</h2> 
<p>全连接神经网络(Fully­Connected Neural Network,FCNN)，是深度神经网络中最基本的一种结构，如图所示。按照神经元所处的位置划分，全连接网络由输入层，隐藏层和输出层组成，通常第一层为输入层，最后一层为输出层，中间部分全为隐藏层。顾名思义，全连接神经网络中每一个神经元都与下一层的神经元全部连接，因而每个层又称为全连接层。网络中同层次的神经元之间无连接，层次间的神经元关系由下述公式得出：<br> <img src="https://images2.imgbox.com/ac/88/eL6uS2pP_o.png" alt="在这里插入图片描述"><br> 其中，当l=1时，h0t=xt即为输入。显然，公式2­1的计算过程是线性的，若每个神经元由该线性计算得出，无论网络的层数有多少，模型解决问题的能力仅限于线性划分问题。为了使神经网络可以处理任意非线性问题，引入了非线性激活函数(Non­linear Activeation Function，下简称为激活函数)，用以增强神经网络的泛化性。常用的激活函数有sigmoid,tanh和ReLU。<br> <img src="https://images2.imgbox.com/3b/f5/effWfZiB_o.png" alt="在这里插入图片描述"><br> 引入激活函数后层次间神经元关系计算为：<br> <img src="https://images2.imgbox.com/36/1d/0LAMZnvx_o.png" alt="在这里插入图片描述"><br> 其中，σ代表激活函数。<br> 全连接网络通过对每一个神经元进行计算，得出输出层神经元的结果。近年来，随着各种特异的网络结构的 出，纯粹由全连接层组成的全连接网络并不多见，通常全连接层和其他结构搭配出现，作为一个线性或非线性的映射层。</p> 
<h2>
<a id="12_14"></a>1.2、循环神经网络</h2> 
<p>在实际场景中，许多问题的建模都是和时间序列信息有关系的。尤其是在自然语言处理的许多任务中，数据间的上下文的依赖性很高。考虑到对这类问题进行建模，Elman出循环神经网络(Recurrent Neural Network,RNN)，其结构如图所示。显然，循环神经网络的输入和输出之间有一个循环过程，将循环过程展开后可以发现，循环神经网络的隐藏层节点关系建立在两个输入之上，一是当前时刻的标准输入，另一个是上一个节点的隐藏层信息，体现为：<br> <img src="https://images2.imgbox.com/eb/21/O4bRRCZ9_o.png" alt="在这里插入图片描述"><br> 其中，o代表循环神经单元的输出值，h代表循环神经单元的隐藏层值。显然，和全连接网络不同的是，循环神经元有两个输出。此外，循环神经网络中的参数是所有输入共享的，也就是说，同一层输入所使用的参数是相同的。这样的参数共享不仅能够使得模型的参数量大大减少，同时还能增强模型的泛化能力，尤其是在自然语言处理的相关问题上，当模型接收到超过训练样本长度的输入时，模型仍然能够 取到输入的特征，但这样的参数共享同时也为模型误差传播带来了一定的障碍。目前，对RNN的训练采用的是时序方向传播方法(Back­propagationThrough Time,BPTT)，从图2­2右边的展开式中可以看出，展开后的RNN在时序上的深度取决于序列的长度，而BPTT算法的求导链和这个长度息息相关。因此当序列变长时，BPTT面临两个问题：梯度消失和梯度爆炸。梯度消失是指BPTT反馈到一定长度之后，出现梯度趋近于零无法学习的问题，梯度爆炸则正好相反，它表示梯度呈现很大的值导致长程神经元学习无用的情况。无论BPTT出现哪一种问题，都会使得序列中的上下文关系无法被体现，背离了RNN结构建模序列关系的初衷。目前解决这两个问题的主流方案是在RNN的神经元上增加门控机制来控制数据流向，保证有用数据的传递，其中最著名的是长短期记忆单元(longshort­Term Memory,LSTM)和门控循环单元(Gated Recurrent Units,GRU)。<br> <img src="https://images2.imgbox.com/3b/0b/k7JOEiBg_o.png" alt="在这里插入图片描述"><br> 长短期记忆单元LSTM在普通的RNN神经元内增加了三个门控单元来控制数据流向，可以形象的将其称为输入门、遗忘门和输出门，三个门分别对应了三个并列的全连接层。最后，输出由三个门的结果和前一个神经元的状态融合产生，具体公式如下：<br> <img src="https://images2.imgbox.com/75/a3/ux3OLLRo_o.png" alt="在这里插入图片描述"><br> 其中，i,f,o分别对应输入门、遗忘门和输出门，c表示神经元的细胞态，h为当前神经元隐藏状态值，也是当前神经元的输出，它融合了三个门的结果和前一个神经元的状态信息。W{i,f,o,c},U{i,f,o,c},b{i,f,o,c}对应不同门的可学习参数，同时，和普通的RNN一样，这些参数也是所有神经元共享的。</p> 
<p><img src="https://images2.imgbox.com/66/f5/DakirVYc_o.png" alt="在这里插入图片描述"><br> 因此，当序列通过采用LSTM单元作为基础单元的RNN时，序列信息流的传播会由门控单元来进行控制，保证有用信息的传递，在一定程度上减少模型学习无用的问题。但LSTM在神经元内部进行的一系列全连接计算也导致了模型效率的降低。Cho等人在基于LSTM的基础上，对门控单元进行简化后 出了一种变体结构：门控循环单元GRU。在GRU中，门控单元被减少到了两个：重置门®和更新门(z)，其结构由图给出，计算如下：<br> <img src="https://images2.imgbox.com/1d/db/099wwOjY_o.png" alt="在这里插入图片描述"><br> 不难发现，GRU直接使用重置门对前一时刻神经元隐藏状态进行刷新,随后融合该状态和当前输入值得到一个新的细胞态，而更新门则根据该细胞态和前一时刻神经元的隐藏状态来控制数据的传递。由于GRU显式的减少了一个门控单元，因此GRU所需的参数量较LSTM更少，在序列任务上的计算效率也比LSTM更高，性能鲜有损失。</p> 
<h3>
<a id="121RNN_27"></a>1.2.1、RNN模型代码</h3> 
<pre><code class="prism language-python"><span class="token keyword">from</span> __future__ <span class="token keyword">import</span> absolute_import
<span class="token keyword">from</span> recurrentshop <span class="token keyword">import</span> LSTMCell<span class="token punctuation">,</span> RecurrentSequential
<span class="token keyword">from</span> <span class="token punctuation">.</span>cells <span class="token keyword">import</span> LSTMDecoderCell<span class="token punctuation">,</span> AttentionDecoderCell
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>models <span class="token keyword">import</span> Sequential<span class="token punctuation">,</span> Model
<span class="token keyword">from</span> keras<span class="token punctuation">.</span>layers <span class="token keyword">import</span> Dense<span class="token punctuation">,</span> Dropout<span class="token punctuation">,</span> TimeDistributed<span class="token punctuation">,</span> Bidirectional<span class="token punctuation">,</span> Input
<span class="token keyword">def</span> <span class="token function">SimpleSeq2Seq</span><span class="token punctuation">(</span>output_dim<span class="token punctuation">,</span> output_length<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                  batch_size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> input_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                  input_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> depth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> unroll<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                  stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token triple-quoted-string string">'''
    Simple model for sequence to sequence learning.
    The encoder encodes the input sequence to vector (called context vector)
    The decoder decodes the context vector in to a sequence of vectors.
    There is no one on one relation between the input and output sequence
    elements. The input sequence and output sequence may differ in length.

    Arguments:

    output_dim : Required output dimension.
    hidden_dim : The dimension of the internal representations of the model.
    output_length : Length of the required output sequence.
    depth : Used to create a deep Seq2seq model. For example, if depth = 3,
            there will be 3 LSTMs on the enoding side and 3 LSTMs on the
            decoding side. You can also specify depth as a tuple. For example,
            if depth = (4, 5), 4 LSTMs will be added to the encoding side and
            5 LSTMs will be added to the decoding side.
    dropout : Dropout probability in between layers.

    '''</span>

    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>depth<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        depth <span class="token operator">=</span> <span class="token punctuation">(</span>depth<span class="token punctuation">,</span> depth<span class="token punctuation">)</span>
    <span class="token keyword">if</span> batch_input_shape<span class="token punctuation">:</span>
        shape <span class="token operator">=</span> batch_input_shape
    <span class="token keyword">elif</span> input_shape<span class="token punctuation">:</span>
        shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> input_shape
    <span class="token keyword">elif</span> input_dim<span class="token punctuation">:</span>
        <span class="token keyword">if</span> input_length<span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_length<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># TODO Proper error message</span>
        <span class="token keyword">raise</span> TypeError
    <span class="token keyword">if</span> hidden_dim <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        hidden_dim <span class="token operator">=</span> output_dim
    encoder <span class="token operator">=</span> RecurrentSequential<span class="token punctuation">(</span>unroll<span class="token operator">=</span>unroll<span class="token punctuation">,</span> stateful<span class="token operator">=</span>stateful<span class="token punctuation">)</span>
    encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> depth<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
        encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>

    decoder <span class="token operator">=</span> RecurrentSequential<span class="token punctuation">(</span>unroll<span class="token operator">=</span>unroll<span class="token punctuation">,</span> stateful<span class="token operator">=</span>stateful<span class="token punctuation">,</span>
                                  decode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> output_length<span class="token operator">=</span>output_length<span class="token punctuation">)</span>
    decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> depth<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>depth<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
            decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
    decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>

    _input <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span>shape<span class="token punctuation">)</span>
    x <span class="token operator">=</span> encoder<span class="token punctuation">(</span>_input<span class="token punctuation">)</span>
    output <span class="token operator">=</span> decoder<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">return</span> Model<span class="token punctuation">(</span>_input<span class="token punctuation">,</span> output<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">Seq2Seq</span><span class="token punctuation">(</span>output_dim<span class="token punctuation">,</span> output_length<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            input_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> input_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
            hidden_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> depth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> broadcast_state<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> unroll<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
            stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> inner_broadcast_state<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> teacher_force<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
            peek<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.</span><span class="token punctuation">)</span><span class="token punctuation">:</span>

    <span class="token triple-quoted-string string">'''
    Seq2seq model based on [1] and [2].
    This model has the ability to transfer the encoder hidden state to the decoder's
    hidden state(specified by the broadcast_state argument). Also, in deep models
    (depth &gt; 1), the hidden state is propogated throughout the LSTM stack(specified by
    the inner_broadcast_state argument. You can switch between [1] based model and [2]
    based model using the peek argument.(peek = True for [2], peek = False for [1]).
    When peek = True, the decoder gets a 'peek' at the context vector at every timestep.

    [1] based model:

            Encoder:
            X = Input sequence
            C = LSTM(X); The context vector

            Decoder:
    y(t) = LSTM(s(t-1), y(t-1)); Where s is the hidden state of the LSTM (h and c)
    y(0) = LSTM(s0, C); C is the context vector from the encoder.

    [2] based model:

            Encoder:
            X = Input sequence
            C = LSTM(X); The context vector

            Decoder:
    y(t) = LSTM(s(t-1), y(t-1), C)
    y(0) = LSTM(s0, C, C)
    Where s is the hidden state of the LSTM (h and c), and C is the context vector
    from the encoder.

    Arguments:

    output_dim : Required output dimension.
    hidden_dim : The dimension of the internal representations of the model.
    output_length : Length of the required output sequence.
    depth : Used to create a deep Seq2seq model. For example, if depth = 3,
                    there will be 3 LSTMs on the enoding side and 3 LSTMs on the
                    decoding side. You can also specify depth as a tuple. For example,
                    if depth = (4, 5), 4 LSTMs will be added to the encoding side and
                    5 LSTMs will be added to the decoding side.
    broadcast_state : Specifies whether the hidden state from encoder should be
                                      transfered to the deocder.
    inner_broadcast_state : Specifies whether hidden states should be propogated
                                                    throughout the LSTM stack in deep models.
    peek : Specifies if the decoder should be able to peek at the context vector
               at every timestep.
    dropout : Dropout probability in between layers.


    '''</span>

    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>depth<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        depth <span class="token operator">=</span> <span class="token punctuation">(</span>depth<span class="token punctuation">,</span> depth<span class="token punctuation">)</span>
    <span class="token keyword">if</span> batch_input_shape<span class="token punctuation">:</span>
        shape <span class="token operator">=</span> batch_input_shape
    <span class="token keyword">elif</span> input_shape<span class="token punctuation">:</span>
        shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> input_shape
    <span class="token keyword">elif</span> input_dim<span class="token punctuation">:</span>
        <span class="token keyword">if</span> input_length<span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_length<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># TODO Proper error message</span>
        <span class="token keyword">raise</span> TypeError
    <span class="token keyword">if</span> hidden_dim <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        hidden_dim <span class="token operator">=</span> output_dim

    encoder <span class="token operator">=</span> RecurrentSequential<span class="token punctuation">(</span>readout<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> state_sync<span class="token operator">=</span>inner_broadcast_state<span class="token punctuation">,</span>
                                  unroll<span class="token operator">=</span>unroll<span class="token punctuation">,</span> stateful<span class="token operator">=</span>stateful<span class="token punctuation">,</span>
                                  return_states<span class="token operator">=</span>broadcast_state<span class="token punctuation">)</span>
    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>depth<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>

    dense1 <span class="token operator">=</span> TimeDistributed<span class="token punctuation">(</span>Dense<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    dense1<span class="token punctuation">.</span>supports_masking <span class="token operator">=</span> <span class="token boolean">True</span>
    dense2 <span class="token operator">=</span> Dense<span class="token punctuation">(</span>output_dim<span class="token punctuation">)</span>

    decoder <span class="token operator">=</span> RecurrentSequential<span class="token punctuation">(</span>readout<span class="token operator">=</span><span class="token string">'add'</span> <span class="token keyword">if</span> peek <span class="token keyword">else</span> <span class="token string">'readout_only'</span><span class="token punctuation">,</span>
                                  state_sync<span class="token operator">=</span>inner_broadcast_state<span class="token punctuation">,</span> decode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
                                  output_length<span class="token operator">=</span>output_length<span class="token punctuation">,</span> unroll<span class="token operator">=</span>unroll<span class="token punctuation">,</span>
                                  stateful<span class="token operator">=</span>stateful<span class="token punctuation">,</span> teacher_force<span class="token operator">=</span>teacher_force<span class="token punctuation">)</span>

    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>depth<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMDecoderCell<span class="token punctuation">(</span>output_dim<span class="token operator">=</span>output_dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span>hidden_dim<span class="token punctuation">,</span>
                                    batch_input_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    _input <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span>shape<span class="token punctuation">)</span>
    _input<span class="token punctuation">.</span>_keras_history<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>supports_masking <span class="token operator">=</span> <span class="token boolean">True</span>
    encoded_seq <span class="token operator">=</span> dense1<span class="token punctuation">(</span>_input<span class="token punctuation">)</span>
    encoded_seq <span class="token operator">=</span> encoder<span class="token punctuation">(</span>encoded_seq<span class="token punctuation">)</span>
    <span class="token keyword">if</span> broadcast_state<span class="token punctuation">:</span>
        <span class="token keyword">assert</span> <span class="token builtin">type</span><span class="token punctuation">(</span>encoded_seq<span class="token punctuation">)</span> <span class="token keyword">is</span> <span class="token builtin">list</span>
        states <span class="token operator">=</span> encoded_seq<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        encoded_seq <span class="token operator">=</span> encoded_seq<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        states <span class="token operator">=</span> <span class="token boolean">None</span>
    encoded_seq <span class="token operator">=</span> dense2<span class="token punctuation">(</span>encoded_seq<span class="token punctuation">)</span>
    inputs <span class="token operator">=</span> <span class="token punctuation">[</span>_input<span class="token punctuation">]</span>
    <span class="token keyword">if</span> teacher_force<span class="token punctuation">:</span>
        truth_tensor <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> output_length<span class="token punctuation">,</span> output_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        truth_tensor<span class="token punctuation">.</span>_keras_history<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>supports_masking <span class="token operator">=</span> <span class="token boolean">True</span>
        inputs <span class="token operator">+=</span> <span class="token punctuation">[</span>truth_tensor<span class="token punctuation">]</span>


    decoded_seq <span class="token operator">=</span> decoder<span class="token punctuation">(</span>encoded_seq<span class="token punctuation">,</span>
                          ground_truth<span class="token operator">=</span>inputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">if</span> teacher_force <span class="token keyword">else</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
                          initial_readout<span class="token operator">=</span>encoded_seq<span class="token punctuation">,</span> initial_state<span class="token operator">=</span>states<span class="token punctuation">)</span>
    
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> decoded_seq<span class="token punctuation">)</span>
    model<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder
    model<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder
    <span class="token keyword">return</span> model


<span class="token keyword">def</span> <span class="token function">AttentionSeq2Seq</span><span class="token punctuation">(</span>output_dim<span class="token punctuation">,</span> output_length<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                     batch_size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> input_shape<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> input_length<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
                     input_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> depth<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
                     bidirectional<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> unroll<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> stateful<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    This is an attention Seq2seq model based on [3].
    Here, there is a soft allignment between the input and output sequence elements.
    A bidirection encoder is used by default. There is no hidden state transfer in this
    model.

    The  math:

            Encoder:
            X = Input Sequence of length m.
            H = Bidirection_LSTM(X); Note that here the LSTM has return_sequences = True,
            so H is a sequence of vectors of length m.

            Decoder:
    y(i) = LSTM(s(i-1), y(i-1), v(i)); Where s is the hidden state of the LSTM (h and c)
    and v (called the context vector) is a weighted sum over H:

    v(i) =  sigma(j = 0 to m-1)  alpha(i, j) * H(j)

    The weight alpha[i, j] for each hj is computed as follows:
    energy = a(s(i-1), H(j))
    alpha = softmax(energy)
    Where a is a feed forward network.

    '''</span>

    <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>depth<span class="token punctuation">,</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        depth <span class="token operator">=</span> <span class="token punctuation">(</span>depth<span class="token punctuation">,</span> depth<span class="token punctuation">)</span>
    <span class="token keyword">if</span> batch_input_shape<span class="token punctuation">:</span>
        shape <span class="token operator">=</span> batch_input_shape
    <span class="token keyword">elif</span> input_shape<span class="token punctuation">:</span>
        shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> input_shape
    <span class="token keyword">elif</span> input_dim<span class="token punctuation">:</span>
        <span class="token keyword">if</span> input_length<span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_length<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            shape <span class="token operator">=</span> <span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token punctuation">(</span>input_dim<span class="token punctuation">,</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        <span class="token comment"># TODO Proper error message</span>
        <span class="token keyword">raise</span> TypeError
    <span class="token keyword">if</span> hidden_dim <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        hidden_dim <span class="token operator">=</span> output_dim

    _input <span class="token operator">=</span> Input<span class="token punctuation">(</span>batch_shape<span class="token operator">=</span>shape<span class="token punctuation">)</span>
    _input<span class="token punctuation">.</span>_keras_history<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>supports_masking <span class="token operator">=</span> <span class="token boolean">True</span>

    encoder <span class="token operator">=</span> RecurrentSequential<span class="token punctuation">(</span>unroll<span class="token operator">=</span>unroll<span class="token punctuation">,</span> stateful<span class="token operator">=</span>stateful<span class="token punctuation">,</span>
                                  return_sequences<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> depth<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
        encoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMCell<span class="token punctuation">(</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">if</span> bidirectional<span class="token punctuation">:</span>
        encoder <span class="token operator">=</span> Bidirectional<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> merge_mode<span class="token operator">=</span><span class="token string">'sum'</span><span class="token punctuation">)</span>
        encoder<span class="token punctuation">.</span>forward_layer<span class="token punctuation">.</span>build<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
        encoder<span class="token punctuation">.</span>backward_layer<span class="token punctuation">.</span>build<span class="token punctuation">(</span>shape<span class="token punctuation">)</span>
        <span class="token comment"># patch</span>
        encoder<span class="token punctuation">.</span>layer <span class="token operator">=</span> encoder<span class="token punctuation">.</span>forward_layer

    encoded <span class="token operator">=</span> encoder<span class="token punctuation">(</span>_input<span class="token punctuation">)</span>
    decoder <span class="token operator">=</span> RecurrentSequential<span class="token punctuation">(</span>decode<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> output_length<span class="token operator">=</span>output_length<span class="token punctuation">,</span>
                                  unroll<span class="token operator">=</span>unroll<span class="token punctuation">,</span> stateful<span class="token operator">=</span>stateful<span class="token punctuation">)</span>
    decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">,</span> batch_input_shape<span class="token operator">=</span><span class="token punctuation">(</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> depth<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>AttentionDecoderCell<span class="token punctuation">(</span>output_dim<span class="token operator">=</span>output_dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>AttentionDecoderCell<span class="token punctuation">(</span>output_dim<span class="token operator">=</span>output_dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>depth<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
            decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMDecoderCell<span class="token punctuation">(</span>output_dim<span class="token operator">=</span>hidden_dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span>
        decoder<span class="token punctuation">.</span>add<span class="token punctuation">(</span>LSTMDecoderCell<span class="token punctuation">(</span>output_dim<span class="token operator">=</span>output_dim<span class="token punctuation">,</span> hidden_dim<span class="token operator">=</span>hidden_dim<span class="token punctuation">)</span><span class="token punctuation">)</span>
    
    inputs <span class="token operator">=</span> <span class="token punctuation">[</span>_input<span class="token punctuation">]</span>
    decoded <span class="token operator">=</span> decoder<span class="token punctuation">(</span>encoded<span class="token punctuation">)</span>
    model <span class="token operator">=</span> Model<span class="token punctuation">(</span>inputs<span class="token punctuation">,</span> decoded<span class="token punctuation">)</span>
    <span class="token keyword">return</span> model

</code></pre> 
<h2>
<a id="13_312"></a>1.3、卷积神经网络</h2> 
<p>和循环神经网络 出的原因一样，卷积神经网络(Convolutional Neural Network,CNN) 出也是为了针对性的处理问题。与循环神经网络的 出是针对序列关系建模不同的是，卷积神经网络最初的 出是为了高效处理非序列数据问题，例如取图像中的特征问题。具体地，图像由于其具有空间三维性且包含的数据量过大，使用全连接网络会导致网络庞大且效率低下，为了 高神经网络对图像等类似数据的高效处理，基于局部特征取的卷积神经网络被出。它通过模拟生物捕捉图像特征的方法，对输入进行局部特征的 取，再依靠深度网络对高层特征的捕捉能力实现对图像的特征取。后来卷积神经网络被证明也可应用于自然语言处理任务，并且效率更高。一个完整的卷积神经网络包含三个部分，卷积层、池化层、全连接层。<br> 卷积层是卷积神经网络的核心计算层，用于取输入的特征。一个卷积层由多个卷积核(Kernel)组成，每个卷积核负责取输入的一部分局部特征，其深度和输入相同，但高度和宽度都远小于输入。一次卷积计算，卷积核仅能和在其视野内，也就是在其高度和宽度范围内的输入数据交互，这个视野也被称之为感受野，是由人为设定的。通过滑动，卷积核的感受野不断变换，从而完成和输入的所有数据交互。感受野滑动的幅度称之为步长，是一个人为设定的经验值，一般设为1。卷积的计算过程用公式表示如下：<br> <img src="https://images2.imgbox.com/33/5e/i7Ibgdkh_o.png" alt="在这里插入图片描述"><br> 其中，h[m,n]代表尺寸为m×n的输入经过卷积后得到的隐藏层输出值，卷积核集合为K×L,一次卷积从中取出一个k×l的卷积核进行操作，I为输入数据，可以是输入源数据，也可以是前一隐藏层的输出数据。σ代表激活函数。图演示了一个卷积核尺寸为3×3的一次卷积计算示例。<br> <img src="https://images2.imgbox.com/7e/32/2FhiI6wm_o.png" alt="在这里插入图片描述"></p> 
<p>显然，若想获得输入的更多特征，则需要设定多种不同的卷积核去捕捉输入，这样仍然会使得模型庞大。为了减少网络的参数量，卷积神经网络引入下采样操作周期性的在卷积层之后进行操作，被称为池化层。池化层的计算方法一般有两种，最大池化(Max Pooling)和平均池化(Average Pooling)，顾名思义，最大池化即是在池化区域中取最大值作为输出，而平均池化是取池化区域中的平均值作为输出。同卷积层操作一样，池化层也需要人工设定工作区域和移动步长，图2演示了一个池化区域为2×2，在步长为2时的最大池化/平均池化示例。<br> 卷积神经网络通过堆叠卷积层和池化层获取到输入的特征，再将该特征拉伸为向量后通过一个全连接层后输出整个网络的结果。<br> <img src="https://images2.imgbox.com/c7/14/eALpN6br_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="14_323"></a>1.4、基于循环神经网络的深度神经序列模型</h2> 
<p>对深度神经序列模型的追溯，最早可到2013年，由Kalchbrenner等人出采用编码­解码的思想解决自然语言生成问题，它为后续序列模型结构奠定了基础，该思想为：将给定输入文本通过一个编码器网络重构成一个新的表示，再采用一个解码器网络解构这个表示并根据解构后的信息生成目标文本。具体地，Kalchbrenner等人在文中采用卷积神经网络作为编码器对输入进行重构，采用循环神经网络作为解码器进行生成。该模型当时虽并未取得非常理想的结果，但却引发了领域内众人的热烈讨论。考虑到模型采用卷积神经网络作为编码器并不能完全切合语言的特点，同时解码器采用的循环神经单元未做门控处理会有梯度消失/爆炸的问题，Sutckever等人将LSTM单元引入该模型替换了原模型中的基本循环神经单元，同时将编码器中的卷积神经网络替换为更贴合语言特点的循环神经网络，该模型如图所示，其在机器翻译任务上取得的成功验证了深度神经网络模型在自动机器翻译任务上的可行性。几乎同时，Cho等人对此种序列到序列的模型归纳为编码­解码模型，并通过对上下文表示的强调进一步升该模型的性能，如图所示。<br> <img src="https://images2.imgbox.com/ad/fa/J6RpPHMR_o.png" alt="在这里插入图片描述"><br> 自此，序列到序列模型在自然语言处理生成问题上奠定了基础，任何可建模为“序列映射到序列”的问题都可采用该框架，并且采用LSTM或GRU的循环神经网络也成为自然语言处理任务的标准结构。计算过程一般分为3步，首先编码器对输入文本X=(x1,x2,…,xm)进行重构，其中xi表示输入文本中第i个最小组成部分，也就是一个标记。对于中文来说，如果是基于中文词语的模型，一个标记就代表一个词语，如果是基于中文分字的模型，一个标记就代表一个字符；对于英文或其它类似西文来说，一个标记代表一个单词。编码器重构的方式为：<br> <img src="https://images2.imgbox.com/3c/56/FdjlWtsw_o.png" alt="在这里插入图片描述"><br> 其中fRNN代表采用了LSTM或GRU的循环神经单元，h为循环神经单元的隐藏值，l代表层数，此处定义i代表当前编码时序，显然i−1是前一个时序。根据循环神经网络的特点，一般采用最后一层最后一个时序计算后的输出作为重构后的向量表示，定义为c，该向量由于融合了前序序列经门控后的特征，被认为是输入文本的高层语义表示。接下来解码器使用c作为上下文语义表示进行解码计算，定义为：<br> <img src="https://images2.imgbox.com/97/b7/UR6GzB0c_o.png" alt="在这里插入图片描述"><br> 其中，gRNN代表采用了LSTM或GRU的循环神经单元，s为循环神经单元的隐藏值，t代表当前解码时序，o为循环神经单元的输出值，y为当前时刻解码器的输入，同时它也是前一时序模型的预测输出，这种模式被称为自回归解码模式，即每一时刻解码器的预测值都是基于前一时刻解码器的预测值计算的。模型的输出部分采用一个线性层对解码器的输出进行线性转换并映射到词表上进行选词：<br> <img src="https://images2.imgbox.com/7d/46/lEWgirgP_o.png" alt="在这里插入图片描述">值得一 的是，目前在文本生成技术中广泛采用teacher­forcing学习算法，该算法在模型训练阶段采用右移标签作为解码器输入，即丢弃训练阶段模型的预测值，转而使用t−1时刻的标签值作为解码器的输入以帮助模型尽快学习。到模型预测阶段时，才真正使用前一时刻的预测值来预测输出。由于这个学习算法能加快模型拟合，因此该方法也是目前自然语言处理任务的标准有监督学习方法。</p> 
<h3>
<a id="141_Lstm_332"></a>1.4.1 Lstm模型实现代码</h3> 
<pre><code class="prism language-python"><span class="token comment"># Copyright (c) Facebook, Inc. and its affiliates.</span>
<span class="token comment">#</span>
<span class="token comment"># This source code is licensed under the MIT license found in the</span>
<span class="token comment"># LICENSE file in the root directory of this source tree.</span>

<span class="token keyword">from</span> typing <span class="token keyword">import</span> Dict<span class="token punctuation">,</span> List<span class="token punctuation">,</span> Optional<span class="token punctuation">,</span> Tuple

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> fairseq <span class="token keyword">import</span> utils
<span class="token keyword">from</span> fairseq<span class="token punctuation">.</span>models <span class="token keyword">import</span> <span class="token punctuation">(</span>
    FairseqEncoder<span class="token punctuation">,</span>
    FairseqEncoderDecoderModel<span class="token punctuation">,</span>
    FairseqIncrementalDecoder<span class="token punctuation">,</span>
    register_model<span class="token punctuation">,</span>
    register_model_architecture<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> fairseq<span class="token punctuation">.</span>modules <span class="token keyword">import</span> AdaptiveSoftmax<span class="token punctuation">,</span> FairseqDropout
<span class="token keyword">from</span> torch <span class="token keyword">import</span> Tensor


DEFAULT_MAX_SOURCE_POSITIONS <span class="token operator">=</span> <span class="token number">1e5</span>
DEFAULT_MAX_TARGET_POSITIONS <span class="token operator">=</span> <span class="token number">1e5</span>


<span class="token decorator annotation punctuation">@register_model</span><span class="token punctuation">(</span><span class="token string">"lstm"</span><span class="token punctuation">)</span>
<span class="token keyword">class</span> <span class="token class-name">LSTMModel</span><span class="token punctuation">(</span>FairseqEncoderDecoderModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">add_args</span><span class="token punctuation">(</span>parser<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Add model-specific arguments to the parser."""</span>
        <span class="token comment"># fmt: off</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--dropout'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'D'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'dropout probability'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-embed-dim'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'encoder embedding dimension'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-embed-path'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'STR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'path to pre-trained encoder embedding'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-freeze-embed'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'freeze encoder embeddings'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-hidden-size'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'encoder hidden size'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-layers'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'number of encoder layers'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-bidirectional'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'make all layers of encoder bidirectional'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-embed-dim'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder embedding dimension'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-embed-path'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'STR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'path to pre-trained decoder embedding'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-freeze-embed'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'freeze decoder embeddings'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-hidden-size'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder hidden size'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-layers'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'number of decoder layers'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-out-embed-dim'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder output embedding dimension'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-attention'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'BOOL'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder attention'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--adaptive-softmax-cutoff'</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'EXPR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'comma separated list of adaptive softmax cutoff points. '</span>
                                 <span class="token string">'Must be used with adaptive_loss criterion'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--share-decoder-input-output-embed'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
                            action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'share decoder input and output embeddings'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--share-all-embeddings'</span><span class="token punctuation">,</span> default<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'share encoder, decoder and output embeddings'</span>
                                 <span class="token string">' (requires shared dictionary and embed dim)'</span><span class="token punctuation">)</span>

        <span class="token comment"># Granular dropout settings (if not specified these default to --dropout)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-dropout-in'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'D'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'dropout probability for encoder input embedding'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-dropout-out'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'D'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'dropout probability for encoder output'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-dropout-in'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'D'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'dropout probability for decoder input embedding'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-dropout-out'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'D'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'dropout probability for decoder output'</span><span class="token punctuation">)</span>
        <span class="token comment"># fmt: on</span>

    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">build_model</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> args<span class="token punctuation">,</span> task<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Build a new model instance."""</span>
        <span class="token comment"># make sure that all args are properly defaulted (in case there are any new ones)</span>
        base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>

        <span class="token keyword">if</span> args<span class="token punctuation">.</span>encoder_layers <span class="token operator">!=</span> args<span class="token punctuation">.</span>decoder_layers<span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"--encoder-layers must match --decoder-layers"</span><span class="token punctuation">)</span>

        max_source_positions <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>
            args<span class="token punctuation">,</span> <span class="token string">"max_source_positions"</span><span class="token punctuation">,</span> DEFAULT_MAX_SOURCE_POSITIONS
        <span class="token punctuation">)</span>
        max_target_positions <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>
            args<span class="token punctuation">,</span> <span class="token string">"max_target_positions"</span><span class="token punctuation">,</span> DEFAULT_MAX_TARGET_POSITIONS
        <span class="token punctuation">)</span>

        <span class="token keyword">def</span> <span class="token function">load_pretrained_embedding_from_file</span><span class="token punctuation">(</span>embed_path<span class="token punctuation">,</span> dictionary<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span><span class="token punctuation">:</span>
            num_embeddings <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
            padding_idx <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>pad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            embed_tokens <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span>
            embed_dict <span class="token operator">=</span> utils<span class="token punctuation">.</span>parse_embedding<span class="token punctuation">(</span>embed_path<span class="token punctuation">)</span>
            utils<span class="token punctuation">.</span>print_embed_overlap<span class="token punctuation">(</span>embed_dict<span class="token punctuation">,</span> dictionary<span class="token punctuation">)</span>
            <span class="token keyword">return</span> utils<span class="token punctuation">.</span>load_embedding<span class="token punctuation">(</span>embed_dict<span class="token punctuation">,</span> dictionary<span class="token punctuation">,</span> embed_tokens<span class="token punctuation">)</span>

        <span class="token keyword">if</span> args<span class="token punctuation">.</span>encoder_embed_path<span class="token punctuation">:</span>
            pretrained_encoder_embed <span class="token operator">=</span> load_pretrained_embedding_from_file<span class="token punctuation">(</span>
                args<span class="token punctuation">.</span>encoder_embed_path<span class="token punctuation">,</span> task<span class="token punctuation">.</span>source_dictionary<span class="token punctuation">,</span> args<span class="token punctuation">.</span>encoder_embed_dim
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            num_embeddings <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>task<span class="token punctuation">.</span>source_dictionary<span class="token punctuation">)</span>
            pretrained_encoder_embed <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>
                num_embeddings<span class="token punctuation">,</span> args<span class="token punctuation">.</span>encoder_embed_dim<span class="token punctuation">,</span> task<span class="token punctuation">.</span>source_dictionary<span class="token punctuation">.</span>pad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token punctuation">)</span>

        <span class="token keyword">if</span> args<span class="token punctuation">.</span>share_all_embeddings<span class="token punctuation">:</span>
            <span class="token comment"># double check all parameters combinations are valid</span>
            <span class="token keyword">if</span> task<span class="token punctuation">.</span>source_dictionary <span class="token operator">!=</span> task<span class="token punctuation">.</span>target_dictionary<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span><span class="token string">"--share-all-embeddings requires a joint dictionary"</span><span class="token punctuation">)</span>
            <span class="token keyword">if</span> args<span class="token punctuation">.</span>decoder_embed_path <span class="token keyword">and</span> <span class="token punctuation">(</span>
                args<span class="token punctuation">.</span>decoder_embed_path <span class="token operator">!=</span> args<span class="token punctuation">.</span>encoder_embed_path
            <span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                    <span class="token string">"--share-all-embed not compatible with --decoder-embed-path"</span>
                <span class="token punctuation">)</span>
            <span class="token keyword">if</span> args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">!=</span> args<span class="token punctuation">.</span>decoder_embed_dim<span class="token punctuation">:</span>
                <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                    <span class="token string">"--share-all-embeddings requires --encoder-embed-dim to "</span>
                    <span class="token string">"match --decoder-embed-dim"</span>
                <span class="token punctuation">)</span>
            pretrained_decoder_embed <span class="token operator">=</span> pretrained_encoder_embed
            args<span class="token punctuation">.</span>share_decoder_input_output_embed <span class="token operator">=</span> <span class="token boolean">True</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># separate decoder input embeddings</span>
            pretrained_decoder_embed <span class="token operator">=</span> <span class="token boolean">None</span>
            <span class="token keyword">if</span> args<span class="token punctuation">.</span>decoder_embed_path<span class="token punctuation">:</span>
                pretrained_decoder_embed <span class="token operator">=</span> load_pretrained_embedding_from_file<span class="token punctuation">(</span>
                    args<span class="token punctuation">.</span>decoder_embed_path<span class="token punctuation">,</span>
                    task<span class="token punctuation">.</span>target_dictionary<span class="token punctuation">,</span>
                    args<span class="token punctuation">.</span>decoder_embed_dim<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
        <span class="token comment"># one last double check of parameter combinations</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>share_decoder_input_output_embed <span class="token keyword">and</span> <span class="token punctuation">(</span>
            args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">!=</span> args<span class="token punctuation">.</span>decoder_out_embed_dim
        <span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string">"--share-decoder-input-output-embeddings requires "</span>
                <span class="token string">"--decoder-embed-dim to match --decoder-out-embed-dim"</span>
            <span class="token punctuation">)</span>

        <span class="token keyword">if</span> args<span class="token punctuation">.</span>encoder_freeze_embed<span class="token punctuation">:</span>
            pretrained_encoder_embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>decoder_freeze_embed<span class="token punctuation">:</span>
            pretrained_decoder_embed<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>

        encoder <span class="token operator">=</span> LSTMEncoder<span class="token punctuation">(</span>
            dictionary<span class="token operator">=</span>task<span class="token punctuation">.</span>source_dictionary<span class="token punctuation">,</span>
            embed_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>encoder_embed_dim<span class="token punctuation">,</span>
            hidden_size<span class="token operator">=</span>args<span class="token punctuation">.</span>encoder_hidden_size<span class="token punctuation">,</span>
            num_layers<span class="token operator">=</span>args<span class="token punctuation">.</span>encoder_layers<span class="token punctuation">,</span>
            dropout_in<span class="token operator">=</span>args<span class="token punctuation">.</span>encoder_dropout_in<span class="token punctuation">,</span>
            dropout_out<span class="token operator">=</span>args<span class="token punctuation">.</span>encoder_dropout_out<span class="token punctuation">,</span>
            bidirectional<span class="token operator">=</span>args<span class="token punctuation">.</span>encoder_bidirectional<span class="token punctuation">,</span>
            pretrained_embed<span class="token operator">=</span>pretrained_encoder_embed<span class="token punctuation">,</span>
            max_source_positions<span class="token operator">=</span>max_source_positions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        decoder <span class="token operator">=</span> LSTMDecoder<span class="token punctuation">(</span>
            dictionary<span class="token operator">=</span>task<span class="token punctuation">.</span>target_dictionary<span class="token punctuation">,</span>
            embed_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_embed_dim<span class="token punctuation">,</span>
            hidden_size<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_hidden_size<span class="token punctuation">,</span>
            out_embed_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_out_embed_dim<span class="token punctuation">,</span>
            num_layers<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_layers<span class="token punctuation">,</span>
            dropout_in<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_dropout_in<span class="token punctuation">,</span>
            dropout_out<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_dropout_out<span class="token punctuation">,</span>
            attention<span class="token operator">=</span>utils<span class="token punctuation">.</span>eval_bool<span class="token punctuation">(</span>args<span class="token punctuation">.</span>decoder_attention<span class="token punctuation">)</span><span class="token punctuation">,</span>
            encoder_output_units<span class="token operator">=</span>encoder<span class="token punctuation">.</span>output_units<span class="token punctuation">,</span>
            pretrained_embed<span class="token operator">=</span>pretrained_decoder_embed<span class="token punctuation">,</span>
            share_input_output_embed<span class="token operator">=</span>args<span class="token punctuation">.</span>share_decoder_input_output_embed<span class="token punctuation">,</span>
            adaptive_softmax_cutoff<span class="token operator">=</span><span class="token punctuation">(</span>
                utils<span class="token punctuation">.</span>eval_str_list<span class="token punctuation">(</span>args<span class="token punctuation">.</span>adaptive_softmax_cutoff<span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">)</span>
                <span class="token keyword">if</span> args<span class="token punctuation">.</span>criterion <span class="token operator">==</span> <span class="token string">"adaptive_loss"</span>
                <span class="token keyword">else</span> <span class="token boolean">None</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            max_target_positions<span class="token operator">=</span>max_target_positions<span class="token punctuation">,</span>
            residuals<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> cls<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        src_tokens<span class="token punctuation">,</span>
        src_lengths<span class="token punctuation">,</span>
        prev_output_tokens<span class="token punctuation">,</span>
        incremental_state<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        encoder_out <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>src_tokens<span class="token punctuation">,</span> src_lengths<span class="token operator">=</span>src_lengths<span class="token punctuation">)</span>
        decoder_out <span class="token operator">=</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>
            prev_output_tokens<span class="token punctuation">,</span>
            encoder_out<span class="token operator">=</span>encoder_out<span class="token punctuation">,</span>
            incremental_state<span class="token operator">=</span>incremental_state<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> decoder_out


<span class="token keyword">class</span> <span class="token class-name">LSTMEncoder</span><span class="token punctuation">(</span>FairseqEncoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""LSTM encoder."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        dictionary<span class="token punctuation">,</span>
        embed_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        hidden_size<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
        dropout_in<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
        dropout_out<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
        bidirectional<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        left_pad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        pretrained_embed<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        padding_idx<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        max_source_positions<span class="token operator">=</span>DEFAULT_MAX_SOURCE_POSITIONS<span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers
        self<span class="token punctuation">.</span>dropout_in_module <span class="token operator">=</span> FairseqDropout<span class="token punctuation">(</span>
            dropout_in <span class="token operator">*</span> <span class="token number">1.0</span><span class="token punctuation">,</span> module_name<span class="token operator">=</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout_out_module <span class="token operator">=</span> FairseqDropout<span class="token punctuation">(</span>
            dropout_out <span class="token operator">*</span> <span class="token number">1.0</span><span class="token punctuation">,</span> module_name<span class="token operator">=</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bidirectional <span class="token operator">=</span> bidirectional
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>max_source_positions <span class="token operator">=</span> max_source_positions

        num_embeddings <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>padding_idx <span class="token operator">=</span> padding_idx <span class="token keyword">if</span> padding_idx <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> dictionary<span class="token punctuation">.</span>pad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> pretrained_embed <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> pretrained_embed

        self<span class="token punctuation">.</span>lstm <span class="token operator">=</span> LSTM<span class="token punctuation">(</span>
            input_size<span class="token operator">=</span>embed_dim<span class="token punctuation">,</span>
            hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>
            num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span>
            dropout<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout_out_module<span class="token punctuation">.</span>p <span class="token keyword">if</span> num_layers <span class="token operator">&gt;</span> <span class="token number">1</span> <span class="token keyword">else</span> <span class="token number">0.0</span><span class="token punctuation">,</span>
            bidirectional<span class="token operator">=</span>bidirectional<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>left_pad <span class="token operator">=</span> left_pad

        self<span class="token punctuation">.</span>output_units <span class="token operator">=</span> hidden_size
        <span class="token keyword">if</span> bidirectional<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>output_units <span class="token operator">*=</span> <span class="token number">2</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        src_tokens<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
        src_lengths<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
        enforce_sorted<span class="token punctuation">:</span> <span class="token builtin">bool</span> <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            src_tokens (LongTensor): tokens in the source language of
                shape `(batch, src_len)`
            src_lengths (LongTensor): lengths of each source sentence of
                shape `(batch)`
            enforce_sorted (bool, optional): if True, `src_tokens` is
                expected to contain sequences sorted by length in a
                decreasing order. If False, this condition is not
                required. Default: True.
        """</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>left_pad<span class="token punctuation">:</span>
            <span class="token comment"># nn.utils.rnn.pack_padded_sequence requires right-padding;</span>
            <span class="token comment"># convert left-padding to right-padding</span>
            src_tokens <span class="token operator">=</span> utils<span class="token punctuation">.</span>convert_padding_direction<span class="token punctuation">(</span>
                src_tokens<span class="token punctuation">,</span>
                torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span><span class="token punctuation">.</span>fill_<span class="token punctuation">(</span>self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span><span class="token punctuation">,</span>
                left_to_right<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>

        bsz<span class="token punctuation">,</span> seqlen <span class="token operator">=</span> src_tokens<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># embed tokens</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_in_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># B x T x C -&gt; T x B x C</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># pack embedded source tokens into a PackedSequence</span>
        packed_x <span class="token operator">=</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>pack_padded_sequence<span class="token punctuation">(</span>
            x<span class="token punctuation">,</span> src_lengths<span class="token punctuation">.</span>cpu<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> enforce_sorted<span class="token operator">=</span>enforce_sorted
        <span class="token punctuation">)</span>

        <span class="token comment"># apply LSTM</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bidirectional<span class="token punctuation">:</span>
            state_size <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            state_size <span class="token operator">=</span> self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size
        h0 <span class="token operator">=</span> x<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token operator">*</span>state_size<span class="token punctuation">)</span>
        c0 <span class="token operator">=</span> x<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token operator">*</span>state_size<span class="token punctuation">)</span>
        packed_outs<span class="token punctuation">,</span> <span class="token punctuation">(</span>final_hiddens<span class="token punctuation">,</span> final_cells<span class="token punctuation">)</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>lstm<span class="token punctuation">(</span>packed_x<span class="token punctuation">,</span> <span class="token punctuation">(</span>h0<span class="token punctuation">,</span> c0<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># unpack outputs and apply dropout</span>
        x<span class="token punctuation">,</span> _ <span class="token operator">=</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>rnn<span class="token punctuation">.</span>pad_packed_sequence<span class="token punctuation">(</span>
            packed_outs<span class="token punctuation">,</span> padding_value<span class="token operator">=</span>self<span class="token punctuation">.</span>padding_idx <span class="token operator">*</span> <span class="token number">1.0</span>
        <span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_out_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">assert</span> <span class="token builtin">list</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token punctuation">[</span>seqlen<span class="token punctuation">,</span> bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>output_units<span class="token punctuation">]</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>bidirectional<span class="token punctuation">:</span>
            final_hiddens <span class="token operator">=</span> self<span class="token punctuation">.</span>combine_bidir<span class="token punctuation">(</span>final_hiddens<span class="token punctuation">,</span> bsz<span class="token punctuation">)</span>
            final_cells <span class="token operator">=</span> self<span class="token punctuation">.</span>combine_bidir<span class="token punctuation">(</span>final_cells<span class="token punctuation">,</span> bsz<span class="token punctuation">)</span>

        encoder_padding_mask <span class="token operator">=</span> src_tokens<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>
            <span class="token punctuation">(</span>
                x<span class="token punctuation">,</span>  <span class="token comment"># seq_len x batch x hidden</span>
                final_hiddens<span class="token punctuation">,</span>  <span class="token comment"># num_layers x batch x num_directions*hidden</span>
                final_cells<span class="token punctuation">,</span>  <span class="token comment"># num_layers x batch x num_directions*hidden</span>
                encoder_padding_mask<span class="token punctuation">,</span>  <span class="token comment"># seq_len x batch</span>
            <span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">combine_bidir</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> outs<span class="token punctuation">,</span> bsz<span class="token punctuation">:</span> <span class="token builtin">int</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> outs<span class="token punctuation">.</span>view<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> bsz<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> out<span class="token punctuation">.</span>view<span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">,</span> bsz<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">reorder_encoder_out</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span> encoder_out<span class="token punctuation">:</span> Tuple<span class="token punctuation">[</span>Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> new_order
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>
            <span class="token punctuation">(</span>
                encoder_out<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">,</span>
                encoder_out<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">,</span>
                encoder_out<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">,</span>
                encoder_out<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">max_positions</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Maximum input length supported by the encoder."""</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>max_source_positions


<span class="token keyword">class</span> <span class="token class-name">AttentionLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_embed_dim<span class="token punctuation">,</span> source_embed_dim<span class="token punctuation">,</span> output_embed_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>input_proj <span class="token operator">=</span> Linear<span class="token punctuation">(</span>input_embed_dim<span class="token punctuation">,</span> source_embed_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>output_proj <span class="token operator">=</span> Linear<span class="token punctuation">(</span>
            input_embed_dim <span class="token operator">+</span> source_embed_dim<span class="token punctuation">,</span> output_embed_dim<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> source_hids<span class="token punctuation">,</span> encoder_padding_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment"># input: bsz x input_embed_dim</span>
        <span class="token comment"># source_hids: srclen x bsz x source_embed_dim</span>

        <span class="token comment"># x: bsz x source_embed_dim</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>input_proj<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>

        <span class="token comment"># compute attention</span>
        attn_scores <span class="token operator">=</span> <span class="token punctuation">(</span>source_hids <span class="token operator">*</span> x<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

        <span class="token comment"># don't attend over padding</span>
        <span class="token keyword">if</span> encoder_padding_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            attn_scores <span class="token operator">=</span> <span class="token punctuation">(</span>
                attn_scores<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">.</span>masked_fill_<span class="token punctuation">(</span>encoder_padding_mask<span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>attn_scores<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>  <span class="token comment"># FP16 support: cast to float and back</span>

        attn_scores <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>attn_scores<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># srclen x bsz</span>

        <span class="token comment"># sum weighted sources</span>
        x <span class="token operator">=</span> <span class="token punctuation">(</span>attn_scores<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span> source_hids<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>

        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tanh<span class="token punctuation">(</span>self<span class="token punctuation">.</span>output_proj<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> attn_scores


<span class="token keyword">class</span> <span class="token class-name">LSTMDecoder</span><span class="token punctuation">(</span>FairseqIncrementalDecoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""LSTM decoder."""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        dictionary<span class="token punctuation">,</span>
        embed_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        hidden_size<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        out_embed_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
        dropout_in<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
        dropout_out<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
        attention<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        encoder_output_units<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        pretrained_embed<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        share_input_output_embed<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        adaptive_softmax_cutoff<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        max_target_positions<span class="token operator">=</span>DEFAULT_MAX_TARGET_POSITIONS<span class="token punctuation">,</span>
        residuals<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout_in_module <span class="token operator">=</span> FairseqDropout<span class="token punctuation">(</span>
            dropout_in <span class="token operator">*</span> <span class="token number">1.0</span><span class="token punctuation">,</span> module_name<span class="token operator">=</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout_out_module <span class="token operator">=</span> FairseqDropout<span class="token punctuation">(</span>
            dropout_out <span class="token operator">*</span> <span class="token number">1.0</span><span class="token punctuation">,</span> module_name<span class="token operator">=</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>hidden_size <span class="token operator">=</span> hidden_size
        self<span class="token punctuation">.</span>share_input_output_embed <span class="token operator">=</span> share_input_output_embed
        self<span class="token punctuation">.</span>need_attn <span class="token operator">=</span> <span class="token boolean">True</span>
        self<span class="token punctuation">.</span>max_target_positions <span class="token operator">=</span> max_target_positions
        self<span class="token punctuation">.</span>residuals <span class="token operator">=</span> residuals
        self<span class="token punctuation">.</span>num_layers <span class="token operator">=</span> num_layers

        self<span class="token punctuation">.</span>adaptive_softmax <span class="token operator">=</span> <span class="token boolean">None</span>
        num_embeddings <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        padding_idx <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>pad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> pretrained_embed <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> pretrained_embed

        self<span class="token punctuation">.</span>encoder_output_units <span class="token operator">=</span> encoder_output_units
        <span class="token keyword">if</span> encoder_output_units <span class="token operator">!=</span> hidden_size <span class="token keyword">and</span> encoder_output_units <span class="token operator">!=</span> <span class="token number">0</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>encoder_hidden_proj <span class="token operator">=</span> Linear<span class="token punctuation">(</span>encoder_output_units<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>encoder_cell_proj <span class="token operator">=</span> Linear<span class="token punctuation">(</span>encoder_output_units<span class="token punctuation">,</span> hidden_size<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>encoder_hidden_proj <span class="token operator">=</span> self<span class="token punctuation">.</span>encoder_cell_proj <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token comment"># disable input feeding if there is no encoder</span>
        <span class="token comment"># input feeding is described in arxiv.org/abs/1508.04025</span>
        input_feed_size <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> encoder_output_units <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> hidden_size
        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span>
            <span class="token punctuation">[</span>
                LSTMCell<span class="token punctuation">(</span>
                    input_size<span class="token operator">=</span>input_feed_size <span class="token operator">+</span> embed_dim
                    <span class="token keyword">if</span> layer <span class="token operator">==</span> <span class="token number">0</span>
                    <span class="token keyword">else</span> hidden_size<span class="token punctuation">,</span>
                    hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
                <span class="token keyword">for</span> layer <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_layers<span class="token punctuation">)</span>
            <span class="token punctuation">]</span>
        <span class="token punctuation">)</span>

        <span class="token keyword">if</span> attention<span class="token punctuation">:</span>
            <span class="token comment"># TODO make bias configurable</span>
            self<span class="token punctuation">.</span>attention <span class="token operator">=</span> AttentionLayer<span class="token punctuation">(</span>
                hidden_size<span class="token punctuation">,</span> encoder_output_units<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>attention <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> hidden_size <span class="token operator">!=</span> out_embed_dim<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>additional_fc <span class="token operator">=</span> Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span> out_embed_dim<span class="token punctuation">)</span>

        <span class="token keyword">if</span> adaptive_softmax_cutoff <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># setting adaptive_softmax dropout to dropout_out for now but can be redefined</span>
            self<span class="token punctuation">.</span>adaptive_softmax <span class="token operator">=</span> AdaptiveSoftmax<span class="token punctuation">(</span>
                num_embeddings<span class="token punctuation">,</span>
                hidden_size<span class="token punctuation">,</span>
                adaptive_softmax_cutoff<span class="token punctuation">,</span>
                dropout<span class="token operator">=</span>dropout_out<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>share_input_output_embed<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>fc_out <span class="token operator">=</span> Linear<span class="token punctuation">(</span>out_embed_dim<span class="token punctuation">,</span> num_embeddings<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout_out<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        prev_output_tokens<span class="token punctuation">,</span>
        encoder_out<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        incremental_state<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        src_lengths<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        x<span class="token punctuation">,</span> attn_scores <span class="token operator">=</span> self<span class="token punctuation">.</span>extract_features<span class="token punctuation">(</span>
            prev_output_tokens<span class="token punctuation">,</span> encoder_out<span class="token punctuation">,</span> incremental_state
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>output_layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> attn_scores

    <span class="token keyword">def</span> <span class="token function">extract_features</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        prev_output_tokens<span class="token punctuation">,</span>
        encoder_out<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tuple<span class="token punctuation">[</span>Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">,</span> Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
        incremental_state<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">None</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Similar to *forward* but only return features.
        """</span>
        <span class="token comment"># get outputs from encoder</span>
        <span class="token keyword">if</span> encoder_out <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            encoder_outs <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
            encoder_hiddens <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
            encoder_cells <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span>
            encoder_padding_mask <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            encoder_outs <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            encoder_hiddens <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            encoder_cells <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
            encoder_padding_mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>
        srclen <span class="token operator">=</span> encoder_outs<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> incremental_state <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>incremental_state<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            prev_output_tokens <span class="token operator">=</span> prev_output_tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>

        bsz<span class="token punctuation">,</span> seqlen <span class="token operator">=</span> prev_output_tokens<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>

        <span class="token comment"># embed tokens</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>prev_output_tokens<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_in_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># B x T x C -&gt; T x B x C</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        <span class="token comment"># initialize previous states (or get from cache during incremental generation)</span>
        <span class="token keyword">if</span> incremental_state <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> <span class="token builtin">len</span><span class="token punctuation">(</span>incremental_state<span class="token punctuation">)</span> <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
            prev_hiddens<span class="token punctuation">,</span> prev_cells<span class="token punctuation">,</span> input_feed <span class="token operator">=</span> self<span class="token punctuation">.</span>get_cached_state<span class="token punctuation">(</span>
                incremental_state
            <span class="token punctuation">)</span>
        <span class="token keyword">elif</span> encoder_out <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># setup recurrent cells</span>
            prev_hiddens <span class="token operator">=</span> <span class="token punctuation">[</span>encoder_hiddens<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
            prev_cells <span class="token operator">=</span> <span class="token punctuation">[</span>encoder_cells<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>encoder_hidden_proj <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                prev_hiddens <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>encoder_hidden_proj<span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token keyword">for</span> y <span class="token keyword">in</span> prev_hiddens<span class="token punctuation">]</span>
                prev_cells <span class="token operator">=</span> <span class="token punctuation">[</span>self<span class="token punctuation">.</span>encoder_cell_proj<span class="token punctuation">(</span>y<span class="token punctuation">)</span> <span class="token keyword">for</span> y <span class="token keyword">in</span> prev_cells<span class="token punctuation">]</span>
            input_feed <span class="token operator">=</span> x<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token comment"># setup zero cells, since there is no encoder</span>
            zero_state <span class="token operator">=</span> x<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span>bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>
            prev_hiddens <span class="token operator">=</span> <span class="token punctuation">[</span>zero_state <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
            prev_cells <span class="token operator">=</span> <span class="token punctuation">[</span>zero_state <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
            input_feed <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">assert</span> <span class="token punctuation">(</span>
            srclen <span class="token operator">&gt;</span> <span class="token number">0</span> <span class="token keyword">or</span> self<span class="token punctuation">.</span>attention <span class="token keyword">is</span> <span class="token boolean">None</span>
        <span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"attention is not supported if there are no encoder outputs"</span>
        attn_scores<span class="token punctuation">:</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>
            x<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span>srclen<span class="token punctuation">,</span> seqlen<span class="token punctuation">,</span> bsz<span class="token punctuation">)</span> <span class="token keyword">if</span> self<span class="token punctuation">.</span>attention <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token punctuation">)</span>
        outs <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
        <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>seqlen<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># input feeding: concatenate context vector from previous time step</span>
            <span class="token keyword">if</span> input_feed <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token builtin">input</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">(</span>x<span class="token punctuation">[</span>j<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> input_feed<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                <span class="token builtin">input</span> <span class="token operator">=</span> x<span class="token punctuation">[</span>j<span class="token punctuation">]</span>

            <span class="token keyword">for</span> i<span class="token punctuation">,</span> rnn <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># recurrent cell</span>
                hidden<span class="token punctuation">,</span> cell <span class="token operator">=</span> rnn<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>prev_hiddens<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> prev_cells<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

                <span class="token comment"># hidden state becomes the input to the next layer</span>
                <span class="token builtin">input</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_out_module<span class="token punctuation">(</span>hidden<span class="token punctuation">)</span>
                <span class="token keyword">if</span> self<span class="token punctuation">.</span>residuals<span class="token punctuation">:</span>
                    <span class="token builtin">input</span> <span class="token operator">=</span> <span class="token builtin">input</span> <span class="token operator">+</span> prev_hiddens<span class="token punctuation">[</span>i<span class="token punctuation">]</span>

                <span class="token comment"># save state for next time step</span>
                prev_hiddens<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> hidden
                prev_cells<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> cell

            <span class="token comment"># apply attention using the last layer's hidden state</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>attention <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                <span class="token keyword">assert</span> attn_scores <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
                out<span class="token punctuation">,</span> attn_scores<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> j<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">(</span>
                    hidden<span class="token punctuation">,</span> encoder_outs<span class="token punctuation">,</span> encoder_padding_mask
                <span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                out <span class="token operator">=</span> hidden
            out <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_out_module<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

            <span class="token comment"># input feeding</span>
            <span class="token keyword">if</span> input_feed <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                input_feed <span class="token operator">=</span> out

            <span class="token comment"># save final output</span>
            outs<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out<span class="token punctuation">)</span>

        <span class="token comment"># Stack all the necessary tensors together and store</span>
        prev_hiddens_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>prev_hiddens<span class="token punctuation">)</span>
        prev_cells_tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>prev_cells<span class="token punctuation">)</span>
        cache_state <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span>
            Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">{<!-- --></span>
                <span class="token string">"prev_hiddens"</span><span class="token punctuation">:</span> prev_hiddens_tensor<span class="token punctuation">,</span>
                <span class="token string">"prev_cells"</span><span class="token punctuation">:</span> prev_cells_tensor<span class="token punctuation">,</span>
                <span class="token string">"input_feed"</span><span class="token punctuation">:</span> input_feed<span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>set_incremental_state<span class="token punctuation">(</span>incremental_state<span class="token punctuation">,</span> <span class="token string">"cached_state"</span><span class="token punctuation">,</span> cache_state<span class="token punctuation">)</span>

        <span class="token comment"># collect outputs across time steps</span>
        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span>outs<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>seqlen<span class="token punctuation">,</span> bsz<span class="token punctuation">,</span> self<span class="token punctuation">.</span>hidden_size<span class="token punctuation">)</span>

        <span class="token comment"># T x B x C -&gt; B x T x C</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token keyword">if</span> <span class="token builtin">hasattr</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token string">"additional_fc"</span><span class="token punctuation">)</span> <span class="token keyword">and</span> self<span class="token punctuation">.</span>adaptive_softmax <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>additional_fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_out_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token comment"># srclen x tgtlen x bsz -&gt; bsz x tgtlen x srclen</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>training <span class="token keyword">and</span> self<span class="token punctuation">.</span>need_attn <span class="token keyword">and</span> self<span class="token punctuation">.</span>attention <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> attn_scores <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            attn_scores <span class="token operator">=</span> attn_scores<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            attn_scores <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> attn_scores

    <span class="token keyword">def</span> <span class="token function">output_layer</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Project features to the vocabulary size."""</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>adaptive_softmax <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>share_input_output_embed<span class="token punctuation">:</span>
                x <span class="token operator">=</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc_out<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        <span class="token keyword">return</span> x

    <span class="token keyword">def</span> <span class="token function">get_cached_state</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        incremental_state<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> Tuple<span class="token punctuation">[</span>List<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> List<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">:</span>
        cached_state <span class="token operator">=</span> self<span class="token punctuation">.</span>get_incremental_state<span class="token punctuation">(</span>incremental_state<span class="token punctuation">,</span> <span class="token string">"cached_state"</span><span class="token punctuation">)</span>
        <span class="token keyword">assert</span> cached_state <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        prev_hiddens_ <span class="token operator">=</span> cached_state<span class="token punctuation">[</span><span class="token string">"prev_hiddens"</span><span class="token punctuation">]</span>
        <span class="token keyword">assert</span> prev_hiddens_ <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        prev_cells_ <span class="token operator">=</span> cached_state<span class="token punctuation">[</span><span class="token string">"prev_cells"</span><span class="token punctuation">]</span>
        <span class="token keyword">assert</span> prev_cells_ <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
        prev_hiddens <span class="token operator">=</span> <span class="token punctuation">[</span>prev_hiddens_<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
        prev_cells <span class="token operator">=</span> <span class="token punctuation">[</span>prev_cells_<span class="token punctuation">[</span>j<span class="token punctuation">]</span> <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>num_layers<span class="token punctuation">)</span><span class="token punctuation">]</span>
        input_feed <span class="token operator">=</span> cached_state<span class="token punctuation">[</span>
            <span class="token string">"input_feed"</span>
        <span class="token punctuation">]</span>  <span class="token comment"># can be None for decoder-only language models</span>
        <span class="token keyword">return</span> prev_hiddens<span class="token punctuation">,</span> prev_cells<span class="token punctuation">,</span> input_feed

    <span class="token keyword">def</span> <span class="token function">reorder_incremental_state</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        incremental_state<span class="token punctuation">:</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        new_order<span class="token punctuation">:</span> Tensor<span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> incremental_state <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">or</span> <span class="token builtin">len</span><span class="token punctuation">(</span>incremental_state<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span>
        prev_hiddens<span class="token punctuation">,</span> prev_cells<span class="token punctuation">,</span> input_feed <span class="token operator">=</span> self<span class="token punctuation">.</span>get_cached_state<span class="token punctuation">(</span>incremental_state<span class="token punctuation">)</span>
        prev_hiddens <span class="token operator">=</span> <span class="token punctuation">[</span>p<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> prev_hiddens<span class="token punctuation">]</span>
        prev_cells <span class="token operator">=</span> <span class="token punctuation">[</span>p<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span> <span class="token keyword">for</span> p <span class="token keyword">in</span> prev_cells<span class="token punctuation">]</span>
        <span class="token keyword">if</span> input_feed <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            input_feed <span class="token operator">=</span> input_feed<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span>
        cached_state_new <span class="token operator">=</span> torch<span class="token punctuation">.</span>jit<span class="token punctuation">.</span>annotate<span class="token punctuation">(</span>
            Dict<span class="token punctuation">[</span><span class="token builtin">str</span><span class="token punctuation">,</span> Optional<span class="token punctuation">[</span>Tensor<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            <span class="token punctuation">{<!-- --></span>
                <span class="token string">"prev_hiddens"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>prev_hiddens<span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">"prev_cells"</span><span class="token punctuation">:</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>prev_cells<span class="token punctuation">)</span><span class="token punctuation">,</span>
                <span class="token string">"input_feed"</span><span class="token punctuation">:</span> input_feed<span class="token punctuation">,</span>
            <span class="token punctuation">}</span><span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>set_incremental_state<span class="token punctuation">(</span>incremental_state<span class="token punctuation">,</span> <span class="token string">"cached_state"</span><span class="token punctuation">,</span> cached_state_new<span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token keyword">return</span>

    <span class="token keyword">def</span> <span class="token function">max_positions</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Maximum output length supported by the decoder."""</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>max_target_positions

    <span class="token keyword">def</span> <span class="token function">make_generation_fast_</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> need_attn<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>need_attn <span class="token operator">=</span> need_attn


<span class="token keyword">def</span> <span class="token function">Embedding</span><span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>padding_idx<span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">[</span>padding_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> m


<span class="token keyword">def</span> <span class="token function">LSTM</span><span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"weight"</span> <span class="token keyword">in</span> name <span class="token keyword">or</span> <span class="token string">"bias"</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> m


<span class="token keyword">def</span> <span class="token function">LSTMCell</span><span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> nn<span class="token punctuation">.</span>LSTMCell<span class="token punctuation">(</span>input_size<span class="token punctuation">,</span> hidden_size<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    <span class="token keyword">for</span> name<span class="token punctuation">,</span> param <span class="token keyword">in</span> m<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token string">"weight"</span> <span class="token keyword">in</span> name <span class="token keyword">or</span> <span class="token string">"bias"</span> <span class="token keyword">in</span> name<span class="token punctuation">:</span>
            param<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> m


<span class="token keyword">def</span> <span class="token function">Linear</span><span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Linear layer (input: N x T x C)"""</span>
    m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>
    m<span class="token punctuation">.</span>weight<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> bias<span class="token punctuation">:</span>
        m<span class="token punctuation">.</span>bias<span class="token punctuation">.</span>data<span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> m


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"lstm"</span><span class="token punctuation">,</span> <span class="token string">"lstm"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">base_architecture</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    args<span class="token punctuation">.</span>dropout <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"dropout"</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_embed_path <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_path"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_freeze_embed <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_freeze_embed"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_hidden_size <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>
        args<span class="token punctuation">,</span> <span class="token string">"encoder_hidden_size"</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>encoder_embed_dim
    <span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_layers"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_bidirectional <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_bidirectional"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_dropout_in <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_dropout_in"</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_dropout_out <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_dropout_out"</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_path <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_path"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_freeze_embed <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_freeze_embed"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_hidden_size <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>
        args<span class="token punctuation">,</span> <span class="token string">"decoder_hidden_size"</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>decoder_embed_dim
    <span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_layers"</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_attention <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_attention"</span><span class="token punctuation">,</span> <span class="token string">"1"</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_dropout_in <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_dropout_in"</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_dropout_out <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_dropout_out"</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>share_decoder_input_output_embed <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>
        args<span class="token punctuation">,</span> <span class="token string">"share_decoder_input_output_embed"</span><span class="token punctuation">,</span> <span class="token boolean">False</span>
    <span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>share_all_embeddings <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"share_all_embeddings"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>adaptive_softmax_cutoff <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>
        args<span class="token punctuation">,</span> <span class="token string">"adaptive_softmax_cutoff"</span><span class="token punctuation">,</span> <span class="token string">"10000,50000,200000"</span>
    <span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"lstm"</span><span class="token punctuation">,</span> <span class="token string">"lstm_wiseman_iwslt_de_en"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">lstm_wiseman_iwslt_de_en</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    args<span class="token punctuation">.</span>dropout <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"dropout"</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_dropout_in <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_dropout_in"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_dropout_out <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_dropout_out"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_dropout_in <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_dropout_in"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_dropout_out <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_dropout_out"</span><span class="token punctuation">,</span> args<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>
    base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"lstm"</span><span class="token punctuation">,</span> <span class="token string">"lstm_luong_wmt_en_de"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">lstm_luong_wmt_en_de</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_layers"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_dropout_out <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_dropout_out"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_layers"</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_dropout_out <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_dropout_out"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="15_1092"></a>1.5、基于卷积神经网络的深度神经网络模型</h2> 
<p>如前所述，编码­解码模型在如机器翻译这种序列到序列任务上得到了很好的效果，很长一段时间内，模型的基本神经元都由LSTM或GRU主宰。但循环神经单元也有其固有的问题，尤其是时间消耗高、计算效率低。一些学者们发现，人类在处理语言理解时，小范围的语序混乱并不妨碍语义的理解。因此，是否能使用更高效的并行处理单元来替换严格时序的循环神经单元成为研究热点。<br> 此前的并行处理模型受限于自然语言生成时的自回归方案，仅仅将卷积神经网络应用到编码器端。Gehring等人出了ConvS2S模型，这是一个解码器编码器均建立在卷积神经网络结构上的模型，如图所示。这个模型采用了由Dauphin等人出的门控线性单元(Gated Linear Units,GLU)来并行化建模标记之间的关系，如下式所示：<br> <img src="https://images2.imgbox.com/dc/80/IDfPHhVk_o.png" alt="在这里插入图片描述"><br> 即对输入文本X进行两次卷积操作，其中卷积B操作使用sigmoid函数进行非线性激活，因此输出可以看作是经过一次门控后的数据，由卷积B决定卷积A中的数据哪些可以流入下一层。采用了门控线性单元的ConvS2S模型证明，在一个卷积核感受野范围内的标记无序并不会影响对一整句话语义的理解。并且，根据卷积神经网络的特点，长距离标记之间的关系可以由堆叠卷积层来达到。例如，堆叠6卷积核为5的卷积层，就能捕获到25个标记之间的关系.因此，序列到序列的问题完全可以由并行化程度更高的卷积神经网络来完成。<br> <img src="https://images2.imgbox.com/78/d0/UmnQbAOu_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="151CNNfconv_1098"></a>1.5.1、CNN（fconv）模型代码</h3> 
<p>由fairseq工具箱提供</p> 
<pre><code class="prism language-python"><span class="token comment"># Copyright (c) Facebook, Inc. and its affiliates.</span>
<span class="token comment">#</span>
<span class="token comment"># This source code is licensed under the MIT license found in the</span>
<span class="token comment"># LICENSE file in the root directory of this source tree.</span>

<span class="token keyword">import</span> math

<span class="token keyword">import</span> torch
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn
<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F
<span class="token keyword">from</span> fairseq <span class="token keyword">import</span> utils
<span class="token keyword">from</span> fairseq<span class="token punctuation">.</span>models <span class="token keyword">import</span> <span class="token punctuation">(</span>
    FairseqEncoder<span class="token punctuation">,</span>
    FairseqEncoderDecoderModel<span class="token punctuation">,</span>
    FairseqIncrementalDecoder<span class="token punctuation">,</span>
    register_model<span class="token punctuation">,</span>
    register_model_architecture<span class="token punctuation">,</span>
<span class="token punctuation">)</span>
<span class="token keyword">from</span> fairseq<span class="token punctuation">.</span>modules <span class="token keyword">import</span> <span class="token punctuation">(</span>
    AdaptiveSoftmax<span class="token punctuation">,</span>
    BeamableMM<span class="token punctuation">,</span>
    FairseqDropout<span class="token punctuation">,</span>
    GradMultiply<span class="token punctuation">,</span>
    LearnedPositionalEmbedding<span class="token punctuation">,</span>
    LinearizedConvolution<span class="token punctuation">,</span>
<span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model</span><span class="token punctuation">(</span><span class="token string">"fconv"</span><span class="token punctuation">)</span>
<span class="token keyword">class</span> <span class="token class-name">FConvModel</span><span class="token punctuation">(</span>FairseqEncoderDecoderModel<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    A fully convolutional model, i.e. a convolutional encoder and a
    convolutional decoder, as described in `"Convolutional Sequence to Sequence
    Learning" (Gehring et al., 2017) &lt;https://arxiv.org/abs/1705.03122&gt;`_.

    Args:
        encoder (FConvEncoder): the encoder
        decoder (FConvDecoder): the decoder

    The Convolutional model provides the following named architectures and
    command-line arguments:

    .. argparse::
        :ref: fairseq.models.fconv_parser
        :prog:
    """</span>

    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">hub_models</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">def</span> <span class="token function">moses_subword</span><span class="token punctuation">(</span>path<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
                <span class="token string">"path"</span><span class="token punctuation">:</span> path<span class="token punctuation">,</span>
                <span class="token string">"tokenizer"</span><span class="token punctuation">:</span> <span class="token string">"moses"</span><span class="token punctuation">,</span>
                <span class="token string">"bpe"</span><span class="token punctuation">:</span> <span class="token string">"subword_nmt"</span><span class="token punctuation">,</span>
            <span class="token punctuation">}</span>

        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"conv.wmt14.en-fr"</span><span class="token punctuation">:</span> moses_subword<span class="token punctuation">(</span>
                <span class="token string">"https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2"</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">"conv.wmt14.en-de"</span><span class="token punctuation">:</span> moses_subword<span class="token punctuation">(</span>
                <span class="token string">"https://dl.fbaipublicfiles.com/fairseq/models/wmt14.en-de.fconv-py.tar.bz2"</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">"conv.wmt17.en-de"</span><span class="token punctuation">:</span> moses_subword<span class="token punctuation">(</span>
                <span class="token string">"https://dl.fbaipublicfiles.com/fairseq/models/wmt17.v2.en-de.fconv-py.tar.bz2"</span>
            <span class="token punctuation">)</span><span class="token punctuation">,</span>
        <span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>encoder<span class="token punctuation">.</span>num_attention_layers <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>
            layer <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">for</span> layer <span class="token keyword">in</span> decoder<span class="token punctuation">.</span>attention
        <span class="token punctuation">)</span>

    <span class="token decorator annotation punctuation">@staticmethod</span>
    <span class="token keyword">def</span> <span class="token function">add_args</span><span class="token punctuation">(</span>parser<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Add model-specific arguments to the parser."""</span>
        <span class="token comment"># fmt: off</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--dropout'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">float</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'D'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'dropout probability'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-embed-dim'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'encoder embedding dimension'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-embed-path'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'STR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'path to pre-trained encoder embedding'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--encoder-layers'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'EXPR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'encoder layers [(dim, kernel_size), ...]'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-embed-dim'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder embedding dimension'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-embed-path'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'STR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'path to pre-trained decoder embedding'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-layers'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'EXPR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder layers [(dim, kernel_size), ...]'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-out-embed-dim'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">int</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'N'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder output embedding dimension'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--decoder-attention'</span><span class="token punctuation">,</span> <span class="token builtin">type</span><span class="token operator">=</span><span class="token builtin">str</span><span class="token punctuation">,</span> metavar<span class="token operator">=</span><span class="token string">'EXPR'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'decoder attention [True, ...]'</span><span class="token punctuation">)</span>
        parser<span class="token punctuation">.</span>add_argument<span class="token punctuation">(</span><span class="token string">'--share-input-output-embed'</span><span class="token punctuation">,</span> action<span class="token operator">=</span><span class="token string">'store_true'</span><span class="token punctuation">,</span>
                            <span class="token builtin">help</span><span class="token operator">=</span><span class="token string">'share input and output embeddings (requires'</span>
                                 <span class="token string">' --decoder-out-embed-dim and --decoder-embed-dim'</span>
                                 <span class="token string">' to be equal)'</span><span class="token punctuation">)</span>
        <span class="token comment"># fmt: on</span>

    <span class="token decorator annotation punctuation">@classmethod</span>
    <span class="token keyword">def</span> <span class="token function">build_model</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> args<span class="token punctuation">,</span> task<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Build a new model instance."""</span>
        <span class="token comment"># make sure that all args are properly defaulted (in case there are any new ones)</span>
        base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>

        encoder_embed_dict <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>encoder_embed_path<span class="token punctuation">:</span>
            encoder_embed_dict <span class="token operator">=</span> utils<span class="token punctuation">.</span>parse_embedding<span class="token punctuation">(</span>args<span class="token punctuation">.</span>encoder_embed_path<span class="token punctuation">)</span>
            utils<span class="token punctuation">.</span>print_embed_overlap<span class="token punctuation">(</span>encoder_embed_dict<span class="token punctuation">,</span> task<span class="token punctuation">.</span>source_dictionary<span class="token punctuation">)</span>

        decoder_embed_dict <span class="token operator">=</span> <span class="token boolean">None</span>
        <span class="token keyword">if</span> args<span class="token punctuation">.</span>decoder_embed_path<span class="token punctuation">:</span>
            decoder_embed_dict <span class="token operator">=</span> utils<span class="token punctuation">.</span>parse_embedding<span class="token punctuation">(</span>args<span class="token punctuation">.</span>decoder_embed_path<span class="token punctuation">)</span>
            utils<span class="token punctuation">.</span>print_embed_overlap<span class="token punctuation">(</span>decoder_embed_dict<span class="token punctuation">,</span> task<span class="token punctuation">.</span>target_dictionary<span class="token punctuation">)</span>

        encoder <span class="token operator">=</span> FConvEncoder<span class="token punctuation">(</span>
            dictionary<span class="token operator">=</span>task<span class="token punctuation">.</span>source_dictionary<span class="token punctuation">,</span>
            embed_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>encoder_embed_dim<span class="token punctuation">,</span>
            embed_dict<span class="token operator">=</span>encoder_embed_dict<span class="token punctuation">,</span>
            convolutions<span class="token operator">=</span><span class="token builtin">eval</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>encoder_layers<span class="token punctuation">)</span><span class="token punctuation">,</span>
            dropout<span class="token operator">=</span>args<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span>
            max_positions<span class="token operator">=</span>args<span class="token punctuation">.</span>max_source_positions<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        decoder <span class="token operator">=</span> FConvDecoder<span class="token punctuation">(</span>
            dictionary<span class="token operator">=</span>task<span class="token punctuation">.</span>target_dictionary<span class="token punctuation">,</span>
            embed_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_embed_dim<span class="token punctuation">,</span>
            embed_dict<span class="token operator">=</span>decoder_embed_dict<span class="token punctuation">,</span>
            convolutions<span class="token operator">=</span><span class="token builtin">eval</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>decoder_layers<span class="token punctuation">)</span><span class="token punctuation">,</span>
            out_embed_dim<span class="token operator">=</span>args<span class="token punctuation">.</span>decoder_out_embed_dim<span class="token punctuation">,</span>
            attention<span class="token operator">=</span><span class="token builtin">eval</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>decoder_attention<span class="token punctuation">)</span><span class="token punctuation">,</span>
            dropout<span class="token operator">=</span>args<span class="token punctuation">.</span>dropout<span class="token punctuation">,</span>
            max_positions<span class="token operator">=</span>args<span class="token punctuation">.</span>max_target_positions<span class="token punctuation">,</span>
            share_embed<span class="token operator">=</span>args<span class="token punctuation">.</span>share_input_output_embed<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">return</span> FConvModel<span class="token punctuation">(</span>encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">FConvEncoder</span><span class="token punctuation">(</span>FairseqEncoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Convolutional encoder consisting of `len(convolutions)` layers.

    Args:
        dictionary (~fairseq.data.Dictionary): encoding dictionary
        embed_dim (int, optional): embedding dimension
        embed_dict (str, optional): filename from which to load pre-trained
            embeddings
        max_positions (int, optional): maximum supported input sequence length
        convolutions (list, optional): the convolutional layer structure. Each
            list item `i` corresponds to convolutional layer `i`. Layers are
            given as ``(out_channels, kernel_width, [residual])``. Residual
            connections are added between layers when ``residual=1`` (which is
            the default behavior).
        dropout (float, optional): dropout to be applied before each conv layer
    """</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        dictionary<span class="token punctuation">,</span>
        embed_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        embed_dict<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        max_positions<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>
        convolutions<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">20</span><span class="token punctuation">,</span>
        dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout_module <span class="token operator">=</span> FairseqDropout<span class="token punctuation">(</span>
            dropout<span class="token punctuation">,</span> module_name<span class="token operator">=</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>num_attention_layers <span class="token operator">=</span> <span class="token boolean">None</span>

        num_embeddings <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>padding_idx <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>pad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span>
        <span class="token keyword">if</span> embed_dict<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> utils<span class="token punctuation">.</span>load_embedding<span class="token punctuation">(</span>
                embed_dict<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dictionary<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_tokens
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>embed_positions <span class="token operator">=</span> PositionalEmbedding<span class="token punctuation">(</span>
            max_positions<span class="token punctuation">,</span>
            embed_dim<span class="token punctuation">,</span>
            self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">,</span>
        <span class="token punctuation">)</span>

        convolutions <span class="token operator">=</span> extend_conv_spec<span class="token punctuation">(</span>convolutions<span class="token punctuation">)</span>
        in_channels <span class="token operator">=</span> convolutions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>projections <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convolutions <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>residuals <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        layer_in_channels <span class="token operator">=</span> <span class="token punctuation">[</span>in_channels<span class="token punctuation">]</span>
        <span class="token keyword">for</span> _<span class="token punctuation">,</span> <span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> residual<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>convolutions<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> residual <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                residual_dim <span class="token operator">=</span> out_channels
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                residual_dim <span class="token operator">=</span> layer_in_channels<span class="token punctuation">[</span><span class="token operator">-</span>residual<span class="token punctuation">]</span>
            self<span class="token punctuation">.</span>projections<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                Linear<span class="token punctuation">(</span>residual_dim<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>
                <span class="token keyword">if</span> residual_dim <span class="token operator">!=</span> out_channels
                <span class="token keyword">else</span> <span class="token boolean">None</span>
            <span class="token punctuation">)</span>
            <span class="token keyword">if</span> kernel_size <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                padding <span class="token operator">=</span> kernel_size <span class="token operator">//</span> <span class="token number">2</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                padding <span class="token operator">=</span> <span class="token number">0</span>
            self<span class="token punctuation">.</span>convolutions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                ConvTBC<span class="token punctuation">(</span>
                    in_channels<span class="token punctuation">,</span>
                    out_channels <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span>
                    kernel_size<span class="token punctuation">,</span>
                    dropout<span class="token operator">=</span>dropout<span class="token punctuation">,</span>
                    padding<span class="token operator">=</span>padding<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>residuals<span class="token punctuation">.</span>append<span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
            in_channels <span class="token operator">=</span> out_channels
            layer_in_channels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src_tokens<span class="token punctuation">,</span> src_lengths<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""
        Args:
            src_tokens (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            src_lengths (LongTensor): lengths of each source sentence of shape
                `(batch)`

        Returns:
            dict:
                - **encoder_out** (tuple): a tuple with two elements, where the
                  first element is the last encoder layer's output and the
                  second element is the same quantity summed with the input
                  embedding (used for attention). The shape of both tensors is
                  `(batch, src_len, embed_dim)`.
                - **encoder_padding_mask** (ByteTensor): the positions of
                  padding elements of shape `(batch, src_len)`
        """</span>
        <span class="token comment"># embed tokens and positions</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>embed_positions<span class="token punctuation">(</span>src_tokens<span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        input_embedding <span class="token operator">=</span> x

        <span class="token comment"># project to size of convolution</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># used to mask padding in input</span>
        encoder_padding_mask <span class="token operator">=</span> src_tokens<span class="token punctuation">.</span>eq<span class="token punctuation">(</span>self<span class="token punctuation">.</span>padding_idx<span class="token punctuation">)</span><span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># -&gt; T x B</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> encoder_padding_mask<span class="token punctuation">.</span><span class="token builtin">any</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            encoder_padding_mask <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token comment"># B x T x C -&gt; T x B x C</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

        residuals <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span>
        <span class="token comment"># temporal convolutions</span>
        <span class="token keyword">for</span> proj<span class="token punctuation">,</span> conv<span class="token punctuation">,</span> res_layer <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>projections<span class="token punctuation">,</span> self<span class="token punctuation">.</span>convolutions<span class="token punctuation">,</span> self<span class="token punctuation">.</span>residuals
        <span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> res_layer <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                residual <span class="token operator">=</span> residuals<span class="token punctuation">[</span><span class="token operator">-</span>res_layer<span class="token punctuation">]</span>
                residual <span class="token operator">=</span> residual <span class="token keyword">if</span> proj <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> proj<span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                residual <span class="token operator">=</span> <span class="token boolean">None</span>

            <span class="token keyword">if</span> encoder_padding_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> x<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>encoder_padding_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

            x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">if</span> conv<span class="token punctuation">.</span>kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">%</span> <span class="token number">2</span> <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>
                <span class="token comment"># padding is implicit in the conv</span>
                x <span class="token operator">=</span> conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                padding_l <span class="token operator">=</span> <span class="token punctuation">(</span>conv<span class="token punctuation">.</span>kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> <span class="token number">2</span>
                padding_r <span class="token operator">=</span> conv<span class="token punctuation">.</span>kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> <span class="token number">2</span>
                x <span class="token operator">=</span> F<span class="token punctuation">.</span>pad<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> padding_l<span class="token punctuation">,</span> padding_r<span class="token punctuation">)</span><span class="token punctuation">)</span>
                x <span class="token operator">=</span> conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>glu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

            <span class="token keyword">if</span> residual <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> residual<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
            residuals<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># T x B x C -&gt; B x T x C</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token comment"># project back to size of embedding</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">if</span> encoder_padding_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            encoder_padding_mask <span class="token operator">=</span> encoder_padding_mask<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># -&gt; B x T</span>
            x <span class="token operator">=</span> x<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>encoder_padding_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

        <span class="token comment"># scale gradients (this only affects backward, not forward)</span>
        x <span class="token operator">=</span> GradMultiply<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token number">1.0</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2.0</span> <span class="token operator">*</span> self<span class="token punctuation">.</span>num_attention_layers<span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># add output to input embedding for attention</span>
        y <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> input_embedding<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>

        <span class="token keyword">return</span> <span class="token punctuation">{<!-- --></span>
            <span class="token string">"encoder_out"</span><span class="token punctuation">:</span> <span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token string">"encoder_padding_mask"</span><span class="token punctuation">:</span> encoder_padding_mask<span class="token punctuation">,</span>  <span class="token comment"># B x T</span>
        <span class="token punctuation">}</span>

    <span class="token keyword">def</span> <span class="token function">reorder_encoder_out</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_out<span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_out"</span><span class="token punctuation">]</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_out"</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">(</span>
                encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_out"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">,</span>
                encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_out"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">if</span> encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_padding_mask"</span><span class="token punctuation">]</span> <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_padding_mask"</span><span class="token punctuation">]</span> <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span>
                <span class="token string">"encoder_padding_mask"</span>
            <span class="token punctuation">]</span><span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span>
        <span class="token keyword">return</span> encoder_out

    <span class="token keyword">def</span> <span class="token function">max_positions</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Maximum input length supported by the encoder."""</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>embed_positions<span class="token punctuation">.</span>max_positions


<span class="token keyword">class</span> <span class="token class-name">AttentionLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> conv_channels<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> bmm<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token comment"># projects from output of convolution to embedding dimension</span>
        self<span class="token punctuation">.</span>in_projection <span class="token operator">=</span> Linear<span class="token punctuation">(</span>conv_channels<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>
        <span class="token comment"># projects from embedding dimension to convolution size</span>
        self<span class="token punctuation">.</span>out_projection <span class="token operator">=</span> Linear<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> conv_channels<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>bmm <span class="token operator">=</span> bmm <span class="token keyword">if</span> bmm <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>bmm

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> target_embedding<span class="token punctuation">,</span> encoder_out<span class="token punctuation">,</span> encoder_padding_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>
        residual <span class="token operator">=</span> x

        <span class="token comment"># attention</span>
        x <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>in_projection<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> target_embedding<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> encoder_out<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># don't attend over padding</span>
        <span class="token keyword">if</span> encoder_padding_mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> <span class="token punctuation">(</span>
                x<span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
                <span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>encoder_padding_mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"-inf"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
                <span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            <span class="token punctuation">)</span>  <span class="token comment"># FP16 support: cast to float and back</span>

        <span class="token comment"># softmax over last dim</span>
        sz <span class="token operator">=</span> x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>sz<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> sz<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> sz<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>sz<span class="token punctuation">)</span>
        attn_scores <span class="token operator">=</span> x

        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>x<span class="token punctuation">,</span> encoder_out<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

        <span class="token comment"># scale attention output (respecting potentially different lengths)</span>
        s <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">if</span> encoder_padding_mask <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token punctuation">(</span>s <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">/</span> s<span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            s <span class="token operator">=</span> s <span class="token operator">-</span> encoder_padding_mask<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>
                dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span>
            <span class="token punctuation">)</span>  <span class="token comment"># exclude padding</span>
            s <span class="token operator">=</span> s<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
            x <span class="token operator">=</span> x <span class="token operator">*</span> <span class="token punctuation">(</span>s <span class="token operator">*</span> s<span class="token punctuation">.</span>rsqrt<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

        <span class="token comment"># project back</span>
        x <span class="token operator">=</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>out_projection<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token operator">+</span> residual<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x<span class="token punctuation">,</span> attn_scores

    <span class="token keyword">def</span> <span class="token function">make_generation_fast_</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> beamable_mm_beam_size<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Replace torch.bmm with BeamableMM."""</span>
        <span class="token keyword">if</span> beamable_mm_beam_size <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">del</span> self<span class="token punctuation">.</span>bmm
            self<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"bmm"</span><span class="token punctuation">,</span> BeamableMM<span class="token punctuation">(</span>beamable_mm_beam_size<span class="token punctuation">)</span><span class="token punctuation">)</span>


<span class="token keyword">class</span> <span class="token class-name">FConvDecoder</span><span class="token punctuation">(</span>FairseqIncrementalDecoder<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Convolutional decoder"""</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span>
        dictionary<span class="token punctuation">,</span>
        embed_dim<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span>
        embed_dict<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        out_embed_dim<span class="token operator">=</span><span class="token number">256</span><span class="token punctuation">,</span>
        max_positions<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span>
        convolutions<span class="token operator">=</span><span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">512</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">20</span><span class="token punctuation">,</span>
        attention<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>
        share_embed<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>
        positional_embeddings<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>
        adaptive_softmax_cutoff<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span>
        adaptive_softmax_dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">"version"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>dropout_module <span class="token operator">=</span> FairseqDropout<span class="token punctuation">(</span>
            dropout<span class="token punctuation">,</span> module_name<span class="token operator">=</span>self<span class="token punctuation">.</span>__class__<span class="token punctuation">.</span>__name__
        <span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>need_attn <span class="token operator">=</span> <span class="token boolean">True</span>

        convolutions <span class="token operator">=</span> extend_conv_spec<span class="token punctuation">(</span>convolutions<span class="token punctuation">)</span>
        in_channels <span class="token operator">=</span> convolutions<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
        <span class="token keyword">if</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>attention<span class="token punctuation">,</span> <span class="token builtin">bool</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment"># expand True into [True, True, ...] and do the same with False</span>
            attention <span class="token operator">=</span> <span class="token punctuation">[</span>attention<span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>convolutions<span class="token punctuation">)</span>
        <span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token builtin">isinstance</span><span class="token punctuation">(</span>attention<span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">)</span> <span class="token keyword">or</span> <span class="token builtin">len</span><span class="token punctuation">(</span>attention<span class="token punctuation">)</span> <span class="token operator">!=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>convolutions<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> ValueError<span class="token punctuation">(</span>
                <span class="token string">"Attention is expected to be a list of booleans of "</span>
                <span class="token string">"length equal to the number of layers."</span>
            <span class="token punctuation">)</span>

        num_embeddings <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>
        padding_idx <span class="token operator">=</span> dictionary<span class="token punctuation">.</span>pad<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embed_dim<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span>
        <span class="token keyword">if</span> embed_dict<span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>embed_tokens <span class="token operator">=</span> utils<span class="token punctuation">.</span>load_embedding<span class="token punctuation">(</span>
                embed_dict<span class="token punctuation">,</span> self<span class="token punctuation">.</span>dictionary<span class="token punctuation">,</span> self<span class="token punctuation">.</span>embed_tokens
            <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>embed_positions <span class="token operator">=</span> <span class="token punctuation">(</span>
            PositionalEmbedding<span class="token punctuation">(</span>
                max_positions<span class="token punctuation">,</span>
                embed_dim<span class="token punctuation">,</span>
                padding_idx<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
            <span class="token keyword">if</span> positional_embeddings
            <span class="token keyword">else</span> <span class="token boolean">None</span>
        <span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>embed_dim<span class="token punctuation">,</span> in_channels<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>projections <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>convolutions <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>attention <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>residuals <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>

        layer_in_channels <span class="token operator">=</span> <span class="token punctuation">[</span>in_channels<span class="token punctuation">]</span>
        <span class="token keyword">for</span> i<span class="token punctuation">,</span> <span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> residual<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>convolutions<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> residual <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
                residual_dim <span class="token operator">=</span> out_channels
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                residual_dim <span class="token operator">=</span> layer_in_channels<span class="token punctuation">[</span><span class="token operator">-</span>residual<span class="token punctuation">]</span>
            self<span class="token punctuation">.</span>projections<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                Linear<span class="token punctuation">(</span>residual_dim<span class="token punctuation">,</span> out_channels<span class="token punctuation">)</span>
                <span class="token keyword">if</span> residual_dim <span class="token operator">!=</span> out_channels
                <span class="token keyword">else</span> <span class="token boolean">None</span>
            <span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>convolutions<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                LinearizedConv1d<span class="token punctuation">(</span>
                    in_channels<span class="token punctuation">,</span>
                    out_channels <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">,</span>
                    kernel_size<span class="token punctuation">,</span>
                    padding<span class="token operator">=</span><span class="token punctuation">(</span>kernel_size <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                    dropout<span class="token operator">=</span>dropout<span class="token punctuation">,</span>
                <span class="token punctuation">)</span>
            <span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>attention<span class="token punctuation">.</span>append<span class="token punctuation">(</span>
                AttentionLayer<span class="token punctuation">(</span>out_channels<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span> <span class="token keyword">if</span> attention<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">else</span> <span class="token boolean">None</span>
            <span class="token punctuation">)</span>
            self<span class="token punctuation">.</span>residuals<span class="token punctuation">.</span>append<span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
            in_channels <span class="token operator">=</span> out_channels
            layer_in_channels<span class="token punctuation">.</span>append<span class="token punctuation">(</span>out_channels<span class="token punctuation">)</span>

        self<span class="token punctuation">.</span>adaptive_softmax <span class="token operator">=</span> <span class="token boolean">None</span>
        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> <span class="token boolean">None</span>

        <span class="token keyword">if</span> adaptive_softmax_cutoff <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">assert</span> <span class="token keyword">not</span> share_embed
            self<span class="token punctuation">.</span>adaptive_softmax <span class="token operator">=</span> AdaptiveSoftmax<span class="token punctuation">(</span>
                num_embeddings<span class="token punctuation">,</span>
                in_channels<span class="token punctuation">,</span>
                adaptive_softmax_cutoff<span class="token punctuation">,</span>
                dropout<span class="token operator">=</span>adaptive_softmax_dropout<span class="token punctuation">,</span>
            <span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_embed_dim<span class="token punctuation">)</span>
            <span class="token keyword">if</span> share_embed<span class="token punctuation">:</span>
                <span class="token keyword">assert</span> out_embed_dim <span class="token operator">==</span> embed_dim<span class="token punctuation">,</span> <span class="token punctuation">(</span>
                    <span class="token string">"Shared embed weights implies same dimensions "</span>
                    <span class="token string">" out_embed_dim={} vs embed_dim={}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>out_embed_dim<span class="token punctuation">,</span> embed_dim<span class="token punctuation">)</span>
                <span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>out_embed_dim<span class="token punctuation">,</span> num_embeddings<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>fc3<span class="token punctuation">.</span>weight <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">.</span>weight
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> Linear<span class="token punctuation">(</span>out_embed_dim<span class="token punctuation">,</span> num_embeddings<span class="token punctuation">,</span> dropout<span class="token operator">=</span>dropout<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>
        self<span class="token punctuation">,</span> prev_output_tokens<span class="token punctuation">,</span> encoder_out<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> incremental_state<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token operator">**</span>unused
    <span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> encoder_out <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            encoder_padding_mask <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_padding_mask"</span><span class="token punctuation">]</span>
            encoder_out <span class="token operator">=</span> encoder_out<span class="token punctuation">[</span><span class="token string">"encoder_out"</span><span class="token punctuation">]</span>

            <span class="token comment"># split and transpose encoder outputs</span>
            encoder_a<span class="token punctuation">,</span> encoder_b <span class="token operator">=</span> self<span class="token punctuation">.</span>_split_encoder_out<span class="token punctuation">(</span>
                encoder_out<span class="token punctuation">,</span> incremental_state
            <span class="token punctuation">)</span>

        <span class="token keyword">if</span> self<span class="token punctuation">.</span>embed_positions <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            pos_embed <span class="token operator">=</span> self<span class="token punctuation">.</span>embed_positions<span class="token punctuation">(</span>prev_output_tokens<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            pos_embed <span class="token operator">=</span> <span class="token number">0</span>

        <span class="token keyword">if</span> incremental_state <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            prev_output_tokens <span class="token operator">=</span> prev_output_tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>_embed_tokens<span class="token punctuation">(</span>prev_output_tokens<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span>

        <span class="token comment"># embed tokens and combine with positional embeddings</span>
        x <span class="token operator">+=</span> pos_embed
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
        target_embedding <span class="token operator">=</span> x

        <span class="token comment"># project to size of convolution</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># B x T x C -&gt; T x B x C</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>_transpose_if_training<span class="token punctuation">(</span>x<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span>

        <span class="token comment"># temporal convolutions</span>
        avg_attn_scores <span class="token operator">=</span> <span class="token boolean">None</span>
        num_attn_layers <span class="token operator">=</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>attention<span class="token punctuation">)</span>
        residuals <span class="token operator">=</span> <span class="token punctuation">[</span>x<span class="token punctuation">]</span>
        <span class="token keyword">for</span> proj<span class="token punctuation">,</span> conv<span class="token punctuation">,</span> attention<span class="token punctuation">,</span> res_layer <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>projections<span class="token punctuation">,</span> self<span class="token punctuation">.</span>convolutions<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attention<span class="token punctuation">,</span> self<span class="token punctuation">.</span>residuals
        <span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token keyword">if</span> res_layer <span class="token operator">&gt;</span> <span class="token number">0</span><span class="token punctuation">:</span>
                residual <span class="token operator">=</span> residuals<span class="token punctuation">[</span><span class="token operator">-</span>res_layer<span class="token punctuation">]</span>
                residual <span class="token operator">=</span> residual <span class="token keyword">if</span> proj <span class="token keyword">is</span> <span class="token boolean">None</span> <span class="token keyword">else</span> proj<span class="token punctuation">(</span>residual<span class="token punctuation">)</span>
            <span class="token keyword">else</span><span class="token punctuation">:</span>
                residual <span class="token operator">=</span> <span class="token boolean">None</span>

            x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> conv<span class="token punctuation">(</span>x<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span>
            x <span class="token operator">=</span> F<span class="token punctuation">.</span>glu<span class="token punctuation">(</span>x<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>

            <span class="token comment"># attention</span>
            <span class="token keyword">if</span> attention <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> self<span class="token punctuation">.</span>_transpose_if_training<span class="token punctuation">(</span>x<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span>

                x<span class="token punctuation">,</span> attn_scores <span class="token operator">=</span> attention<span class="token punctuation">(</span>
                    x<span class="token punctuation">,</span> target_embedding<span class="token punctuation">,</span> <span class="token punctuation">(</span>encoder_a<span class="token punctuation">,</span> encoder_b<span class="token punctuation">)</span><span class="token punctuation">,</span> encoder_padding_mask
                <span class="token punctuation">)</span>

                <span class="token keyword">if</span> <span class="token keyword">not</span> self<span class="token punctuation">.</span>training <span class="token keyword">and</span> self<span class="token punctuation">.</span>need_attn<span class="token punctuation">:</span>
                    attn_scores <span class="token operator">=</span> attn_scores <span class="token operator">/</span> num_attn_layers
                    <span class="token keyword">if</span> avg_attn_scores <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                        avg_attn_scores <span class="token operator">=</span> attn_scores
                    <span class="token keyword">else</span><span class="token punctuation">:</span>
                        avg_attn_scores<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>attn_scores<span class="token punctuation">)</span>

                x <span class="token operator">=</span> self<span class="token punctuation">.</span>_transpose_if_training<span class="token punctuation">(</span>x<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span>

            <span class="token comment"># residual</span>
            <span class="token keyword">if</span> residual <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
                x <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">+</span> residual<span class="token punctuation">)</span> <span class="token operator">*</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span>
            residuals<span class="token punctuation">.</span>append<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token comment"># T x B x C -&gt; B x T x C</span>
        x <span class="token operator">=</span> self<span class="token punctuation">.</span>_transpose_if_training<span class="token punctuation">(</span>x<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span>

        <span class="token comment"># project back to size of vocabulary if not using adaptive softmax</span>
        <span class="token keyword">if</span> self<span class="token punctuation">.</span>fc2 <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span> <span class="token keyword">and</span> self<span class="token punctuation">.</span>fc3 <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout_module<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

        <span class="token keyword">return</span> x<span class="token punctuation">,</span> avg_attn_scores

    <span class="token keyword">def</span> <span class="token function">reorder_incremental_state</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> incremental_state<span class="token punctuation">,</span> new_order<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reorder_incremental_state<span class="token punctuation">(</span>incremental_state<span class="token punctuation">,</span> new_order<span class="token punctuation">)</span>
        encoder_out <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_incremental_state<span class="token punctuation">(</span>
            self<span class="token punctuation">,</span> incremental_state<span class="token punctuation">,</span> <span class="token string">"encoder_out"</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> encoder_out <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            encoder_out <span class="token operator">=</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>eo<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> new_order<span class="token punctuation">)</span> <span class="token keyword">for</span> eo <span class="token keyword">in</span> encoder_out<span class="token punctuation">)</span>
            utils<span class="token punctuation">.</span>set_incremental_state<span class="token punctuation">(</span>
                self<span class="token punctuation">,</span> incremental_state<span class="token punctuation">,</span> <span class="token string">"encoder_out"</span><span class="token punctuation">,</span> encoder_out
            <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">max_positions</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Maximum output length supported by the decoder."""</span>
        <span class="token keyword">return</span> <span class="token punctuation">(</span>
            self<span class="token punctuation">.</span>embed_positions<span class="token punctuation">.</span>max_positions
            <span class="token keyword">if</span> self<span class="token punctuation">.</span>embed_positions <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span>
            <span class="token keyword">else</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"inf"</span><span class="token punctuation">)</span>
        <span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">upgrade_state_dict</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> state_dict<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> utils<span class="token punctuation">.</span>item<span class="token punctuation">(</span>state_dict<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">"decoder.version"</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token operator">&lt;</span> <span class="token number">2</span><span class="token punctuation">:</span>
            <span class="token comment"># old models use incorrect weight norm dimension</span>
            <span class="token keyword">for</span> i<span class="token punctuation">,</span> conv <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>convolutions<span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token comment"># reconfigure weight norm</span>
                nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>remove_weight_norm<span class="token punctuation">(</span>conv<span class="token punctuation">)</span>
                self<span class="token punctuation">.</span>convolutions<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span>conv<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
            state_dict<span class="token punctuation">[</span><span class="token string">"decoder.version"</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> state_dict

    <span class="token keyword">def</span> <span class="token function">make_generation_fast_</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> need_attn<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>need_attn <span class="token operator">=</span> need_attn

    <span class="token keyword">def</span> <span class="token function">_embed_tokens</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> tokens<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> incremental_state <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token comment"># keep only the last token for incremental forward pass</span>
            tokens <span class="token operator">=</span> tokens<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>embed_tokens<span class="token punctuation">(</span>tokens<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">_split_encoder_out</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder_out<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">"""Split and transpose encoder outputs.

        This is cached when doing incremental inference.
        """</span>
        cached_result <span class="token operator">=</span> utils<span class="token punctuation">.</span>get_incremental_state<span class="token punctuation">(</span>
            self<span class="token punctuation">,</span> incremental_state<span class="token punctuation">,</span> <span class="token string">"encoder_out"</span>
        <span class="token punctuation">)</span>
        <span class="token keyword">if</span> cached_result <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            <span class="token keyword">return</span> cached_result

        <span class="token comment"># transpose only once to speed up attention layers</span>
        encoder_a<span class="token punctuation">,</span> encoder_b <span class="token operator">=</span> encoder_out
        encoder_a <span class="token operator">=</span> encoder_a<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span>
        result <span class="token operator">=</span> <span class="token punctuation">(</span>encoder_a<span class="token punctuation">,</span> encoder_b<span class="token punctuation">)</span>

        <span class="token keyword">if</span> incremental_state <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            utils<span class="token punctuation">.</span>set_incremental_state<span class="token punctuation">(</span>self<span class="token punctuation">,</span> incremental_state<span class="token punctuation">,</span> <span class="token string">"encoder_out"</span><span class="token punctuation">,</span> result<span class="token punctuation">)</span>
        <span class="token keyword">return</span> result

    <span class="token keyword">def</span> <span class="token function">_transpose_if_training</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> incremental_state<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">if</span> incremental_state <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
            x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> x


<span class="token keyword">def</span> <span class="token function">extend_conv_spec</span><span class="token punctuation">(</span>convolutions<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    Extends convolutional spec that is a list of tuples of 2 or 3 parameters
    (kernel size, dim size and optionally how many layers behind to look for residual)
    to default the residual propagation param if it is not specified
    """</span>
    extended <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> spec <span class="token keyword">in</span> convolutions<span class="token punctuation">:</span>
        <span class="token keyword">if</span> <span class="token builtin">len</span><span class="token punctuation">(</span>spec<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">3</span><span class="token punctuation">:</span>
            extended<span class="token punctuation">.</span>append<span class="token punctuation">(</span>spec<span class="token punctuation">)</span>
        <span class="token keyword">elif</span> <span class="token builtin">len</span><span class="token punctuation">(</span>spec<span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
            extended<span class="token punctuation">.</span>append<span class="token punctuation">(</span>spec <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">else</span><span class="token punctuation">:</span>
            <span class="token keyword">raise</span> Exception<span class="token punctuation">(</span>
                <span class="token string">"invalid number of parameters in convolution spec "</span>
                <span class="token operator">+</span> <span class="token builtin">str</span><span class="token punctuation">(</span>spec<span class="token punctuation">)</span>
                <span class="token operator">+</span> <span class="token string">". expected 2 or 3"</span>
            <span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token builtin">tuple</span><span class="token punctuation">(</span>extended<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">Embedding</span><span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token operator">=</span>padding_idx<span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">[</span>padding_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> m


<span class="token keyword">def</span> <span class="token function">PositionalEmbedding</span><span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    m <span class="token operator">=</span> LearnedPositionalEmbedding<span class="token punctuation">(</span>num_embeddings<span class="token punctuation">,</span> embedding_dim<span class="token punctuation">,</span> padding_idx<span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">[</span>padding_idx<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> m


<span class="token keyword">def</span> <span class="token function">Linear</span><span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Weight-normalized Linear layer (input: N x T x C)"""</span>
    m <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">-</span> dropout<span class="token punctuation">)</span> <span class="token operator">/</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span>m<span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">LinearizedConv1d</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Weight-normalized Conv1d layer optimized for decoding"""</span>
    m <span class="token operator">=</span> LinearizedConvolution<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    std <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> dropout<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>m<span class="token punctuation">.</span>kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> in_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span>std<span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span>m<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>


<span class="token keyword">def</span> <span class="token function">ConvTBC</span><span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""Weight-normalized Conv1d layer"""</span>
    <span class="token keyword">from</span> fairseq<span class="token punctuation">.</span>modules <span class="token keyword">import</span> ConvTBC

    m <span class="token operator">=</span> ConvTBC<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span> out_channels<span class="token punctuation">,</span> kernel_size<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>
    std <span class="token operator">=</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">4</span> <span class="token operator">*</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> dropout<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>m<span class="token punctuation">.</span>kernel_size<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> in_channels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>normal_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> mean<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> std<span class="token operator">=</span>std<span class="token punctuation">)</span>
    nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>constant_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>bias<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>weight_norm<span class="token punctuation">(</span>m<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"fconv"</span><span class="token punctuation">,</span> <span class="token string">"fconv"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">base_architecture</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    args<span class="token punctuation">.</span>dropout <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"dropout"</span><span class="token punctuation">,</span> <span class="token number">0.1</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_embed_path <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_path"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_layers"</span><span class="token punctuation">,</span> <span class="token string">"[(512, 3)] * 20"</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_path <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_path"</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_layers"</span><span class="token punctuation">,</span> <span class="token string">"[(512, 3)] * 20"</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_attention <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_attention"</span><span class="token punctuation">,</span> <span class="token string">"True"</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>share_input_output_embed <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"share_input_output_embed"</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"fconv"</span><span class="token punctuation">,</span> <span class="token string">"fconv_iwslt_de_en"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">fconv_iwslt_de_en</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_layers"</span><span class="token punctuation">,</span> <span class="token string">"[(256, 3)] * 4"</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_layers"</span><span class="token punctuation">,</span> <span class="token string">"[(256, 3)] * 3"</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">)</span>
    base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"fconv"</span><span class="token punctuation">,</span> <span class="token string">"fconv_wmt_en_ro"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">fconv_wmt_en_ro</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"fconv"</span><span class="token punctuation">,</span> <span class="token string">"fconv_wmt_en_de"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">fconv_wmt_en_de</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    convs <span class="token operator">=</span> <span class="token string">"[(512, 3)] * 9"</span>  <span class="token comment"># first 9 layers have 512 units</span>
    convs <span class="token operator">+=</span> <span class="token string">" + [(1024, 3)] * 4"</span>  <span class="token comment"># next 4 layers have 1024 units</span>
    convs <span class="token operator">+=</span> <span class="token string">" + [(2048, 1)] * 2"</span>  <span class="token comment"># final 2 layers use 1x1 convolutions</span>

    args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">768</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_layers"</span><span class="token punctuation">,</span> convs<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">768</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_layers"</span><span class="token punctuation">,</span> convs<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>


<span class="token decorator annotation punctuation">@register_model_architecture</span><span class="token punctuation">(</span><span class="token string">"fconv"</span><span class="token punctuation">,</span> <span class="token string">"fconv_wmt_en_fr"</span><span class="token punctuation">)</span>
<span class="token keyword">def</span> <span class="token function">fconv_wmt_en_fr</span><span class="token punctuation">(</span>args<span class="token punctuation">)</span><span class="token punctuation">:</span>
    convs <span class="token operator">=</span> <span class="token string">"[(512, 3)] * 6"</span>  <span class="token comment"># first 6 layers have 512 units</span>
    convs <span class="token operator">+=</span> <span class="token string">" + [(768, 3)] * 4"</span>  <span class="token comment"># next 4 layers have 768 units</span>
    convs <span class="token operator">+=</span> <span class="token string">" + [(1024, 3)] * 3"</span>  <span class="token comment"># next 3 layers have 1024 units</span>
    convs <span class="token operator">+=</span> <span class="token string">" + [(2048, 1)] * 1"</span>  <span class="token comment"># next 1 layer uses 1x1 convolutions</span>
    convs <span class="token operator">+=</span> <span class="token string">" + [(4096, 1)] * 1"</span>  <span class="token comment"># final 1 layer uses 1x1 convolutions</span>

    args<span class="token punctuation">.</span>encoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">768</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>encoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"encoder_layers"</span><span class="token punctuation">,</span> convs<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">768</span><span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_layers <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_layers"</span><span class="token punctuation">,</span> convs<span class="token punctuation">)</span>
    args<span class="token punctuation">.</span>decoder_out_embed_dim <span class="token operator">=</span> <span class="token builtin">getattr</span><span class="token punctuation">(</span>args<span class="token punctuation">,</span> <span class="token string">"decoder_out_embed_dim"</span><span class="token punctuation">,</span> <span class="token number">512</span><span class="token punctuation">)</span>
    base_architecture<span class="token punctuation">(</span>args<span class="token punctuation">)</span>

</code></pre>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>