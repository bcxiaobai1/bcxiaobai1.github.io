<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>神经网络与深度学习（二） pytorch入门——张量 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">神经网络与深度学习（二） pytorch入门——张量</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <h1 id="%E6%9C%AC%E6%96%87%E7%AB%A0%E9%80%9A%E8%BF%87%E5%8F%82%E8%80%83%E9%A3%9E%E6%A1%A8AI%20Studio%20-%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%AD%A6%E4%B9%A0%E4%B8%8E%E5%AE%9E%E8%AE%AD%E7%A4%BE%E5%8C%BA%C2%A0%20%E6%95%99%E7%A8%8B%E8%BF%9B%E8%A1%8Cpytorch%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0%E3%80%82">本文章通过参考<a href="https://aistudio.baidu.com/aistudio/education/group/info/25793" title="飞桨AI Studio - 人工智能学习与实训社区">飞桨AI Studio - 人工智能学习与实训社区</a>  教程进行pytorch相关学习。</h1> 
<h2 id="main-toc">目录</h2> 
<p id="%E4%B8%80.%20%E6%A6%82%E5%BF%B5%EF%BC%9A%E5%BC%A0%E9%87%8F%E3%80%81%E7%AE%97%E5%AD%90-toc" style="margin-left:0px"><a href="#%E4%B8%80.%20%E6%A6%82%E5%BF%B5%EF%BC%9A%E5%BC%A0%E9%87%8F%E3%80%81%E7%AE%97%E5%AD%90">一. 概念：张量、算子</a></p> 
<p id="%E4%BA%8C.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97-toc" style="margin-left:0px"><a href="#%E4%BA%8C.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97">二. 使用pytorch实现张量运算</a></p> 
<p id="1.2.1%20%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F-toc" style="margin-left:40px"><a href="#1.2.1%20%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">1.2.1 创建张量</a></p> 
<p id="1.2.1.1%20%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F-toc" style="margin-left:80px"><a href="#1.2.1.1%20%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">1.2.1.1 指定数据创建张量</a></p> 
<p id="1.2.1.2%20%E6%8C%87%E5%AE%9A%E5%BD%A2%E7%8A%B6%E5%88%9B%E5%BB%BA-toc" style="margin-left:80px"><a href="#1.2.1.2%20%E6%8C%87%E5%AE%9A%E5%BD%A2%E7%8A%B6%E5%88%9B%E5%BB%BA">1.2.1.2 指定形状创建</a></p> 
<p id="1.2.1.3%20%E6%8C%87%E5%AE%9A%E5%8C%BA%E9%97%B4%E5%88%9B%E5%BB%BA-toc" style="margin-left:80px"><a href="#1.2.1.3%20%E6%8C%87%E5%AE%9A%E5%8C%BA%E9%97%B4%E5%88%9B%E5%BB%BA">1.2.1.3 指定区间创建</a></p> 
<p id="1.2.2%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7-toc" style="margin-left:40px"><a href="#1.2.2%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7">1.2.2 张量的属性</a></p> 
<p id="1.2.2.1%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BD%A2%E7%8A%B6-toc" style="margin-left:80px"><a href="#1.2.2.1%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BD%A2%E7%8A%B6">1.2.2.1 张量的形状</a></p> 
<p id="1.2.2.2%20%E5%BD%A2%E7%8A%B6%E7%9A%84%E6%94%B9%E5%8F%98-toc" style="margin-left:80px"><a href="#1.2.2.2%20%E5%BD%A2%E7%8A%B6%E7%9A%84%E6%94%B9%E5%8F%98">1.2.2.2 形状的改变</a></p> 
<p id="1.2.2.3%20%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B-toc" style="margin-left:80px"><a href="#1.2.2.3%20%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">1.2.2.3 张量的数据类型</a></p> 
<p id="1.2.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BE%E5%A4%87%E4%BD%8D%E7%BD%AE-toc" style="margin-left:80px"><a href="#1.2.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BE%E5%A4%87%E4%BD%8D%E7%BD%AE">1.2.2.4 张量的设备位置</a></p> 
<p id="1.2.3%20%E5%BC%A0%E9%87%8F%E4%B8%8ENumpy%E6%95%B0%E7%BB%84%E8%BD%AC%E6%8D%A2-toc" style="margin-left:40px"><a href="#1.2.3%20%E5%BC%A0%E9%87%8F%E4%B8%8ENumpy%E6%95%B0%E7%BB%84%E8%BD%AC%E6%8D%A2">1.2.3 张量与Numpy数组转换</a></p> 
<p id="1.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BF%E9%97%AE-toc" style="margin-left:40px"><a href="#1.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BF%E9%97%AE">1.2.4 张量的访问</a></p> 
<p id="1.2.4.1%20%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87-toc" style="margin-left:80px"><a href="#1.2.4.1%20%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87">1.2.4.1 索引和切片</a></p> 
<p id="1.2.4.2%20%E8%AE%BF%E9%97%AE%E5%BC%A0%E9%87%8F-toc" style="margin-left:80px"><a href="#1.2.4.2%20%E8%AE%BF%E9%97%AE%E5%BC%A0%E9%87%8F">1.2.4.2 访问张量</a></p> 
<p id="1.2.4.3%20%E4%BF%AE%E6%94%B9%E5%BC%A0%E9%87%8F-toc" style="margin-left:80px"><a href="#1.2.4.3%20%E4%BF%AE%E6%94%B9%E5%BC%A0%E9%87%8F">1.2.4.3 修改张量</a></p> 
<p id="1.2.5%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%BF%90%E7%AE%97-toc" style="margin-left:40px"><a href="#1.2.5%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%BF%90%E7%AE%97">1.2.5 张量的运算</a></p> 
<p id="1.2.5.1%20%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97-toc" style="margin-left:80px"><a href="#1.2.5.1%20%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97">1.2.5.1 数学运算</a></p> 
<p id="1.2.5.2%20%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97-toc" style="margin-left:80px"><a href="#1.2.5.2%20%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97">1.2.5.2 逻辑运算</a></p> 
<p id="1.2.5.3%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97-toc" style="margin-left:80px"><a href="#1.2.5.3%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97">1.2.5.3 矩阵运算</a></p> 
<p id="1.2.5.4%20%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6-toc" style="margin-left:80px"><a href="#1.2.5.4%20%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6">1.2.5.4 广播机制</a></p> 
<p id="%E4%B8%89.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86-toc" style="margin-left:40px"><a href="#%E4%B8%89.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">三. 使用pytorch实现数据预处理</a></p> 
<hr id="hr-toc">
<p></p> 
<h1 id="%E4%B8%80.%20%E6%A6%82%E5%BF%B5%EF%BC%9A%E5%BC%A0%E9%87%8F%E3%80%81%E7%AE%97%E5%AD%90">一. 概念：张量、算子</h1> 
<p><br><strong>张量：</strong>张量为我们提供了描述具有任意数量轴的n维数组的通用方法，是矩阵的扩展与延伸。张量类似于Numpy的多维数组(ndarray)的概念，可以具有任意多的维度。<br><strong>算子：</strong>算子是构建复杂机器学习模型的基础组件，包含一个函数f(x)的前向函数和反向函数。深度学习算法由一个个计算单元组成，我们称这些计算单元为算子。</p> 
<p></p> 
<h1 id="%E4%BA%8C.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E5%BC%A0%E9%87%8F%E8%BF%90%E7%AE%97">二. 使用pytorch实现张量运算</h1> 
<h2 id="1.2.1%20%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">
<br> 1.2.1 创建张量</h2> 
<p>创建一个张量可以有多种方式，如：指定数据创建、指定形状创建、指定区间创建等。</p> 
<h3 id="1.2.1.1%20%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E5%88%9B%E5%BB%BA%E5%BC%A0%E9%87%8F">1.2.1.1 指定数据创建张量</h3> 
<p>通过给定Python列表数据，可以创建任意维度的张量。</p> 
<p>（1）通过指定的Python列表数据[2.0, 3.0, 4.0]，创建一个一维张量。</p> 
<pre><code class="language-python">import torch
X = torch.tensor([2.0, 3.0, 4.0])
print(X)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="61" src="https://images2.imgbox.com/f0/19/gzXmgE2p_o.png" width="217"></p> 
<p>（2）通过指定的Python列表数据来创建类似矩阵（matrix）的二维张量。 </p> 
<pre><code class="language-python">import torch
X = torch.tensor([[1.0, 2.0, 3.0],
                 [4.0, 5.0, 6.0]])
print(X)
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="82" src="https://images2.imgbox.com/39/34/tmbkVKbN_o.png" width="233"></p> 
<p> （3）同样地，还可以创建维度为3、4...N等更复杂的多维张量。</p> 
<pre><code class="language-python">import torch
X = torch.tensor([[[1, 2, 3, 4, 5],
                   [6, 7, 8, 9, 10]],
                  [[11, 12, 13, 14, 15],
                   [16, 17, 18, 19, 20]]])
print(X)
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="166" src="https://images2.imgbox.com/81/36/3GGWVkPQ_o.png" width="336"></p> 
<p><span style="color:#fe2c24">（注：需要注意的是，张量在任何一个维度上的元素数量必须相等。下面尝试定义一个在同一维度上元素数量不等的张量。 ）</span></p> 
<h3 id="1.2.1.2%20%E6%8C%87%E5%AE%9A%E5%BD%A2%E7%8A%B6%E5%88%9B%E5%BB%BA">1.2.1.2 指定形状创建</h3> 
<p>如果要创建一个指定形状、元素数据相同的张量，可以使用torch.zeros、<code>torch.ones</code>、<code>torch.full</code>等。</p> 
<pre><code class="language-python">import torch
m, n = 2, 3
zeros_tensor = torch.zeros([m, n])
ones_tensor = torch.ones([m, n])
full_tensor = torch.full([m, n], 10)
print('zeros Tensor:n', zeros_tensor)
print('ones Tensor:n', ones_tensor)
print('full Tensor:n', full_tensor)</code></pre> 
<p>运行结果：<br>  <img alt="" height="280" src="https://images2.imgbox.com/1f/8a/PNftnXrA_o.png" width="250"></p> 
<h3 id="1.2.1.3%20%E6%8C%87%E5%AE%9A%E5%8C%BA%E9%97%B4%E5%88%9B%E5%BB%BA">1.2.1.3 指定区间创建</h3> 
<p>如果要在指定区间内创建张量，可以使用<code>torch.arange</code>、<code>torch.linspace</code>等。</p> 
<pre><code class="language-python">import torch
arange_tensor = torch.arange(start=1, end=5, step=1)
linspace_tensor = torch.linspace(start=1, end=5, steps=5)
print('arange Tensor:n', arange_tensor)
print('linspace Tensor:n', linspace_tensor)
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="140" src="https://images2.imgbox.com/1d/e5/S7vN4nkK_o.png" width="302"></p> 
<p></p> 
<h2 id="1.2.2%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%B1%9E%E6%80%A7">1.2.2 张量的属性</h2> 
<h3 id="1.2.2.1%20%E5%BC%A0%E9%87%8F%E7%9A%84%E5%BD%A2%E7%8A%B6">1.2.2.1 张量的形状</h3> 
<p>张量具有如下形状属性：</p> 
<ul>
<li>
<code>Tensor.ndim</code>：张量的维度，例如向量的维度为1，矩阵的维度为2。</li>
<li>
<code>Tensor.shape</code>： 张量每个维度上元素的数量。</li>
<li>
<code>Tensor.shape[n]</code>：张量第n维的大小。第n维也称为轴。</li>
<li>
<code>Tensor.numel()</code>：张量中全部元素的个数。</li>
</ul>
<pre><code class="language-python">import torch
X = torch.ones([2, 3, 4, 5])
print("Number of dimensions:", X.ndim)
print("Shape of Tensor:", X.shape)
print("Elements number along axis 0 of Tensor:", X.shape[0])
print("Elements number along the last axis of Tensor:", X.shape[-1])
print('Number of elements in Tensor: ', X.numel())</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="171" src="https://images2.imgbox.com/01/27/XTegqNED_o.png" width="497"></p> 
<p><span style="color:#fe2c24">（注：pytorch中Tensor.size（）显示的是矩阵的规模）</span></p> 
<h3 id="1.2.2.2%20%E5%BD%A2%E7%8A%B6%E7%9A%84%E6%94%B9%E5%8F%98">1.2.2.2 形状的改变</h3> 
<p>除了查看张量的形状外，重新设置张量的在实际编程中也具有重要意义。</p> 
<pre><code class="language-python">import torch
X = torch.tensor([[[1, 2, 3, 4, 5],
                   [6, 7, 8, 9, 10]],
                  [[11, 12, 13, 14, 15],
                   [16, 17, 18, 19, 20]],
                  [[21, 22, 23, 24, 25],
                   [26, 27, 28, 29, 30]]])
print('the shape of X:', X.shape)
print('after reshape:n', X.view([2, 5, 3]))</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="382" src="https://images2.imgbox.com/08/5b/cahPGrm8_o.png" width="386"></p> 
<p><span style="color:#fe2c24">（注：张量数据形状发生改变，而张量内的数据和元素顺序则没有发生改变）</span></p> 
<p>分别对上文定义的X进行reshape为[-1]操作和reshape为[1, 5, 6]两种操作，观察新张量的形状。</p> 
<pre><code class="language-python">import torch
ndim_3_Tensor = torch.tensor([[[1, 2, 3, 4, 5],
                                   [6, 7, 8, 9, 10]],
                                  [[11, 12, 13, 14, 15],
                                   [16, 17, 18, 19, 20]],
                                  [[21, 22, 23, 24, 25],
                                   [26, 27, 28, 29, 30]]])

new_Tensor1 = ndim_3_Tensor.reshape([-1])
print('new Tensor 1 shape: ', new_Tensor1.shape)
new_Tensor2 = ndim_3_Tensor.reshape([1, 5, 6])
print('new Tensor 2 shape: ', new_Tensor2.shape)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="82" src="https://images2.imgbox.com/63/79/xpZ0qkvn_o.png" width="442"></p> 
<p>从输出结果看，第一行代码中的第一个reshape操作将张量<code>reshape</code>为元素数量为30的一维向量；第四行代码中的第二个<code>reshape</code>操作中，0对应的维度的元素个数与原张量在该维度上的元素个数相同。</p> 
<p>除使用torch<code>.reshape</code>进行张量形状的改变外，还可以通过torch<code>.unsqueeze</code>将张量中的一个或多个维度中插入尺寸为1的维度。</p> 
<pre><code class="language-python">import torch
ones_Tensor = torch.ones([5, 10])
new_Tensor1 = torch.unsqueeze(ones_Tensor, dim=0)
print('new Tensor 1 shape: ', new_Tensor1.shape)
new_Tensor2 = torch.unsqueeze(ones_Tensor, dim=1)
print('new Tensor 2 shape: ', new_Tensor2.shape)
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="78" src="https://images2.imgbox.com/44/d2/UQpJIzR7_o.png" width="457"></p> 
<p><span style="color:#fe2c24">（注：torch.unsqueeze()中，dim表示插入维度的索引，即在哪里插入）</span></p> 
<h3 id="1.2.2.3%20%E5%BC%A0%E9%87%8F%E7%9A%84%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B">1.2.2.3 张量的数据类型</h3> 
<p>pytorch中可以通过<code>Tensor.dtype</code>来查看数据类型。</p> 
<p>Tensor的最基本数据类型有：</p> 
<ul>
<li>32位浮点型：torch.float32 (最常用）</li>
<li>64位浮点型：torch.float64 (最常用）</li>
<li>32位整型：torch.int32</li>
<li>16位整型：torch.int16</li>
<li>64位整型：torch.int64</li>
</ul>
<p>1）通过Python元素创建的张量，如果未指定：</p> 
<ul>
<li>对于Python整型数据，则会创建int64型张量。</li>
<li>对于Python浮点型数据，默认会创建float32型张量。</li>
</ul>
<p>2）通过Numpy数组创建的张量，则与其原来的数据类型保持相同。通过torch.tensor（）函数可以将Numpy数组转化为张量。</p> 
<pre><code class="language-python">import torch
print('Tensor dtype from python integers:', torch.tensor(1).dtype)
print('Tensor dtype from python floating point:', torch.tensor(1.0).dtype)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="88" src="https://images2.imgbox.com/ac/47/zXZh8Mll_o.png" width="557"></p> 
<p> 如果想改变张量的数据类型，可以通过调用Tensor.type来实现。</p> 
<pre><code class="language-python">import torch
float32_tensor = torch.tensor(1.0)
int64_tensor = float32_tensor.type(torch.int64)
print('Tensor after cast to int64:', int64_tensor.dtype)
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="57" src="https://images2.imgbox.com/3c/0e/jlFrOYFJ_o.png" width="407"></p> 
<h3 id="1.2.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BE%E5%A4%87%E4%BD%8D%E7%BD%AE"><strong>1.2.2.4 张量的设备位置</strong></h3> 
<p>初始化张量时可以通过place来指定其分配的设备位置，可支持的设备位置有三种：CPU、GPU和固定内存。固定内存也称为不可分页内存或锁页内存，它与GPU之间具有更高的读写效率，并且支持异步传输，这对网络整体性能会有进一步提升，但它的缺点是分配空间过多时可能会降低主机系统的性能，因为它减少了用于存储虚拟内存数据的可分页内存。当未指定设备位置时，张量默认设备位置和安装的pytorch版本一致，如安装了GPU版本的pytorch，则设备位置默认为GPU。</p> 
<p>如下代码创建了CPU上的张量，并通过<code>Tensor.device</code>查看张量所在的设备位置。</p> 
<pre><code class="language-python">import torch
# 创建CPU上的Tensor
cpu_Tensor = torch.tensor(1, device=torch.device('cpu'))
# 通过Tensor.place查看张量所在设备位置
print('cpu Tensor: ', cpu_Tensor.device)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="51" src="https://images2.imgbox.com/91/58/FNMqFOTC_o.png" width="187"></p> 
<p></p> 
<h2 id="1.2.3%20%E5%BC%A0%E9%87%8F%E4%B8%8ENumpy%E6%95%B0%E7%BB%84%E8%BD%AC%E6%8D%A2"><strong>1.2.3 张量与Numpy数组转换</strong></h2> 
<p>张量和Numpy数组可以相互转换。第1.2.2.3节中我们了解到torch.tensor()函数可以将Numpy数组转化为张量，也可以通过<code>Tensor.numpy()</code>函数将张量转化为Numpy数组。</p> 
<pre><code class="language-python">import torch
ndim_1_Tensor = torch.tensor([1., 2.])
# 将当前Tensor转化为numpy.ndarray
print('Tensor to convert:', ndim_1_Tensor.numpy())</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="60" src="https://images2.imgbox.com/8c/bc/OOX4epPE_o.png" width="272"></p> 
<p></p> 
<h2 id="1.2.4%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%AE%BF%E9%97%AE">1.2.4 张量的访问</h2> 
<h3 id="1.2.4.1%20%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87">1.2.4.1 索引和切片</h3> 
<p>我们可以通过索引或切片方便地访问或修改张量。pytorch使用标准的Python索引规则与Numpy索引规则，具有以下特点：</p> 
<ul>
<li>基于0−n的下标进行索引，如果下标为负数，则从尾部开始计算。</li>
<li>通过冒号“:”分隔切片参数start:stop:step来进行切片操作，也就是访问start到stop范围内的部分元素并生成一个新的序列。其中start为切片的起始位置，stop为切片的截止位置，step是切片的步长，这三个参数均可缺省。</li>
</ul>
<h3 id="1.2.4.2%20%E8%AE%BF%E9%97%AE%E5%BC%A0%E9%87%8F">1.2.4.2 访问张量</h3> 
<p>针对一维张量，对单个轴进行索引和切片。</p> 
<pre><code class="language-python">import torch
ndim_1_Tensor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])
print("Origin Tensor:", ndim_1_Tensor)
print("First element:", ndim_1_Tensor[0])
print("Last element:", ndim_1_Tensor[-1])
print("All element:", ndim_1_Tensor[:])
print("Before 3:", ndim_1_Tensor[:3])
print("Interval of 3:", ndim_1_Tensor[::3])</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="198" src="https://images2.imgbox.com/27/c5/I7JoSnIu_o.png" width="513"></p> 
<p>========================================================================= </p> 
<p><span style="color:#fe2c24"> （注：Pytorch不支持负数步长）</span></p> 
<pre><code class="language-python">import torch
ndim_1_Tensor = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])
print("Reverse:", ndim_1_Tensor[::-1])
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="142" src="https://images2.imgbox.com/8f/92/V9sK6Z2E_o.png" width="836"></p> 
<p>由运行结果可见，在pytorch中step不可为负。</p> 
<p>========================================================================= </p> 
<p>针对二维及以上维度的张量，在多个维度上进行索引或切片。索引或切片的第一个值对应第0维，第二个值对应第1维，以此类推，如果某个维度上未指定索引，则默认为“:”。</p> 
<pre><code class="language-python">import torch
ndim_2_Tensor = torch.tensor([[0, 1, 2, 3],
                              [4, 5, 6, 7],
                              [8, 9, 10, 11]])
print("Origin Tensor:n", ndim_2_Tensor)
print("First row:", ndim_2_Tensor[0])
print("First row:", ndim_2_Tensor[0, :])
print("First column:", ndim_2_Tensor[:, 0])
print("Last column:", ndim_2_Tensor[:, -1])
print("All element:n", ndim_2_Tensor[:])
print("First row and second column:", ndim_2_Tensor[0, 1])</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="382" src="https://images2.imgbox.com/5a/c8/H1GvHNKk_o.png" width="407"></p> 
<h3 id="1.2.4.3%20%E4%BF%AE%E6%94%B9%E5%BC%A0%E9%87%8F">1.2.4.3 修改张量</h3> 
<p>与访问张量类似，可以在单个或多个轴上通过索引或切片操作来修改张量。</p> 
<p><span style="color:#fe2c24">（注：索引或切片操作是在原地修改该张量的数值，原值并不会被保存，因此在修改张量时，应慎重使用索引或切片操作）</span></p> 
<p></p> 
<pre><code class="language-python">import torch
ndim_2_Tensor = torch.ones([2, 3], dtype=torch.float32)
print('Origin Tensor:n ', ndim_2_Tensor)
# 修改第1维为0
ndim_2_Tensor[0] = 0
print('change Tensor:n ', ndim_2_Tensor)
# 修改第1维为2.1
ndim_2_Tensor[0:1] = 2.1
print('change Tensor: n', ndim_2_Tensor)
# 修改全部Tensor
ndim_2_Tensor[...] = 3
print('change Tensor:n ', ndim_2_Tensor)</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="358" src="https://images2.imgbox.com/0b/6b/5S4ls6nG_o.png" width="357"></p> 
<p></p> 
<h2 id="1.2.5%20%E5%BC%A0%E9%87%8F%E7%9A%84%E8%BF%90%E7%AE%97">1.2.5 张量的运算</h2> 
<p>张量支持包括基础数学运算、逻辑运算、矩阵运算等100余种运算操作，以加法为例，有如下两种实现方式：<br> 1）使用pytorch <code>torch.add(x,y)</code>。<br> 2）使用张量类成员函数<code>x.add(y)</code>。</p> 
<pre><code class="language-python">import torch
x = torch.tensor([[1.1, 2.2], [3.3, 4.4]], dtype=torch.float64)
y = torch.tensor([[5.5, 6.6], [7.7, 8.8]], dtype=torch.float64)
print('Method 1:n', torch.add(x, y))
print('Method 2:n', x.add(y))
</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="197" src="https://images2.imgbox.com/1b/81/G0s3u82o_o.png" width="512"></p> 
<p> 从输出结果看，使用张量类成员函数x.add(y)和torch.add(x,y)具有相同的效果。</p> 
<h3 id="1.2.5.1%20%E6%95%B0%E5%AD%A6%E8%BF%90%E7%AE%97">1.2.5.1 数学运算</h3> 
<p>张量类的基础数学函数如下：</p> 
<pre><code class="language-python">x.abs()                       # 逐元素取绝对值
x.ceil()                      # 逐元素向上取整
x.floor()                     # 逐元素向下取整
x.round()                     # 逐元素四舍五入
x.exp()                       # 逐元素计算自然常数为底的指数
x.log()                       # 逐元素计算x的自然对数
x.reciprocal()                # 逐元素求倒数
x.square()                    # 逐元素计算平方
x.sqrt()                      # 逐元素计算平方根
x.sin()                       # 逐元素计算正弦
x.cos()                       # 逐元素计算余弦
x.add(y)                      # 逐元素加
x.subtract(y)                 # 逐元素减
x.multiply(y)                 # 逐元素乘（积）
x.divide(y)                   # 逐元素除
x.mod(y)                      # 逐元素除并取余
x.pow(y)                      # 逐元素幂
x.max()                       # 指定维度上元素最大值，默认为全部维度
x.min()                       # 指定维度上元素最小值，默认为全部维度
x.prod()                      # 指定维度上元素累乘，默认为全部维度
x.sum()                       # 指定维度上元素的和，默认为全部维度</code></pre> 
<p>同时，为了更方便地使用张量，pytorch对Python数学运算相关的魔法函数进行了重写，以下操作与上述结果相同。</p> 
<pre><code class="language-python">x + y  -&gt; x.add(y)            # 逐元素加
x - y  -&gt; x.subtract(y)       # 逐元素减
x * y  -&gt; x.multiply(y)       # 逐元素乘（积）
x / y  -&gt; x.divide(y)         # 逐元素除
x % y  -&gt; x.mod(y)            # 逐元素除并取余
x ** y -&gt; x.pow(y)            # 逐元素幂</code></pre> 
<h3 id="1.2.5.2%20%E9%80%BB%E8%BE%91%E8%BF%90%E7%AE%97">1.2.5.2 逻辑运算</h3> 
<p>张量类的逻辑运算函数如下：</p> 
<pre><code class="language-python">x.isfinite()                  # 判断Tensor中元素是否是有限的数字，即不包括inf与nan
x.equal_all(y)                # 判断两个Tensor的全部元素是否相等，并返回形状为[1]的布尔类Tensor
x.equal(y)                    # 判断两个Tensor的每个元素是否相等，并返回形状相同的布尔类Tensor
x.not_equal(y)                # 判断两个Tensor的每个元素是否不相等
x.less_than(y)                # 判断Tensor x的元素是否小于Tensor y的对应元素
x.less_equal(y)               # 判断Tensor x的元素是否小于或等于Tensor y的对应元素
x.greater_than(y)             # 判断Tensor x的元素是否大于Tensor y的对应元素
x.greater_equal(y)            # 判断Tensor x的元素是否大于或等于Tensor y的对应元素
x.allclose(y)                 # 判断两个Tensor的全部元素是否接近</code></pre> 
<p>同样地，pytorch对Python逻辑比较相关的魔法函数也进行了重写，这里不再赘述。</p> 
<h3 id="1.2.5.3%20%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97">1.2.5.3 矩阵运算</h3> 
<p>张量类还包含了矩阵运算相关的函数，如矩阵的转置、范数计算和乘法等。</p> 
<pre><code class="language-python">x.t()                         # 矩阵转置
x.transpose([1, 0])           # 交换第 0 维与第 1 维的顺序
x.norm('fro')                 # 矩阵的弗罗贝尼乌斯范数
x.dist(y, p=2)                # 矩阵（x-y）的2范数
x.matmul(y)                   # 矩阵乘法</code></pre> 
<p>有些矩阵运算中也支持大于两维的张量，比如matmul函数，对最后两个维度进行矩阵乘。比如x是形状为[j,k,n,m]的张量，另一个y是[j,k,m,p]的张量，则x.matmul(y)输出的张量形状为[j,k,n,p]。</p> 
<h3 id="1.2.5.4%20%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6">1.2.5.4 广播机制</h3> 
<p>pytorch的一些API在计算时支持广播(Broadcasting)机制，允许在一些运算时使用不同形状的张量。通常来讲，如果有一个形状较小和一个形状较大的张量，会希望多次使用较小的张量来对较大的张量执行某些操作，看起来像是形状较小的张量首先被扩展到和较大的张量形状一致，然后再做运算。</p> 
<p><strong>广播机制的条件</strong></p> 
<p>pytorch的广播机制主要遵循如下规则（参考Numpy广播机制）：</p> 
<p>1）每个张量至少为一维张量。</p> 
<p>2）从后往前比较张量的形状，当前维度的大小要么相等，要么其中一个等于1，要么其中一个不存在。</p> 
<pre><code class="language-python">import torch
x = torch.ones((2, 3, 4))
y = torch.ones((2, 3, 4))
z = x + y
print('broadcasting with two same shape tensor: ', z.shape)

x = torch.ones((2, 3, 1, 5))
y = torch.ones((3, 4, 1))
# 从后往前依次比较：
# 第一次：y的维度大小是1
# 第二次：x的维度大小是1
# 第三次：x和y的维度大小相等，都为3
# 第四次：y的维度不存在
# 所以x和y是可以广播的
z = x + y
print('broadcasting with two different shape tensor:', z.shape)
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="82" src="https://images2.imgbox.com/d6/9c/VPDbVpuQ_o.png" width="722"></p> 
<p>从输出结果看，x与y在上述两种情况中均遵循广播规则，因此在张量相加时可以广播。我们再定义两个shape分别为[2, 3, 4]和[2, 3, 6]的张量，观察这两个张量是否能够通过广播操作相加。</p> 
<pre><code class="language-python">import torch
x = torch.ones((2, 3, 4))
y = torch.ones((2, 3, 6))
z = x + y</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="137" src="https://images2.imgbox.com/f5/70/ltNMW2Mz_o.png" width="1062"></p> 
<p>从输出结果看，此时x和y是不能广播的，因为在第一次从后往前的比较中，4和6不相等，不符合广播规则。</p> 
<blockquote> 
 <p><strong>广播机制的计算规则</strong></p> 
 <p>现在我们知道在什么情况下两个张量是可以广播的。两个张量进行广播后的结果张量的形状计算规则如下：</p> 
 <p>1）如果两个张量shape的长度不一致，那么需要在较小长度的shape前添加1，直到两个张量的形状长度相等。</p> 
 <p>2） 保证两个张量形状相等之后，每个维度上的结果维度就是当前维度上较大的那个。</p> 
 <p>以张量x和y进行广播为例，x的shape为[2, 3, 1，5]，张量y的shape为[3，4，1]。首先张量y的形状长度较小，因此要将该张量形状补齐为[1, 3, 4, 1]，再对两个张量的每一维进行比较。从第一维看，x在一维上的大小为2，y为1，因此，结果张量在第一维的大小为2。以此类推，对每一维进行比较，得到结果张量的形状为[2, 3, 4, 5]。</p> 
 <p>由于矩阵乘法函数torch.matmul在深度学习中使用非常多，这里需要特别说明一下它的广播规则：</p> 
 <p>1）如果两个张量均为一维，则获得点积结果。</p> 
 <p>2） 如果两个张量都是二维的，则获得矩阵与矩阵的乘积。</p> 
 <p>3） 如果张量x是一维，y是二维，则将x的shape转换为[1, D]，与y进行矩阵相乘后再删除前置尺寸。</p> 
 <p>4） 如果张量x是二维，y是一维，则获得矩阵与向量的乘积。</p> 
 <p>5） 如果两个张量都是N维张量（N &gt; 2），则根据广播规则广播非矩阵维度（除最后两个维度外其余维度）。比如：如果输入x是形状为[j,1,n,m]的张量，另一个y是[k,m,p]的张量，则输出张量的形状为[j,k,n,p]。</p> 
</blockquote> 
<pre><code class="language-python">import torch
x = torch.ones([10, 1, 5, 2])
y = torch.ones([3, 2, 5])
z = torch.matmul(x, y)
print('After matmul:', z.shape)
</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="57" src="https://images2.imgbox.com/82/ce/2N6h8ZhW_o.png" width="407"></p> 
<p>从输出结果看，计算张量乘积时会使用到广播机制。 </p> 
<p></p> 
<h2 id="%E4%B8%89.%20%E4%BD%BF%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86">三. 使用pytorch实现数据预处理</h2> 
<p><strong>一、house_tiny.csv数据集处理：</strong><br> （1）读取数据集 house_tiny.csv</p> 
<pre><code class="language-python">import pandas as pd
data = pd.read_csv('house_tiny.csv')
print(data)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="167" src="https://images2.imgbox.com/86/f4/Q9T7RcZc_o.png" width="286"></p> 
<p>（2） 处理缺失值</p> 
<pre><code class="language-python">import pandas as pd
data = pd.read_csv('house_tiny.csv')
# 将data分成inputs和outputs
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
# 对于inputs中缺少的数值，用同一列的均值替换“NaN”项
# mean()求均值
inputs = inputs.fillna(inputs.mean())
print(inputs)
inputs = pd.get_dummies(inputs, dummy_na=True)
print(inputs)</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="322" src="https://images2.imgbox.com/f2/99/X7eQbHm6_o.png" width="375"></p> 
<p>（3）转换为张量格式</p> 
<pre><code class="language-python">import torch
import pandas as pd
data = pd.read_csv('house_tiny.csv')
inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]
inputs = inputs.fillna(inputs.mean())
inputs = pd.get_dummies(inputs, dummy_na=True)
x, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
print(x, 'n', y)</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="178" src="https://images2.imgbox.com/fd/e3/jOUNvS08_o.png" width="445"></p> 
<p></p> 
<p><strong>二、boston_house_prices.csv数据集处理：</strong></p> 
<p>（1）读取数据集 boston_house_prices.csv</p> 
<pre><code class="language-python">import pandas as pd
data = pd.read_csv('boston_house_prices.csv')
print(data)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="415" src="https://images2.imgbox.com/8b/7c/baIBXMxK_o.png" width="783"></p> 
<p>（2）无缺失值</p> 
<p>（3）转换为张量格式</p> 
<pre><code class="language-python">import torch
import pandas as pd
data = pd.read_csv('boston_house_prices.csv')
x = torch.tensor(data.values)
print(x)</code></pre> 
<p>运行结果：</p> 
<p><img alt="" height="387" src="https://images2.imgbox.com/cb/c1/KbNEP55C_o.png" width="757"></p> 
<p></p> 
<p><strong>三、Iris.csv数据集处理：</strong></p> 
<p>（1）读取数据集<strong> </strong>Iris.csv</p> 
<pre><code class="language-python">import pandas as pd
data = pd.read_csv('Iris.csv')
print(data)</code></pre> 
<p>运行结果：</p> 
<p> <img alt="" height="416" src="https://images2.imgbox.com/8c/74/yNvELqZL_o.png" width="607"></p> 
<p>（2）无缺失值</p> 
<p>（3）转换为张量格式</p> 
<pre><code class="language-python">import torch
import pandas as pd
data = pd.read_csv('Iris.csv')
inputs, outputs = data.iloc[:, 0:5], data.iloc[:, 5]
outputs = pd.get_dummies(outputs, dummy_na=True)
x, y = torch.tensor(inputs.values), torch.tensor(outputs.values)
print(x)
print('n')
print(y)</code></pre> 
<p>运行结果：</p> 
<pre><code class="language-python">tensor([[1.0000e+00, 5.1000e+00, 3.5000e+00, 1.4000e+00, 2.0000e-01],
        [2.0000e+00, 4.9000e+00, 3.0000e+00, 1.4000e+00, 2.0000e-01],
        [3.0000e+00, 4.7000e+00, 3.2000e+00, 1.3000e+00, 2.0000e-01],
        [4.0000e+00, 4.6000e+00, 3.1000e+00, 1.5000e+00, 2.0000e-01],
        [5.0000e+00, 5.0000e+00, 3.6000e+00, 1.4000e+00, 2.0000e-01],
        [6.0000e+00, 5.4000e+00, 3.9000e+00, 1.7000e+00, 4.0000e-01],
        [7.0000e+00, 4.6000e+00, 3.4000e+00, 1.4000e+00, 3.0000e-01],
        [8.0000e+00, 5.0000e+00, 3.4000e+00, 1.5000e+00, 2.0000e-01],
        [9.0000e+00, 4.4000e+00, 2.9000e+00, 1.4000e+00, 2.0000e-01],
        [1.0000e+01, 4.9000e+00, 3.1000e+00, 1.5000e+00, 1.0000e-01],
        [1.1000e+01, 5.4000e+00, 3.7000e+00, 1.5000e+00, 2.0000e-01],
        [1.2000e+01, 4.8000e+00, 3.4000e+00, 1.6000e+00, 2.0000e-01],
        [1.3000e+01, 4.8000e+00, 3.0000e+00, 1.4000e+00, 1.0000e-01],
        [1.4000e+01, 4.3000e+00, 3.0000e+00, 1.1000e+00, 1.0000e-01],
        [1.5000e+01, 5.8000e+00, 4.0000e+00, 1.2000e+00, 2.0000e-01],
        [1.6000e+01, 5.7000e+00, 4.4000e+00, 1.5000e+00, 4.0000e-01],
        [1.7000e+01, 5.4000e+00, 3.9000e+00, 1.3000e+00, 4.0000e-01],
        [1.8000e+01, 5.1000e+00, 3.5000e+00, 1.4000e+00, 3.0000e-01],
        [1.9000e+01, 5.7000e+00, 3.8000e+00, 1.7000e+00, 3.0000e-01],
        [2.0000e+01, 5.1000e+00, 3.8000e+00, 1.5000e+00, 3.0000e-01],
        [2.1000e+01, 5.4000e+00, 3.4000e+00, 1.7000e+00, 2.0000e-01],
        [2.2000e+01, 5.1000e+00, 3.7000e+00, 1.5000e+00, 4.0000e-01],
        [2.3000e+01, 4.6000e+00, 3.6000e+00, 1.0000e+00, 2.0000e-01],
        [2.4000e+01, 5.1000e+00, 3.3000e+00, 1.7000e+00, 5.0000e-01],
        [2.5000e+01, 4.8000e+00, 3.4000e+00, 1.9000e+00, 2.0000e-01],
        [2.6000e+01, 5.0000e+00, 3.0000e+00, 1.6000e+00, 2.0000e-01],
        [2.7000e+01, 5.0000e+00, 3.4000e+00, 1.6000e+00, 4.0000e-01],
        [2.8000e+01, 5.2000e+00, 3.5000e+00, 1.5000e+00, 2.0000e-01],
        [2.9000e+01, 5.2000e+00, 3.4000e+00, 1.4000e+00, 2.0000e-01],
        [3.0000e+01, 4.7000e+00, 3.2000e+00, 1.6000e+00, 2.0000e-01],
        [3.1000e+01, 4.8000e+00, 3.1000e+00, 1.6000e+00, 2.0000e-01],
        [3.2000e+01, 5.4000e+00, 3.4000e+00, 1.5000e+00, 4.0000e-01],
        [3.3000e+01, 5.2000e+00, 4.1000e+00, 1.5000e+00, 1.0000e-01],
        [3.4000e+01, 5.5000e+00, 4.2000e+00, 1.4000e+00, 2.0000e-01],
        [3.5000e+01, 4.9000e+00, 3.1000e+00, 1.5000e+00, 1.0000e-01],
        [3.6000e+01, 5.0000e+00, 3.2000e+00, 1.2000e+00, 2.0000e-01],
        [3.7000e+01, 5.5000e+00, 3.5000e+00, 1.3000e+00, 2.0000e-01],
        [3.8000e+01, 4.9000e+00, 3.1000e+00, 1.5000e+00, 1.0000e-01],
        [3.9000e+01, 4.4000e+00, 3.0000e+00, 1.3000e+00, 2.0000e-01],
        [4.0000e+01, 5.1000e+00, 3.4000e+00, 1.5000e+00, 2.0000e-01],
        [4.1000e+01, 5.0000e+00, 3.5000e+00, 1.3000e+00, 3.0000e-01],
        [4.2000e+01, 4.5000e+00, 2.3000e+00, 1.3000e+00, 3.0000e-01],
        [4.3000e+01, 4.4000e+00, 3.2000e+00, 1.3000e+00, 2.0000e-01],
        [4.4000e+01, 5.0000e+00, 3.5000e+00, 1.6000e+00, 6.0000e-01],
        [4.5000e+01, 5.1000e+00, 3.8000e+00, 1.9000e+00, 4.0000e-01],
        [4.6000e+01, 4.8000e+00, 3.0000e+00, 1.4000e+00, 3.0000e-01],
        [4.7000e+01, 5.1000e+00, 3.8000e+00, 1.6000e+00, 2.0000e-01],
        [4.8000e+01, 4.6000e+00, 3.2000e+00, 1.4000e+00, 2.0000e-01],
        [4.9000e+01, 5.3000e+00, 3.7000e+00, 1.5000e+00, 2.0000e-01],
        [5.0000e+01, 5.0000e+00, 3.3000e+00, 1.4000e+00, 2.0000e-01],
        [5.1000e+01, 7.0000e+00, 3.2000e+00, 4.7000e+00, 1.4000e+00],
        [5.2000e+01, 6.4000e+00, 3.2000e+00, 4.5000e+00, 1.5000e+00],
        [5.3000e+01, 6.9000e+00, 3.1000e+00, 4.9000e+00, 1.5000e+00],
        [5.4000e+01, 5.5000e+00, 2.3000e+00, 4.0000e+00, 1.3000e+00],
        [5.5000e+01, 6.5000e+00, 2.8000e+00, 4.6000e+00, 1.5000e+00],
        [5.6000e+01, 5.7000e+00, 2.8000e+00, 4.5000e+00, 1.3000e+00],
        [5.7000e+01, 6.3000e+00, 3.3000e+00, 4.7000e+00, 1.6000e+00],
        [5.8000e+01, 4.9000e+00, 2.4000e+00, 3.3000e+00, 1.0000e+00],
        [5.9000e+01, 6.6000e+00, 2.9000e+00, 4.6000e+00, 1.3000e+00],
        [6.0000e+01, 5.2000e+00, 2.7000e+00, 3.9000e+00, 1.4000e+00],
        [6.1000e+01, 5.0000e+00, 2.0000e+00, 3.5000e+00, 1.0000e+00],
        [6.2000e+01, 5.9000e+00, 3.0000e+00, 4.2000e+00, 1.5000e+00],
        [6.3000e+01, 6.0000e+00, 2.2000e+00, 4.0000e+00, 1.0000e+00],
        [6.4000e+01, 6.1000e+00, 2.9000e+00, 4.7000e+00, 1.4000e+00],
        [6.5000e+01, 5.6000e+00, 2.9000e+00, 3.6000e+00, 1.3000e+00],
        [6.6000e+01, 6.7000e+00, 3.1000e+00, 4.4000e+00, 1.4000e+00],
        [6.7000e+01, 5.6000e+00, 3.0000e+00, 4.5000e+00, 1.5000e+00],
        [6.8000e+01, 5.8000e+00, 2.7000e+00, 4.1000e+00, 1.0000e+00],
        [6.9000e+01, 6.2000e+00, 2.2000e+00, 4.5000e+00, 1.5000e+00],
        [7.0000e+01, 5.6000e+00, 2.5000e+00, 3.9000e+00, 1.1000e+00],
        [7.1000e+01, 5.9000e+00, 3.2000e+00, 4.8000e+00, 1.8000e+00],
        [7.2000e+01, 6.1000e+00, 2.8000e+00, 4.0000e+00, 1.3000e+00],
        [7.3000e+01, 6.3000e+00, 2.5000e+00, 4.9000e+00, 1.5000e+00],
        [7.4000e+01, 6.1000e+00, 2.8000e+00, 4.7000e+00, 1.2000e+00],
        [7.5000e+01, 6.4000e+00, 2.9000e+00, 4.3000e+00, 1.3000e+00],
        [7.6000e+01, 6.6000e+00, 3.0000e+00, 4.4000e+00, 1.4000e+00],
        [7.7000e+01, 6.8000e+00, 2.8000e+00, 4.8000e+00, 1.4000e+00],
        [7.8000e+01, 6.7000e+00, 3.0000e+00, 5.0000e+00, 1.7000e+00],
        [7.9000e+01, 6.0000e+00, 2.9000e+00, 4.5000e+00, 1.5000e+00],
        [8.0000e+01, 5.7000e+00, 2.6000e+00, 3.5000e+00, 1.0000e+00],
        [8.1000e+01, 5.5000e+00, 2.4000e+00, 3.8000e+00, 1.1000e+00],
        [8.2000e+01, 5.5000e+00, 2.4000e+00, 3.7000e+00, 1.0000e+00],
        [8.3000e+01, 5.8000e+00, 2.7000e+00, 3.9000e+00, 1.2000e+00],
        [8.4000e+01, 6.0000e+00, 2.7000e+00, 5.1000e+00, 1.6000e+00],
        [8.5000e+01, 5.4000e+00, 3.0000e+00, 4.5000e+00, 1.5000e+00],
        [8.6000e+01, 6.0000e+00, 3.4000e+00, 4.5000e+00, 1.6000e+00],
        [8.7000e+01, 6.7000e+00, 3.1000e+00, 4.7000e+00, 1.5000e+00],
        [8.8000e+01, 6.3000e+00, 2.3000e+00, 4.4000e+00, 1.3000e+00],
        [8.9000e+01, 5.6000e+00, 3.0000e+00, 4.1000e+00, 1.3000e+00],
        [9.0000e+01, 5.5000e+00, 2.5000e+00, 4.0000e+00, 1.3000e+00],
        [9.1000e+01, 5.5000e+00, 2.6000e+00, 4.4000e+00, 1.2000e+00],
        [9.2000e+01, 6.1000e+00, 3.0000e+00, 4.6000e+00, 1.4000e+00],
        [9.3000e+01, 5.8000e+00, 2.6000e+00, 4.0000e+00, 1.2000e+00],
        [9.4000e+01, 5.0000e+00, 2.3000e+00, 3.3000e+00, 1.0000e+00],
        [9.5000e+01, 5.6000e+00, 2.7000e+00, 4.2000e+00, 1.3000e+00],
        [9.6000e+01, 5.7000e+00, 3.0000e+00, 4.2000e+00, 1.2000e+00],
        [9.7000e+01, 5.7000e+00, 2.9000e+00, 4.2000e+00, 1.3000e+00],
        [9.8000e+01, 6.2000e+00, 2.9000e+00, 4.3000e+00, 1.3000e+00],
        [9.9000e+01, 5.1000e+00, 2.5000e+00, 3.0000e+00, 1.1000e+00],
        [1.0000e+02, 5.7000e+00, 2.8000e+00, 4.1000e+00, 1.3000e+00],
        [1.0100e+02, 6.3000e+00, 3.3000e+00, 6.0000e+00, 2.5000e+00],
        [1.0200e+02, 5.8000e+00, 2.7000e+00, 5.1000e+00, 1.9000e+00],
        [1.0300e+02, 7.1000e+00, 3.0000e+00, 5.9000e+00, 2.1000e+00],
        [1.0400e+02, 6.3000e+00, 2.9000e+00, 5.6000e+00, 1.8000e+00],
        [1.0500e+02, 6.5000e+00, 3.0000e+00, 5.8000e+00, 2.2000e+00],
        [1.0600e+02, 7.6000e+00, 3.0000e+00, 6.6000e+00, 2.1000e+00],
        [1.0700e+02, 4.9000e+00, 2.5000e+00, 4.5000e+00, 1.7000e+00],
        [1.0800e+02, 7.3000e+00, 2.9000e+00, 6.3000e+00, 1.8000e+00],
        [1.0900e+02, 6.7000e+00, 2.5000e+00, 5.8000e+00, 1.8000e+00],
        [1.1000e+02, 7.2000e+00, 3.6000e+00, 6.1000e+00, 2.5000e+00],
        [1.1100e+02, 6.5000e+00, 3.2000e+00, 5.1000e+00, 2.0000e+00],
        [1.1200e+02, 6.4000e+00, 2.7000e+00, 5.3000e+00, 1.9000e+00],
        [1.1300e+02, 6.8000e+00, 3.0000e+00, 5.5000e+00, 2.1000e+00],
        [1.1400e+02, 5.7000e+00, 2.5000e+00, 5.0000e+00, 2.0000e+00],
        [1.1500e+02, 5.8000e+00, 2.8000e+00, 5.1000e+00, 2.4000e+00],
        [1.1600e+02, 6.4000e+00, 3.2000e+00, 5.3000e+00, 2.3000e+00],
        [1.1700e+02, 6.5000e+00, 3.0000e+00, 5.5000e+00, 1.8000e+00],
        [1.1800e+02, 7.7000e+00, 3.8000e+00, 6.7000e+00, 2.2000e+00],
        [1.1900e+02, 7.7000e+00, 2.6000e+00, 6.9000e+00, 2.3000e+00],
        [1.2000e+02, 6.0000e+00, 2.2000e+00, 5.0000e+00, 1.5000e+00],
        [1.2100e+02, 6.9000e+00, 3.2000e+00, 5.7000e+00, 2.3000e+00],
        [1.2200e+02, 5.6000e+00, 2.8000e+00, 4.9000e+00, 2.0000e+00],
        [1.2300e+02, 7.7000e+00, 2.8000e+00, 6.7000e+00, 2.0000e+00],
        [1.2400e+02, 6.3000e+00, 2.7000e+00, 4.9000e+00, 1.8000e+00],
        [1.2500e+02, 6.7000e+00, 3.3000e+00, 5.7000e+00, 2.1000e+00],
        [1.2600e+02, 7.2000e+00, 3.2000e+00, 6.0000e+00, 1.8000e+00],
        [1.2700e+02, 6.2000e+00, 2.8000e+00, 4.8000e+00, 1.8000e+00],
        [1.2800e+02, 6.1000e+00, 3.0000e+00, 4.9000e+00, 1.8000e+00],
        [1.2900e+02, 6.4000e+00, 2.8000e+00, 5.6000e+00, 2.1000e+00],
        [1.3000e+02, 7.2000e+00, 3.0000e+00, 5.8000e+00, 1.6000e+00],
        [1.3100e+02, 7.4000e+00, 2.8000e+00, 6.1000e+00, 1.9000e+00],
        [1.3200e+02, 7.9000e+00, 3.8000e+00, 6.4000e+00, 2.0000e+00],
        [1.3300e+02, 6.4000e+00, 2.8000e+00, 5.6000e+00, 2.2000e+00],
        [1.3400e+02, 6.3000e+00, 2.8000e+00, 5.1000e+00, 1.5000e+00],
        [1.3500e+02, 6.1000e+00, 2.6000e+00, 5.6000e+00, 1.4000e+00],
        [1.3600e+02, 7.7000e+00, 3.0000e+00, 6.1000e+00, 2.3000e+00],
        [1.3700e+02, 6.3000e+00, 3.4000e+00, 5.6000e+00, 2.4000e+00],
        [1.3800e+02, 6.4000e+00, 3.1000e+00, 5.5000e+00, 1.8000e+00],
        [1.3900e+02, 6.0000e+00, 3.0000e+00, 4.8000e+00, 1.8000e+00],
        [1.4000e+02, 6.9000e+00, 3.1000e+00, 5.4000e+00, 2.1000e+00],
        [1.4100e+02, 6.7000e+00, 3.1000e+00, 5.6000e+00, 2.4000e+00],
        [1.4200e+02, 6.9000e+00, 3.1000e+00, 5.1000e+00, 2.3000e+00],
        [1.4300e+02, 5.8000e+00, 2.7000e+00, 5.1000e+00, 1.9000e+00],
        [1.4400e+02, 6.8000e+00, 3.2000e+00, 5.9000e+00, 2.3000e+00],
        [1.4500e+02, 6.7000e+00, 3.3000e+00, 5.7000e+00, 2.5000e+00],
        [1.4600e+02, 6.7000e+00, 3.0000e+00, 5.2000e+00, 2.3000e+00],
        [1.4700e+02, 6.3000e+00, 2.5000e+00, 5.0000e+00, 1.9000e+00],
        [1.4800e+02, 6.5000e+00, 3.0000e+00, 5.2000e+00, 2.0000e+00],
        [1.4900e+02, 6.2000e+00, 3.4000e+00, 5.4000e+00, 2.3000e+00],
        [1.5000e+02, 5.9000e+00, 3.0000e+00, 5.1000e+00, 1.8000e+00]],
       dtype=torch.float64)


tensor([[1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [1, 0, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 1, 0, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0],
        [0, 0, 1, 0]], dtype=torch.uint8)

</code></pre>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>