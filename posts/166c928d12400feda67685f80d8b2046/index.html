<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>基于YOLOv4的口罩检测的系统研究和实现 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">基于YOLOv4的口罩检测的系统研究和实现</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p></p> 
<p id="main-toc"><strong>目录</strong></p> 
<p id="%E4%B8%80%E3%80%81%E5%AE%9E%E7%8E%B0%E6%95%88%E6%9E%9C-toc" style="margin-left:0px"><a href="#%E4%B8%80%E3%80%81%E5%AE%9E%E7%8E%B0%E6%95%88%E6%9E%9C">一、实现效果</a></p> 
<p id="1.1%E3%80%81%E7%94%A8%E6%88%B7%E6%8F%90%E4%BA%A4%E5%9B%BE%E7%89%87%EF%BC%8C%E6%A3%80%E6%B5%8B%E6%98%AF%E5%90%A6%E5%B8%A6%E5%8F%A3%E7%BD%A9-toc" style="margin-left:80px"><a href="#1.1%E3%80%81%E7%94%A8%E6%88%B7%E6%8F%90%E4%BA%A4%E5%9B%BE%E7%89%87%EF%BC%8C%E6%A3%80%E6%B5%8B%E6%98%AF%E5%90%A6%E5%B8%A6%E5%8F%A3%E7%BD%A9">1.1、用户提交图片，检测是否带口罩</a></p> 
<p id="%C2%A01.2%E3%80%81%E5%AE%9E%E6%97%B6%E6%A3%80%E6%B5%8B-toc" style="margin-left:80px"><a href="#%C2%A01.2%E3%80%81%E5%AE%9E%E6%97%B6%E6%A3%80%E6%B5%8B"> 1.2、实时检测</a></p> 
<p id="1.3%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%EF%BC%9A-toc" style="margin-left:80px"><a href="#1.3%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%EF%BC%9A">1.3、数据集介绍</a></p> 
<p id="%E4%BA%8C%E3%80%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86-toc" style="margin-left:0px"><a href="#%E4%BA%8C%E3%80%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86">二、算法原理</a></p> 
<p id="2.1%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%EF%BC%9AMosaic-toc" style="margin-left:80px"><a href="#2.1%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%EF%BC%9AMosaic">2.1数据增强：Mosaic</a></p> 
<p id="2.2%E7%BD%91%E8%B7%AF%E6%A8%A1%E5%9E%8B-toc" style="margin-left:80px"><a href="#2.2%E7%BD%91%E8%B7%AF%E6%A8%A1%E5%9E%8B">2.2网路模型</a></p> 
<p id="2.2.1SPP-toc" style="margin-left:80px"><a href="#2.2.1SPP">2.2.1SPP</a></p> 
<p id="2.2.2Modified%20PAN-toc" style="margin-left:80px"><a href="#2.2.2Modified%20PAN">2.2.2Modified PAN</a></p> 
<p id="2.2.3%E5%AE%8C%E6%95%B4%E7%BB%93%E6%9E%84-toc" style="margin-left:80px"><a href="#2.2.3%E5%AE%8C%E6%95%B4%E7%BB%93%E6%9E%84">2.2.3完整结构</a></p> 
<p id="%E5%AE%9E%E9%AA%8C%E8%BF%87%E7%A8%8B-toc" style="margin-left:0px"><a href="#%E5%AE%9E%E9%AA%8C%E8%BF%87%E7%A8%8B">三、实验过程</a></p> 
<p id="3.1%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5-toc" style="margin-left:80px"><a href="#3.1%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5">3.1推理阶段</a></p> 
<p id="3.1.1%E8%AF%BB%E5%8F%96%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6-toc" style="margin-left:80px"><a href="#3.1.1%E8%AF%BB%E5%8F%96%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6">3.1.1读取并加载权重文件</a></p> 
<p id="3.1.2%E8%AF%BB%E5%8F%96%E5%9B%BE%E7%89%87%E5%B9%B6%E5%B0%86%E8%BE%93%E5%85%A5%E7%9A%84%E5%9B%BE%E7%89%87%E6%8C%89%E7%85%A7%E8%A6%81%E6%B1%82%E7%9A%84%E5%A4%A7%E5%B0%8F%E8%BF%9B%E8%A1%8C%E8%B0%83%E6%95%B4-toc" style="margin-left:80px"><a href="#3.1.2%E8%AF%BB%E5%8F%96%E5%9B%BE%E7%89%87%E5%B9%B6%E5%B0%86%E8%BE%93%E5%85%A5%E7%9A%84%E5%9B%BE%E7%89%87%E6%8C%89%E7%85%A7%E8%A6%81%E6%B1%82%E7%9A%84%E5%A4%A7%E5%B0%8F%E8%BF%9B%E8%A1%8C%E8%B0%83%E6%95%B4">3.1.2读取图片并将输入的图片按照要求的大小进行调整</a></p> 
<p id="3.1.4%E4%BB%A3%E7%A0%81-toc" style="margin-left:80px"><a href="#3.1.4%E4%BB%A3%E7%A0%81">3.1.4代码</a></p> 
<p id="3.2%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5-toc" style="margin-left:80px"><a href="#3.2%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5">3.2训练阶段</a></p> 
<p id="3.2.1%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84-toc" style="margin-left:80px"><a href="#3.2.1%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84">3.2.1 网络结构</a></p> 
<p id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90-toc" style="margin-left:0px"><a href="#%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">四、结果分析</a></p> 
<p id="4.1%EF%BC%9A%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%E6%95%88%E6%9E%9C-toc" style="margin-left:80px"><a href="#4.1%EF%BC%9A%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%E6%95%88%E6%9E%9C">4.1：推理阶段效果</a></p> 
<p id="%E4%BA%94%E3%80%81%E7%BB%93%E8%AF%AD-toc" style="margin-left:0px"><a href="#%E4%BA%94%E3%80%81%E7%BB%93%E8%AF%AD">五、结语</a></p> 
<p id="%E5%85%AD%E3%80%81%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE-toc" style="margin-left:0px"><a href="#%E5%85%AD%E3%80%81%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">六、参考文献</a></p> 
<hr id="hr-toc">
<h1>一、实现效果</h1> 
<h3 id="1.1%E3%80%81%E7%94%A8%E6%88%B7%E6%8F%90%E4%BA%A4%E5%9B%BE%E7%89%87%EF%BC%8C%E6%A3%80%E6%B5%8B%E6%98%AF%E5%90%A6%E5%B8%A6%E5%8F%A3%E7%BD%A9">1.1、用户提交图片，检测是否带口罩</h3> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/81/50/HPQ3Z69I_o.png" width="1200"></p> 
<h3 id="%C2%A01.2%E3%80%81%E5%AE%9E%E6%97%B6%E6%A3%80%E6%B5%8B"> 1.2、实时检测</h3> 
<p><img alt="" height="1200" src="https://images2.imgbox.com/28/9c/fafdspi9_o.png" width="1200"></p> 
<p></p> 
<h3 id="%E2%80%8B"><img alt="" height="1200" src="https://images2.imgbox.com/90/c5/WC1MBrVd_o.png" width="1200"></h3> 
<h3 id="1.3%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D%EF%BC%9A">1.3、数据集介绍</h3> 
<p>训练集（<a class="link-info" href="https://pan.baidu.com/s/1lnBp1aqAXgUEcW1vs0IhUg" title="下载地址">下载地址</a>（提取码：067d））</p> 
<p><img alt="" height="848" src="https://images2.imgbox.com/97/66/iMuzgUYU_o.png" width="1200"></p> 
<p>测试集</p> 
<p><img alt="" height="803" src="https://images2.imgbox.com/fd/f2/lLHTjHTV_o.png" width="1200"></p> 
<h1 id="%E4%BA%8C%E3%80%81%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86"><strong>二、算法原理</strong></h1> 
<p style="margin-left:0">以YOLOv4为例的深度学习的目标检测算法，是通过卷积神经网络学习预选框与真实框之间的坐标、长宽的回归模型和类别的分类模型，以此完成定位和分类任务。</p> 
<h3 id="2.1%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%EF%BC%9AMosaic" style="margin-left:0px"><strong>2.1数据增强：Mosaic</strong></h3> 
<p style="margin-left:0">YOLOv4中提出了新的数据增强方式。通过将四张随机选取的图片进行拼接得到一张新的图片。设要生成新图片大小为（W,H），随机得到左上图片的尺寸为（cutX，cutY）,则在选取的四种图片中的第一张图片随机切割长宽为（cutX，cutY）的图片填充到左上角，其他三次同理，切割大小分别为(cutX，H-cutY)，(W-cutX，cutY)，(W-cutX，H-cutY)。</p> 
<h3 id="2.2%E7%BD%91%E8%B7%AF%E6%A8%A1%E5%9E%8B" style="margin-left:0px"><strong>2.2网路模型</strong></h3> 
<p style="margin-left:0">YOLOv4在先前的网络基础上进行了修改，加入SPP结构,将FPN结构改为采用Modified PAN结构。</p> 
<h3 id="2.2.1SPP" style="margin-left:0"><strong>2.2.1SPP</strong></h3> 
<p style="margin-left:0">SPP全称为Spatial Pyramid Pooling ，即空间金字塔池化。如图二所示</p> 
<p style="margin-left:0">YOLOv4中的实现是对进行 5 × 5 、 9 × 9  、 13 × 13  的最大池化，完成池化后，将三者进行concatenete，连接成一个特征图。SPP网络用在YOLOv4中的目的是增加网络的感受野。</p> 
<p style="margin-left:0px;text-align:center"><img alt="" height="420" src="https://images2.imgbox.com/a2/0a/Iv0sZd7W_o.png" width="638"></p> 
<p style="text-align:center">图二：SPP</p> 
<h3 id="2.2.2Modified%20PAN" style="margin-left:0px"><strong>2.2.2Modified PAN</strong></h3> 
<p style="margin-left:0cm;text-align:justify">PAN是在FPN的基础上新添加了一条由下至上的路径。FPN即特征金字塔网络,如图三（a）所示，PAN如图三（a）（b）所示</p> 
<p style="margin-left:0">FPN认为网络路径中靠前的网络层，即采样倍数较小的层，图片更为清晰，但语义能力差，相反网络路径中靠后的网络层，即采样倍数较大的层，图片较模糊，但语义能力强。使用短接将二者连接起来形成一个特征金字塔。</p> 
<p style="margin-left:0">而PAN的研究发现，靠前网络层的传播路径仍然很漫长，如红线所示。注意不能以图片看见的层数作为实际的层数。绿色框代表了网络中的很多层。于是又新添加了一条有下至上的路径，使得信息更快的传播。</p> 
<p style="margin-left:0"><img alt="" height="332" src="https://images2.imgbox.com/c0/4c/QUrZHNrN_o.png" width="718"></p> 
<p style="margin-left:0;text-align:center">图三：FPN与PAN</p> 
<p style="margin-left:0">YOLOv4对于原本的PAN又进行了修改，改变了原本的最终多层的特征融合方式，由数值的相加改为了维度的链接。使得多层的信息更好的融合保存下来。其如图 4所示</p> 
<p style="text-align:center"><img alt="" height="390" src="https://images2.imgbox.com/3c/50/LV3rUpbb_o.png" width="644"></p> 
<p style="margin-left:0;text-align:center">图四：PAN与Modifed PAN</p> 
<h3 id="2.2.3%E5%AE%8C%E6%95%B4%E7%BB%93%E6%9E%84" style="margin-left:0"><strong>2.2.3完整结构</strong></h3> 
<p style="margin-left:0"><img alt="" height="584" src="https://images2.imgbox.com/ce/9e/yqwYoE5X_o.png" width="830"></p> 
<p style="margin-left:0;text-align:center">图五：YOLOvv4输入图片大小为608*608时的网络结构。</p> 
<p style="margin-left:0">如图所示，YOLOv4的网络大致可以分为CSPDarknet53，SPP，PANet，和最终输出结果的YoLo Head四个部分组成。可以看到YOLOv4的最终输出是由三个YoLo Head的输出组成，它们分别在网络结构的不同地方采样，采样倍数分别为，8倍，16倍，32倍。靠前的采样感受野较小，更加倾向于检测小目标，靠后的采样感受野大，能更好的检测大目标。三者的联合便使得此算法对各种大小的目标都能很好的进行检测。</p> 
<h1 id="%E5%AE%9E%E9%AA%8C%E8%BF%87%E7%A8%8B"><strong>三、实验过程</strong></h1> 
<h3 id="3.1%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5" style="margin-left:0px"><strong>3.1推理阶段</strong></h3> 
<p style="margin-left:0">需要指定运行时参数，设定要检测的类数量、权重文件位置、图片位置、检测结果图片的大小。如：</p> 
<pre><code class="language-python">D:/cv/yolo4/pytorch-YOLOv4/models.py 80 weight/yolov4.pth data/dog.jpg 608 608</code></pre> 
<p style="margin-left:0px">需要指定<strong>表示检测dog.jpg这张图片中包含coco数据集中的80个类别中的目标。</strong></p> 
<h3 id="3.1.1%E8%AF%BB%E5%8F%96%E5%B9%B6%E5%8A%A0%E8%BD%BD%E6%9D%83%E9%87%8D%E6%96%87%E4%BB%B6" style="margin-left:0"><strong>3.1.1读取并加载权重文件</strong></h3> 
<pre><code class="language-python">model = Yolov4(yolov4weight=None,n_classes=n_classes, inference=True)
#加载模型参数
pretrained_dict = torch.load(weightfile, map_location=torch.device('cuda' if use_cuda else 'cpu'))
model.load_state_dict(pretrained_dict)
if use_cuda:
               model.cuda()</code></pre> 
<h3 id="3.1.2%E8%AF%BB%E5%8F%96%E5%9B%BE%E7%89%87%E5%B9%B6%E5%B0%86%E8%BE%93%E5%85%A5%E7%9A%84%E5%9B%BE%E7%89%87%E6%8C%89%E7%85%A7%E8%A6%81%E6%B1%82%E7%9A%84%E5%A4%A7%E5%B0%8F%E8%BF%9B%E8%A1%8C%E8%B0%83%E6%95%B4" style="margin-left:0"><strong>3.1.2读取图片并将输入的图片按照要求的大小进行调整</strong></h3> 
<pre><code class="language-python">img = cv2.imread(imgfile)
sizedimg = cv2.resize(img, (width, height))
sizedimg = cv2.cvtColor(sizedimg, cv2.COLOR_BGR2RGB)</code></pre> 
<p id="3.1.3%E6%A0%B9%E6%8D%AE%E6%A8%A1%E5%9E%8B%E8%BE%93%E5%87%BA%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A1%86%E4%BF%A1%E6%81%AF%E8%BF%9B%E8%A1%8CNMS%E5%90%8E%E7%BB%98%E5%88%B6" style="margin-left:.0001pt"><strong><span style="background-color:#ffffff"><span style="color:#000000">3.1.3根据模型输出的预测框信息进行</span><span style="color:#000000">NMS</span><span style="color:#000000">后绘制</span></span></strong></p> 
<p>      NMS即非极大值抑制，抑制不是极大值的元素。通过NMS去除重复的预测框。<br> 非极大值抑制的方法是：先假设有6个矩形框，根据分类器的类别分类概率做排序，假设从小到大属于某一类的概率分别为A、B、C、D、E、F。<br> (1)从最大概率矩形框F开始，分别判断A~E与F的重叠度IOU是否大于某个设定的阈值;<br> (2)假设B、D与F的重叠度超过阈值，那么就扔掉B、D；并标记第一个矩形框F，是我们保留下来的。<br> (3)从剩下的矩形框A、C、E中，选择概率最大的E，然后判断E与A、C的重叠度，重叠度大于一定的阈值，那么就扔掉；并标记E是我们保留下来的第二个矩形框。<br> 就这样一直重复，找到所有被保留下来的矩形框。<br> NMS的代码如下：</p> 
<pre><code class="language-python">def nms_cpu(boxes, confs, nms_thresh=0.5, min_mode=False):
    # print(boxes.shape)
    x1 = boxes[:, 0]
    y1 = boxes[:, 1]
    x2 = boxes[:, 2]
    y2 = boxes[:, 3]
    areas = (x2 - x1) * (y2 - y1)
    order = confs.argsort()[::-1]
    keep = []
    while order.size &gt; 0:
        idx_self = order[0]
        idx_other = order[1:]
        keep.append(idx_self)
        xx1 = np.maximum(x1[idx_self], x1[idx_other])
        yy1 = np.maximum(y1[idx_self], y1[idx_other])
        xx2 = np.minimum(x2[idx_self], x2[idx_other])
        yy2 = np.minimum(y2[idx_self], y2[idx_other])
        w = np.maximum(0.0, xx2 - xx1)
        h = np.maximum(0.0, yy2 - yy1)
        inter = w * h
        if min_mode:
            over = inter / np.minimum(areas[order[0]], areas[order[1:]])
        else:
            over = inter / (areas[order[0]] + areas[order[1:]] - inter)
        inds = np.where(over &lt;= nms_thresh)[0]
        order = order[inds + 1]
    return np.array(keep)</code></pre> 
<h3 id="3.1.4%E4%BB%A3%E7%A0%81" style="margin-left:0"><strong>3.1.4代码</strong></h3> 
<pre><code class="language-python">"""
Run a rest API exposing the yolov5s object detection model
"""
import argparse
import io

import torch
from PIL import Image
from flask import Flask, request

app = Flask(__name__)

DETECTION_URL = "/v1/object-detection/yolov5s"


@app.route(DETECTION_URL, methods=["POST"])
def predict():
    if not request.method == "POST":
        return

    if request.files.get("image"):
        image_file = request.files["image"]
        image_bytes = image_file.read()

        img = Image.open(io.BytesIO(image_bytes))

        results = model(img, size=640)  # reduce size=320 for faster inference
        return results.pandas().xyxy[0].to_json(orient="records")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Flask API exposing YOLOv5 model")
    parser.add_argument("--port", default=5000, type=int, help="port number")
    args = parser.parse_args()

    # model = torch.hub.load("ultralytics/yolov5", "yolov5s", force_reload=True)  # force_reload to recache
    model=torch.hub.load('../../', 'custom', path='./best.pt', source='local')
    app.run(host="0.0.0.0", port=args.port)  # debug=True causes Restarting with stat
</code></pre> 
<h3 id="3.2%E8%AE%AD%E7%BB%83%E9%98%B6%E6%AE%B5" style="margin-left:0px"><strong>3.2训练阶段</strong></h3> 
<p style="margin-left:0">需要指定学习率，预训练模型参数，类别数量，训练数据。其他通用设定都已在cfg文件中指定。如输入：</p> 
<pre><code>D:/cv/yolo4/pytorch-YOLOv4/train.py -l 0.001  -pretrained weight/yolov4.pth -classes 3 -dir D:/cv/yolo4/pytorch-YOLOv4/data1</code></pre> 
<p style="margin-left:0">表示训练一个二分类的网络模型，本次实验为检测有无。</p> 
<h3 id="3.2.1%20%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84" style="margin-left:0"><strong>3.2.1 网络结构</strong></h3> 
<p style="margin-left:0">根据图 来进行网路的搭建。</p> 
<p style="margin-left:0">因为每次卷积后都要进行Batch Normalization操作，所以将其抽象出来为一个模块。以及每个残差块的构建。每个残差块都为3层，将输入进行两次卷积，第三层为输入和两次卷积后结果的和。</p> 
<p style="margin-left:0">卷积BN模块代码</p> 
<pre><code class="language-python">class Conv_Bn_Activation(nn.Module):

    def __init__(self, in_channels, out_channels, kernel_size, stride, activation, bn=True, bias=False):

        super().__init__()

        # N = (W − F + 2P )/S+1步长为1时图片卷积后的尺寸不变

        #步长为2时图片卷积后的尺寸减半

        pad = (kernel_size - 1) // 2

        self.conv = nn.ModuleList()

        if bias:

            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad))

        else:

            self.conv.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride, pad, bias=False))

        if bn:

            self.conv.append(nn.BatchNorm2d(out_channels))

        if activation == "mish":

            self.conv.append(Mish())

        elif activation == "relu":

            self.conv.append(nn.ReLU(inplace=True))

        elif activation == "leaky":

            self.conv.append(nn.LeakyReLU(0.1, inplace=True))

        elif activation == "linear":

            pass

        else:

            print("activate error !!! {} {} {}".format(sys._getframe().f_code.co_filename,

                                                       sys._getframe().f_code.co_name, sys._getframe().f_lineno))



    def forward(self, x):

        for l in self.conv:

            x = l(x)

        return x</code></pre> 
<p style="margin-left:0">将真实框和预选框之间进行IoU计算，将真实框和预选框对应起来，每个真实框与和其Iou值最大的预选框所对应，得到target数据，与预测数据output之间进行loss计算。</p> 
<pre><code>loss_xy += F.binary_cross_entropy(input=output[..., :2], target=target[..., :2],weight=tgt_scale * tgt_scale, reduction='sum')

loss_wh += F.mse_loss(input=output[..., 2:4], target=target[..., 2:4], reduction='sum') / 2

loss_obj += F.binary_cross_entropy(input=output[..., 4], target=target[..., 4], reduction='sum')

loss_cls += F.binary_cross_entropy(input=output[..., 5:], target=target[..., 5:], reduction='sum')

loss = loss_xy + loss_wh + loss_obj + loss_cls</code></pre> 
<p style="margin-left:0"><strong>3.2.2   </strong><strong>迁移学习模型加载</strong></p> 
<p></p> 
<p style="margin-left:0">由于目标检测coco数据集数量还是较少，与用于分类的ImageNet数量相比差距很多。于是先在ImageNet上训练模型的前一段，作为目标检测的初始参数，从而达到迁移学习。本次实验下载的是论文中作者已经预训练好的权重模型darknet.pth。</p> 
<p style="margin-left:0">代码：</p> 
<pre><code class="language-python">_model = nn.Sequential(self.down1, self.down2, self.down3, self.down4, self.down5, self.neek)

pretrained_dict = torch.load(yolov4weight)



model_dict = _model.state_dict()

#  过滤掉不需要的参数

pretrained_dict = {k1: v for (k, v), k1 in zip(pretrained_dict.items(), model_dict)}

           

model_dict.update(pretrained_dict)

_model.load_state_dict(model_dict)</code></pre> 
<h1 id="%E5%9B%9B%E3%80%81%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90">四、结果分析</h1> 
<h3 id="4.1%EF%BC%9A%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%E6%95%88%E6%9E%9C" style="margin-left:0px">
<strong>4.1：</strong><strong>推理阶段效果</strong>
</h3> 
<p style="margin-left:0">输入图片进行测试</p> 
<p style="margin-left:0"><img alt="" height="998" src="https://images2.imgbox.com/82/e1/2bKc5w6X_o.png" width="1200"></p> 
<p><img alt="" height="732" src="https://images2.imgbox.com/44/dd/EXk2iw3H_o.png" width="1200"></p> 
<p style="margin-left:0;text-align:center">图七：推理左图为输入图片，右图为输出图片。</p> 
<p style="margin-left:0"><strong>4.2  </strong><strong>训练阶段效果</strong></p> 
<p style="margin-left:0">输入命令</p> 
<pre><code>tensorboard.py --logdir=log</code></pre> 
<p style="margin-left:0">可视化查看loss的下降效果</p> 
<p style="margin-left:0px;text-align:center"><img alt="" height="498" src="https://images2.imgbox.com/47/fe/9hAya9Is_o.png" width="850"> 图九：训练损失函数</p> 
<p style="margin-left:0">将训练好的模型加载，输入图片进行推理测试</p> 
<pre><code>D:/cv/yolo4/pytorch-YOLOv4/models.py 3 checkpoints/Yolov4_epoch200.pth data/test.jpg 608 608</code></pre> 
<h1 id="%E4%BA%94%E3%80%81%E7%BB%93%E8%AF%AD"><strong>五、结语</strong></h1> 
<p style="margin-left:0">通过本次复现实现，更加入深入的了解了一阶段目标检测，特别是YOLO系列的一步步改进。YOLOv4论文中也做了大量的实验，通过此论文了解了很多实验技巧，网路的残差连接以融合特征，学习率的动态改变，新的激活函数等通用的技巧，还学习到YOLOv4中的loss的一步步修改变形和为什么要如此变化。理解预选框的作用和loss的设计，对将此通用目标检测算法用到特定任务上有了一定的经验。了解使用了Netron、tensorboard等可视化图形工具，使得网路模型可视化，理解起来更加清晰（完整的网路模型图见附件）。训练过程可视化，可以清晰的看见loss的变化，便于更好的调整。</p> 
<h1 id="%E5%85%AD%E3%80%81%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE" style="margin-left:0px"><strong>六、参考文献</strong></h1> 
<p style="margin-left:0">1.Bochkovskiy, A., Wang, C.-Y., Liao, H.-Y.M., 2020. YOLOv4: Optimal Speed and Accuracy of Object Detection. CoRR abs/2004.10934.</p> 
<p style="margin-left:0">2.Huang, Z., Wang, J., 2019. DC-SPP-YOLO: Dense Connection and Spatial Pyramid Pooling Based YOLO for Object Detection. CoRR abs/1903.08589.</p> 
<p style="margin-left:0">3.Lin, T.-Y., Dollár, P., Girshick, R.B., He, K., Hariharan, B., Belongie, S.J., 2017. Feature Pyramid Networks for Object Detection., in: 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017. pp. 936–944. <a href="https://doi.org/10.1109/CVPR.2017.106" title="https://doi.org/10.1109/CVPR.2017.106">https://doi.org/10.1109/CVPR.2017.106</a></p> 
<p style="margin-left:0">4.Lin, T.-Y., Goyal, P., Girshick, R., He, K., Dollar, P., 2020. Focal Loss for Dense Object Detection. IEEE Trans. Pattern Anal. Mach. Intell. 42, 318–327. <a href="https://doi.org/10.1109/TPAMI.2018.2858826" title="Focal Loss for Dense Object Detection | IEEE Journals &amp; Magazine | IEEE Xplore">Focal Loss for Dense Object Detection | IEEE Journals &amp; Magazine | IEEE Xplore</a></p> 
<p style="margin-left:0">5.Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. Path Aggregation Network for Instance Segmentation., in: 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018. pp. 8759–8768.</p> 
<p style="margin-left:0">6.Misra, D., 2020. Mish: A Self Regularized Non-Monotonic Activation Function., in: 31st British Machine Vision Conference 2020, BMVC 2020, Virtual Event, UK, September 7-10, 2020.</p> 
<p style="margin-left:0">7.Zheng, Z., Wang, P., Liu, W., Li, J., Ye, R., Ren, D., 2020. Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression., in: The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020. pp. 12993–13000.</p> 
<p style="margin-left:0"></p> 
<p style="margin-left:0"></p> 
<p style="margin-left:0"></p> 
<p style="margin-left:0"></p> 
<p style="margin-left:0"></p> 
<p style="margin-left:0"></p> 
<p style="margin-left:0"></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>