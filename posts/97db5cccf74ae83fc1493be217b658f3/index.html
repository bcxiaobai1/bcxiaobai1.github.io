<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Elasticsearch 介绍及java集成 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Elasticsearch 介绍及java集成</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <h2>一、Elasticsearch 基础介绍</h2> 
<p>ElasticSearch 是分布式实时搜索、实时分析、实时存储引擎，简称（ES)， 成立于2012年，是一家来自荷兰的、开源的大数据搜索、分析服务提供商，为企业提供实时搜索、数据分析服务，支持PB级的大数据。</p> 
<p>基于Apache Lucene 开源搜索引擎，Lucene是目前公认的性能最好，最先进的，功能最全的搜索引擎。</p> 
<p>Elasticsearch使用Java开发并使用Lucene作为其核心来实现所有索引和搜索的功能，通过简单RESTfulAPI来隐藏Lucene的复杂性，从而让全文搜索变得简单。 速度超出你的想像，从10亿的数据中查出一条只需要1-2秒</p> 
<p>除了Lucene 和全文搜索，还有以下功能</p> 
<p>        分布式的实时文件存储，每个字段都被索引并可被搜索</p> 
<p>         分布式的实时分析搜索引擎</p> 
<p>        可以扩展到上百台服务器，处理PB级结构化或非结构化数据</p> 
<p>而且，所有的这些功能被集成到一个服务里面，你的应用可以通过简单的RESTful API、各种语言的客户端甚至命令行与之交互。</p> 
<p><strong>为什么要用ElasticSearch？</strong></p> 
<p>全文检索开始使用SQL来写，使用like进行模糊查询。如果数据量比较大，用这种方法就会特别慢，可以使用索引使得速度相对提高，但还是达不到对大数据搜索的要求，所以要使用分布式的全文搜索引擎ElasticSearch。</p> 
<h3>1)、ES原理剖析</h3> 
<p>索引和搜索流程图</p> 
<p><img alt="" height="517" src="https://images2.imgbox.com/6a/06/IUpNQzey_o.png" width="1200"></p> 
<p></p> 
<p>绿色代表索引过程，对要检索的内容进行索引构建一个索引库，</p> 
<p>索引过程包括：确定的原始内容即要搜索的内容——&gt;采集文档——&gt;创建文档——&gt;分析文档——&gt;索引文档</p> 
<p>红色代表搜索过程：从索引库中搜索内容，</p> 
<p>搜索过程：用户通过搜索界面——&gt;创建查询——&gt;执行搜索，从索引库搜索——&gt;渲染搜索结果</p> 
<h2>二、Elasticsearch基本概念：</h2> 
<h3><strong> 索引（Index）</strong></h3> 
<p>一个索引就是一个拥有几分相似特征的文档的集合。比如说，你可以有一个客户数据的索引，另一个产品目录的索引，还有一个订单数据的索引。一个索引由一个名字来标识（必须全部是小写字母的），并且当我们要对对应于这个索引中的文档进行索引、搜索、更新和删除的时候，都要使用到这个名字。</p> 
<h3>
<strong>类型（Type）</strong>6.0.0版本中弃用</h3> 
<p>类型，曾经是索引的逻辑类别/分区，允许您在同一索引中存储不同类型的文档，例如，一种类型用于用户，另一种类型用于博客帖子。</p> 
<p>在一个索引中，你可以定义一种或多种类型。一个类型是你的索引的一个逻辑上的分类/分区，其语义完全由你来定。</p> 
<h3><strong>文档（Document）</strong></h3> 
<p>一个文档是一个可被索引的基础信息单元。比如，你可以拥有某一个客户的文档，某一个产品的一个文档，当然，也可以拥有某个订单的一个文档。文档以JSON（Javascript Object Notation）格式来表示。文档必须被索引/赋予一个索引的type。</p> 
<p><img alt="" height="348" src="https://images2.imgbox.com/6e/ba/FY0zJIB8_o.png" width="1042"></p> 
<h3><strong style="color:#4f4f4f;font-size:22px">分片(Shards)</strong></h3> 
<p>索引可能存储大量可能超过单个节点的硬件限制的数据。</p> 
<p>如果我们的索引数据量很大，超过硬件存放单个文件的限制，就会影响查询请求的速度，Es引入了分片技术。一个分片本身就是一个完成的搜索引擎，文档存储在分片中，而分片会被分配到集群中的各个节点中，随着集群的扩大和缩小，ES会自动的将分片在节点之间进行迁移，以保证集群能保持一种平衡。分片有以下特点：</p> 
<ol>
<li>ES的一个索引可以包含多个分片（shard）；</li>
<li>每一个分片（shard）都是一个最小的工作单元，承载部分数据；</li>
<li>每个shard都是一个lucene实例，有完整的简历索引和处理请求的能力；</li>
<li>增减节点时，shard会自动在nodes中负载均衡；</li>
<li>一个文档只能完整的存放在一个shard上</li>
<li>一个索引中含有shard的数量，默认值为5，在索引创建后这个值是不能被更改的。</li>
<li>优点：水平分割和扩展我们存放的内容索引；分发和并行跨碎片操作提高性能/吞吐量；</li>
<li>每一个shard关联的副本分片（replica shard）的数量，默认值为1，这个设置在任何时候都可以修改。</li>
</ol> 
<h3><strong>副本(Replicasedit)</strong></h3> 
<p>副本，是对分片的复制。目的是为了当分片/节点发生故障时提供高可用性，它允许您扩展搜索量/吞吐量，因为可以在所有副本上并行执行搜索。</p> 
<p>一个分片可以有多个复制分片，也可以无复制分片。它的作用主要是防止分片故障，加速查询索引等功能，提供了高可用性。另外，复制分片是不和主分片在一起的，一个主分片在一台机器上，它的复制分片可能分布在其它N台机器上。在这里，我们可以把它理解为，一个分片的复制，就叫复制分片。每个分片会包含部分索引文件。文件由sgment组成 。</p> 
<p>副本（replica shard）就是shard的冗余备份，它的主要作用：</p> 
<p>1）、冗余备份，防止数据丢失；</p> 
<p>2）、shard异常时负责容错和负载均衡；</p> 
<p><img alt="" height="353" src="https://images2.imgbox.com/a7/0f/ecQopR85_o.png" width="861"></p> 
<p> </p> 
<p>注意：副本是乘法，越多越浪费，但也越保险。分片是除法，分片越多，单分片数据就越少也越分散。</p> 
<p></p> 
<h3>集群</h3> 
<p>多台ES服务器的结合的统称叫ES集群，一个集群包含多台服务器，多个节点。</p> 
<p>节点</p> 
<p>一个节点是你集群中的一个服务器，作为集群的一部分，它存储你的数据，参与集群的索引和搜索功能。</p> 
<p><strong>节点种类</strong></p> 
<p><strong>主节点</strong>：负责集群范围内轻量级的操作，例如创建或删除索引。跟踪那些节点是集群的一部分以及确定将哪些碎片分配给哪些节点</p> 
<p><strong>数据节点</strong>：包含已创建的索引文档的分片。数据节点处理及数据相关的操作。例如CRUD，搜索和聚合</p> 
<p><strong>调节节点</strong>：仅可路由请求，处理搜索缩减阶段并分配批量索引。本质上，仅协调节点充当智能负载平衡器</p> 
<p><strong>节点和分片如何工作？</strong></p> 
<p>一个集群至少有一个节点，而一个节点就是一个ElasticSearch进程，节点可以有多个默认索引，如果创建索引，索引将会由5个分片（primary shard，又称主分片）构成，每一个分片会有一个复制分片</p> 
<p><img alt="" height="415" src="https://images2.imgbox.com/00/c7/xdJPTPl4_o.png" width="1200"></p> 
<h2>三、与传统的关系型数据库中的库、表、行、列等概念进行对比</h2> 
<p>关系型数据库 -&gt; Databases(库) -&gt; Tables(表) -&gt; Rows(行) -&gt; Columns(列)。</p> 
<p>Elasticsearch -&gt; Indeces(索引) -&gt; Types(类型) -&gt; Documents(文档) -&gt; Fields(属性)。</p> 
<table><tbody>
<tr>
<td colspan="1" rowspan="1"> <p>RDBS</p> </td>
<td colspan="1" rowspan="1"> <p>ES</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>数据库（database）</p> </td>
<td colspan="1" rowspan="1"> <p>索引（index）</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>表（table）</p> </td>
<td colspan="1" rowspan="1"> <p>类型（type）（ES6.0之后被废弃，es7中完全删除）</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>表结构（schema）</p> </td>
<td colspan="1" rowspan="1"> <p>映射（mapping）</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>行（row）</p> </td>
<td colspan="1" rowspan="1"> <p>文档（document）</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>列（column）</p> </td>
<td colspan="1" rowspan="1"> <p>字段（field）</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>索引（Schema）</p> </td>
<td colspan="1" rowspan="1"> <p>反向索引（Mapping）</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>SQL</p> </td>
<td colspan="1" rowspan="1"> <p>查询DSL</p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>SELECT * FROM table</p> </td>
<td colspan="1" rowspan="1"> <p>GET <a href="" title="http://.....">http://.....</a></p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>UPDATE table SET</p> </td>
<td colspan="1" rowspan="1"> <p>PUT  <a href="" title="http://......">http://......</a></p> </td>
</tr>
<tr>
<td colspan="1" rowspan="1"> <p>DELETE</p> </td>
<td colspan="1" rowspan="1"> <p>DELETE  <a href="" title="http://......">http://......</a></p> </td>
</tr>
</tbody></table> 
<p>（1）、关系型数据库中的数据库(database)，等价与ES索引（index）</p> 
<p>（2）、一个数据库下面有N张表（table）,等价与1个索引Index下面有N多类型（Type）</p> 
<p><strong>                备注：（ES6.0之后被废弃，es7中完全删除）</strong></p> 
<p>（3）、一个数据库表（table）下的数据有多行（row）多列（colum）组成，等价与一个Type由多文档（document）多字段（field）组成</p> 
<p>（4）、在一个关系型数据库中，索引（Schema）定义了表，每个表的字段，还有表和字段的之间关系，与之对应，在ES中：反向索引（Mapping）定义索引下的Ttype的字段的处理规则，即如何建立、索引类型、是否保存原始索引JSON文档、是否压缩原始JSON文档、是否需要分词处理、如何进行分词处理等</p> 
<p>（5）、在数据库中新增 INSERT、删除 DELTE、修改 UPDATE、查询 SEARCH操作等价于ES中的新增PUT/POST、删除DELETE、修改_update、查询GET</p> 
<p><strong>        ES内置的RREST接口</strong></p> 
<p><img alt="" height="591" src="https://images2.imgbox.com/9b/0c/hmHHD9yA_o.png" width="1088"></p> 
<h3>搜索原理</h3> 
<p><img alt="" height="324" src="https://images2.imgbox.com/7c/30/YTPjGr0p_o.png" width="1077"></p> 
<h2><span style="color:#4d4d4d;font-size:16px;font-weight:normal">（1）、客户端给DODE1发送请求，查询名字叫张三的数据</span></h2> 
<p>（2）、P1节点接收到请求，判断出当前数据的_ID对应的分片0，且分片P1中的数据对应复制分片R0,R1都有，就会将请求转发到R0进行处理</p> 
<p>（3）、取出文档数据返回给P1，最终返回给前端</p> 
<h3>更新原理</h3> 
<p><img alt="" height="246" src="https://images2.imgbox.com/e4/d9/4fakzdkj_o.png" width="1041"></p> 
<p>（1）、客户端给NODE1发送更新请求</p> 
<p>（2）、它转发请求到主分片所在的节点NODE3</p> 
<p>（3）、NODE3从主分片检索出文档，修改_soure字段的JSON，然后在主分片上重建索引，如果有其他进程修改了文档，它以retry_on_conflict设置的次数重复步骤3，都未成功则放弃</p> 
<p>（4）、如何NODE3更新文档成功，它同时转发文档的新版本到NODE1和NODE2上的复制节点以重建索引。当所有复制节点更新成功，NODE3返回成功给请求节点，然后返回用户端</p> 
<h3>创建/删除原理</h3> 
<p><img alt="" height="336" src="https://images2.imgbox.com/8f/e6/BosuiG6Y_o.png" width="1193"></p> 
<p>（1）、客户端发送请求创建、删除请求</p> 
<p>（2）、根据文档ID，它将转发请求到主分片所在节点NODE3</p> 
<p>（3）、NODE3在主分片上执行请求，如果成功将转发请求到NODE1和NODE2的复制分片上，当所有复制分片成功，则NODE3返回成功信息给请求节点。在将信息返回给客户端</p> 
<h3>字段数据类型：</h3> 
<p>字符型：text（分词，不能用于排序、过滤查询、聚合查询）、keyWord</p> 
<p>数字型：byte、short、integer、float、double</p> 
<p>布尔型：boolean</p> 
<p>日期型：date</p> 
<p>二进制型：binary</p> 
<p>对象类型：object</p> 
<h3>字段属性：</h3> 
<p>store：是否储存字段原始值（独立于_source字段）</p> 
<p>index：是否参与索引</p> 
<p>analyzer：指定分词器</p> 
<p>boost：字段级别的分数加权</p> 
<p>doc_values：是否对不分词建立正排序索引</p> 
<p>fleld_data：是否对分词器建立正排序索引</p> 
<p>properties：类型映射</p> 
<p>ignore_above：超过指定字符的文本将忽略不被索引</p> 
<p>include_in_all：是否包含该字段到_all字段中</p> 
<p>index_optionss：倒排序索引的可选参数</p> 
<p>norms：是否储存长度因子和分数加权（boost）</p> 
<p>null_value：初始值</p> 
<p>position_increment_gap：指定多字段的多个值之间的位置间隔</p> 
<p>search_analyzer：指定搜索时分词器</p> 
<p>similarity：指定评分策略</p> 
<p>term_vector：指定返回哪些关于词条的统计信息</p> 
<p>normalizer：标注化处理器</p> 
<p>coerce：强制类型转换器</p> 
<p>copy_to：创建自定义的_all属性</p> 
<p>dynamic：动态映射策略</p> 
<p>enabled：是否处理字段（正排序索引和倒排序索引）</p> 
<p>eager_global_ordinals：是否立即加载全局序号</p> 
<p>format：指定日期格式</p> 
<p>ignore_malformed：忽略格式错误的字段</p> 
<h2>四、ES的特性：</h2> 
<p>速度快、易扩展、弹性、灵活、操作简单、多语言客户端、X-Pack、hadoop/spark强强联手、开箱即用。</p> 
<ul>
<li>
<strong>分布式：</strong>横向扩展非常灵活</li>
<li>
<strong>全文检索：</strong>基于lucene的强大的全文检索能力；</li>
<li>
<strong>近实时搜索和分析：</strong>数据进入ES，可达到近实时搜索，还可进行聚合分析</li>
<li>
<strong>高可用：</strong>容错机制，自动发现新的或失败的节点，重组和重新平衡数据</li>
<li>
<strong>模式自由：</strong>ES的动态mapping机制可以自动检测数据的结构和类型，创建索引并使数据可搜索。</li>
<li>
<strong>RESTful API：</strong>JSON + HTTP</li>
</ul> 
<h2>五、索引的应用</h2> 
<h3>创建索引</h3> 
<pre><code class="language-Scala">PUT project_v1
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1
  },
  "mappings": {
    "properties": {
      "name_cn": {
        "type": "text"
      },
      "name_en": {
        "type": "keyword"
      },
      "project_type": {
        "type": "keyword"
      },
      "people_count": {
        "type": "integer"
      },
      "order_count": {
        "type": "long"
      },
      "date": {
        "type": "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||yyyy-MM||epoch_millis"
      }
    }
  }
}

</code></pre> 
<p><strong>备注：</strong>text 用于索引全文值的字段，例如电子邮件正文或产品说明。它们通过分词器传递 ，以在被索引之前将字符串转换为单个术语的列表。分析过程允许Elasticsearch搜索单个单词中 每个完整的文本字段。文本字段不用于排序，很少用于聚合。</p> 
<p>keyword 用于索引结构化内容的字段，例如电子邮件地址，主机名，状态代码，邮政编码或标签。它们通常用于过滤，排序，和聚合。keyword字段只能按其确切值进行搜索。</p> 
<p>有时候一个字段同时拥有全文类型（text）和关键字类型（keyword）是有用的：一个用于全文搜索，另一个用于聚合和排序。</p> 
<p><strong>number_of_shards</strong> 是指索引要做多少个分片，只能在创建索引时指定，后期无法修改。</p> 
<p><strong>number_of_replica</strong>s 是指每个分片有多少个副本，后期可以动态修改</p> 
<p>primary shard（主分片）：每个文档都存储在一个分片中，当你存储一个文档的时候，系统会首先存储在主分片中，然后会复制到不同的副本中。默认情况下，一个索引有5个主分片。你可以在事先制定分片的数量，<strong>当分片一旦建立，分片的数量则不能修改。</strong></p> 
<p>replica shard（副本分片）：每一个分片有零个或多个副本。副本主要是主分片的复制，可以 增加高可用性，提高性能。</p> 
<p>默认情况下，一个主分配有一个副本，但副本的数量可以在后面动态的配置增加。</p> 
<p>副本必须部署在不同的节点上，不能部署在和主分片相同的节点上。</p> 
<h3>新增索引数据</h3> 
<pre><code class="language-Scala">PUT /project_v1/_doc/1
{
  "name_en":"encourage",
  "name_cn":"营销码",
  "project_type":"营销",
  "people_count":4,
  "order_count":1000000,
  "date":"2019-04-01"
}

</code></pre> 
<h3>查询索引数据</h3> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "query": {
    "match_all": {}
  }
}
</code></pre> 
<h3>匹配查询 match</h3> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "query": {
    "match": {
      "name_cn": "营销"
    }
  }
}

</code></pre> 
<h3>过滤查询 Filter</h3> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "query": {
    "bool": {
      "filter": {
        "range": {
          "date": {
            "gte": "2020-04-01"
          }
        }
      }
    }
  }
}

</code></pre> 
<h2>六、Elasticsearch 聚合查询</h2> 
<h3>1.聚合的概念</h3> 
<p>官方对聚合有四个关键字：Metric(指标)、Bucketing(桶)、Pipeline(管道)、Matrix(矩阵)，在查询请求体中以aggregations语法来定义聚合分析，也可简写成aggs</p> 
<p>Metric(指标)：指标分析类型，如计算最大值、最小值、平均值等（对桶内的文档进行聚合分析的操作）</p> 
<p>Bucket(桶)：分桶类型，类似sql中的group by语法（满足特定条件的文档的集合）</p> 
<p>Pipeline(管道)：管道分析类型，基于上一级的聚合分析结果进行再分析</p> 
<p>Matrix(矩阵)：矩阵分析类型（聚合是一种面向数值型的聚合，用于计算一组文档字段中的统计信息）</p> 
<h3>2.Metric(指标)聚合</h3> 
<p>Metric聚合分析分为单值分析和多值分析两类</p> 
<h4>1、单值分析，只输出一个分析结果</h4> 
<p>关键字有min， max，avg，sum，cardinality</p> 
<h4>2、多值分析，输出多个分析结果</h4> 
<p>关键字有stats，extended_stats，percentile_rank，top hits</p> 
<h5>1)、min， max，avg， sum</h5> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "min_people_count": {
      "min": {
        "field": "people_count"
      }
    },
    "max_order_count":{
      "max": {
        "field": "order_count"
      }
    },
    "avg_order_count":{
      "avg": {
        "field": "order_count"
      }
    },
    "sum_order_count":{
      "sum": {
        "field": "order_count"
      }
    }
  }
}

</code></pre> 
<h5>2)、cardinality</h5> 
<p>cardinality 关键字: 求唯一值，即不重复的字段有多少（相当于sql中的distinct）</p> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "cardinality_project_type": {
      "cardinality": {
        "field": "project_type"
      }
    }
  }
}

</code></pre> 
<h5>3)、stats</h5> 
<p>统计信息，请求后会直接显示各种聚合结果(count，min，max，avg，sum)</p> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "stats_order_count": {
      "stats": {
        "field": "order_count"
      }
    }
  }
}

</code></pre> 
<h5>4)、extended_stats</h5> 
<p>扩展的统计信息，比stats返回更多的统计信息</p> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "extended_stats_order_count": {
      "extended_stats": {
        "field": "order_count"
      }
    }
  }
}</code></pre> 
<h4>3.Bucket(桶)聚合</h4> 
<p>Bucket可以理解为一个桶，它会遍历文档中的内容，凡是符合某一要求的就放在一个桶中，分桶相当于sql中的group by</p> 
<p>关键字有Terms Aggregation，Filter Aggregation，Histogram Aggregation，Date Aggregation</p> 
<h5>1)、Terms Aggregation</h5> 
<p>根据某一项的每个唯一的值来聚合</p> 
<pre><code class="language-Scala">GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "terms_project_type": {
      "terms": {
        "field": "project_type",
        "size": 3
      }
    }
  }
}

</code></pre> 
<h5>2)、Filter Aggregation</h5> 
<p>指具体的域和具体的值，可以在Terms Aggregation 的基础上进行了过滤，只对特定的值进行了聚合</p> 
<pre><code class="language-Scala">#查营销类型的总订单数
GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "filter_project_type": {
      "filter": {
        "term": {
          "project_type": "营销"
        }
      },
      "aggs": {
        "sum_order_count": {
          "sum": {
            "field": "order_count"
          }
        }
      }
    }
  }
}

</code></pre> 
<h5>3)、Histogram Aggregation</h5> 
<p>Histogram与Terms聚合类似，都是数据分组，区别是Terms是按照Field的值分组，而Histogram可以按照指定的间隔对Field进行分组</p> 
<pre><code class="language-Scala">#项目规模
GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "project_scale": {
      "histogram": {
        "field": "people_count",
        "interval": 1
      }
    }
  }
}

</code></pre> 
<h5>4)、Date Aggregation</h5> 
<p>针对时间格式数据的直方图聚合，基本特性与Histogram Aggregation一致</p> 
<pre><code class="language-Scala">#项目发展史
GET /project_v1/_search
{
  "size": 0,
  "aggs": {
    "date_by_day": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "day",
        "min_doc_count": 1
      }
    }
  }
}

</code></pre> 
<h4>4.Pipeline(管道)聚合</h4> 
<p>管道的概念：支持对聚合分析的结果，再次进行聚合分析</p> 
<pre><code class="language-Scala">#查项目类型最少人数的项目类型
GET /project_v1/_search
{
  "size":0,
  "aggs":{
    "project_type":{
      "terms": {
        "field": "project_type",
        "size": 3
      },
      "aggs":{
        "sum_people_count":{
          "sum": {
            "field": "people_count"
          }
        }
      }
    },
    "min_people_count_by_project_type":{
      "min_bucket": {
        "buckets_path": "project_type&gt;sum_people_count"
      }
    }
  }
}

</code></pre> 
<h4>5.总结</h4> 
<p>Metric(指标)：分类并对一组文档进行sum、avg等数学运算</p> 
<p>Bucketing(桶)：桶聚合，常规的分类然后计算每个分类的文档数量</p> 
<p>Pipeline(管道)：对聚合的结果再次聚合</p> 
<p>Matrix(矩阵)：可在多个字段上计算，生成矩阵结果</p> 
<h2>七，通过SQL查询Elasticsearch</h2> 
<h3>1.为什么用SQL查询</h3> 
<p>Elasticsearch 的官方查询语言是 Query DSL，既然是官方指定的，说明最吻合 ES 的强大功能，为ES做支撑。</p> 
<p>其实，SQL 作为一个数据库查询语言，它语法简洁，书写方便而且大部分服务端程序员都清楚了解和熟知它的写法。但是作为一个 ES 新人来说，就算他已经是一位编程界的老江湖，但是如果他不熟悉 ES ，那么他如果要使用公司已经搭好的 ES 服务，他必须要先学习 Query DSL，学习成本也是一项影响技术开发进度的因素而且不稳定性高。但是如果 ES 查询支持 SQL的话，那么也许就算他是工作一两年的同学，他虽然不懂 ES的复杂概念，他也能很好的使用 ES 而且顺利的参加到开发的队伍中，毕竟SQL 都会写</p> 
<h3>2.Elasticsearch-SQL</h3> 
<p>Elasticsearch-SQL不属于 Elasticsearch 官方的，它是 NLPChina（中国自然语言处理开源组织）开源的一个 ES 插件，主要功能是通过 SQL 来查询 ES，其实它的底层是通过解释 SQL，将SQL 转换为 DSL 语法，再通过DSL 查询。</p> 
<p>查询语法</p> 
<pre><code class="language-Scala">SELECT fields from indexName/type WHERE conditions</code></pre> 
<p>表名 tableName 的地方现在改为了索引名 indexName，如果有索引Type ，则indexName/type</p> 
<pre><code class="language-Scala">POST _sql?format=txt
{
  "query": "select * from project_index limit 10"
}</code></pre> 
<p>SQL翻译成DSL语句</p> 
<pre><code class="language-Scala">POST _sql/translate
{
  "query": "select name_en,COUNT(*) from project_index GROUP BY name_en"
}</code></pre> 
<h2>八、注意点</h2> 
<h3>1.版本问题</h3> 
<p>es 5到7的版本变动很大，其中包括type的变动</p> 
<ul>
<li>5.x 支持多种type</li>
<li>6.x 只能有一种type</li>
<li>7.x 将去除type 没有类型的概念了</li>
</ul> 
<h3>2.ES并不可靠</h3> 
<p>ES不是可靠的存储系统，不是数据库，它有丢数据的风险。ES不是实时系统，数据写入成功只是trans log成功（类似于mysql的bin log），写入成功后立刻查询查不到是正常的。因为数据此刻可能还在内存里而不是进入存储引擎里。</p> 
<h3>3.同步问题</h3> 
<p>在需要添加新数据与新字段的时候，如果elasticSearch进行搜索是可能需要重新修改格式。之前的数据需要重新同步，对数据的管理有很多困难。</p> 
<h2>九、SpringBoot集成Elasticsearch</h2> 
<h3>1.引入依赖</h3> 
<pre><code class="language-Scala">&lt;properties&gt;
        &lt;!--告诉springboot我们处理的ES的版本--&gt;
        &lt;elasticsearch.version&gt;7.10.2&lt;/elasticsearch.version&gt;
&lt;/properties&gt;


&lt;dependencies&gt;
	&lt;dependency&gt;
        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;
        &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;
    &lt;/dependency&gt;
&lt;/dependencies&gt;</code></pre> 
<h3>2.yml配置es集群</h3> 
<pre><code class="language-Scala">spring:
  elasticsearch:
    rest:
      uris:
        - 192.168.53.112:9200
        - 192.168.53.113:9200
        - 192.168.53.114:9200
</code></pre> 
<h3>3.简单Test</h3> 
<h4>3.1 创建索引以及分片设置</h4> 
<pre><code class="language-Scala">@Test
public void createIndex() throws Exception{
  //1 创建索引并设置分片
  //1.1 创建一个RestHightLevelClient对象，相当于和服务端建立连接。
  RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(
  		  //没有集群的话  此处可new 一个即可。
          new HttpHost("192.168.53.112",9200)
          new HttpHost("192.168.53.113",9200),
          new HttpHost("192.168.53.114",9200),
  ));


  //1.2 使用client的索引管理的对象,indices()返回索引管理对象。
  IndicesClient indicesClient = client.indices();
  //两个参数
  //1.2.1 创建索引请求对象  参数：创建的索引库的名称
  CreateIndexRequest request = new CreateIndexRequest("hello")
  			    .settings(Settings.builder()
                        .put("number_of_shards", 5)
                        .put("number_of_replicas", 1)
                        .build()
                );
  //1.2.2 请求选项，使用默认值。配置请求头，主要用于认证。
  CreateIndexResponse response = indicesClient.create(request,   RequestOptions.DEFAULT);


  //显示结果
  System.out.println(response.toString());
 }
</code></pre> 
<h4></h4> 
<h4>3.2 创建索引库并设置mapping信息</h4> 
<pre><code class="language-Scala">@Test
public void createIndexAndMapping() throws Exception{
	RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(
  		  //没有集群的话  此处可new 一个即可。
          new HttpHost("192.168.53.112",9200)
          new HttpHost("192.168.53.113",9200),
          new HttpHost("192.168.53.114",9200),
  	));
    //创建json数据
    XContentBuilder mappings = XContentFactory.jsonBuilder()
            .startObject()
               .startObject("properties")
                    .startObject("id")
                        .field("type","long")
                    .endObject()
                    .startObject("title")
                        .field("type","text")
                        .field("analyzer","ik_smart")
                        .field("store",true)
                    .endObject()
                .endObject()
            .endObject();


    //创建索引请求对象  参数：创建的索引库的名称，分片副片数量以及mapping信息
    CreateIndexRequest request = new CreateIndexRequest("hello1")
            .settings(Settings.builder()
                    .put("number_of_shards", 5)
                    .put("number_of_replicas", 1)
                    .build()
            )
            .mapping(mappings);


   //两个参数
    //1 创建索引请求对象  参数：创建的索引库的名称
    //2 请求选项，使用默认值。配置请求头，主要用于认证。
    CreateIndexResponse response = client.indices().create(request, RequestOptions.DEFAULT);


    //显示结果
    System.out.println(response.toString());
	}
}
</code></pre> 
<h4></h4> 
<h4>3.3删除索引库</h4> 
<pre><code class="language-Scala">@Test
public void deleteIndex() throws Exception{
	RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(
  		  //没有集群的话  此处可new 一个即可。
          new HttpHost("192.168.53.112",9200)
          new HttpHost("192.168.53.113",9200),
          new HttpHost("192.168.53.114",9200),
  	));
		//删除索引库
        AcknowledgedResponse response = client.indices().delete(new DeleteIndexRequest("hello"), RequestOptions.DEFAULT);


        //显示结果
        System.out.println(response.toString());
  	
}  	
</code></pre> 
<h4></h4> 
<h4>3.4 添加索引库字段信息</h4> 
<pre><code class="language-Scala">@Test
public void putIndex() throws Exception{
	RestHighLevelClient client = new RestHighLevelClient(RestClient.builder(
  		  //没有集群的话  此处可new 一个即可。
          new HttpHost("192.168.53.112",9200)
          new HttpHost("192.168.53.113",9200),
          new HttpHost("192.168.53.114",9200),
  	));
  	
  	String mappings = "{n" +
                "ttt"properties":{n" +
                "tttt"id":{n" +
                "ttttt"type" : "long"n" +
                "tttt},n" +
                "tttt"title" :{n" +
                "ttttt"type" : "text",n" +
                "ttttt"analyzer" : "ik_smart",n" +
                "ttttt"store" : truen" +
                "tttt},n" +
                "tttt" content" :{n" +
                "ttttt"type" : "text",n" +
                "ttttt"analyzer" : "ik_smart",n" +
                "ttttt"store" :truen" +
                "tttt}n" +
                "ttt}n" +
                "tt}";
		//将字符串以json形式发送
        PutMappingRequest request = new PutMappingRequest("hello1")
                .source(mappings, XContentType.JSON);


        //修改索引库
        AcknowledgedResponse response = client.indices().putMapping(request, RequestOptions.DEFAULT);


        //显示结果
        System.out.println(response.toString());
}
</code></pre> 
<h2></h2> 
<h2>十、Elasticsearch文档管理</h2> 
<h3>0.抽取ES连接对象的公共方法</h3> 
<pre><code class="language-Scala">//原生客户端类，即ESjava客户端。
private RestHighLevelClient client;
public void init(){
   //1.1 创建一个RestHightLevelClient对象，相当于和服务端建立连接。
   client = new RestHighLevelClient(RestClient.builder(
          //没有集群的话  此处可new 一个即可。
          new HttpHost("192.168.53.112",9200)
          new HttpHost("192.168.53.113",9200),
          new HttpHost("192.168.53.114",9200),
   ));
}
</code></pre> 
<h3>1.添加文档</h3> 
<p>使用RestHightLevelClient对象。</p> 
<p>使用client对象的index方法添加文档</p> 
<p>创建IndexRequest对象，其中包含了索引库名称、文档id、文档的内容</p> 
<p>{“id”:“1”,“title”:“测试文档1”,“content”:“测试文档中的内容”}</p> 
<pre><code class="language-Scala">public void addDocument() throws Exception{


   String document = "{"id":1, "title":"这是测试文章", "content":"xxxxx"}";


   //创建IndexRequest对象，其中包含索引库名称，文档id，文档内容
   IndexRequest request = new IndexRequest()
           .index("hello1")
           .id("1")
           .source(document, XContentType.JSON);


   IndexResponse response = client.index(request, RequestOptions.DEFAULT);


   System.out.println(response.toString());


}</code></pre> 
<h3>2.更新文档</h3> 
<p>使用client对象的update方法。</p> 
<p>需要UpdateRequest参数：</p> 
<p>1.更新的索引</p> 
<p>2.更新的文档的id</p> 
<p>3.更新的文档内容</p> 
<pre><code class="language-Scala">public void updateDocument() throws Exception{


    String document = "{"id":1, "title":"这是测试文章更细的", "content":"new update"}";


    //创建IndexRequest对象，其中包含索引库名称，文档id，文档内容
    UpdateRequest request = new UpdateRequest()
            .index("hello1")
            .id("1")
            .doc(document, XContentType.JSON);


    UpdateResponse response = client.update(request, RequestOptions.DEFAULT);


    System.out.println(response.toString());


}
</code></pre> 
<h3>3.删除文档</h3> 
<p>使用client的delete方法</p> 
<p>需要DeleteRequest对象，需要两个参数</p> 
<p>1.操作的索引</p> 
<p>2.文档的id</p> 
<pre><code class="language-Scala">public void deleteDocument() throws Exception{


   //创建IndexRequest对象，其中包含索引库名称，文档id，文档内容
    DeleteRequest request = new DeleteRequest("hello1", "1");


    DeleteResponse response = client.delete(request, RequestOptions.DEFAULT);


    System.out.println(response.toString());


}</code></pre> 
<h3>4.根据id查询文档</h3> 
<p>使用client对象的get方法。</p> 
<p>需要使用GetRequest对象，两个参数：</p> 
<p>1.操作的索引</p> 
<p>2.文档的id</p> 
<pre><code class="language-Scala">public void getDocument() throws Exception{


    //创建IndexRequest对象，其中包含索引库名称，文档id，文档内容
    GetRequest request = new GetRequest("hello1", "1");


    GetResponse response = client.get(request, RequestOptions.DEFAULT);


    System.out.println(response.toString());


}</code></pre> 
<h3>5.批量查询文档</h3> 
<p>使用client对象的bulk方法。</p> 
<p>BulkRequest对象，使用add方法，添加要批量处理的请求。</p> 
<p>支持的处理：IndexRequest，DeleteRequest，UpdateRequest</p> 
<pre><code class="language-Scala">public void bulkDocument() throws Exception{
    //json数据
    String jsonData = "[" +
            "{"id":3, "title":"这是测试文章1", "content":"xxxxx", "comment":"备注信息", "mobile":"13344556677"}n" +
            "{"id":4, "title":"这是一篇文章2", "content":"xxxxx", "comment":"备注信息", "mobile":"13344556677"}n" +
            "{"id":5, "title":"这是一篇文章3", "content":"xxxxx", "comment":"备注信息", "mobile":"13344556677"}]";


    //转换成json格式字符串
    JSONArray jsonArray = JSONObject.parseArray(jsonData);


    //创建IndexRequest对象，其中包含索引库名称，文档id，文档内容
    BulkRequest request = new BulkRequest();


    jsonArray.stream()
            .forEach(json -&gt; {
                IndexRequest r = new IndexRequest()
                        .index("hello1")
                        .id(((JSONObject) json).getString("id"))
                        .source(((JSONObject) json).toJSONString(), XContentType.JSON);
                request.add(r);
    });




    BulkResponse response = client.bulk(request, RequestOptions.DEFAULT);


    System.out.println(response.toString());


}</code></pre> 
<h2>十一、ElasticsearchRestTemplate类与ElasticsearchRepository类</h2> 
<p>SpringData对ES的封装ElasticsearchRestTemplate类，可直接使用，此类在ElasticsearchRestTemplate基础上进行性一定程度的封装，使用起来更方便灵活，拓展性更强。</p> 
<p>ElasticsearchRepository可以被继承操作ES，是SpringBoot对ES的高度封装，操作最为方便，但牺牲了灵活性。</p> 
<p><strong>索引库实体类</strong></p> 
<pre><code class="language-Scala">@Data
@Document(indexName = "blog_1", shards = 5, replicas = 1)
public class Blog {
    @Field(type = FieldType.Long, store = true)
    private Long id;
    
    //type = FieldType.Text     字段类型为text
    //analyzer = "ik_max_word"  分词器为"ik_max_word"
    //store = true              存储 =&gt; 是
    @Field(type = FieldType.Text, analyzer = "ik_max_word", store = true)
    private String title;


    @Field(type = FieldType.Text, analyzer = "ik_max_word", store = true)
    private String content;


    @Field(type = FieldType.Text, analyzer = "ik_max_word", store = true)
    private String comment;


    @Field(type = FieldType.Keyword, store = true)
    private String mobile;


}
</code></pre> 
<h3>1、使用ElasticsearchRestTemplate类</h3> 
<h4>a)、创建索引库</h4> 
<pre><code class="language-Scala">@Autowired
private ElasticsearchRestTemplate template;
	/**
	* 创建索引库
	*/
	public void createIndex(){
	   //创建索引库
	   template. indexOps(IndexCoordinates.of("mytest")).create();
    }</code></pre> 
<h4>b)、创建索引库并实体类设置mapping</h4> 
<h5>1）创建索引库</h5> 
<pre><code class="language-Scala">template.indexOps(IndexCoordinates.of(“mytest”)).create();</code></pre> 
<h5>2）设置mapping信息</h5> 
<p>需要创建一个实体类，其中配置实体类和文档的映射关系，使用注解配置。</p> 
<p>可以从Entity中生成mapping信息。</p> 
<pre><code class="language-Scala">public void putMapping(){
     //创建索引库
     Document mapping = template.indexOps(IndexCoordinates.of("mytest")).createMapping(Blog.class);
     template.indexOps(IndexCoordinates.of("mytest")).putMapping(mapping);
}
</code></pre> 
<h4>c)、删除索引库</h4> 
<pre><code class="language-Scala">	//删除索引库
    public void deleteIndex(){
        template.indexOps(IndexCoordinates.of("hello1")).delete();
    }
</code></pre> 
<h4>d)、索引库查询</h4> 
<pre><code class="language-Scala">public void maxQueryTest(){
        NativeSearchQuery builder = new NativeSearchQueryBuilder()
                //多字段查询  （高亮跟查询条件有关）
                .withQuery(QueryBuilders.multiMatchQuery("8", "id","title"))
                //增加过滤条件, 可以设置多个
                .withFilter(QueryBuilders.boolQuery()
                        //增加bool查询：should的term关键字查询
                        .should(QueryBuilders.termQuery("title", "文章"))
                        .should(QueryBuilders.termQuery("content","xxx"))
                )
                //增加过滤条件的关键字查询
                .withFilter(QueryBuilders.termQuery("mobile", "13344556677"))
                //分页设置
                .withPageable(PageRequest.of(0,5))
                //设置高亮
                .withHighlightBuilder(new HighlightBuilder()
                        //高亮显示的字段
                        .field("title")
                        //高亮显示的字段
                        .field("content")
                        //高亮显示的前缀
                        .preTags("&lt;em&gt;")
                        //高亮显示的后缀
                        .postTags("&lt;/em&gt;")
                )
                //添加聚合查询
                .addAggregation(new TermsAggregationBuilder("mobile_group").field("mobile"))
                .build();


        //基于Blog.class 类型返回的结果
        SearchHits&lt;Blog&gt; searchHits = template.search(builder, Blog.class);


        //从searchHits取相关数据
        long totalHits = searchHits.getTotalHits();               //取总记录数
        List&lt;SearchHit&lt;Blog&gt;&gt; list = searchHits.getSearchHits();  //取每条数据放入集合中
        System.out.println("总记录数为：" + totalHits);
        list.forEach(blogSearchHit -&gt; {
            //取原生文档对象
            Blog blog = blogSearchHit.getContent();
            System.out.println(blog);
            //取高亮对象
            Map&lt;String, List&lt;String&gt;&gt; highlightFields = blogSearchHit.getHighlightFields();
            System.out.println(highlightFields);


            //取高亮对象 放到Blog里去 这样就将Blog和高亮结合输出了
            String title = highlightFields.get("title").get(0);
            //String content = highlightFields.get("content").get(0);
            blog.setTitle(title);
            //blog.setContent(content);
            System.out.println(blog);
        });


        //取聚合结果
        Aggregations aggregations = searchHits.getAggregations();
        System.out.println(aggregations.toString());
    }</code></pre> 
<h3>2、使用ElasticsearchRepository类</h3> 
<h4>a)、创建接口继承ElasticsearchRepository</h4> 
<pre><code class="language-Scala">public interface BlogRepository extends ElasticsearchRepository&lt;Blog, Long&gt; {
    /**
     * 定义一个方法查询：根据title查询es
     *
     * 原因：  ElasticsearchRepository会分析方法名,参数对应es中的field（这就是灵活之处）
     * @param title
     * @return java.util.List&lt;com.yt.cubemall.search.model.Blog&gt;
     */
    List&lt;Blog&gt; findByTitle(String title);


	/**
     * 定义一个方法查询： 根据title，content查询es
     */
    List&lt;Blog&gt; findByTitleAndContent(String title, String content);


}</code></pre> 
<h4>b)、使用BlogRepository接口</h4> 
<pre><code class="language-Scala">public class BlogRepositoryTest {


    @Autowired
    private BlogRepository blogRepository;


    /**
     * 添加文档
     */
    @Test
    public void addDocument(){
        Blog blog = new Blog();
        for (int i = 0; i &lt; 10; i++) {
            blog.setId((long)i+1);
            blog.setTitle("测试spring集成es"+i+1);
            blog.setContent("sjihfapf"+i+1);
            blog.setComment("注释内容"+i+1);
            blog.setMobile("12345678901");
            blogRepository.save(blog);
        }
    }


    /**
     * 更新文档
     */
    @Test
    public void updateDocument(){
        Optional&lt;Blog&gt; optional = blogRepository.findById(1l);
        if (optional.isPresent()){
            Blog blog = optional.get();
            blog.setTitle("hello update");
            blogRepository.save(blog);
        }
    }


    /**
     * 删除文档
     */
    @Test
    public void deleteDocument() {
        blogRepository.deleteById(1l);
    }




    /**
     * 查询所有 文档
     */
    @Test
    public void getDocument() {
        //根据id查找
        //Optional&lt;Blog&gt; optional = blogRepository.findById(1l);
        //Blog blog = optional.get();
        //System.out.println(blog);


        //查找全部
        //Iterable&lt;Blog&gt; all = blogRepository.findAll();
        //all.forEach(blog -&gt; System.out.println(blog));


        //分页查找全部
        Iterable&lt;Blog&gt; all = blogRepository.findAll(PageRequest.of(1,10));
        all.forEach(blog -&gt; System.out.println(blog));
    }




	/**
     * 自定义方法：根据title内容查询索引库
     * /
    @Test
    public void testFindByTitle(){
        List&lt;Blog&gt; blogList = blogRepository.findByTitle("测试");
        blogList.stream().forEach(System.out::println);
    }


	/**
     * 自定义方法：根据title，content内容查询索引库
     * /
    @Test
    public void testFindByTitleAndContent(){
        List&lt;Blog&gt; blogList = blogRepository.findByTitleAndContent("测试", "sjihfapf");
        blogList.stream().forEach(System.out::println);
    }
}


}</code></pre>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>