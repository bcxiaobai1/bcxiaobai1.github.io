<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>PyTorch深度学习（30）OpenCV图像处理 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PyTorch深度学习（30）OpenCV图像处理</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <h2>一、安装OpenCV </h2> 
<blockquote> 
 <p>3.4.2以上需要专利</p> 
 <p>pip install opencv-python==3.4.1.15</p> 
 <p>pip install opencv-contrib-python==3.4.1.15    增加额外拓展包</p> 
 <p></p> 
 <p>import cv2</p> 
 <p>cv2.__version__</p> 
</blockquote> 
<h3>环境配置</h3> 
<p>Anaconda：<a href="https://www.anaconda.com/" title="Anaconda | The World's Most Popular Data Science Platform">Anaconda | The World's Most Popular Data Science Platform</a></p> 
<p>Python_whl：<a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv" title="https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv">https://www.lfd.uci.edu/~gohlke/pythonlibs/#opencv</a> </p> 
<p></p> 
<h2>二、图像处理</h2> 
<h3>1、截取部分图像数据</h3> 
<p><strong>ROI：region of interest</strong></p> 
<p> 颜色通道提取</p> 
<p>b,g,r=cv2.split(img)</p> 
<p>img.shape  形状大小  H，W，C</p> 
<p></p> 
<h3>2、图像格式</h3> 
<p>cv2.IMREAD_COLOR：彩色图像<br> cv2.IMREAD_GRAYSCALE:灰色图像<br><strong>cv2.imread('路径+图像名称')  type=numpy array  h,w,c</strong></p> 
<p>opencv读取格式：BGR<br> cv2.imshow('窗口名称', img)<br> cv2.waitKey(0)  0表示任意键终止，等待时间为毫秒级<br> cv2.destroyAllWindows()</p> 
<p>cv2.VideoCapture(.mp4文件路径+名称)</p> 
<p><strong>ROI  range of interest感兴趣区域</strong><br> img[0:50, 0:200]</p> 
<p>颜色通道提取<br> b,g,r = cv2.split(img)<br><strong>cv2.merge((b, g, r))</strong></p> 
<p></p> 
<h3>3、边界填充</h3> 
<p><strong>cv2.copyMakeBorder(img, top_size, bottom_size, left_size, right_size, borderType=cv2.BORDER_REPLICATE)</strong></p> 
<p>BORDER_REPLICATE：复制法，复制最边缘像素<br> BORDER_RELECT：反射法，对感兴趣的图像中的像素在两边进行复制<br> BORDER_REFLECT_101：反射法，以最边缘像素为轴，对称<br> BORDER_WRAP：外包装法cdefgh|abcdefgh|abcdefg<br> BORDER_CONSTANT：常量法，常量值填充</p> 
<p><strong>图像叠加<br> cv2.add(img_1,img_2)  [[255,255,.]...]]</strong></p> 
<p><strong>重置图像大小<br> cv2.resize(img, (500, 300))<br> cv2.resize(img, (0,0), fx=3, fy=1)  指定长宽倍数</strong></p> 
<p><strong>cv2.addWeighted(img_1, 0.4, img_2, 0.6, 0)  制定权重融合</strong></p> 
<p></p> 
<h3>4、图像阈值</h3> 
<p><strong>ret,dst = cv2.threshold(src, thresh, maxval, type)</strong><br> src：输入图像，只能输入单通道图像（灰度图）<br> dst：输出图<br> thresh：阈值<br> maxval：当像素值超出了阈值（或小于阈值，根据type来决定），所赋予的值<br> type：二值化操作的类型，类型如下<br> cv2.THRESH_BINARY 超过阈值部分取maxval(最大值)，否则取0<br> cv2.THRESH_BINARY_INV 上一个的反转<br> cv2.THRESH_TRUNC 大于阈值部分设为阈值，否则不变<br> cv2.THRESH_TOZERO 大于阈值部分不变，否则为0<br> cv2.THRESH_TOZERO_INV 上一个的反转</p> 
<h3></h3> 
<h3>5、图像平滑</h3> 
<p><strong>均值滤波 简单的平均卷积操作<br> cv2.blur(img, (3,3))  卷积核大小<br> 方框滤波<br> cv2.boxFilter(img, -1, (3,3), normalize=True)<br> 高斯滤波<br> cv2.GaussianBlur(img, (5,5), -1)<br> 中值滤波<br> cv2.medianBlur(img, 5)</strong></p> 
<p></p> 
<h3>6、形态学</h3> 
<p><strong>腐蚀操作——边界越来越小，损害<br> cv2.erode(img, np.ones((5,5),np.uint8),iterations=1)<br> 膨胀操作——损害还原<br> cv2.dilate(img, np.ones((5,5),np.uint8), iterations=1)</strong></p> 
<p><strong>开运算：先腐蚀，后膨胀</strong><br> kernel = np.ones((5,5)<br><strong>cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)<br> 闭运算：先膨胀，后腐蚀<br> cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)<br> 梯度运算=膨胀-腐蚀  得到轮廓<br> cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)</strong></p> 
<p><strong>礼帽=原始输入-开运算结果</strong><br> (如原始为带刺，减去不带刺，得到带刺的图像）<br><strong>cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)<br> 黑帽=闭运算-原始输入</strong><br> (如膨胀带刺图-原始带刺图，得到原图像的边缘)<br><strong>cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel)</strong></p> 
<p></p> 
<h3>7、图像梯度</h3> 
<p><strong>Sobel算子(Gx Gy 水平 竖直之间的像素差异)</strong><br> cv2.Sobel(src,ddepth,dx,dy,ksize)<br> ddepth：图像的深度<br> dx和dy分别水平和竖值方向<br> ksize是Sobel算子的大小<br><strong>cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)<br> cv2.convertScaleAbs(sobelx)<br> cv2.convertScaleAbs(sobely)<br> cv2.addWeighted(sobelx, 0.5, sobely, 0.5, 0)</strong></p> 
<p><strong>Scharr算子<br> cv2.Scharr(img,cv2.CV_64F,1,0)<br> cv2.Scharr(img,cv2.CV_64F,0,1)</strong></p> 
<p><strong>Laplacian算子<br> cv2.Laplacian(img,cv2.CV_64F)</strong></p> 
<p><strong>Canny边缘检测</strong><br> 1）使用高斯滤波器，以平滑图像，滤除噪声<br> 2）计算图像中每个像素点的梯度强度和方向<br> 3）应用非极大值(Non-Maximum Suppression)抑制，以消除边缘检测带来的杂散响应<br> 4）应用双阈值(Double-Threshold)检测来确定真实和潜在的边缘<br> 5）通过抑制孤立的弱边缘最终完成边缘检测<br><strong>cv2.Canny(img,80,150)</strong></p> 
<p></p> 
<h3>8、图像金字塔</h3> 
<h4><strong>（1）高斯金字塔</strong></h4> 
<p>向下采样方法（缩小）<br> cv2.pyrDown(img)<br> 将Gi与高斯内核卷积<br> 将所有偶数行和列去除<br> 向上采样方法（放大）<br> cv2.pyrUp(img)<br> 将图像在每个方向扩大为原来的两倍，新增的行和列以0填充<br> 使用先前同样的内核(乘以4)与放大后的图像卷积，获得近似值</p> 
<h4><strong>（2）拉普拉斯金字塔</strong></h4> 
<p><strong>Li=Gi-PyrUp(PyrDown(Gi))</strong><br> 1、低通滤波 2、缩小尺寸 3、放大尺寸 4、图像相减<br> down = cv2.pyrDown(img)<br> down_up = cv2.pyrUp(down)<br> l_1 = img - down_up</p> 
<p></p> 
<h3><strong>9、图像轮廓</strong></h3> 
<p><strong>cv2.findContours(img,mode,method)</strong><br> mode：轮廓检测模式<br> RETR_EXTERNAL：只检测最外面的轮廓<br> RETR_LIST：检测所有的轮廓，并将其保存到一条链表当中<br> RETR_CCOMP：检索所有的轮廓，并将他们组织为两层，顶层是个部分的外部边界，第二层是空洞的边界<br> RETR_TREE：检索所有的轮廓，并重构嵌套轮廓的整个层次<br> method：轮廓逼近方法<br> CHAIN_APPROX_NONE：以Freeman链码的方式输出轮廓，所有其他方法输出多边形(顶点的序列)<br> CHAIN_APPROX_SIMPLE：压缩水平的、垂直的和斜的部分，函数只保留他们的终点部分<br> ret,thresh = cv2.threshold(img,127,255,cv2.THRESH_BINARY)<br><strong>binary,contours,hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)</strong></p> 
<p><strong>轮廓特征<br> 面积<br> cv2.contourArea(contours[0])<br> 周长，True表示闭合<br> cv2.arcLength(contours[0])<br> 轮廓近似</strong><br> epsilon = 0.1 * cv2.arcLength(contours[0],True)<br><strong>cv2.approxPolyDP(contours[0],epsilon,True)</strong><br> 哪个轮廓，周长的百分比<br><strong>边界矩形</strong><br> x,y,w,h = cv2.boundingRect(contours[0])<br> 外接圆<br> (x,y),radius = cv2.minEnclosingCircle(contours[0])</p> 
<p><strong>模板匹配</strong><br> res = cv2.matchTemplate(img,template,cv2.TM_CCOEFF_NORMED)<br> TM_SQDIFF：计算平方不同，计算出来的值越小越相关<br> TM_CCORR：计算相关性，计算出来的值越大越相关<br> TM_CCOEFF：计算相关系数，计算出来的值越大越相关<br> TM_SQDIFF_NORMED：计算归一化平方不同，计算出来的值越接近0越相关<br> TM_CCORR_NORMED：计算归一化相关性，计算出来的值越接近1越相关<br> TM_CCOEFF_NORMED：计算归一化相关系数，计算出来的值越接近1越相关<br> min_val,max_val,min_loc,max_loc = cv2.minMaxLoc(res)</p> 
<h3>
<br> 10、直方图</h3> 
<p><strong>cv2.calcHist(images,channels,mask,histSize,ranges)</strong><br> images：源图像格式为uint8或float32<br> channels：如果是彩色图像,则输入参数为[0~2] BGR<br> mask：掩模图像<br> histSize：bin的数目<br> ranges：像素值范围为0~256<br><strong>cv2.calcHist([img],[0],None,[256],[0,256])</strong></p> 
<p>直方图均衡化<br><strong>cv2.equalizeHist(img)</strong><br> 自适应直方图均衡化(分块均衡化)<br><strong>cv2.createCLAHE(clipLimit=2.0,tileGridSize=(8,8))</strong></p> 
<p>傅里叶变换：以时间为参照就是时域分析，但在频域中是静止的<br> 高频：变化剧烈的灰度分量，如边界<br> 低频：变化缓慢的灰度分量，如一片大海<br> 滤波<br> 低通滤波器：只保留低频会使得图像模糊<br> 高通滤波器：只保留高频会使得图像细节增强<br><strong>cv2.dft()  cv2.idft()  输入图像先转换成np.float32<br> cv2.dtf()返回结果是双通道的(实部，虚部)<br> dft=cv2.dft(img_float32,flags=cv2.DFT_COMPLEX_OUTPUT)</strong><br> dft_shift=np.fft.fftshift(dft)<br> 将值映射回0~255<br> magnitude_spectrum=20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))<br> Magnitude_Specturm图，中心为低频，边缘为高频</p> 
<h3>
<br> 11、文档文字识别</h3> 
<p>OCR Optical Character Recognition<br> 光学字符识别<br><strong>pip install pytesseract</strong><br> 如果环境变量未找到，在Anaconda的Lib中，找到pytesseract.py tesseract_cmd，将斜杠进行更改<br><strong>tesseract.image_to_string(图片)</strong></p> 
<h3>
<br> 12、图像特征</h3> 
<h4>harris角点检测</h4> 
<p>角点响应R值<br> R=detM-a(traceM)^2  detM=x1x2  traceM=x1+x2<br><strong>cv2.cornerHarris()</strong><br> img：数据类型为float32的图像<br> blockSize：角点检测中指定区域的大小<br> ksize：Sobel求导中使用的窗口大小<br> k:取值参数为[0.04, 0.06]<br><strong>cv2.cornerHarris(cv2.cvtColor(cv2.imread(图像名),cv2.COLOR_BGR2GRAY),2,3,0.04)</strong></p> 
<h4>Scale Invariant Feature Transform(SIFT)</h4> 
<p>获取关键点，关键点向量化<br> 平移不变性特征转换<br> 图像尺度空间--高斯模糊实现</p> 
<h4>多分辨率金字塔 DOG Difference of Gaussian 高斯差分金字塔</h4> 
<p>关键点的精确定位<br> 候选点是DOG空间的局部极值点，这些点是离散的，需要对尺度空间DOG函数进行曲线拟合，计算极值点，实现关键点精确定位<br> (可以利用泰勒级数进行展开)<br>  </p> 
<p><strong>消除边界响应</strong><br> Hessian矩阵，另a=xmain为最大特征值，b=ymin为最小特征值<br> 特征点的主方向--一个特征点产生了多个坐标、尺度、方向<br> 生成特征描述--使用直方图统计邻域内像素的梯度和方向<br> opencv 3.4.3以上版本无法使用SIFT函数，需要3.4.3版本以下，pip install opencv-python==3.4.1.15 pip install opencv-contrib-python==3.4.1.15<br><strong>sift = cv2.xfeatures2d.SIFT_create()</strong><br> kp = sift.detect(灰度图, None)</p> 
<h3></h3> 
<h3>13、特征匹配(可用于拼图)</h3> 
<h4>（1）Brute-Force蛮力匹配</h4> 
<p>kp1,des1 = sift.detectAndCompute(img1,None)<br> kp2,des2 = sift.detectAndCompute(img2,None)<br> crossCheck表示两个特征点要户像匹配，如A中的第i个特征点与B中的第j个特征点最近，且B中第j个特征点与A中第i个特征点最近<br> NORM_L2：归一化数组的(欧几里德距离)，如果其他特征计算方法需要考虑不同的匹配计算方式<br><strong>bf = cv2.BFMatcher(crossCheck=True)</strong></p> 
<h4>（2）1对1匹配</h4> 
<p>matches = bf.match(des1,des2)<br> matches = sorted(matches,key=lambda x:x.distance)<br> 两张图像关键点匹配连线<br><strong>cv2.drawMatches(img1,kp1,img2,kp2,matches[:10],None,flags=2)</strong></p> 
<h4>（3）k对最佳匹配</h4> 
<p><strong>bf.knnMatch(des1,des2,k=2)</strong></p> 
<p>如果需要更快速完成操作，可以尝试使用cv2.FlannBasedMatcher</p> 
<h3>
<br> 14、图像拼接</h3> 
<p>随机抽样一致算法(Random sample consensus RANSAC)<br> 容忍范围不断迭代<br> 单应性矩阵<br> 视觉变换矩阵RANSAC+单应性矩阵<br><strong>cv2.findHomography(np.float32(kpsA[i] for(_,i) in matches]), np.float32(kpsB[i] for(_,i) in matches]), cv2.RANSAC, reprojThresh)</strong></p> 
<h3>
<br> 15、背景建模</h3> 
<h4>（1）帧差法</h4> 
<p>场景中目标在运动，对连续两帧图像进行差分运算，不同帧对应的像素点相减，判断灰度差绝对值，当绝对值超过一定阈值时，即为运动目标，从而实现目标的检测功能。<br> 帧差法会引入噪音和空洞问题</p> 
<h4>（2）混合高斯模型</h4> 
<p>背景的实际分布应当是多个高斯分布混合在一起，每个高斯模型也可以带有权重<br> 混合高斯模型学习方法<br> 1、首先初始化每个高斯模型矩阵参数<br> 2、取视频中T帧数据图像用来训练高斯混合模型，第一个像素之后来当第一个高斯分布<br> 3、后面的像素值与前面已有的高斯均值进行比较，如果像素点值与其模型均值差3倍方差，则属于该分布，对其进行参数更新<br> 4、如果后一个像素不满足当前高斯分布，用它来创建一个新的高斯分布<br> 例如测试：对新来的像素点值与混合高斯模型中的每一个均值进行比较，如果其差值在2倍方差之间，则认为是背景，否则是前景。前景赋值为255，背景赋值为0。形成一幅前景二值图。<br><strong>形态学操作<br> cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))<br> 创建混合高斯模型用于背景建模<br> cv2.createBackgroundSubtractorMOG2()</strong></p> 
<p><strong>cv2.morphologyEx(fg.apply(cv2.VideoCapture(视频名)),cv2.MORPH_OPEN,kernel)</strong></p> 
<h3>
<br> 16、光流估计</h3> 
<p>光流：空间运动物体在观测成像平面上的像素运动的瞬时速度，根据各个像素点的速度矢量特征，可以对图像进行动态分析，例如目标跟踪<br> 亮度恒定：同一点随着时间的变化，其亮度不会发生改变<br> 小运动：随着时间的变化不会引起位置的距离变化，只有小运动情况下才能用前后帧之间单位位置变化引起的灰度变化去近似灰度对位置的偏导数<br> 空间一致：一个场景上附近的点投影到图像上也是邻近点，且邻近点速度一直。因为光流基本方程约束只有一个，而要求x,y的方向的速度，需要连立多个方程求解。</p> 
<p><strong>Lucas-kanade算法</strong></p> 
<p>角点检测所需参数<br> feature_params = dict(maxCorners=100,qualityLevel=0.3,minDistance=7)<br> lk_params = dict(winSize=(15,15),maxLevel=2)<br> 角点最大数量(效率)，品质因子(特征值越大的越好，筛选)，距离<br> p0 = cv2.goodFeaturesToTrack(gray_image, mask=None, **feature_params)<br> frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)<br><strong>cv2.calcOpticalFlowPyrLK(gray_image, frame_gray, p0, None, **lk_params)</strong><br> 参数 prevImage前一帧图像 nextImage 当前帧图像<br> prevPts 待跟踪的特征点向量 winSize 搜索窗口的大小 maxLevel 最大的金字塔层数<br> 返回 nextPts 输出跟踪特征点向量<br> status 特征点是否找到，找到状态为1，未找到状态为0</p> 
<p><strong>OpenCV读取Caffe模型<br> cv2.dnn.readNetFromCaffe(txt, model)<br> cv2.dnn.blobFromImage(cv2.resize(image, (224,224), (224,224), (104,117,123)))</strong></p> 
<p><strong>OpenCV追踪算法</strong><br> csrt：cv2.TrackerCSRT_create<br> kcf：cv2.TrackerKCF_create<br> boosting：cv2.TrackerBoosting_create<br> mil：cv2.TrackerMIL_create<br> tld：cv2.TrackerTLD_create<br> medianflow：cv2.TrackerMedianFlow_create<br> mosse：cv2.TrackerMOSSE_create</p> 
<p><strong>多目标追踪</strong><br> trackers = cv2.MultiTracker_create()</p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>