<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Ensembling Off-the-shelf Models for GAN Training（GAN模型迎来预训练时代，仅需1%的训练样本） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Ensembling Off-the-shelf Models for GAN Training（GAN模型迎来预训练时代，仅需1%的训练样本）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p></p> 
<h1 style="text-align:center">Ensembling Off-the-shelf Models for GAN Training</h1> 
<p style="text-align:center">（<strong>集成现成的GAN训练模型</strong>）</p> 
<p style="text-align:center">论文链接：<a class="link-info" href="https://arxiv.org/abs/2112.09130" title="https://arxiv.org/abs/2112.09130">https://arxiv.org/abs/2112.09130</a></p> 
<p style="text-align:center">项目链接：<a class="link-info" href="https://github.com/nupurkmr9/vision-aided-gan" title="https://github.com/nupurkmr9/vision-aided-gan">https://github.com/nupurkmr9/vision-aided-gan</a></p> 
<p style="text-align:center">视频链接：<a class="link-info" href="https://www.youtube.com/watch?v=oHdyJNdQ9E4" title="https://www.youtube.com/watch?v=oHdyJNdQ9E4">https://www.youtube.com/watch?v=oHdyJNdQ9E4</a></p> 
<hr>
<h1>解决了什么问题？</h1> 
<p>每次GAN模型都要从头训练的日子过去了！最近CMU联手Adobe提出了一种新的模型集成策略，<span style="color:#fe2c24"><strong>让GAN模型也能用上预训练，成功解决「判别器过拟合（</strong></span><span style="color:#a2e043"><strong>训练集性能很强，但在验证集上表现得很差</strong></span><span style="color:#fe2c24"><strong>）」这个老大难问题</strong></span>。</p> 
<p></p> 
<p>图像生成本身就需要能够捕捉和模拟真实世界视觉现象中的复杂统计数据，不然生成出来的图片不符合物理世界规律，直接一眼鉴定为「假」。</p> 
<p> 预训练模型提供知识、GAN模型提供生成能力，二者强强联合，多是一件美事！</p> 
<p>问题来了，哪些预训练模型、以及如何结合起来才能改善GAN模型的生成能力？</p> 
<p>最近来自CMU和Adobe的研究人员在CVPR 2022发表了一篇文章，通过「选拔」的方式将预训练模型与GAN模型的训练相结合。</p> 
<p></p> 
<p>GAN模型的训练过程由一个判别器和一个生成器组成，其中判别器用来学习区分真实样本和生成样本的相关统计数据，而生成器的目标则是让生成的图像与真实分布尽可能相同。</p> 
<p>理想情况下，判别器应当能够测量生成图像和真实图像之间的分布差距。</p> 
<p>但在数据量十分有限的情况下，直接上大规模预训练模型作为判别器，非常容易导致生成器被「无情碾压」，然后就「过拟合」了。</p> 
<p>通过在FFHQ 1k数据集上的实验来看，即使采用最新的可微分数据增强方法，判别器仍然会过拟合，训练集性能很强，但在验证集上表现得很差。</p> 
<blockquote> 
 <div class="img-center"> 
  <figure class="image">
   <img alt="" height="347" src="https://images2.imgbox.com/04/3e/6BpZnNgB_o.png" width="942">
   <figcaption>
    <strong>Training and validation accuracy w.r.t. training iterations for our DINO [11] based discriminator vs. baseline StyleGAN2-ADA discriminator on FFHQ 1k dataset.</strong>
   </figcaption>
  </figure>
 </div> 
 <p><strong> Figure 3. </strong> Our discriminator based on pretrained features has higher accuracy on validation real images and thus shows better generalization. In the above training, vision aided adversarial loss is added at the 2M iteration.</p> 
</blockquote> 
<p style="margin-left:.0001pt;text-align:justify"></p> 
<p>此外，判别器可能会关注那些人类无法辨别但对机器来说很明显的伪装。 </p> 
<p></p> 
<p>为了平衡判别器和生成器的能力，研究人员提出将一组不同的预训练模型的表征集合起来作为判别器。</p> 
<p></p> 
<div class="img-center"> 
 <figure class="image">
  <img alt="" height="599" src="https://images2.imgbox.com/ec/03/KQjRveDM_o.png" width="613">
  <figcaption>
   <strong>Vision-aided GAN training. </strong>
  </figcaption>
 </figure>
</div> 
<p> Figure 1. The model bank F consists of widely used and state-of-the-art pretrained networks. We automatically select a subset <img alt="{hat{F}}_{k=1}^{K}" class="mathcode" src="https://images2.imgbox.com/06/08/JIey4Vr6_o.gif"> from F, which can best distinguish between real and fake distribution.<br> Our training procedure consists of creating an ensemble of the <span style="color:#4da8ee"><strong>original discriminator D</strong></span> and <span style="color:#4da8ee"><strong>discriminators <img alt="hat{D}_{k}=hat{C}_{k} circ hat{F}_{k}" class="mathcode" src="https://images2.imgbox.com/39/61/dRXGNUP1_o.gif"></strong></span> based on the feature space of selected off-the-shelf models.<br><img alt="hat{C}_{k}" class="mathcode" src="https://images2.imgbox.com/14/23/7yQBafaR_o.gif"> is a shallow trainable network over the frozen pretrained features.</p> 
<p></p> 
<p>这种方法有两个好处：</p> 
<p>1、在预训练的特征上训练一个浅层分类器是使深度网络适应小规模数据集的常见方法，同时可以减少过拟合。</p> 
<p>也就是说只要把预训练模型的参数固定住，再在顶层加入轻量级的分类网络就可以提供稳定的训练过程。</p> 
<p>比如上面实验中的Ours曲线，可以看到验证集的准确率相比StyleGAN2-ADA要提升不少。</p> 
<p>2、最近也有一些研究证明了，深度网络可以捕获有意义的视觉概念，从低级别的视觉线索（边缘和纹理）到高级别的概念（物体和物体部分）都能捕获。</p> 
<p>建立在这些特征上的判别器可能更符合人类的感知能力。</p> 
<p>并且将多个预训练模型组合在一起后，可以促进生成器在不同的、互补的特征空间中匹配真实的分布。</p> 
<p>为了选择效果最好的预训练网络，研究人员首先搜集了多个sota模型组成一个「模型银行」，包括用于分类的VGG-16，用于检测和分割的Swin-T等。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f6/d8/ASjVzJV4_o.jpg"></p> 
<p> 然后基于特征空间中真实和虚假图像的线性分割，提出一个自动的模型搜索策略，并使用标签平滑和可微分的增强技术来进一步稳定模型训练，减少过拟合。</p> 
<p>具体来说，就是将真实训练样本和生成的图像的并集分成训练集和验证集。</p> 
<p>对于每个预训练的模型，训练一个逻辑线性判别器来分类样本是来自真实样本还是生成的，并在验证分割上使用「负二元交叉熵损失」测量分布差距，并返回误差最小的模型。</p> 
<p>一个较低的验证误差与更高的线性探测精度相关，表明这些特征对于区分真实样本和生成的样本是有用的，使用这些特征可以为生成器提供更有用的反馈。</p> 
<p>研究人员我们用FFHQ和LSUN CAT数据集的1000个训练样本对GAN训练进行了经验验证。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/24/6a/PxEX4I9D_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a9/6a/uDTFUhgl_o.png"></p> 
<p>结果显示，用预训练模型训练的GAN具有更高的线性探测精度，一般来说，可以实现更好的FID指标。</p> 
<p>为了纳入多个现成模型的反馈，文中还探索了两种模型选择和集成策略</p> 
<p>1）K-fixed模型选择策略，在训练开始时选择K个最好的现成模型并训练直到收敛；</p> 
<p>2）K-progressive模型选择策略，在固定的迭代次数后迭代选择并添加性能最佳且未使用的模型。</p> 
<p>实验结果可以发现，与K-fixed策略相比，progressive的方式具有更低的计算复杂度，也有助于选择预训练的模型，从而捕捉到数据分布的不同。例如，通过progressive策略选择的前两个模型通常是一对自监督和监督模型。</p> 
<p>文章中的实验主要以progressive为主。</p> 
<p>最终的训练算法首先训练一个具有标准对抗性损失的GAN。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/5b/e6/CkimOHfg_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/57/f5/6SLXGgn9_o.png"></p> 
<p>给定一个基线生成器，可以使用线性探测搜索到最好的预训练模型，并在训练中引入损失目标函数。</p> 
<p>在K-progressive策略中，在训练了与可用的真实训练样本数量成比例的固定迭代次数后，把一个新的视觉辅助判别器被添加到前一阶段具有最佳训练集FID的快照中。</p> 
<p>在训练过程中，通过水平翻转进行数据增强，并使用可微分的增强技术和单侧标签平滑作为正则化项。</p> 
<p>还可以观察到，只使用现成的模型作为判别器会导致散度（divergence），而原始判别器和预训练模型的组合则可以改善这一情况。</p> 
<p>最终实验展示了在FFHQ、LSUN CAT和LSUN CHURCH数据集的训练样本从1k到10k变化时的结果。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/38/75/3YEuSzaF_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/ca/3e/N03G9SZE_o.png"></p> 
<p>在所有设置中，FID都能获得显著提升，证明了该方法在有限数据场景中的有效性。</p> 
<p>为了定性分析该方法和StyleGAN2-ADA之间的差异，根据两个方法生成的样本质量来看，文中提出的新方法能够提高最差样本的质量，特别是对于FFHQ和LSUN CAT</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/b3/a8/k4fQqKEm_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/96/2e/tdEBiq85_o.png"></p> 
<p>当我们逐步增加下一个判别器时，可以看到线性探测对预训练模型的特征的准确性在逐渐下降，也就是说生成器更强了。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/54/8f/W8ZnysAn_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f0/b1/GLYVwzKj_o.png"></p> 
<p>总的来说，在只有1万个训练样本的情况下，该方法在LSUN CAT上的FID与在160万张图像上训练的StyleGAN2性能差不多。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/f8/d5/r45C2qJU_o.jpg"></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/82/22/zTtAzday_o.png"></p> 
<p>而在完整的数据集上，该方法在LSUN的猫、教堂和马的类别上提高了1.5到2倍的FID。</p> 
<p></p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/79/6d/Coi8l0Ma_o.png"></p> 
<p>​</p> 
<p></p> 
<p><a href="https://www.51cto.com/article/710307.html" title="CMU联手Adobe：GAN模型迎来预训练时代，仅需1%的训练样本-51CTO.COM">CMU联手Adobe：GAN模型迎来预训练时代，仅需1%的训练样本-51CTO.COM</a></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p> 
<p></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>