<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>《沉默的真 相》的舆情分析及文本挖掘（一）——以微博、爱奇艺弹幕、bilibili为例 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《沉默的真 相》的舆情分析及文本挖掘（一）——以微博、爱奇艺弹幕、bilibili为例</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>目录</h3>
 <ul>
<li><a href="#1_9">1.引言</a></li>
<li><a href="#2_13">2.数据来源与研究方法</a></li>
<li>
<ul>
<li><a href="#21%09_15">2.1. 数据来源</a></li>
<li><a href="#22%09_22">2.2. 技术路线</a></li>
</ul>
  </li>
<li><a href="#3%09_33">3. 影视作品《沉默的真相》在社交平台的数据分析结果</a></li>
<li>
<ul>
<li><a href="#31%09_34">3.1. 总体关注情况分析</a></li>
<li>
<ul>
<li><a href="#311%09_35">3.1.1. 确定重要时间节点</a></li>
<li><a href="#312%09_44">3.1.2. 微博上的关注度走势</a></li>
</ul>
   </li>
<li><a href="#32%09_50">3.2. 微博平台关于《沉默的真相》的关注点</a></li>
<li>
<ul>
<li><a href="#321%09TFIDFWordCloud_57">3.2.1. TF-IDF词频统计及WordCloud可视化分析</a></li>
<li><a href="#322%09_87">3.2.2. 文本聚类</a></li>
<li><a href="#323%09LDA_154">3.2.3. LDA主题模型</a></li>
</ul>
  </li>
</ul>
  </li>
<li><a href="#4%09_233">4. 影视作品《沉默的真相》在视频平台的数据分析结果</a></li>
<li>
<ul>
<li><a href="#41%09_234">4.1. 爱奇艺视频网站的弹幕数据分析</a></li>
<li>
<ul>
<li><a href="#411%09_235">4.1.1. 弹幕的总体趋势</a></li>
<li><a href="#412%09_327">4.1.2. 绘制角色词云图</a></li>
</ul>
   </li>
<li><a href="#42%09_332">4.2. 哔哩哔哩平台上的相关视频分析</a></li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<p>本文从社交平台（以新浪微博为例）和视频平台（以爱奇艺、哔哩哔哩为例）两个角度对网络剧《沉默的真相》进行网络舆情和弹幕的文本挖掘分析。同时基于原著小说《长夜难明》绘制了主要角色的人物关系图，并对该商品在京东某店的评论进行了情感分析处理。基于以上对网络剧《沉默的真相》进行了全方位理解，洞察观众的关注点，为其高分评价做出了合理解释。</p> 
<h1>
<a id="1_9"></a>1.引言</h1> 
<p>爱奇艺“迷雾剧场”推出的悬疑短剧《隐秘的角落》，改编自紫金陈的小说《长夜难明》。<br> 由于字数限制，分两篇文章展示。</p> 
<h1>
<a id="2_13"></a>2.数据来源与研究方法</h1> 
<h2>
<a id="21%09_15"></a>2.1. 数据来源</h2> 
<p>本文采用<strong>新浪微博数据</strong>作为数据源进行分析。本文选取的时间段为2020年9月15日18时—27日24时共计12天，利用八爪鱼工具以“#沉默的真相#”为关键词抓取原创微博数据，每条数据包含微博用户名、博文内容、发文时间、转发数、评论数、点赞数、发布设备7个字段，经过数据清洗得到可用数据40875条。<br> <strong>视频网站弹幕功能</strong>的出现正好促进了用户的实时互动性，即用户可以发表和交流自己的看法通过弹幕。爱奇艺是《沉默的真相》的独播平台，因此其弹幕数据是极具代表性的。截止2020年11月11日，利用Python爬虫采集了全集共12集的弹幕数据241554条，每条数据包括集数、用户名、弹幕ID、弹幕内容、出现时间、点赞数6个字段。<br> <strong>哔哩哔哩</strong>，设定B站关键词为“沉默的真相”，截止2020年11月11日，去除涉及版权原因等不正常数据、空值后得到534条相关视频数据，每条数据包含标题、视频时长、发布日期、发布时间、总播放数、总弹幕数、硬币、收藏数、点赞数、分享数、视频标签、发布者、投稿数、粉丝数14个字段。<br> **原著小说商品*选择销量最高，即浦睿文化京东自营店的《长夜难明》，对其商品评论进行采集，得到好、中、差评各504条、24条、21条，会员名称、会员级别、评价星级、评价内容、日期时间5个字段。</p> 
<h2>
<a id="22%09_22"></a>2.2. 技术路线</h2> 
<p><img src="https://images2.imgbox.com/91/fe/JExOMAny_o.png" alt="图2.1 本文的技术路线图"></p> 

 图2.1 本文的技术路线图
. 
<p>本文主要分为三个板块，网剧《沉默的真相》基于社交平台（新浪微博）的舆情分析、在视频平台（爱奇艺、哔哩哔哩）的数据分析，以及最后的关于原著小说《长夜难明》的文本挖掘。大部分都遵循以下四个步骤（见图2.1），从爬虫获取数据、数据预处理、可视化分析到最后的文本挖掘。</p> 
<h1>
<a id="3%09_33"></a>3. 影视作品《沉默的真相》在社交平台的数据分析结果</h1> 
<h2>
<a id="31%09_34"></a>3.1. 总体关注情况分析</h2> 
<h3>
<a id="311%09_35"></a>3.1.1. 确定重要时间节点</h3> 
<p>截止11.06，通过爬<strong>取官方微博号——“网剧沉默的真相”</strong>，去除64条无效微博后共得到257条博文内容，其转发、评论、点赞总量分别高达65万+、7万、76万+。通过统计所有博文的评论数和转发数，可以明显看出6个重要的时间节点（见图3.1），总体可以划分为宣传阶段（6.8-6.9.15）、播出阶段（9.16-9.27）、回落阶段（9.28-10.17），其中7、8月份没有任何发博记录。在宣传阶段的转发量都是极其高的，本文主要对9.15日及播出阶段的数据进行分析。<br> 本文主要分析9.15-9.27公众对于《沉默的真相》的关注点及其关注情况，在此基础上将其划分为定档期和播出期，对应到舆情发展阶段，并对**#沉默的真相#话题下的所有原创微博**进行了统计（见表3.1），为尽量避免水军的影响，该表中的对数据的去重依据是博文内容完全重复，接下来针对微博平台上的分析都是基于该表。<br> <img src="https://images2.imgbox.com/9b/aa/cceGsSl7_o.png" alt="图3.1 官方微博“网剧沉默的真相”微博数据分析结果"></p> 

 图3.1 官方微博“网剧沉默的真相”微博数据分析结果
. 
<p><img src="https://images2.imgbox.com/47/f9/v3TNu3tK_o.png" alt="表3.1 根据重要时间节点划分微博舆情的发展阶段"></p> 

 表3.1 根据重要时间节点划分微博舆情的发展阶段
. 
<h3>
<a id="312%09_44"></a>3.1.2. 微博上的关注度走势</h3> 
<p>本文选择的时间段为该剧的定档期和播出期（2020.9.15-2020.9.27），来观察这段时间内该剧在微博平台上的信息量走势。从微博博文数量走势图（见图3.2）来看，在网剧播出前的关注量较少，而在播出期间的关注度（有关该剧的网络信息量，这里<strong>只考虑#沉默的真相#话题中的原创微博内容</strong>）陡然上升，在播出的第一天就出现明显上升，并在9.20达到峰值；在剧播出后期，信息量逐渐平稳；播出结束后，逐渐减少，呈下降趋势，这在图1.1也有所展现。<br> <img src="https://images2.imgbox.com/56/aa/Go0RDq9K_o.png" alt="图3.2  9.15-9.27#沉默的真相#微博博文数量走势图"></p> 

 图3.2 9.15-9.27#沉默的真相#微博博文数量走势图
. 
<h2>
<a id="32%09_50"></a>3.2. 微博平台关于《沉默的真相》的关注点</h2> 
<p>将爬取的相关微博数据接着进行中文分词处理，TF-IDF词频统计及词云可视化及文本聚类、LDA主题模型分析，来获取微博平台上公众对《沉默的真相》的关注点。<br> 首先进行数据预处理，前文已完成对重复值、缺失值的清洗，接下来需要对文本数据进行中文分词处理，这里我使用jieba实现中文分词，在分词前首先对中文文本进行了同义词替换，同时添加自定义词典、去除停用词（部分重要停用词和替换词见表3.2）。</p> 
<p><img src="https://images2.imgbox.com/2c/95/jYdrnT8y_o.png" alt="在这里插入图片描述"></p> 

 表3.2 部分重要停用词和替换词
. 
<h3>
<a id="321%09TFIDFWordCloud_57"></a>3.2.1. TF-IDF词频统计及WordCloud可视化分析</h3> 
<p>在进行数据预处理和中文分词处理后进行TF-IDF词频统计，定档期和播出期TF-IDF词频前20见图3.7，说明观众对剧集内容的讨论开始加重，而不是宣传阶段仅仅因为明星效应的缘故去讨论该话题。<br> <img src="https://images2.imgbox.com/1a/f7/4P5Wi2xE_o.png" alt="在这里插入图片描述"></p> 

 图3.7 定档期与播出期TF-IDF高频关键词对比
. 
<p>单看TF-IDF排名图中并不容易对比出两个时期的差异，这里我又通过PyEcharts来分别绘制两个阶段的WordCloud（图3.8），这里考虑到对比差异，因此去掉江阳、白宇，词云图中展示了TOP100的关键词，且可以将其重要性归一化地展现。<br> <img src="https://images2.imgbox.com/78/18/Yj3LB89v_o.png" alt="在这里插入图片描述"></p> 

 图3.8 定档期（左）与播出期（右）的词云图
. 
<pre><code class="prism language-python"><span class="token comment"># 微博数据的词云图</span>
<span class="token keyword">from</span> pyecharts <span class="token keyword">import</span> options <span class="token keyword">as</span> opts
<span class="token keyword">from</span> pyecharts<span class="token punctuation">.</span>charts <span class="token keyword">import</span> WordCloud
<span class="token keyword">from</span> pyecharts<span class="token punctuation">.</span><span class="token builtin">globals</span> <span class="token keyword">import</span> SymbolType
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token comment"># 数据</span>
wb <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_excel<span class="token punctuation">(</span><span class="token string">r'D:LearningLPythonbigDataClass_2020Fallpaper_weibo9.16开播-9.27结束_TFIDF关键词前100.xlsx'</span><span class="token punctuation">)</span>
data_wb <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span>wb<span class="token punctuation">[</span><span class="token string">'词语'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>wb<span class="token punctuation">[</span><span class="token string">'重要性'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 渲染图</span>
<span class="token keyword">def</span> <span class="token function">wordcloud_base</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">-</span><span class="token operator">&gt;</span> WordCloud<span class="token punctuation">:</span>
    c <span class="token operator">=</span> <span class="token punctuation">(</span>
        WordCloud<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token punctuation">.</span>add<span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">,</span> data_wb<span class="token punctuation">,</span> word_size_range<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">]</span><span class="token punctuation">,</span> shape<span class="token operator">=</span><span class="token string">'triangle-forward'</span><span class="token punctuation">)</span>  <span class="token comment"># SymbolType.ROUND_RECT</span>
        <span class="token punctuation">.</span>set_global_opts<span class="token punctuation">(</span>title_opts<span class="token operator">=</span>opts<span class="token punctuation">.</span>TitleOpts<span class="token punctuation">(</span>title<span class="token operator">=</span><span class="token string">'9.16-9.27 WordCloud词云'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>
    <span class="token keyword">return</span> c
<span class="token comment"># 生成图</span>
wordcloud_base<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>render<span class="token punctuation">(</span><span class="token string">'9.16-9.27词云图.html'</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="322%09_87"></a>3.2.2. 文本聚类</h3> 
<p>（1）K-Means聚类<br> 选择定档期的全部数据，以及播出期的随机10000条数据进行K-Means分析，预设n=3时的输出图像如图3.9所示。可以观察到K-Means的聚类效果并不是很好，且简单的聚类是无法进行深入分析的，需要结合具体数据集进行分析，但其解释性始终不是很好。但是实际的数据分析会引入类标签或注释，因此我们引入主题关键词聚类和LDA主题模型分析，这有助于理解文本挖掘和主题分析。<br> <img src="https://images2.imgbox.com/d9/5d/lzh0b92I_o.png" alt="在这里插入图片描述"></p> 
<p>（2）层次聚类<br> 本文选用Ward方法，即沃德方差最小化算法进行凝聚层析聚类，由于这里的主题词太多，所以这里采用jieba分词提取每条微博数据（对应一行数据）的Top100特征词，再存储至txt文件中进行层次聚类分析。通过不断地调节关键性参数以更好地展示聚类结果，我需要调节min_df用于删除不经常出现的术语，以及max_df用于删除过于频繁出现的术语，最终设置定档期为[40,200]，播出期为[500,2000]，最终分别得到31个词、61个词的聚类结果如图3.10，3.11所示。</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> jieba
<span class="token keyword">import</span> jieba<span class="token punctuation">.</span>analyse
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">from</span> pylab <span class="token keyword">import</span> mpl
<span class="token keyword">from</span> collections <span class="token keyword">import</span> Counter
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>pairwise <span class="token keyword">import</span> cosine_similarity
<span class="token keyword">from</span> scipy<span class="token punctuation">.</span>cluster<span class="token punctuation">.</span>hierarchy <span class="token keyword">import</span> ward<span class="token punctuation">,</span> dendrogram
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer<span class="token punctuation">,</span> TfidfTransformer

mpl<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'SimHei'</span><span class="token punctuation">]</span>

<span class="token comment">#------------------------------  第一步 获取TOP100 ------------------------------</span>
<span class="token comment">#------------------------------ 第二步 中文分词过滤 ------------------------------</span>
<span class="token comment">#------------------------------ 第三步 相相关计算 ------------------------------ </span>
text <span class="token operator">=</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'C-key.txt'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>read<span class="token punctuation">(</span><span class="token punctuation">)</span>
list1 <span class="token operator">=</span> text<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">"n"</span><span class="token punctuation">)</span>

mytext_list <span class="token operator">=</span> list1
count_vec <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span>min_df<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> max_df<span class="token operator">=</span><span class="token number">2000</span><span class="token punctuation">)</span>
xx1 <span class="token operator">=</span> count_vec<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>list1<span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span>
word <span class="token operator">=</span> count_vec<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span> 
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"word feature length: {}"</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>word<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>xx1<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>xx1<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
titles <span class="token operator">=</span> word
<span class="token comment">#------------------------------ 第四步 相似度计算 ------------------------------ </span>
df <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>xx1<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token string">'spearman'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>df<span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token string">'kendall'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

dist <span class="token operator">=</span> df<span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dist<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>dist<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>dist<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

<span class="token comment">#------------------------------ 第五步 可视化分析 ------------------------------ </span>
<span class="token comment"># define the linkage_matrix using ward clustering pre-computed distances</span>
linkage_matrix <span class="token operator">=</span> ward<span class="token punctuation">(</span>dist<span class="token punctuation">)</span>
fig<span class="token punctuation">,</span> ax <span class="token operator">=</span> plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># set size</span>
ax <span class="token operator">=</span> dendrogram<span class="token punctuation">(</span>linkage_matrix<span class="token punctuation">,</span> orientation<span class="token operator">=</span><span class="token string">"right"</span><span class="token punctuation">,</span> labels<span class="token operator">=</span>titles<span class="token punctuation">)</span><span class="token punctuation">;</span>

<span class="token comment"># how plot with tight layout</span>
plt<span class="token punctuation">.</span>xticks<span class="token punctuation">(</span>fontsize<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>yticks<span class="token punctuation">(</span>fontsize<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>tight_layout<span class="token punctuation">(</span><span class="token punctuation">)</span> 

<span class="token comment"># save figure as ward_clusters</span>
plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'9.16-9.27 Tree_word4.png'</span><span class="token punctuation">,</span> dpi<span class="token operator">=</span><span class="token number">200</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ef/87/H6LQfqTV_o.png" alt="在这里插入图片描述"></p> 
<p>播出期可以提取出6个小主题，从上至下分别是原著小说（紫金陈、隐秘的角落、还原、原著）；演员（廖凡、张超、宁理）；评价（不错、值得安利、细节、三线并行）；广告植入；升华主题（信念、光明、希望、付出代价）；情感（眼泪、好哭、暴哭、压抑）。总的来说，定档期主要是对其播出平台和时间的宣传以及原著粉丝、主演粉丝对剧集的期待；而播出期主要是对剧集的正面评价、主旨理解、情感输出等。</p> 
<h3>
<a id="323%09LDA_154"></a>3.2.3. LDA主题模型</h3> 
<p>LDA主题分布分析需要设置不同的主题值，本文指定抽取3个主题中的前20个高频词汇，对于播出期，去掉关键词“白宇、江阳”，设定<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        λ
       
       
        =
       
       
        0.88
       
      
      
       lambda=0.88
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault">λ</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">8</span><span class="mord">8</span></span></span></span></span>时可视化效果如图3.13所示。此时困惑度为534.498，困惑度较定档期低些，说明聚类的效果稍好。从结果来看，3个主题是比较清晰的，第1主题代表网剧的定档时间为9月16日晚20点，第2主题为公众对网剧的期待、喜欢，第3主题为“今晚一定追剧”。</p> 
<pre><code class="prism language-python"><span class="token comment"># 微博信息-LDA主题模型</span>
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> TfidfVectorizer<span class="token punctuation">,</span> CountVectorizer
<span class="token keyword">import</span> sys
<span class="token keyword">import</span> codecs
<span class="token keyword">import</span> importlib
importlib<span class="token punctuation">.</span><span class="token builtin">reload</span><span class="token punctuation">(</span>sys<span class="token punctuation">)</span>
<span class="token comment">#---------------------  第一步 读取数据(已分词)  ----------------------</span>
corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token comment"># 读取预料 一行预料为一个文档</span>
<span class="token keyword">for</span> line <span class="token keyword">in</span> <span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">'9.15定档-9.16开播_分词.txt'</span><span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    corpus<span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        
<span class="token comment">#-----------------------  第二步 计算TF-IDF值  ----------------------- </span>
<span class="token comment"># 设置特征数</span>
n_features <span class="token operator">=</span> <span class="token number">500</span>
tf_vectorizer <span class="token operator">=</span> TfidfVectorizer<span class="token punctuation">(</span>strip_accents <span class="token operator">=</span> <span class="token string">'unicode'</span><span class="token punctuation">,</span>
                                max_features<span class="token operator">=</span>n_features<span class="token punctuation">,</span>
                                stop_words<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'沉默'</span><span class="token punctuation">,</span><span class="token string">'white'</span><span class="token punctuation">,</span><span class="token string">'真相'</span><span class="token punctuation">,</span><span class="token string">'我要'</span><span class="token punctuation">,</span><span class="token string">'想到'</span><span class="token punctuation">,</span><span class="token string">'看到'</span><span class="token punctuation">,</span><span class="token string">'看起来'</span><span class="token punctuation">,</span>
                                            <span class="token string">'可以'</span><span class="token punctuation">,</span><span class="token string">'微博'</span><span class="token punctuation">,</span><span class="token string">'比较'</span><span class="token punctuation">,</span><span class="token string">'这里'</span><span class="token punctuation">,</span><span class="token string">'视频'</span><span class="token punctuation">,</span><span class="token string">'真的'</span><span class="token punctuation">,</span><span class="token string">'一个'</span><span class="token punctuation">,</span>
                                            <span class="token string">'自己'</span><span class="token punctuation">,</span><span class="token string">'两集'</span><span class="token punctuation">,</span><span class="token string">'同事'</span><span class="token punctuation">,</span><span class="token string">'沉真'</span><span class="token punctuation">,</span><span class="token string">'老师'</span><span class="token punctuation">,</span><span class="token string">'没有'</span><span class="token punctuation">,</span><span class="token string">'时候'</span><span class="token punctuation">,</span>
                                            <span class="token string">'为了'</span><span class="token punctuation">,</span><span class="token string">'他们'</span><span class="token punctuation">,</span><span class="token string">'什么'</span><span class="token punctuation">,</span><span class="token string">'全文'</span><span class="token punctuation">,</span><span class="token string">'收起'</span><span class="token punctuation">,</span><span class="token string">'我们'</span><span class="token punctuation">,</span><span class="token string">'知道'</span><span class="token punctuation">,</span>
                                            <span class="token string">'不是'</span><span class="token punctuation">,</span><span class="token string">'就是'</span><span class="token punctuation">,</span><span class="token string">'这样'</span><span class="token punctuation">,</span><span class="token string">'有人'</span><span class="token punctuation">,</span><span class="token string">'怎么'</span><span class="token punctuation">,</span><span class="token string">'这个'</span><span class="token punctuation">,</span><span class="token string">'那个'</span><span class="token punctuation">,</span>
                                            <span class="token string">'这部'</span><span class="token punctuation">,</span><span class="token string">'有点'</span><span class="token punctuation">,</span><span class="token string">'那么'</span><span class="token punctuation">,</span><span class="token string">'白宇'</span><span class="token punctuation">,</span><span class="token string">'by'</span><span class="token punctuation">,</span><span class="token string">'下三土'</span><span class="token punctuation">,</span><span class="token string">'江阳'</span><span class="token punctuation">,</span>
                                            <span class="token string">'微博'</span><span class="token punctuation">,</span><span class="token string">'微博'</span><span class="token punctuation">,</span><span class="token string">'微博'</span><span class="token punctuation">,</span><span class="token string">'微博'</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
                                max_df <span class="token operator">=</span> <span class="token number">0.8</span><span class="token punctuation">,</span>
                                min_df <span class="token operator">=</span> <span class="token number">0.002</span><span class="token punctuation">)</span> <span class="token comment">#去除文档内出现几率过大或过小的词汇</span>
tf <span class="token operator">=</span> tf_vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>
<span class="token comment">#-------------------------  第三步 LDA分析  ------------------------ </span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>decomposition <span class="token keyword">import</span> LatentDirichletAllocation
<span class="token comment"># 设置主题数</span>
n_topics <span class="token operator">=</span> <span class="token number">3</span>

lda <span class="token operator">=</span> LatentDirichletAllocation<span class="token punctuation">(</span>n_components<span class="token operator">=</span>n_topics<span class="token punctuation">,</span>
                                max_iter<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span>
                                learning_method<span class="token operator">=</span><span class="token string">'online'</span><span class="token punctuation">,</span>
                                learning_offset<span class="token operator">=</span><span class="token number">50</span><span class="token punctuation">,</span>
                                random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
lda<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>tf<span class="token punctuation">)</span>

<span class="token comment"># 显示主题数 model.topic_word_</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lda<span class="token punctuation">.</span>components_<span class="token punctuation">)</span>
<span class="token comment"># 几个主题就是几行 多少个关键词就是几列 </span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lda<span class="token punctuation">.</span>components_<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>                         

<span class="token comment"># 计算困惑度</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">u'困惑度：'</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lda<span class="token punctuation">.</span>perplexity<span class="token punctuation">(</span>tf<span class="token punctuation">,</span>sub_sampling <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 主题-关键词分布</span>
<span class="token keyword">def</span> <span class="token function">print_top_words</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> tf_feature_names<span class="token punctuation">,</span> n_top_words<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> topic_idx<span class="token punctuation">,</span>topic <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>components_<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># lda.component相当于model.topic_word_</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Topic #%d:'</span> <span class="token operator">%</span> topic_idx<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span><span class="token punctuation">[</span>tf_feature_names<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> topic<span class="token punctuation">.</span>argsort<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token operator">-</span>n_top_words<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">""</span><span class="token punctuation">)</span>

<span class="token comment"># 定义好函数之后 暂定每个主题输出前20个关键词</span>
n_top_words <span class="token operator">=</span> <span class="token number">20</span>
tf_feature_names <span class="token operator">=</span> tf_vectorizer<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token comment"># 调用函数</span>
print_top_words<span class="token punctuation">(</span>lda<span class="token punctuation">,</span> tf_feature_names<span class="token punctuation">,</span> n_top_words<span class="token punctuation">)</span>
<span class="token comment">#------------------------  第四步 可视化分析  ------------------------- </span>
<span class="token keyword">import</span> pyLDAvis
<span class="token keyword">import</span> pyLDAvis<span class="token punctuation">.</span>sklearn
data <span class="token operator">=</span> pyLDAvis<span class="token punctuation">.</span>sklearn<span class="token punctuation">.</span>prepare<span class="token punctuation">(</span>lda<span class="token punctuation">,</span>tf<span class="token punctuation">,</span>tf_vectorizer<span class="token punctuation">)</span>
<span class="token comment">#print(data)</span>
<span class="token comment">#显示图形</span>
pyLDAvis<span class="token punctuation">.</span>show<span class="token punctuation">(</span>data<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/3e/21/f4zLEj4T_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/7c/7b/ii6hZVEP_o.png" alt="在这里插入图片描述"></p> 

 图3.13 播出期的LDA主题模型分析结果
. 
<h1>
<a id="4%09_233"></a>4. 影视作品《沉默的真相》在视频平台的数据分析结果</h1> 
<h2>
<a id="41%09_234"></a>4.1. 爱奇艺视频网站的弹幕数据分析</h2> 
<h3>
<a id="411%09_235"></a>4.1.1. 弹幕的总体趋势</h3> 
<p>利用Python爬虫，首先寻找获取每集视频的tvid参数，进一步发起请求，获取爱奇艺12集的弹幕评论信息分别保存到excel文档，爬取到共90032个用户发送的241554条弹幕。</p> 
<pre><code class="prism language-python"><span class="token comment">#爱奇异 获取弹幕</span>
<span class="token keyword">import</span> requests
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">import</span> zlib
<span class="token keyword">import</span> re
<span class="token keyword">import</span> time

<span class="token keyword">def</span> <span class="token function">get_aiqiyi_danmu</span><span class="token punctuation">(</span>tvid<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">"""
    功能：给定tvid，获取爱奇艺一集的弹幕评论信息
    """</span>
    <span class="token comment"># 建立空df</span>
    df_all <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment"># 初始page_num</span>
    page_num <span class="token operator">=</span> <span class="token number">1</span>
    <span class="token keyword">while</span> <span class="token boolean">True</span><span class="token punctuation">:</span>
        <span class="token comment"># 打印进度</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'正在获取第</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>page_num<span class="token punctuation">}</span></span><span class="token string">页的弹幕数据'</span></span><span class="token punctuation">)</span>

        <span class="token keyword">try</span><span class="token punctuation">:</span>
            <span class="token comment"># 获取URL</span>
            url <span class="token operator">=</span> <span class="token string-interpolation"><span class="token string">f'https://cmts.iqiyi.com/bullet/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">str</span><span class="token punctuation">(</span>tvid<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token format-spec">-2]</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">str</span><span class="token punctuation">(</span>tvid<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">:</span><span class="token format-spec">]</span><span class="token punctuation">}</span></span><span class="token string">/</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span><span class="token builtin">str</span><span class="token punctuation">(</span>tvid<span class="token punctuation">)</span><span class="token punctuation">}</span></span><span class="token string">_300_</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>page_num<span class="token punctuation">}</span></span><span class="token string">.z'</span></span>

            <span class="token comment"># 添加headers</span>
            headers <span class="token operator">=</span> <span class="token punctuation">{<!-- --></span>
                <span class="token string">'user-agent'</span><span class="token punctuation">:</span> <span class="token string">'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36'</span>
            <span class="token punctuation">}</span>
            <span class="token comment"># 发起请求</span>
            <span class="token keyword">try</span><span class="token punctuation">:</span>
                r <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> timeout<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
            <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>
                r <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> timeout<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>
            <span class="token comment"># 转换为arrry</span>
            zarray <span class="token operator">=</span> <span class="token builtin">bytearray</span><span class="token punctuation">(</span>r<span class="token punctuation">.</span>content<span class="token punctuation">)</span>
            <span class="token comment"># 解压字符串</span>
            xml <span class="token operator">=</span> zlib<span class="token punctuation">.</span>decompress<span class="token punctuation">(</span>zarray<span class="token punctuation">,</span> <span class="token number">15</span> <span class="token operator">+</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>
            <span class="token comment"># 用户名</span>
            name <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">'&lt;name&gt;(.*?)&lt;/name&gt;'</span><span class="token punctuation">,</span> xml<span class="token punctuation">)</span>
            <span class="token comment"># 评论ID</span>
            contentId <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">'&lt;contentId&gt;(.*?)&lt;/contentId&gt;'</span><span class="token punctuation">,</span> xml<span class="token punctuation">)</span>
            <span class="token comment"># 评论信息</span>
            content <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">'&lt;content&gt;(.*?)&lt;/content&gt;'</span><span class="token punctuation">,</span> xml<span class="token punctuation">)</span>
            <span class="token comment"># 展示时间</span>
            showTime <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">'&lt;showTime&gt;(.*?)&lt;/showTime&gt;'</span><span class="token punctuation">,</span> xml<span class="token punctuation">)</span>
            <span class="token comment"># 点赞次数</span>
            likeCount <span class="token operator">=</span> re<span class="token punctuation">.</span>findall<span class="token punctuation">(</span><span class="token string">'&lt;likeCount&gt;(.*?)&lt;/likeCount&gt;'</span><span class="token punctuation">,</span> xml<span class="token punctuation">)</span>
            <span class="token comment"># 保存数据</span>
            df_one <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span><span class="token punctuation">{<!-- --></span>
                <span class="token string">'name'</span><span class="token punctuation">:</span> name<span class="token punctuation">,</span>
                <span class="token string">'contentId'</span><span class="token punctuation">:</span> contentId<span class="token punctuation">,</span>
                <span class="token string">'content'</span><span class="token punctuation">:</span> content<span class="token punctuation">,</span>
                <span class="token string">'showTime'</span><span class="token punctuation">:</span> showTime<span class="token punctuation">,</span>
                <span class="token string">'likeCount'</span><span class="token punctuation">:</span> likeCount
            <span class="token punctuation">}</span><span class="token punctuation">)</span>
            <span class="token comment"># 循环追加</span>
            df_all <span class="token operator">=</span> df_all<span class="token punctuation">.</span>append<span class="token punctuation">(</span>df_one<span class="token punctuation">,</span> ignore_index<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
            <span class="token comment"># 休眠一秒</span>
            time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
            <span class="token comment"># 页数+1</span>
            page_num <span class="token operator">+=</span> <span class="token number">1</span>
        <span class="token keyword">except</span> Exception <span class="token keyword">as</span> e<span class="token punctuation">:</span>
            <span class="token keyword">print</span><span class="token punctuation">(</span>e<span class="token punctuation">)</span>
            <span class="token keyword">break</span>
    <span class="token keyword">return</span> df_all
<span class="token comment"># 抓包获取视频tvid</span>
tvid_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">8928607238794800</span><span class="token punctuation">,</span> <span class="token number">1633711019810300</span><span class="token punctuation">,</span> <span class="token number">8520304822314600</span><span class="token punctuation">,</span>
             <span class="token number">2168755640240300</span><span class="token punctuation">,</span> <span class="token number">8992780506613300</span><span class="token punctuation">,</span> <span class="token number">2406998896996700</span><span class="token punctuation">,</span>
             <span class="token number">6095357464723600</span><span class="token punctuation">,</span> <span class="token number">1664193322029900</span><span class="token punctuation">,</span> <span class="token number">4568835856548000</span><span class="token punctuation">,</span>
             <span class="token number">2473578662358100</span><span class="token punctuation">,</span> <span class="token number">3822019452341300</span><span class="token punctuation">,</span> <span class="token number">2827333801193900</span><span class="token punctuation">]</span>
episodes_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'第一集 张超强闯地铁站'</span><span class="token punctuation">,</span> <span class="token string">'第二集 张晓倩收到匿名信'</span><span class="token punctuation">,</span> <span class="token string">'第三集 张晓倩收到匿名信'</span><span class="token punctuation">,</span>
                 <span class="token string">'第四集 严良再访李静'</span><span class="token punctuation">,</span> <span class="token string">'第五集 江阳接连受到恐吓'</span><span class="token punctuation">,</span> <span class="token string">'第六集 江阳朱伟开始走访'</span><span class="token punctuation">,</span>
                 <span class="token string">'第七集 严良知晓爆炸原理'</span><span class="token punctuation">,</span> <span class="token string">'第八集 侯贵平案实情曝光'</span><span class="token punctuation">,</span> <span class="token string">'第九集 江阳得到重要线索'</span><span class="token punctuation">,</span>
                 <span class="token string">'第十集 江阳遭污蔑索取贿赂'</span><span class="token punctuation">,</span> <span class="token string">'第十一集 严良找到匿名寄信人'</span><span class="token punctuation">,</span> <span class="token string">'第十二集 江阳死因揭秘'</span><span class="token punctuation">]</span>

<span class="token comment"># 循环获取所有集数据</span>
<span class="token keyword">for</span> tvid<span class="token punctuation">,</span> episodes <span class="token keyword">in</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>tvid_list<span class="token punctuation">,</span> episodes_list<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>tvid<span class="token punctuation">,</span> episodes<span class="token punctuation">)</span>
    <span class="token comment"># 获取数据</span>
    df <span class="token operator">=</span> get_aiqiyi_danmu<span class="token punctuation">(</span>tvid<span class="token operator">=</span>tvid<span class="token punctuation">)</span>
    <span class="token comment"># 插入列</span>
    df<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'episodes'</span><span class="token punctuation">,</span> episodes<span class="token punctuation">)</span>
    <span class="token comment"># 导出数据</span>
    df<span class="token punctuation">.</span>to_excel<span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'aiqiyi_</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>episodes<span class="token punctuation">}</span></span><span class="token string">.xlsx'</span></span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="412%09_327"></a>4.1.2. 绘制角色词云图</h3> 
<p>首先制作角色词典（见表4.1），接下来通过提取与主角相关的弹幕，即包含了姓名的关键字，来分别绘制江阳、朱伟、严良、张超、侯贵平的角色词云图（见图4.4），其中白宇塑造的正义形象江阳，提及频率远高于其他角色。<br> <img src="https://images2.imgbox.com/f1/c3/J4SNol7E_o.png" alt="在这里插入图片描述"><br> 通过观察五个主角的词云图，可以发现一些有趣的现象。第一，不同角色对应的感情色彩是不同的，例如江阳是哭泣（&amp;#128557）、严良是笑哭（&amp;#128514）、加入廖凡后是点赞（&amp;#128077）。第二，明显观察到与其他网剧相关联的地方，例如严良词云图出现大量《隐秘的角落》中的角色，朱朝阳、普、/秦昊等，因为《隐秘的角落》中也同样出现了名为“严良”的角色；侯贵平词云图中也出现《人民的名义》角色侯亮平，很可能是因为名字相仿而已。第三，只有张超词云图中李丰田的比重几乎与角色名称重要性一样大，“李丰田”是网络剧《无证之罪》中登场的虚拟角色，“人狠话不多”也是对其的特有评价。第四，紫金陈“社会派”推理作品三部曲有《无证之罪》、《坏小孩》、《长夜难明》，分别对应于网剧《无证之罪》、《隐秘的角落》、《沉默的真相》，这三部剧很可能拥有共同的影迷和粉丝，具有很强的关联性。<br> <img src="https://images2.imgbox.com/83/ab/55kxnYkb_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="42%09_332"></a>4.2. 哔哩哔哩平台上的相关视频分析</h2> 
<p>首先爬取得到哔哩哔哩534条相关视频数据，观察到总播放数、总弹幕数、硬币、收藏数、点赞数、分享数这6个字段中既包含有“182.0万”也有“354”两种不一致的表示方法；视频时长中也有“01:09”和“01:06:11”两种方式，数据质量并不高。因此我进行了数据转换消除异构数据的影响，例如将“182.0万”改为“1820000”，将“01:09”改为“00:01:09”。<br> 观察到变量总播放数、总弹幕数、硬币、收藏数、点赞数、分享数以及与UP主的投稿数、粉丝数可能存在某种关联，因此本文主要采用皮尔逊Pearson相关系数来度量连续变量之间的线性相关强度；同时利用seaborn.heatmap生成热力图（见图4.7），颜色越深表示相关性越强。可以明显看出投稿数与其他变量的相关性都不是很强，都没有超过0.4；收藏数与硬币的相关性达到最高，为0.96，与B站的“一键三连”机制，即同时“点赞、投币、收藏”有一定的关系；总播放数与除投稿数的6个变量相关性都很高，都超过了0.8。</p> 
<pre><code class="prism language-python"><span class="token comment"># 相关系数矩阵</span>
<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd
<span class="token keyword">from</span> pandas <span class="token keyword">import</span> Series<span class="token punctuation">,</span> DataFrame

df_train <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_excel<span class="token punctuation">(</span><span class="token string">r'D:LearningLPythonbigDataClass_2020Fallpaper_aiyiqibilibili_视频采集_清洗后.xlsx'</span><span class="token punctuation">,</span> header<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> skiprows<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>
df_train<span class="token punctuation">.</span>info<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 显示数据信息</span>

t <span class="token operator">=</span> np<span class="token punctuation">.</span>around<span class="token punctuation">(</span>df_train<span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> decimals<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">)</span>  <span class="token comment"># 这里是将矩阵结果保留4位小数</span>
tt <span class="token operator">=</span> df_train<span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 默认保留6位小数</span>
mm <span class="token operator">=</span> df_train<span class="token punctuation">[</span><span class="token string">'总播放数'</span><span class="token punctuation">]</span><span class="token punctuation">.</span>corr<span class="token punctuation">(</span>df_train<span class="token punctuation">[</span><span class="token string">'粉丝数'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 进行两列之间的相关性分析</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>mm<span class="token punctuation">)</span>
oo <span class="token operator">=</span> df_train<span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">'总播放数'</span><span class="token punctuation">,</span> <span class="token string">'总弹幕数'</span><span class="token punctuation">,</span> <span class="token string">'硬币'</span><span class="token punctuation">,</span> <span class="token string">'收藏数'</span><span class="token punctuation">,</span> <span class="token string">'分享数'</span><span class="token punctuation">,</span> <span class="token string">'粉丝数'</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">.</span>corr<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 计算指定多列相关系数</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>oo<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>subplots<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">11</span><span class="token punctuation">,</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 设置画面大小</span>
plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'font.sans-serif'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'SimHei'</span><span class="token punctuation">]</span>  <span class="token comment"># 指定默认字体</span>
plt<span class="token punctuation">.</span>rcParams<span class="token punctuation">[</span><span class="token string">'axes.unicode_minus'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token boolean">False</span>  <span class="token comment"># 解决保存图像是负号'-'显示为方块的问题,负号正常显示</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'变量相关系数 - 热图n'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">)</span>  <span class="token comment"># 添加图表标题“变量相关系数 - 热图”,fontsize=18 字体大小 可省略</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'变量相关系数 - 热图n'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>t<span class="token punctuation">,</span> annot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> vmax<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> vmin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> xticklabels<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> yticklabels<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> square<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"YlGnBu"</span><span class="token punctuation">,</span>
            linewidths<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> linecolor<span class="token operator">=</span><span class="token string">'white'</span><span class="token punctuation">,</span> mask<span class="token operator">=</span>t <span class="token operator">&lt;</span> <span class="token number">0.8</span><span class="token punctuation">)</span>  <span class="token comment"># mask=t &lt; 0.8等价于mask=(t &lt; 0.8)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">'bilibili变量相关系数热图n'</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">18</span><span class="token punctuation">)</span>
sns<span class="token punctuation">.</span>heatmap<span class="token punctuation">(</span>t<span class="token punctuation">,</span> annot<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> vmax<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> vmin<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> xticklabels<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> yticklabels<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> square<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"YlGnBu"</span><span class="token punctuation">,</span>
            linewidths<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> linecolor<span class="token operator">=</span><span class="token string">'white'</span><span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span>

plt<span class="token punctuation">.</span>savefig<span class="token punctuation">(</span><span class="token string">'./bilibili热图.png'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/2c/fe/t1sdORI6_o.png" alt="在这里插入图片描述"></p> 
<p>由于篇幅限制，关于本剧的原著小说《长夜难明》的数据分析结果将在下一篇文章中进行展示。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>