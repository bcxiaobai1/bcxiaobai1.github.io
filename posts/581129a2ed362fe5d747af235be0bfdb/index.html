<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【BBuf的CUDA笔记】一，解析OneFlow Element-Wise 算子实现 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【BBuf的CUDA笔记】一，解析OneFlow Element-Wise 算子实现</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="0x0__1"></a>0x0. 前言</h1> 
<p>由于CUDA水平太菜，所以一直没写过这方面的笔记。现在日常的工作中已经不能离开写CUDA代码，所以准备学习ZZK随缘做一做CUDA的笔记记录一下学习到的知识和技巧。这篇文章记录的是阅读OneFlow的Element-Wise系列CUDA算子实现方案学习到的技巧，希望可以帮助到一起入门CUDA的小伙伴们。Elemet-Wise算子指的是针对输入Tensor进行逐元素操作，比如ReLU就是针对输入Tensor的每个值进行判断是否大于0，大于0的话输出就是输入否则就是0。用CUDA来表达最简单的写法就是：</p> 
<pre><code class="prism language-cpp">__global__ <span class="token keyword">void</span> <span class="token function">relu_kernel</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token operator">*</span> input<span class="token punctuation">,</span> <span class="token keyword">float</span><span class="token operator">*</span> output<span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
  <span class="token keyword">int32_t</span> idx <span class="token operator">=</span> blockIdx<span class="token punctuation">.</span>x <span class="token operator">*</span> blockDim<span class="token punctuation">.</span>x <span class="token operator">+</span> threadIdx<span class="token punctuation">.</span>x<span class="token punctuation">;</span>
  output<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span> <span class="token operator">&lt;</span> <span class="token number">0</span> <span class="token operator">?</span> <span class="token number">0</span> <span class="token operator">:</span> input<span class="token punctuation">[</span>idx<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">{<!-- --></span>
  <span class="token keyword">float</span><span class="token operator">*</span> input<span class="token punctuation">;</span>
  <span class="token keyword">float</span><span class="token operator">*</span> output<span class="token punctuation">;</span>
  <span class="token keyword">int32_t</span> elem_cnt <span class="token operator">=</span> <span class="token number">3</span><span class="token operator">*</span><span class="token number">224</span><span class="token operator">*</span><span class="token number">224</span><span class="token punctuation">;</span>
  
  <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>input<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>elem_cnt<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaMalloc</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>output<span class="token punctuation">,</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span><span class="token keyword">float</span><span class="token punctuation">)</span><span class="token operator">*</span>elem_cnt<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">int32_t</span> thread_num <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">;</span>
  <span class="token keyword">int32_t</span> grid_size <span class="token operator">=</span> <span class="token punctuation">(</span>elem_cnt <span class="token operator">+</span> thread_num <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> thread_num<span class="token punctuation">;</span>
  relu_kernel<span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>grid_size<span class="token punctuation">,</span> thread_num<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>src<span class="token punctuation">,</span> dst<span class="token punctuation">)</span><span class="token punctuation">;</span> 
  
  <span class="token function">cudaDeviceSynchronize</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>src<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token function">cudaFree</span><span class="token punctuation">(</span>dst<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>虽然这种写法非常简单明了，但却存在明显的性能问题。所以这篇文章将基于OneFlow开源的Element-Wise CUDA算子方案来解释如何写一个高性能的Element-Wise CUDA算子。</p> 
<h1>
<a id="0x1__30"></a>0x1. 性能</h1> 
<p>以GELU激活函数为例子，分别测试 dtype = float32，不同shape下的前向耗时以及带宽利用率（NVIDIA A100-PCIE-40GB）。性能情况如下图所示：</p> 
<p><img src="https://images2.imgbox.com/e7/b8/Wknawjl4_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/e5/2a/pPOi2fNi_o.png" alt="在这里插入图片描述"></p> 
<p>可以看到对于 GeLU 来说，无论是性能还是带宽 OneFlow 的实现都是更优的，接下来我们就来了解一下为什么 OneFlow 的 Element-Wise 算子性能可以做到更优。</p> 
<h1>
<a id="0x2__40"></a>0x2. 用法</h1> 
<p>OneFlow在 <a href="https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh">elementwise.cuh</a> 文件中分别针对一元，二元，三元运算的 Element-Wise 操作实现了模板函数。在包含这个头文件之后我们可以使用 <code>cuda::elementwise::Unary/Binary/Ternary</code> 这几个模板函数来针对我们自己定义的 Element-Wise 操作进行计算。注意，这里说的一元，二元，三元代表的是这个 Element-Wise 操作有几个输入 Tensor。</p> 
<p>我们举个例子，假设我们要做的 Element-Wise 操作是逐点乘法，也即有 2 个输入Tensor x 和 y，然后 x 和 y的形状和数据类型都是一致的。那么我们可以定义一个模板类：</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">MultiplyFunctor</span> <span class="token punctuation">{<!-- --></span>
  OF_DEVICE_FUNC T <span class="token keyword">operator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>T x<span class="token punctuation">,</span> T y<span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">return</span> x<span class="token operator">*</span>y<span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre> 
<p>这里 OF_DEVICE_FUNC 表示我们定义的这个函数既可以运行在 CPU 又可以运行在 GPU 上，它的定义是：</p> 
<pre><code class="prism language-cpp"><span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">if</span> <span class="token expression"><span class="token function">defined</span><span class="token punctuation">(</span>__CUDACC__<span class="token punctuation">)</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">OF_DEVICE_FUNCTION</span> <span class="token expression">__device__ __host__ __forceinline__</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">else</span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">define</span> <span class="token macro-name">OF_DEVICE_FUNCTION</span> <span class="token expression"><span class="token keyword">inline</span></span></span>
<span class="token macro property"><span class="token directive-hash">#</span><span class="token directive keyword">endif</span></span>
</code></pre> 
<p>然后我们就可以使用 <code>cuda::elementwise::Binary</code> 这个模板函数来完成这个二元的 Element-Wise 算子了。示例代码如下：</p> 
<pre><code class="prism language-cpp">    <span class="token keyword">const</span> user_op<span class="token double-colon punctuation">::</span>Tensor<span class="token operator">*</span> x <span class="token operator">=</span> ctx<span class="token operator">-&gt;</span><span class="token function">Tensor4ArgNameAndIndex</span><span class="token punctuation">(</span><span class="token string">"x"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">const</span> user_op<span class="token double-colon punctuation">::</span>Tensor<span class="token operator">*</span> y <span class="token operator">=</span> ctx<span class="token operator">-&gt;</span><span class="token function">Tensor4ArgNameAndIndex</span><span class="token punctuation">(</span><span class="token string">"y"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    user_op<span class="token double-colon punctuation">::</span>Tensor<span class="token operator">*</span> out <span class="token operator">=</span> ctx<span class="token operator">-&gt;</span><span class="token function">Tensor4ArgNameAndIndex</span><span class="token punctuation">(</span><span class="token string">"out"</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">const</span> <span class="token keyword">int64_t</span> elem_cnt <span class="token operator">=</span> x<span class="token operator">-&gt;</span><span class="token function">shape</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">elem_cnt</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    OF_CUDA_CHECK（cuda<span class="token double-colon punctuation">::</span>elementwise<span class="token double-colon punctuation">::</span><span class="token function">Binary</span><span class="token punctuation">(</span><span class="token generic-function"><span class="token function">MultiplyFunctor</span><span class="token generic class-name"><span class="token operator">&lt;</span>T<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> elem_cnt<span class="token punctuation">,</span> out<span class="token operator">-&gt;</span><span class="token generic-function"><span class="token function">mut_dptr</span><span class="token generic class-name"><span class="token operator">&lt;</span>T<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                            x<span class="token operator">-&gt;</span><span class="token generic-function"><span class="token function">dptr</span><span class="token generic class-name"><span class="token operator">&lt;</span>T<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> 
                                            y<span class="token operator">-&gt;</span><span class="token generic-function"><span class="token function">dptr</span><span class="token generic class-name"><span class="token operator">&lt;</span>T<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
                                            ctx<span class="token operator">-&gt;</span><span class="token function">device_ctx</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">-&gt;</span><span class="token function">cuda_stream</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>这里的 x, y, out 分别代表这个 Element-Wise 操作的输入输出 Tensor，然后 <code>element_cnt</code> 表示 Tensor 的元素个数，输出张量的数据首地址 <code>out-&gt;mut_dptr&lt;T&gt;()</code>, 输入张量的数据首地址 <code>x-&gt;dptr&lt;T&gt;()</code> &amp;&amp; <code>y-&gt;dptr&lt;T&gt;()</code> ，最后一个参数则是当前 Kernel 运行的 cuda Stream对象。</p> 
<h1>
<a id="0x3__79"></a>0x3. 原理&amp;&amp;代码实现解析</h1> 
<p>我个人认为这里有几个要点，分别是一个线程处理多个数据，向量化数据访问提升带宽，设置合理的Block数量（GridSize）和线程数量（BlockSize）以及在合适的地方进行循环展开（unrool）以及一些编程上的技巧。</p> 
<h2>
<a id="0x31__ElementWise__GridSize__BlockSize_82"></a>0x3.1 给 Element-Wise 操作设置合理的 GridSize 和 BlockSize</h2> 
<p>下面这段代码展示了 OneFlow 针对 Element-Wise 算子是如何设置 GridSize 和 BlockSize 的。对应的源码地址为：https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh#L30-L52 。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">constexpr</span> <span class="token keyword">int</span> kBlockSize <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">;</span>
<span class="token keyword">constexpr</span> <span class="token keyword">int</span> kNumWaves <span class="token operator">=</span> <span class="token number">32</span><span class="token punctuation">;</span>

<span class="token keyword">inline</span> cudaError_t <span class="token function">GetNumBlocks</span><span class="token punctuation">(</span><span class="token keyword">int64_t</span> n<span class="token punctuation">,</span> <span class="token keyword">int</span><span class="token operator">*</span> num_blocks<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token keyword">int</span> dev<span class="token punctuation">;</span>
  <span class="token punctuation">{<!-- --></span>
    cudaError_t err <span class="token operator">=</span> <span class="token function">cudaGetDevice</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>dev<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>err <span class="token operator">!=</span> cudaSuccess<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> err<span class="token punctuation">;</span> <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
  <span class="token keyword">int</span> sm_count<span class="token punctuation">;</span>
  <span class="token punctuation">{<!-- --></span>
    cudaError_t err <span class="token operator">=</span> <span class="token function">cudaDeviceGetAttribute</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>sm_count<span class="token punctuation">,</span> cudaDevAttrMultiProcessorCount<span class="token punctuation">,</span> dev<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>err <span class="token operator">!=</span> cudaSuccess<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> err<span class="token punctuation">;</span> <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
  <span class="token keyword">int</span> tpm<span class="token punctuation">;</span>
  <span class="token punctuation">{<!-- --></span>
    cudaError_t err <span class="token operator">=</span> <span class="token function">cudaDeviceGetAttribute</span><span class="token punctuation">(</span><span class="token operator">&amp;</span>tpm<span class="token punctuation">,</span> cudaDevAttrMaxThreadsPerMultiProcessor<span class="token punctuation">,</span> dev<span class="token punctuation">)</span><span class="token punctuation">;</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>err <span class="token operator">!=</span> cudaSuccess<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> err<span class="token punctuation">;</span> <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
  <span class="token operator">*</span>num_blocks <span class="token operator">=</span> std<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">max</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> std<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">min</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">int64_t</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">(</span>n <span class="token operator">+</span> kBlockSize <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> kBlockSize<span class="token punctuation">,</span>
                                                   sm_count <span class="token operator">*</span> tpm <span class="token operator">/</span> kBlockSize <span class="token operator">*</span> kNumWaves<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token keyword">return</span> cudaSuccess<span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>这个地方 BlockSize 直接被设置为了 256 ，对应 <code>constexpr int kBlockSize = 256;</code> 这行代码，也就是说每个 Block 有 256 个线程。为什么是 256 ？大家不妨读一下俊丞大佬这篇经典的 <a href="https://mp.weixin.qq.com/s/1_ao9xM6Qk3JaavptChXew">给CUDA Kernel设置合适的 GridSize 和 Block Size 的文章</a> 。文章中通过对 SM 的资源分析确定在主流的GPU上将 BlockSize 设置为 128 或者 256 是比较合适，在这里直接设置为了 256 。</p> 
<p>确定了 BlockSize 之后需要确定 Kernel 启动线程块的数量，我一直觉得上述文章中对这一段的分析是尤其精彩的，这里再截图展示一下：</p> 
<p><img src="https://images2.imgbox.com/c4/03/sQRkmPOC_o.png" alt="选自OneFlow CUDA Kernel 中 grid_size 和 block_size 应该怎么设置 一文"></p> 
<p>根据这里的分析，对于 Element-Wise 操作要设置合适的 GridSize 不仅需要考虑元素的数量还要考虑由于 SM 硬件本身带来的限制。如下公式所述：</p> 
<pre><code class="prism language-cpp"><span class="token operator">*</span>num_blocks <span class="token operator">=</span> std<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">max</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">int</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> std<span class="token double-colon punctuation">::</span><span class="token generic-function"><span class="token function">min</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">int64_t</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">(</span>n <span class="token operator">+</span> kBlockSize <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> kBlockSize<span class="token punctuation">,</span>
                                                   sm_count <span class="token operator">*</span> tpm <span class="token operator">/</span> kBlockSize <span class="token operator">*</span> kNumWaves<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
</code></pre> 
<p>这里的 <code>(n + kBlockSize - 1) / kBlockSize</code> 就是根据 Element-Wise 操作的元素个数来计算需要启动多少个线程块，比如在文章开头的例子中有 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        224
       
       
        ×
       
       
        224
       
       
        ×
       
       
        3
       
      
      
       224 times 224 times 3
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span> = <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        150528
       
      
      
       150528
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">1</span><span class="mord">5</span><span class="mord">0</span><span class="mord">5</span><span class="mord">2</span><span class="mord">8</span></span></span></span></span> 个元素，那么就一共需要 <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        (
       
       
        150528
       
       
        +
       
       
        256
       
       
        −
       
       
        1
       
       
        )
       
       
        /
       
       
        256
       
       
        =
       
       
        588
       
      
      
       (150528+256-1)/256=588
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mord">5</span><span class="mord">0</span><span class="mord">5</span><span class="mord">2</span><span class="mord">8</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/</span><span class="mord">2</span><span class="mord">5</span><span class="mord">6</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">5</span><span class="mord">8</span><span class="mord">8</span></span></span></span></span>个线程块。然后这里以GTX 3080Ti为例，它的SM个数也就是<code>sm_count=80</code>，每个SM最多调度的线程数<code>tpm=1536</code>，那么<code>sm_count * tpm / kBlockSize * kNumWaves = 80 * 1536 / 256 * 32 = 15360</code>，所以在这个例子中我们最终设置的线程块个数为 588 个。</p> 
<p>通过上述讲解和分析我们已经确定了启动 Element-Wise CUDA Kernel 的 GridSize 和 BlockSize。</p> 
<h2>
<a id="0x32__129"></a>0x3.2 向量化数据访问提升带宽</h2> 
<p>对于大多数 Element-Wise 算子来说，一般它们的计算量不会太大，所以它们的瓶颈一般在GPU的带宽上。在 NVIDIA 的性能优化博客 https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/ 中提到，对于很多 CUDA 核函数我们都可以通过向量化数据访问的方式来提升带宽受限的 Kernel 的性能，特别是对于架构比较新的 GPU 向量化数据访问的效果会更加明显。</p> 
<p>在 OneFlow 的 Element-Wise 系列算子中，为了更好的进行向量化的数据访问，俊丞设计了如下的 Pack 数据结构（代码位置：https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh#L54-L70）：</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token punctuation">,</span> <span class="token keyword">int</span> pack_size<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">GetPackType</span> <span class="token punctuation">{<!-- --></span>
  <span class="token keyword">using</span> type <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token double-colon punctuation">::</span>aligned_storage<span class="token operator">&lt;</span>pack_size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> pack_size <span class="token operator">*</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token operator">&gt;</span><span class="token double-colon punctuation">::</span>type<span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token punctuation">,</span> <span class="token keyword">int</span> pack_size<span class="token operator">&gt;</span>
<span class="token keyword">using</span> PackType <span class="token operator">=</span> <span class="token keyword">typename</span> <span class="token class-name">GetPackType</span><span class="token operator">&lt;</span>T<span class="token punctuation">,</span> pack_size<span class="token operator">&gt;</span><span class="token double-colon punctuation">::</span>type<span class="token punctuation">;</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token punctuation">,</span> <span class="token keyword">int</span> pack_size<span class="token operator">&gt;</span>
<span class="token keyword">union</span> Pack <span class="token punctuation">{<!-- --></span>
  <span class="token keyword">static_assert</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span>PackType<span class="token operator">&lt;</span>T<span class="token punctuation">,</span> pack_size<span class="token operator">&gt;</span><span class="token punctuation">)</span> <span class="token operator">==</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span> <span class="token operator">*</span> pack_size<span class="token punctuation">,</span> <span class="token string">""</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
  __device__ <span class="token function">Pack</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// do nothing</span>
  <span class="token punctuation">}</span>
  PackType<span class="token operator">&lt;</span>T<span class="token punctuation">,</span> pack_size<span class="token operator">&gt;</span> storage<span class="token punctuation">;</span>
  T elem<span class="token punctuation">[</span>pack_size<span class="token punctuation">]</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre> 
<p>首先 GetPackType 结构体中使用了 <code>std::aligned_storage</code> 先声明了一个内存对齐的数据类型 type ，注意这个 type 的内存长度为 <code>pack_size * sizeof(T) </code> 。然后这里的 T 是我们需要进行 Pack 的数据类型，而 <code>pack_size</code> 则表示我们需要 Pack 的元素个数。接下来我们看到 Pack 联合体中声明了 <code>storage</code> 和 <code>elem</code> 两个数组，它们公用同一段对齐的内存。然后 Pack 联合体的入口有一个检查: <code>static_assert(sizeof(PackType&lt;T, pack_size&gt;) == sizeof(T) * pack_size, "");</code> 这是用来判断我们之前声明的 <code>type</code> 的内存长度是否符合预期。</p> 
<p>接下来我们从 https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh#L155-L194 这里可以看到这个 Pack 联合体主要是用在 Kernel 启动之前判断 Element-Wise 操作的输入输出 Tensor 对应的数据指针地址是否满足内存对齐的条件，如果不满足则这个 Element-Wise 操作无法执行数据 Pack 。对应下图2个画红色框的地方。<br> <img src="https://images2.imgbox.com/cc/ea/s3RGGebo_o.png" alt="在这里插入图片描述"></p> 
<p>接下来，OneFlow 定义了真正要执行数据 Pack 的数据结构 Packed 并且定义了计算 PackSize 的工具函数。代码位置为：https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh#L72-L95 。</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token punctuation">,</span> <span class="token keyword">int</span> pack_size<span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token keyword">alignas</span><span class="token punctuation">(</span><span class="token keyword">sizeof</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span> <span class="token operator">*</span> pack_size<span class="token punctuation">)</span> Packed <span class="token punctuation">{<!-- --></span>
  __device__ <span class="token function">Packed</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
    <span class="token comment">// do nothing</span>
  <span class="token punctuation">}</span>
  <span class="token keyword">union</span> <span class="token punctuation">{<!-- --></span>
    T elem<span class="token punctuation">[</span>pack_size<span class="token punctuation">]</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">constexpr</span> <span class="token keyword">int</span> kMaxPackBytes <span class="token operator">=</span> <span class="token number">128</span> <span class="token operator">/</span> <span class="token number">8</span><span class="token punctuation">;</span>
<span class="token keyword">constexpr</span> <span class="token keyword">int</span> kMaxPackSize <span class="token operator">=</span> <span class="token number">8</span><span class="token punctuation">;</span>

<span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token function">Min</span><span class="token punctuation">(</span><span class="token keyword">int</span> a<span class="token punctuation">,</span> <span class="token keyword">int</span> b<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> a <span class="token operator">&lt;</span> b <span class="token operator">?</span> a <span class="token operator">:</span> b<span class="token punctuation">;</span> <span class="token punctuation">}</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token operator">&gt;</span>
<span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token function">PackSize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token keyword">return</span> <span class="token function">Min</span><span class="token punctuation">(</span>kMaxPackBytes <span class="token operator">/</span> <span class="token keyword">sizeof</span><span class="token punctuation">(</span>T<span class="token punctuation">)</span><span class="token punctuation">,</span> kMaxPackSize<span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">T</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">U</span><span class="token punctuation">,</span> <span class="token keyword">typename</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> Args<span class="token operator">&gt;</span>
<span class="token keyword">constexpr</span> <span class="token keyword">int</span> <span class="token function">PackSize</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token keyword">return</span> <span class="token function">Min</span><span class="token punctuation">(</span><span class="token generic-function"><span class="token function">PackSize</span><span class="token generic class-name"><span class="token operator">&lt;</span>T<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token generic-function"><span class="token function">PackSize</span><span class="token generic class-name"><span class="token operator">&lt;</span>U<span class="token punctuation">,</span> Args<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<p>这里需要注意的是对于 CUDA 来说，最多支持 128 个 bit 的访问粒度，也就是说 PackSize 的大小不能超过 128 个bit。然后对于各种数据类型来说，Half 数据类型的 bit 数是最少的即 16，所以一次性可以支持 Pack 8个half类型的数据，4个float32的数据，以此类推。所以这里的定义的 kMaxPackSize 表示 128/16=8 ，然后 kMaxPackBytes 则表示最大可以 Pack 的 byte 数 。</p> 
<blockquote> 
 <p>请注意区分 bit 和 byte 。</p> 
</blockquote> 
<p>接下来 https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh#L97-L144 则是真正的为 Element-Wise 操作完成数据 Pack 并执行计算。</p> 
<p>首先来看这段充满技巧的代码：</p> 
<p><img src="https://images2.imgbox.com/0e/59/n4NK3gG9_o.png" alt="在这里插入图片描述"></p> 
<p>首先这里定义了一个 HasApply2 类用来判断 Functor是否支持两个两个的操作，比如half2。如果 Functor 里定义了apply2，那么test就会匹配到one函数，返回的是sizeof char，value是true。比如 half 可以 Pack 一次8个，但是有 <code>__half22float2</code> 这种针对 half2 的操作，那它就可以两个两个的做。可以看到对于 half2 类型的 Element-Wise 操作我们需要给对应的 Functor 定义一个 Apply2 函数，比如对于 Cast 操作的 Functor 定义如下：</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">To</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">From</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token operator">=</span> <span class="token keyword">void</span><span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">CastFunctor</span> <span class="token punctuation">{<!-- --></span>
  __device__ To <span class="token keyword">operator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>From from<span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> <span class="token generic-function"><span class="token function">static_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span>To<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>from<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>

<span class="token keyword">template</span><span class="token operator">&lt;</span><span class="token keyword">typename</span> <span class="token class-name">To</span><span class="token operator">&gt;</span>
<span class="token keyword">struct</span> <span class="token class-name">CastFunctor</span><span class="token operator">&lt;</span>To<span class="token punctuation">,</span> half<span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">std</span><span class="token double-colon punctuation">::</span>enable_if<span class="token operator">&lt;</span><span class="token operator">!</span>std<span class="token double-colon punctuation">::</span>is_same<span class="token operator">&lt;</span>To<span class="token punctuation">,</span> half<span class="token operator">&gt;</span><span class="token double-colon punctuation">::</span>value<span class="token operator">&gt;</span><span class="token double-colon punctuation">::</span>type<span class="token operator">&gt;</span> <span class="token punctuation">{<!-- --></span>
  __device__ To <span class="token keyword">operator</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>half from<span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> <span class="token generic-function"><span class="token function">static_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span>To<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span><span class="token generic-function"><span class="token function">static_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">float</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>from<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token punctuation">}</span>

  __device__ <span class="token keyword">void</span> <span class="token function">Apply2</span><span class="token punctuation">(</span>To<span class="token operator">*</span> to<span class="token punctuation">,</span> <span class="token keyword">const</span> half<span class="token operator">*</span> from<span class="token punctuation">)</span> <span class="token keyword">const</span> <span class="token punctuation">{<!-- --></span>
    <span class="token keyword">const</span> float2 f2 <span class="token operator">=</span> <span class="token function">__half22float2</span><span class="token punctuation">(</span><span class="token operator">*</span><span class="token generic-function"><span class="token function">reinterpret_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">const</span> half2<span class="token operator">*</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>from<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
    to<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token generic-function"><span class="token function">static_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span>To<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>f2<span class="token punctuation">.</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span>
    to<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token generic-function"><span class="token function">static_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span>To<span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>f2<span class="token punctuation">.</span>y<span class="token punctuation">)</span><span class="token punctuation">;</span>
  <span class="token punctuation">}</span>
<span class="token punctuation">}</span><span class="token punctuation">;</span>
</code></pre> 
<h2>
<a id="0x33__Kernel_219"></a>0x3.3 启动 Kernel</h2> 
<p>我们接下来看一下 Element-Wise 的 Kernel 实现：https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh#L133-L144 。</p> 
<p><img src="https://images2.imgbox.com/e8/41/6g66AICY_o.png" alt="在这里插入图片描述"></p> 
<p>在 Kernel 中我们发现每一个线程实际上处理了多个 Pack 后的数据，也即：<code>for (int64_t i = global_tid; i &lt; n_pack; i += blockDim.x * gridDim.x)</code> 。初学者看到这个循环也许会比较疑惑，为什么它的步幅是 <code>blockDim.x * gridDim.x</code> ? 这个 <code>blockDim.x * gridDim.x</code> 表示的是 CUDA 线程网格中的线程总数。假设线程网格中有 1280 个线程，线程 0 将计算元素 0、1280、2560 等。通过使用步幅等于网格大小的循环，确保了 warp 中的所有寻址都是单位步幅，可以获得最大的内存合并。想了解更多细节可以查看：https://zhuanlan.zhihu.com/p/571320529 。</p> 
<p>除此之外，使用这种技巧的还有个好处就是如果对于 Kernel 中存在每个线程都包含一个公共的操作，那么线程数的增多，也代表着这部分的开销变大。这个时候我们减少线程的数量并循环进行处理的话那么这个公共操作的开销就会更低。</p> 
<p>最后，在循环之外，我们还需要根据传入的 n_tail 参数，看一下还有没有因为没有被 pack_size 整除的剩余元素，如果有的话就单独调用 functor 进行处理。</p> 
<h2>
<a id="0x34_unroll_231"></a>0x3.4 unroll</h2> 
<p>实际上就是代码中的 <code>#pragma unroll</code> ，这个宏会对我们的 for 循环做循环展开，让更多的指令可以并行执行。但容易想到，只有处理的数据没有前后依赖关系的时候我们可以做。对于大多数的 ElementWise 算子来说一般是满足这个条件的。</p> 
<h2>
<a id="0x35_Kernel_Launch_235"></a>0x3.5 Kernel Launch的细节</h2> 
<p>在 https://github.com/Oneflow-Inc/oneflow/blob/master/oneflow/core/cuda/elementwise.cuh#L166-L181 这个位置 OneFlow 展示了 Element-Wise Kernel 的启动细节，我们简单注释一下：</p> 
<pre><code class="prism language-cpp"><span class="token keyword">template</span><span class="token operator">&lt;</span>size_t pack_size<span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">FactoryT</span><span class="token punctuation">,</span> <span class="token keyword">typename</span> <span class="token class-name">R</span><span class="token punctuation">,</span> <span class="token keyword">typename</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> IN<span class="token operator">&gt;</span>
cudaError_t <span class="token function">LaunchKernel</span><span class="token punctuation">(</span>FactoryT factory<span class="token punctuation">,</span> <span class="token keyword">int64_t</span> n<span class="token punctuation">,</span> R<span class="token operator">*</span> r<span class="token punctuation">,</span> <span class="token keyword">const</span> IN<span class="token operator">*</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> in<span class="token punctuation">,</span> cudaStream_t stream<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span>
  <span class="token keyword">const</span> <span class="token keyword">int64_t</span> n_pack <span class="token operator">=</span> n <span class="token operator">/</span> pack_size<span class="token punctuation">;</span> <span class="token comment">// 根据元素个数和pack_size，计算pack数目，比如1026 / 4 = 256。</span>
  <span class="token keyword">const</span> <span class="token keyword">int64_t</span> tail_offset <span class="token operator">=</span> n_pack <span class="token operator">*</span> pack_size<span class="token punctuation">;</span> <span class="token comment">// 如果存在不被整除的情况，我们计算使用pack的偏移量：256*4； </span>
  <span class="token keyword">const</span> <span class="token keyword">int64_t</span> n_tail <span class="token operator">=</span> n <span class="token operator">-</span> tail_offset<span class="token punctuation">;</span> <span class="token comment">// // 元素数目-偏移量 = 剩下的元素个数-&gt; 1026-1024 = 2</span>
  <span class="token keyword">int</span> num_blocks<span class="token punctuation">;</span>
  <span class="token punctuation">{<!-- --></span>
    cudaError_t err <span class="token operator">=</span> <span class="token function">GetNumBlocks</span><span class="token punctuation">(</span>n_pack<span class="token punctuation">,</span> <span class="token operator">&amp;</span>num_blocks<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token comment">// 计算线程块数目</span>
    <span class="token keyword">if</span> <span class="token punctuation">(</span>err <span class="token operator">!=</span> cudaSuccess<span class="token punctuation">)</span> <span class="token punctuation">{<!-- --></span> <span class="token keyword">return</span> err<span class="token punctuation">;</span> <span class="token punctuation">}</span>
  <span class="token punctuation">}</span>
  ApplyGeneric<span class="token operator">&lt;</span>pack_size<span class="token punctuation">,</span> FactoryT<span class="token punctuation">,</span> R<span class="token punctuation">,</span> IN<span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token operator">&gt;</span><span class="token operator">&lt;&lt;</span><span class="token operator">&lt;</span>num_blocks<span class="token punctuation">,</span> kBlockSize<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> stream<span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span><span class="token punctuation">(</span>
      factory<span class="token punctuation">,</span> n_pack<span class="token punctuation">,</span> <span class="token generic-function"><span class="token function">reinterpret_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span>Packed<span class="token operator">&lt;</span>R<span class="token punctuation">,</span> pack_size<span class="token operator">&gt;</span><span class="token operator">*</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>r<span class="token punctuation">)</span><span class="token punctuation">,</span>
      <span class="token punctuation">(</span><span class="token generic-function"><span class="token function">reinterpret_cast</span><span class="token generic class-name"><span class="token operator">&lt;</span><span class="token keyword">const</span> Packed<span class="token operator">&lt;</span>IN<span class="token punctuation">,</span> pack_size<span class="token operator">&gt;</span><span class="token operator">*</span><span class="token operator">&gt;</span></span></span><span class="token punctuation">(</span>in<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> n_tail<span class="token punctuation">,</span> r <span class="token operator">+</span> tail_offset<span class="token punctuation">,</span>
      <span class="token punctuation">(</span>in <span class="token operator">+</span> tail_offset<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">;</span> 
  <span class="token keyword">return</span> <span class="token function">cudaPeekAtLastError</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>
<span class="token punctuation">}</span>
</code></pre> 
<h1>
<a id="0x4__258"></a>0x4. 总结</h1> 
<p>以上就是我对 OneFlow Element-Wise 系列 CUDA 算子实现的解析，后续有空会持续更新学习到的新知识。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>