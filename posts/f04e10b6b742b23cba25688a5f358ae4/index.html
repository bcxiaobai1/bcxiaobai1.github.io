<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【LeNet、AlexNet、VGG】 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【LeNet、AlexNet、VGG】</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <h1>
<a id="LeNet_0"></a>LeNet</h1> 
<p>  LeNet是最早用于图像处理的神经网络，主要是为了解决手写数字识别的问题，著名的数据集Minist就是伴随着LeNet的诞生而出现的。下面是其基本架构：<br> <img src="https://images2.imgbox.com/a9/2d/9QVVo2tJ_o.jpg" alt="在这里插入图片描述"><br>   其结构相对简单，其中的Pooling层可以使用MaxPooling，也可以使用AvgPooling，激活函数原始模型使用的是Sigmoid，不过也可以换成Relu，tanh等。</p> 
<p><strong>总结</strong></p> 
<ul>
<li>Lenet是是最早发布的卷积神经网络之一，因其在计算机视觉任务中的高效性能而受到广泛关注</li>
<li>先用卷积层来学习图片的空间信息，通过池化层降低图片的敏感度</li>
<li>然后使用全连接层来转换到类别空间，得到10类</li>
<li>两个卷积层再加一个多层感知机，最终得到从图片到类别的映射</li>
</ul> 
<p><strong>代码实现</strong></p> 
<pre><code class="prism language-python"><span class="token operator">%</span>matplotlib inline
<span class="token keyword">import</span> torch
<span class="token keyword">from</span> torch <span class="token keyword">import</span> nn
<span class="token keyword">import</span> torchvision
<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils <span class="token keyword">import</span> data
<span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot <span class="token keyword">as</span> plt
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
</code></pre> 
<pre><code class="prism language-python">trans <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>

train_data <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span><span class="token string">'../data/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">)</span>

test_data <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span><span class="token string">'../data/'</span><span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>trans<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">train_data<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> test_data<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape
</code></pre> 
<pre><code>(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_dataloader</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> train_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_dataloader <span class="token operator">=</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>train_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    test_dataloader <span class="token operator">=</span> data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span>test_data<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> train_dataloader<span class="token punctuation">,</span> test_dataloader
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">class</span> <span class="token class-name">LeNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span>LeNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        
    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>net<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">get_optimizer</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>
    <span class="token keyword">return</span> optimizer
</code></pre> 
<pre><code class="prism language-python"><span class="token comment"># 定义评判方法，对于分类问题，我们常使用准确率来判定</span>
<span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    padding <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    right <span class="token operator">=</span> <span class="token punctuation">(</span>padding <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> right <span class="token operator">/</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">init_weights</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Linear <span class="token keyword">or</span> <span class="token builtin">type</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">==</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">:</span>
        nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform_<span class="token punctuation">(</span>m<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">train</span><span class="token punctuation">(</span>epoches<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> LeNet<span class="token punctuation">(</span><span class="token punctuation">)</span>
    model<span class="token punctuation">.</span><span class="token builtin">apply</span><span class="token punctuation">(</span>init_weights<span class="token punctuation">)</span>
    
    loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span>
    optimizer <span class="token operator">=</span> get_optimizer<span class="token punctuation">(</span>model<span class="token punctuation">,</span> lr<span class="token punctuation">)</span>
    train_loader<span class="token punctuation">,</span> test_loader <span class="token operator">=</span> get_dataloader<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> train_data<span class="token punctuation">,</span> test_data<span class="token punctuation">)</span>
    
    loss_lis <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    train_acc_lis <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    test_acc_lis <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epoches<span class="token punctuation">)</span><span class="token punctuation">:</span>
        acc <span class="token operator">=</span> <span class="token number">0</span>
        l_sum <span class="token operator">=</span> <span class="token number">0</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_loader<span class="token punctuation">:</span>
            y_hat <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
            l <span class="token operator">=</span> loss<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
            l<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
            acc <span class="token operator">+=</span> accuracy<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
            l_sum <span class="token operator">+=</span> l<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        
        acc <span class="token operator">=</span> acc <span class="token operator">/</span> <span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> batch_size<span class="token punctuation">)</span>
        l_sum <span class="token operator">=</span> l_sum <span class="token operator">/</span> <span class="token punctuation">(</span>train_data<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> batch_size<span class="token punctuation">)</span>
        
        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        acc_eval <span class="token operator">=</span> <span class="token number">0</span>
        <span class="token keyword">for</span> x<span class="token punctuation">,</span> Y <span class="token keyword">in</span> test_loader<span class="token punctuation">:</span>
            Y_hat <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
            acc_eval <span class="token operator">+=</span> accuracy<span class="token punctuation">(</span>Y_hat<span class="token punctuation">,</span> Y<span class="token punctuation">)</span>
        acc_eval <span class="token operator">/=</span> <span class="token punctuation">(</span>test_data<span class="token punctuation">.</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">/</span> batch_size<span class="token punctuation">)</span>
        
        loss_lis<span class="token punctuation">.</span>append<span class="token punctuation">(</span>l_sum<span class="token punctuation">)</span>
        train_acc_lis<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc<span class="token punctuation">)</span>
        test_acc_lis<span class="token punctuation">.</span>append<span class="token punctuation">(</span>acc_eval<span class="token punctuation">)</span>
        
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'epoch is </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">}</span></span><span class="token string">, the loss is </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>l_sum<span class="token punctuation">}</span></span><span class="token string"> and the accuracy on train data is </span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>acc<span class="token punctuation">}</span></span><span class="token string">, on test data is</span><span class="token interpolation"><span class="token punctuation">{<!-- --></span>acc_eval<span class="token punctuation">}</span></span><span class="token string">'</span></span><span class="token punctuation">)</span>
        
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> epoches <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> loss_lis<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'blue'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'loss'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> epoches <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> train_acc_lis<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'grey'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'train_acc'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> epoches <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> test_acc_lis<span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">'red'</span><span class="token punctuation">,</span> linestyle<span class="token operator">=</span><span class="token string">'--'</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">'test_acc'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>grid<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">'upper right'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code class="prism language-python">train<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">0.9</span><span class="token punctuation">)</span>
</code></pre> 
<pre><code>epoch is 1, the loss is 1.8979717952728272 and the accuracy on train data is 0.2590388888888889, on test data is0.6222
epoch is 2, the loss is 0.7648378531773885 and the accuracy on train data is 0.699338888888889, on test data is0.7426
epoch is 3, the loss is 0.5863379270553589 and the accuracy on train data is 0.7725944444444444, on test data is0.7907
epoch is 4, the loss is 0.4983555362065633 and the accuracy on train data is 0.8092, on test data is0.8301
epoch is 5, the loss is 0.4429962953249613 and the accuracy on train data is 0.8350277777777777, on test data is0.8435
epoch is 6, the loss is 0.4026765632947286 and the accuracy on train data is 0.8518888888888889, on test data is0.8468
epoch is 7, the loss is 0.37838911752700805 and the accuracy on train data is 0.8591833333333333, on test data is0.8668
epoch is 8, the loss is 0.35928820660909017 and the accuracy on train data is 0.8668888888888889, on test data is0.8727
epoch is 9, the loss is 0.34128332163492836 and the accuracy on train data is 0.8742888888888889, on test data is0.8638
epoch is 10, the loss is 0.32974659884770713 and the accuracy on train data is 0.8775555555555556, on test data is0.8824
</code></pre> 
<p><img src="https://images2.imgbox.com/91/e1/5mTjxw2h_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="AlexNet_167"></a>AlexNet</h1> 
<p>   AlexNet诞生于2012年，与另一种观察图像特征的提取方法不同，它认为特征本身应该被学习，在合理的复杂性前提下，特征应该由多个共同学习的神经网络层组成，每个层都有可学习的参数。在机器视觉中，最底层可能检测边缘、颜色和纹理；更高层建立在底层表示的基础上，以表示更大的特征，更高层可以检测整个物体；最终的隐藏神经元可以学习图像的综合表示，从而使不同类别的数据易于区分</p> 
<ul>
<li>AlexNet赢了2012年ImageNet竞赛</li>
<li>本质上是更深更大的LeNet</li>
<li>主要改进： 
  <ul>
<li>丢弃法</li>
<li>ReLu</li>
<li>MaxPooling</li>
</ul> </li>
<li>计算机视觉方法论的改变（不再需要人工特征提取，而是让CNN去自己进行特征学习）<br> <img src="https://images2.imgbox.com/44/03/2MdF5YqD_o.png" alt="在这里插入图片描述"><br> <strong>AlexNet和LeNet的对比</strong><br> <img src="https://images2.imgbox.com/55/b0/qVAv7qgX_o.png" alt="在这里插入图片描述">
</li>
<li>AlexNet其实就是一个更大、更深的LeNet，由八层组成：5个卷积层、2个全连接隐藏层和一个全连接输出层</li>
<li>AlexNet的输入是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         224
        
        
         ×
        
        
         224
        
        
         ×
        
        
         3
        
       
       
        224times224times3
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mord">2</span><span class="mord">4</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>的3通道RGB图片，LeNet的输入是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         32
        
        
         ×
        
        
         32
        
        
         ×
        
        
         1
        
       
       
        32times32times1
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">1</span></span></span></span></span>的单通道灰度图片</li>
<li>第一层：AlexNet使用了更大的核窗口（因为图片更大了，需要用更大的卷积窗口来捕获目标），通道数也更多了，从6变成了96（希望能够在第一层识别更多的模式，所以用了比较大的通道数），stride从2变成了4（这是由于当时GPU性能的限制，如果stride比较小的话，计算就会变得非常困难）</li>
<li>第二层：AlexNet使用了更大的池化层，stride都是2，因为LeNet的池化层窗口大小也是2，所以它每次看到的内容是不重叠的。<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         2
        
        
         ×
        
        
         2
        
       
       
        2times2
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">2</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         3
        
        
         ×
        
        
         3
        
       
       
        3times3
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>的主要区别是： 
  <ul><li>
<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           2
          
          
           ×
          
          
           2
          
         
         
          2times2
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">2</span></span></span></span></span>允许一个像素往一边平移一点而不影响输出，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           3
          
          
           ×
          
          
           3
          
         
         
          3times3
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>的话就允许一个像素左移或者右移都不影响输出；stride都等于2使得输出的高和宽都减半</li></ul> </li>
<li>第三层：AlexNet有一个padding为2的操作，它的作用就是使得输入和输出的大小是一样的；AlexNet的输出通道是256，使用了更多的输出通道来识别更多的模式</li>
<li>ALexNet新加了3个卷积层</li>
<li>AlexNet的全连接层也用了两个隐藏层，但是隐藏层更大（在最后一个卷积层后有两个全连接层，分别有4096个输出。 这两个巨大的全连接层拥有将近1GB的模型参数。 由于早期GPU显存有限，原版的AlexNet采用了双数据流设计，使得每个GPU只负责存储和计算模型的一半参数）</li>
<li>Alex的激活函数从sigmoid变成了ReLu： 
  <ul>
<li>1、ReLU激活函数的计算更简单，它不需要sigmoid激活函数那般复杂的求幂运算；</li>
<li>2、当使用不同的参数初始化方法时，ReLU激活函数使训练模型更加容易；</li>
<li>3、当sigmoid激活函数的输出非常接近于0或1时，这些区域的梯度几乎为0，因此反向传播无法继续更新一些模型参数，相反，ReLU激活函数在正区间的梯度总是1。 因此，如果模型参数没有正确初始化，sigmoid函数可能在正区间内得到几乎为0的梯度，从而使模型无法得到有效的训练。<br> LeNet只使用了权重衰减，而AlexNet在全连接层的两个隐藏层之后加入了丢弃层（dropout、暂退法），来做模型的正则化，控制全连接层的模型复杂度</li>
</ul> </li>
<li>为了进一步扩充数据，AlexNet还做了数据的增强：对样本图片进行随机截取、随机调节亮度、随即调节色温（因为卷积对位置、光照等比较敏感，所以在输入图片中增加大量的变种，来模拟预测物体形状或者颜色的变化；因为神经网络能够记住所有的数据，通过这种变换之后来降低神经网络的这种能力，因为每次变换之后的物体都是不一样的）</li>
</ul> 
<p><strong>总结</strong></p> 
<ul>
<li>AlexNet是更大更深的LeNet，但是整个架构是一样的，AlexNet的参数个数比LeNet多了10倍，计算复杂度多了260倍</li>
<li>AlexNet新加入了一些小技巧使得训练更加容易：丢弃法（dropout）、ReLu、最大池化层、数据增强</li>
<li>AlexNet首次证明了学习到的特征可以超越手工设计的特征，以很大的优势赢下了2012年的ImageNet竞赛之后，标志着新一轮的神经网络热潮的开始</li>
<li>尽管今天AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步</li>
<li>Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤</li>
</ul> 
<h1>
<a id="VGG_202"></a>VGG</h1> 
<p>   Alexnet虽然证明了深层神经网络是有效果的，但是它最大的问题是模型不规则，结构不是很清晰，没有提供一个通用的模板来指导后续的研究人员设计新的网络。如果模型想要变得更大、更深，则需要很好的设计思想，使得整个框架更加规则</p> 
<p><strong>如何使模型更大更深</strong></p> 
<ul>
<li>更多的全连接层（缺点是全连接层很大的话会占用很多的内存）</li>
<li>更多的卷积层（AlexNet是先将LeNet的模型给扩大之后，再加了三个卷积层，不好实现对模型进一步的加大、加深；VGG的思想是先将卷积层组成小块，然后再将卷积层进行堆叠）</li>
<li>将卷积层组合成块（VGG提出了VGG块的概念，其实就是AlexNet思路的拓展：AlexNet中是三个一模一样的卷积层（<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         3
        
        
         ×
        
        
         3
        
       
       
        3times3
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>，384通道，padding等于1）加上一个池化层（<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         3
        
        
         ×
        
        
         3
        
       
       
        3times3
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>，最大池化层，stride=2）组成了一个小块：VGG块是在此基础上的拓展，它并不限制块中卷积层的层数和通道数），最大池化层重新用回了LeNet中的最大池化层窗口（<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         2
        
        
         ×
        
        
         2
        
       
       
        2times2
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">2</span></span></span></span></span>，最大池化层，stride=2）</li>
</ul> 
<p><strong>VGG块</strong><br> <img src="https://images2.imgbox.com/4b/c8/3AMUIACq_o.png" alt="在这里插入图片描述"></p> 
<ul>
<li> <p>VGG的核心思想是使用大量由一定数目的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          3
         
         
          ×
         
         
          3
         
        
        
         3times3
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>的卷积层和一个最大池化层组成的VGG块进行堆叠，最终得到最后的网络</p> 
  <ul><li>为什么使用的卷积层是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           3
          
          
           ×
          
          
           3
          
         
         
          3times3
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>，而不是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           5
          
          
           ×
          
          
           5
          
         
         
          5times5
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">5</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">5</span></span></span></span></span>？ 
    <ul><li>
<span class="katex--inline"><span class="katex"><span class="katex-mathml">
         
          
           
            
             5
            
            
             ×
            
            
             5
            
           
           
            5times5
           
          
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">5</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">5</span></span></span></span></span>的卷积层也用过，但是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
         
          
           
            
             5
            
            
             ×
            
            
             5
            
           
           
            5times5
           
          
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">5</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">5</span></span></span></span></span>的卷积层的计算量更大，所以层数就不会太大，VGG块就会变得浅一点，最终通过对比发现，在同样的计算开销之下，大量的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
         
          
           
            
             3
            
            
             ×
            
            
             3
            
           
           
            3times3
           
          
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>的卷积层堆叠起来比少量的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
         
          
           
            
             5
            
            
             ×
            
            
             5
            
           
           
            5times5
           
          
         </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">5</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">5</span></span></span></span></span>的卷积层堆叠起来的效果更好，也就是说模型更深、卷积窗口更小的情况下，效果会更好一点</li></ul> </li></ul> </li>
<li> <p>VGG块由两部分组成：多个填充为1的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          3
         
         
          ×
         
         
          3
         
        
        
         3times3
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">3</span></span></span></span></span>卷积层（它有两个超参数：层数n、通道数m）和一个步幅为2的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          2
         
         
          ×
         
         
          2
         
        
        
         2times2
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.64444em;vertical-align: 0em"></span><span class="mord">2</span></span></span></span></span>最大池化层</p> </li>
</ul> 
<p><strong>VGG架构</strong></p> 
<ul>
<li>其实就是使用多个VGG块进行堆叠来替换掉AlexNet中的卷积部分<br> <img src="https://images2.imgbox.com/37/6a/e5vniQ1z_o.png" alt="在这里插入图片描述">
</li>
<li>VGG块重复的次数不同可以得到不同的架构，比如VGG-16、VGG-19，···</li>
<li>最后还是使用了两个4096的全连接层得到输出</li>
<li>VGG对AlexNet最大的改进是：将AlexNet在LeNet的基础上新加的卷积层抽象出了VGG块，替换掉了AlexNet中原先并不规则的部分</li>
<li>类似于AlexNet、LeNet，VGG网络也可以分成两部分：第一部分主要由卷积层和汇聚层组成，第二部分由全连接层组成。从AlexNet到VGG，本质上都是块设计</li>
<li>原始的VGG网络有5个块，前2个块各有一个卷积层，后3个块个包含两个卷积层；第一个模块有64个输出通道，每个后续模块将输出通道的数量翻倍，直到达到512，由于该网络使用了8个卷积层和三个全连接层，因此通常被称为VGG-11（这里为什么是5块？因为原始输入图像的大小是224，每经过一个VGG块，输出的通道数会翻倍、高宽会减半，当减到第五次时输出的高宽为7，就不能再经过VGG块进行减半了）</li>
</ul>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>