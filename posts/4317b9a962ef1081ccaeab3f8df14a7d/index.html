<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>pytorch快速入门中文——06(torch.nn) - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">pytorch快速入门中文——06(torch.nn)</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <h1>
<a id="torchnn_1"></a><code>torch.nn</code>到底是什么？</h1> 
<blockquote> 
 <p>原文：<a href="https://pytorch.org/tutorials/beginner/nn_tutorial.html">https://pytorch.org/tutorials/beginner/nn_tutorial.html</a></p> 
</blockquote> 
<p>作者：Jeremy Howard，<a href="https://www.fast.ai">fast.ai</a>。 感谢 Rachel Thomas 和 Francisco Ingham。</p> 
<p>我们建议将本教程作为笔记本而不是脚本来运行。 要下载笔记本（<code>.ipynb</code>）文件，请单击页面顶部的链接。</p> 
<p>PyTorch 提供设计精美的模块和类<a href="https://pytorch.org/docs/stable/nn.html"><code>torch.nn</code></a>，<a href="https://pytorch.org/docs/stable/optim.html"><code>torch.optim</code></a>，<a href="https://pytorch.org/docs/stable/data.html?highlight=dataset#torch.utils.data.Dataset"><code>Dataset</code></a>和<a href="https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader"><code>DataLoader</code></a>神经网络。 为了充分利用它们的功能并针对您的问题对其进行自定义，您需要真正了解它们在做什么。 为了建立这种理解，我们将首先在 MNIST 数据集上训练基本神经网络，而无需使用这些模型的任何功能。 我们最初将仅使用最基本的 PyTorch 张量函数。 然后，我们将一次从<code>torch.nn</code>，<code>torch.optim</code>，<code>Dataset</code>或<code>DataLoader</code>中逐个添加一个函数，以准确显示每个函数，以及如何使代码更简洁或更有效。 灵活。</p> 
<p><strong>本教程假定您已经安装了 PyTorch，并且熟悉张量操作的基础知识。</strong> （如果您熟悉 Numpy 数组操作，将会发现此处使用的 PyTorch 张量操作几乎相同）。</p> 
<h2>
<a id="MNIST__13"></a>MNIST 数据集</h2> 
<p>我们将使用经典的 <a href="http://deeplearning.net/data/mnist/">MNIST</a> 数据集，该数据集由手绘数字的黑白图像组成（0 到 9 之间）。</p> 
<p>我们将使用<a href="https://docs.python.org/3/library/pathlib.html"><code>pathlib</code></a>处理路径（Python 3 标准库的一部分），并使用<a href="http://docs.python-requests.org/en/master/"><code>requests</code></a>下载数据集。 我们只会在使用模块时才导入它们，因此您可以确切地看到每个位置上正在使用的模块。</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path
<span class="token keyword">import</span> requests

DATA_PATH <span class="token operator">=</span> Path<span class="token punctuation">(</span><span class="token string">"data"</span><span class="token punctuation">)</span>
PATH <span class="token operator">=</span> DATA_PATH <span class="token operator">/</span> <span class="token string">"mnist"</span>

PATH<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span>parents<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> exist_ok<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

URL <span class="token operator">=</span> <span class="token string">"https://github.com/pytorch/tutorials/raw/master/_static/"</span>
FILENAME <span class="token operator">=</span> <span class="token string">"mnist.pkl.gz"</span>

<span class="token keyword">if</span> <span class="token keyword">not</span> <span class="token punctuation">(</span>PATH <span class="token operator">/</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span>exists<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        content <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>URL <span class="token operator">+</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span>content
        <span class="token punctuation">(</span>PATH <span class="token operator">/</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token string">"wb"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>write<span class="token punctuation">(</span>content<span class="token punctuation">)</span>

</code></pre> 
<p>该数据集为 numpy 数组格式，并已使用<code>pickle</code>（一种用于序列化数据的 python 特定格式）存储。</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> pickle
<span class="token keyword">import</span> gzip

<span class="token keyword">with</span> gzip<span class="token punctuation">.</span><span class="token builtin">open</span><span class="token punctuation">(</span><span class="token punctuation">(</span>PATH <span class="token operator">/</span> FILENAME<span class="token punctuation">)</span><span class="token punctuation">.</span>as_posix<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"rb"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>
        <span class="token punctuation">(</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span><span class="token punctuation">,</span> _<span class="token punctuation">)</span> <span class="token operator">=</span> pickle<span class="token punctuation">.</span>load<span class="token punctuation">(</span>f<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"latin-1"</span><span class="token punctuation">)</span>

</code></pre> 
<p>每个图像为<code>28 x 28</code>，并存储为长度为<code>784 = 28x28</code>的扁平行。 让我们来看一个； 我们需要先将其重塑为 2d。</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> matplotlib <span class="token keyword">import</span> pyplot
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

pyplot<span class="token punctuation">.</span>imshow<span class="token punctuation">(</span>x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> cmap<span class="token operator">=</span><span class="token string">"gray"</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

</code></pre> 
<p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-M06fS5fl-1688002252943)(img/7c783def0bbe536f41ed172041b7e89e.png)]</p> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token punctuation">(</span><span class="token number">50000</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">)</span>

</code></pre> 
<p>PyTorch 使用<code>torch.tensor</code>而不是 numpy 数组，因此我们需要转换数据。</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> torch

x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_valid<span class="token punctuation">,</span> y_valid <span class="token operator">=</span> <span class="token builtin">map</span><span class="token punctuation">(</span>
    torch<span class="token punctuation">.</span>tensor<span class="token punctuation">,</span> <span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
n<span class="token punctuation">,</span> c <span class="token operator">=</span> x_train<span class="token punctuation">.</span>shape
x_train<span class="token punctuation">,</span> x_train<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> y_train<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_train<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x_train<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y_train<span class="token punctuation">.</span><span class="token builtin">min</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y_train<span class="token punctuation">.</span><span class="token builtin">max</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">50000</span><span class="token punctuation">,</span> <span class="token number">784</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="torchnn_99"></a>从零开始的神经网络（没有<code>torch.nn</code>）</h2> 
<p>首先，我们仅使用 PyTorch 张量操作创建模型。 我们假设您已经熟悉神经网络的基础知识。 （如果不是，则可以在 <a href="https://course.fast.ai">course.fast.ai</a> 中学习它们）。</p> 
<p>PyTorch 提供了创建随机或零填充张量的方法，我们将使用它们来为简单的线性模型创建权重和偏差。 这些只是常规张量，还有一个非常特殊的附加值：我们告诉 PyTorch 它们需要梯度。 这使 PyTorch 记录了在张量上完成的所有操作，因此它可以在反向传播时<em>自动计算</em>的梯度！</p> 
<p><strong>对于权重，我们在初始化之后设置<code>requires_grad</code>，因为我们不希望该步骤包含在梯度中。 （请注意，PyTorch 中的尾随<code>_</code>表示该操作是原地执行的。）</strong></p> 
<p>注意</p> 
<p>我们在这里用 <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">Xavier 初始化</a>（通过乘以<code>1 / sqrt(n)</code>）来初始化权重。</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> math

weights <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">)</span>
weights<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>
bias <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

</code></pre> 
<p>由于 PyTorch 具有自动计算梯度的功能，我们可以将任何标准的 Python 函数（或可调用对象）用作模型！ 因此，我们只需编写一个普通矩阵乘法和广播加法即可创建一个简单的线性模型。 我们还需要激活函数，因此我们将编写并使用<code>log_softmax</code>。 请记住：尽管 PyTorch 提供了许多预写的损失函数，激活函数等，但是您可以使用纯 Python 轻松编写自己的函数。 PyTorch 甚至会自动为您的函数创建快速 GPU 或向量化的 CPU 代码。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">log_softmax</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x <span class="token operator">-</span> x<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> log_softmax<span class="token punctuation">(</span>xb @ weights <span class="token operator">+</span> bias<span class="token punctuation">)</span>

</code></pre> 
<p>在上面，<code>@</code>代表点积运算。 我们将对一批数据（在本例中为 64 张图像）调用函数。 这是一个<em>正向传播</em>。 请注意，由于我们从随机权重开始，因此在这一阶段，我们的预测不会比随机预测更好。</p> 
<pre><code class="prism language-py">bs <span class="token operator">=</span> <span class="token number">64</span>  <span class="token comment"># batch size</span>

xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>bs<span class="token punctuation">]</span>  <span class="token comment"># a mini-batch from x</span>
preds <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>  <span class="token comment"># predictions</span>
preds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> preds<span class="token punctuation">.</span>shape
<span class="token keyword">print</span><span class="token punctuation">(</span>preds<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> preds<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.5964</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.3153</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.1321</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4480</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.2930</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.9507</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.1289</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.4175</span><span class="token punctuation">,</span>
        <span class="token operator">-</span><span class="token number">2.5332</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.3967</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SelectBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

</code></pre> 
<p>如您所见，<code>preds</code>张量不仅包含张量值，还包含梯度函数。 稍后我们将使用它进行反向传播。</p> 
<p>让我们实现负对数可能性作为损失函数（同样，我们只能使用标准 Python）：</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">nll</span><span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> target<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token operator">-</span><span class="token builtin">input</span><span class="token punctuation">[</span><span class="token builtin">range</span><span class="token punctuation">(</span>target<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span> target<span class="token punctuation">]</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

loss_func <span class="token operator">=</span> nll

</code></pre> 
<p>让我们使用随机模型来检查损失，以便我们稍后查看反向传播后是否可以改善我们的损失。</p> 
<pre><code class="prism language-py">yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">:</span>bs<span class="token punctuation">]</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">2.3735</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NegBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<p>我们还实现一个函数来计算模型的准确率。 对于每个预测，如果具有最大值的索引与目标值匹配，则该预测是正确的。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">accuracy</span><span class="token punctuation">(</span>out<span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">:</span>
    preds <span class="token operator">=</span> torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>out<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>preds <span class="token operator">==</span> yb<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>让我们检查一下随机模型的准确率，以便我们可以看出随着损失的增加，准确率是否有所提高。</p> 
<pre><code class="prism language-py"><span class="token keyword">print</span><span class="token punctuation">(</span>accuracy<span class="token punctuation">(</span>preds<span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">0.0938</span><span class="token punctuation">)</span>

</code></pre> 
<p>现在，我们可以运行一个训练循环。 对于每次迭代，我们将：</p> 
<ul>
<li>选择一个小批量数据（大小为<code>bs</code>）</li>
<li>使用模型进行预测</li>
<li>计算损失</li>
<li>
<code>loss.backward()</code>更新模型的梯度，在这种情况下为<code>weights</code>和<code>bias</code>。</li>
</ul> 
<p>现在，我们使用这些梯度来更新权重和偏差。 我们在<code>torch.no_grad()</code>上下文管理器中执行此操作，因为我们不希望在下一步的梯度计算中记录这些操作。 <a href="https://pytorch.org/docs/stable/notes/autograd.html">您可以在这里阅读有关 PyTorch 的 Autograd 如何记录操作的更多信息</a>。</p> 
<p>然后，将梯度设置为零，以便为下一个循环做好准备。 否则，我们的梯度会记录所有已发生操作的运行记录（即<code>loss.backward()</code>将梯度添加到已存储的内容中，而不是替换它们）。</p> 
<p>tips:</p> 
<p>您可以使用标准的 python 调试器逐步浏览 PyTorch 代码，从而可以在每一步检查各种变量值。 取消注释以下<code>set_trace()</code>即可尝试。</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> IPython<span class="token punctuation">.</span>core<span class="token punctuation">.</span>debugger <span class="token keyword">import</span> set_trace

lr <span class="token operator">=</span> <span class="token number">0.5</span>  <span class="token comment"># learning rate</span>
epochs <span class="token operator">=</span> <span class="token number">2</span>  <span class="token comment"># how many epochs to train for</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token punctuation">(</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> bs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token comment">#         set_trace()</span>
        start_i <span class="token operator">=</span> i <span class="token operator">*</span> bs
        end_i <span class="token operator">=</span> start_i <span class="token operator">+</span> bs
        xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>
        yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            weights <span class="token operator">-=</span> weights<span class="token punctuation">.</span>grad <span class="token operator">*</span> lr
            bias <span class="token operator">-=</span> bias<span class="token punctuation">.</span>grad <span class="token operator">*</span> lr
            weights<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
            bias<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>就是这样：我们完全从头开始创建并训练了一个最小的神经网络（在这种情况下，是逻辑回归，因为我们没有隐藏层）！</p> 
<p>让我们检查损失和准确率，并将其与我们之前获得的进行比较。 我们希望损失会减少，准确率会增加，而且确实如此。</p> 
<pre><code class="prism language-py"><span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">0.0811</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NegBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token number">1.</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="torchnnfunctional_257"></a>使用<code>torch.nn.functional</code>
</h2> 
<p>现在，我们将重构代码，使其执行与以前相同的操作，只是我们将开始利用 PyTorch 的<code>nn</code>类使其更加简洁和灵活。 从这里开始的每一步，应该使我们的代码占一个或多个特点：更简洁、更易理解、和/或更灵活。</p> 
<p>第一步也是最简单的步骤，就是用<code>torch.nn.functional</code>（通常按照惯例将其导入到名称空间<code>F</code>中）替换我们的手写激活和损失函数，从而缩短代码长度。 该模块包含<code>torch.nn</code>库中的所有函数（而该库的其他部分包含类）。 除了广泛的损失和激活函数外，您还会在这里找到一些方便的函数来创建神经网络，例如池化函数。 （还有一些用于进行卷积，线性层等的函数，但是正如我们将看到的那样，通常可以使用库的其他部分来更好地处理这些函数。）</p> 
<p>如果您使用的是负对数似然损失和对数 softmax 激活，那么 Pytorch 会提供结合了两者的单一函数<code>F.cross_entropy</code>。 因此，我们甚至可以从模型中删除激活函数。</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F

loss_func <span class="token operator">=</span> F<span class="token punctuation">.</span>cross_entropy

<span class="token keyword">def</span> <span class="token function">model</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> xb @ weights <span class="token operator">+</span> bias

</code></pre> 
<p>请注意，我们不再在<code>model</code>函数中调用<code>log_softmax</code>。 让我们确认我们的损失和准确率与以前相同：</p> 
<pre><code class="prism language-py"><span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">0.0811</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token number">1.</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="nnModule_289"></a>使用<code>nn.Module</code>重构</h2> 
<p>接下来，我们将使用<code>nn.Module</code>和<code>nn.Parameter</code>进行更清晰，更简洁的训练循环。 我们将<code>nn.Module</code>子类化（它本身是一个类并且能够跟踪状态）。 在这种情况下，我们要创建一个类，该类包含前进步骤的权重，偏置和方法。 <code>nn.Module</code>具有许多我们将要使用的属性和方法（例如<code>.parameters()</code>和<code>.zero_grad()</code>）。</p> 
<p>注意</p> 
<p><code>nn.Module</code>（大写<code>M</code>）是 PyTorch 的特定概念，并且是我们将经常使用的一类。 不要将<code>nn.Module</code>与<a href="https://docs.python.org/3/tutorial/modules.html">模块</a>（小写<code>m</code>）的 Python 概念混淆，该模块是可以导入的 Python 代码文件。</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> torch <span class="token keyword">import</span> nn

<span class="token keyword">class</span> <span class="token class-name">Mnist_Logistic</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>weights <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span> <span class="token operator">/</span> math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xb<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> xb @ self<span class="token punctuation">.</span>weights <span class="token operator">+</span> self<span class="token punctuation">.</span>bias

</code></pre> 
<p>由于我们现在使用的是对象而不是仅使用函数，因此我们首先必须实例化模型：</p> 
<pre><code class="prism language-py">model <span class="token operator">=</span> Mnist_Logistic<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>现在我们可以像以前一样计算损失。 请注意，<code>nn.Module</code>对象的使用就好像它们是函数一样（即，它们是<em>可调用的</em>），但是在后台 Pytorch 会自动调用我们的<code>forward</code>方法。</p> 
<pre><code class="prism language-py"><span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">2.3903</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<p>以前，在我们的训练循环中，我们必须按名称更新每个参数的值，并手动将每个参数的梯度分别归零，如下所示：</p> 
<pre><code class="prism language-py"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    weights <span class="token operator">-=</span> weights<span class="token punctuation">.</span>grad <span class="token operator">*</span> lr
    bias <span class="token operator">-=</span> bias<span class="token punctuation">.</span>grad <span class="token operator">*</span> lr
    weights<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    bias<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>现在我们可以利用<code>model.parameters()</code>和<code>model.zero_grad()</code>（它们都由 PyTorch 为<code>nn.Module</code>定义）来使这些步骤更简洁，并且更不会出现忘记某些参数的错误，尤其是当我们有一个更复杂的模型的时候：</p> 
<pre><code class="prism language-py"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> p <span class="token operator">-=</span> p<span class="token punctuation">.</span>grad <span class="token operator">*</span> lr
    model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>我们将把小的训练循环包装在<code>fit</code>函数中，以便稍后再运行。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token punctuation">(</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> bs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            start_i <span class="token operator">=</span> i <span class="token operator">*</span> bs
            end_i <span class="token operator">=</span> start_i <span class="token operator">+</span> bs
            xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>
            yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>
            pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
            loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

            loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
            <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
                    p <span class="token operator">-=</span> p<span class="token punctuation">.</span>grad <span class="token operator">*</span> lr
                model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

fit<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>让我们仔细检查一下我们的损失是否减少了：</p> 
<pre><code class="prism language-py"><span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">0.0808</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="nnLinear_389"></a>使用<code>nn.Linear</code>重构</h2> 
<p>我们继续重构我们的代码。 代替手动定义和初始化<code>self.weights</code>和<code>self.bias</code>并计算<code>xb @ self.weights + self.bias</code>，我们将对线性层使用 Pytorch 类<a href="https://pytorch.org/docs/stable/nn.html#linear-layers"><code>nn.Linear</code></a>，这将为我们完成所有工作。 Pytorch 具有许多类型的预定义层，可以大大简化我们的代码，并且通常也可以使其速度更快。</p> 
<pre><code class="prism language-py"><span class="token keyword">class</span> <span class="token class-name">Mnist_Logistic</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>lin <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">784</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xb<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>lin<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>

</code></pre> 
<p>我们以与以前相同的方式实例化模型并计算损失：</p> 
<pre><code class="prism language-py">model <span class="token operator">=</span> Mnist_Logistic<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">2.4215</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<p>我们仍然可以使用与以前相同的<code>fit</code>方法。</p> 
<pre><code class="prism language-py">fit<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">0.0824</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="optim_435"></a>使用<code>optim</code>重构</h2> 
<p>Pytorch 还提供了一个包含各种优化算法的包<code>torch.optim</code>。 我们可以使用优化器中的<code>step</code>方法采取向前的步骤，而不是手动更新每个参数。</p> 
<p>这将使我们替换之前的手动编码优化步骤：</p> 
<pre><code class="prism language-py"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span> p <span class="token operator">-=</span> p<span class="token punctuation">.</span>grad <span class="token operator">*</span> lr
    model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>而是只使用：</p> 
<pre><code class="prism language-py">opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

</code></pre> 
<p>（<code>optim.zero_grad()</code>将梯度重置为 0，我们需要在计算下一个小批量的梯度之前调用它。）</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> torch <span class="token keyword">import</span> optim

</code></pre> 
<p>我们将定义一个小函数来创建模型和优化器，以便将来重用。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">get_model</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    model <span class="token operator">=</span> Mnist_Logistic<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model<span class="token punctuation">,</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>

model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token punctuation">(</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> bs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        start_i <span class="token operator">=</span> i <span class="token operator">*</span> bs
        end_i <span class="token operator">=</span> start_i <span class="token operator">+</span> bs
        xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>
        yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">2.2999</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>
tensor<span class="token punctuation">(</span><span class="token number">0.0823</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="Dataset_498"></a>使用<code>Dataset</code>重构</h2> 
<p>PyTorch 有一个抽象的<code>Dataset</code>类。 数据集可以是具有<code>__len__</code>函数（由 Python 的标准<code>len</code>函数调用）和具有<code>__getitem__</code>函数作为对其进行索引的一种方法。 <a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html">本教程</a>演示了一个不错的示例，该示例创建一个自定义<code>FacialLandmarkDataset</code>类作为<code>Dataset</code>的子类。</p> 
<p>PyTorch 的<a href="https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset"><code>TensorDataset</code></a>是一个数据集包装张量。 通过定义索引的长度和方式，这也为我们提供了沿张量的第一维进行迭代，索引和切片的方法。 这将使我们在训练的同一行中更容易访问自变量和因变量。</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset

</code></pre> 
<p><code>x_train</code>和<code>y_train</code>都可以合并为一个<code>TensorDataset</code>，这将更易于迭代和切片。</p> 
<pre><code class="prism language-py">train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>

</code></pre> 
<p>以前，我们不得不分别遍历<code>x</code>和<code>y</code>值的小批量：</p> 
<pre><code class="prism language-py">xb <span class="token operator">=</span> x_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>
yb <span class="token operator">=</span> y_train<span class="token punctuation">[</span>start_i<span class="token punctuation">:</span>end_i<span class="token punctuation">]</span>

</code></pre> 
<p>现在，我们可以一起执行以下两个步骤：</p> 
<pre><code class="prism language-py">xb<span class="token punctuation">,</span>yb <span class="token operator">=</span> train_ds<span class="token punctuation">[</span>i<span class="token operator">*</span>bs <span class="token punctuation">:</span> i<span class="token operator">*</span>bs<span class="token operator">+</span>bs<span class="token punctuation">]</span>

</code></pre> 
<pre><code class="prism language-py">model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token punctuation">(</span>n <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">//</span> bs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        xb<span class="token punctuation">,</span> yb <span class="token operator">=</span> train_ds<span class="token punctuation">[</span>i <span class="token operator">*</span> bs<span class="token punctuation">:</span> i <span class="token operator">*</span> bs <span class="token operator">+</span> bs<span class="token punctuation">]</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">0.0819</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="DataLoader_555"></a>使用<code>DataLoader</code>重构</h2> 
<p>Pytorch 的<code>DataLoader</code>负责批量管理。 您可以从任何<code>Dataset</code>创建一个<code>DataLoader</code>。 <code>DataLoader</code>使迭代迭代变得更加容易。 不必使用<code>train_ds[i*bs : i*bs+bs]</code>，<code>DataLoader</code>会自动为我们提供每个小批量。</p> 
<pre><code class="prism language-py"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader

train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs<span class="token punctuation">)</span>

</code></pre> 
<p>以前，我们的循环遍历如下批量<code>(xb, yb)</code>：</p> 
<pre><code class="prism language-py"><span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token punctuation">(</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">//</span>bs <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    xb<span class="token punctuation">,</span>yb <span class="token operator">=</span> train_ds<span class="token punctuation">[</span>i<span class="token operator">*</span>bs <span class="token punctuation">:</span> i<span class="token operator">*</span>bs<span class="token operator">+</span>bs<span class="token punctuation">]</span>
    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>

</code></pre> 
<p>现在，我们的循环更加简洁了，因为<code>(xb, yb)</code>是从数据加载器自动加载的：</p> 
<pre><code class="prism language-py"><span class="token keyword">for</span> xb<span class="token punctuation">,</span>yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
    pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>

</code></pre> 
<pre><code class="prism language-py">model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py">tensor<span class="token punctuation">(</span><span class="token number">0.0821</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>NllLossBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span>

</code></pre> 
<p>得益于 Pytorch 的<code>nn.Module</code>，<code>nn.Parameter</code>，<code>Dataset</code>和<code>DataLoader</code>，我们的训练循环现在变得更小，更容易理解。 现在，让我们尝试添加在实践中创建有效模型所需的基本功能。</p> 
<h2>
<a id="_609"></a>添加验证</h2> 
<p>在第 1 节中，我们只是试图建立一个合理的训练循环以用于我们的训练数据。 实际上，您也应该<strong>始终</strong>具有<a href="https://www.fast.ai/2017/11/13/validation-sets/">验证集</a>，以便识别您是否过拟合。</p> 
<p><a href="https://www.quora.com/Does-the-order-of-training-data-matter-when-training-neural-networks">对训练数据进行打乱</a>对于防止批量与过拟合之间的相关性很重要。 另一方面，无论我们是否打乱验证集，验证损失都是相同的。 由于打乱需要花费更多时间，因此打乱验证数据没有任何意义。</p> 
<p>我们将验证集的批量大小设为训练集的两倍。 这是因为验证集不需要反向传播，因此占用的内存更少（不需要存储梯度）。 我们利用这一优势来使用更大的批量，并更快地计算损失。</p> 
<pre><code class="prism language-py">train_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

valid_ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>x_valid<span class="token punctuation">,</span> y_valid<span class="token punctuation">)</span>
valid_dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>valid_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span>

</code></pre> 
<p>我们将在每个周期结束时计算并打印验证损失。</p> 
<p>（请注意，我们总是在训练之前调用<code>model.train()</code>，并在推理之前调用<code>model.eval()</code>，因为诸如<code>nn.BatchNorm2d</code>和<code>nn.Dropout</code>之类的层会使用它们，以确保这些不同阶段的行为正确。）</p> 
<pre><code class="prism language-py">model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
        pred <span class="token operator">=</span> model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span>
        loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>pred<span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

    model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        valid_loss <span class="token operator">=</span> <span class="token builtin">sum</span><span class="token punctuation">(</span>loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">)</span>

    <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> valid_loss <span class="token operator">/</span> <span class="token builtin">len</span><span class="token punctuation">(</span>valid_dl<span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token number">0</span> tensor<span class="token punctuation">(</span><span class="token number">0.3743</span><span class="token punctuation">)</span>
<span class="token number">1</span> tensor<span class="token punctuation">(</span><span class="token number">0.3316</span><span class="token punctuation">)</span>

</code></pre> 
<h2>
<a id="fitget_data_659"></a>创建<code>fit()</code>和<code>get_data()</code>
</h2> 
<p>现在，我们将自己进行一些重构。 由于我们经历了两次相似的过程来计算训练集和验证集的损失，因此我们将其设为自己的函数<code>loss_batch</code>，该函数可计算一批损失。</p> 
<p>我们将优化器传入训练集中，然后使用它执行反向传播。 对于验证集，我们没有通过优化程序，因此该方法不会执行反向传播。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">loss_batch</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    loss <span class="token operator">=</span> loss_func<span class="token punctuation">(</span>model<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">,</span> yb<span class="token punctuation">)</span>

    <span class="token keyword">if</span> opt <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>
        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>
        opt<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>xb<span class="token punctuation">)</span>

</code></pre> 
<p><code>fit</code>运行必要的操作来训练我们的模型，并计算每个周期的训练和验证损失。</p> 
<pre><code class="prism language-py"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np

<span class="token keyword">def</span> <span class="token function">fit</span><span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> train_dl<span class="token punctuation">:</span>
            loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">,</span> opt<span class="token punctuation">)</span>

        model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
            losses<span class="token punctuation">,</span> nums <span class="token operator">=</span> <span class="token builtin">zip</span><span class="token punctuation">(</span>
                <span class="token operator">*</span><span class="token punctuation">[</span>loss_batch<span class="token punctuation">(</span>model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> xb<span class="token punctuation">,</span> yb<span class="token punctuation">)</span> <span class="token keyword">for</span> xb<span class="token punctuation">,</span> yb <span class="token keyword">in</span> valid_dl<span class="token punctuation">]</span>
            <span class="token punctuation">)</span>
        val_loss <span class="token operator">=</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>np<span class="token punctuation">.</span>multiply<span class="token punctuation">(</span>losses<span class="token punctuation">,</span> nums<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> np<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>nums<span class="token punctuation">)</span>

        <span class="token keyword">print</span><span class="token punctuation">(</span>epoch<span class="token punctuation">,</span> val_loss<span class="token punctuation">)</span>

</code></pre> 
<p><code>get_data</code>返回训练和验证集的数据加载器。</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">get_data</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> <span class="token punctuation">(</span>
        DataLoader<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        DataLoader<span class="token punctuation">(</span>valid_ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>bs <span class="token operator">*</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

</code></pre> 
<p>现在，我们获取数据加载器和拟合模型的整个过程可以在 3 行代码中运行：</p> 
<pre><code class="prism language-py">train_dl<span class="token punctuation">,</span> valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span>
model<span class="token punctuation">,</span> opt <span class="token operator">=</span> get_model<span class="token punctuation">(</span><span class="token punctuation">)</span>
fit<span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token number">0</span> <span class="token number">0.3120644524335861</span>
<span class="token number">1</span> <span class="token number">0.28915613491535186</span>

</code></pre> 
<p>您可以使用这些基本的 3 行代码来训练各种各样的模型。 让我们看看是否可以使用它们来训练卷积神经网络（CNN）！</p> 
<h2>
<a id="_CNN_730"></a>切换到 CNN</h2> 
<p>现在，我们将构建具有三个卷积层的神经网络。 由于上一节中的任何功能都不假设任何有关模型形式的信息，因此我们将能够使用它们来训练 CNN，而无需进行任何修改。</p> 
<p>我们将使用 Pytorch 的预定义<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d"><code>Conv2d</code></a>类作为我们的卷积层。 我们定义了具有 3 个卷积层的 CNN。 每个卷积后跟一个 ReLU。 最后，我们执行平均池化。 （请注意，<code>view</code>是 numpy 的<code>reshape</code>的 PyTorch 版本）</p> 
<pre><code class="prism language-py"><span class="token keyword">class</span> <span class="token class-name">Mnist_CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> xb<span class="token punctuation">)</span><span class="token punctuation">:</span>
        xb <span class="token operator">=</span> xb<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>
        xb <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">)</span>
        xb <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">)</span>
        xb <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>xb<span class="token punctuation">)</span><span class="token punctuation">)</span>
        xb <span class="token operator">=</span> F<span class="token punctuation">.</span>avg_pool2d<span class="token punctuation">(</span>xb<span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> xb<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> xb<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

lr <span class="token operator">=</span> <span class="token number">0.1</span>

</code></pre> 
<p><a href="https://cs231n.github.io/neural-networks-3/#sgd">动量</a>是随机梯度下降的一种变体，它也考虑了以前的更新，通常可以加快训练速度。</p> 
<pre><code class="prism language-py">model <span class="token operator">=</span> Mnist_CNN<span class="token punctuation">(</span><span class="token punctuation">)</span>
opt <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

fit<span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token number">0</span> <span class="token number">0.32337012240886687</span>
<span class="token number">1</span> <span class="token number">0.25021172934770586</span>

</code></pre> 
<h2>
<a id="nnSequential_774"></a><code>nn.Sequential</code>
</h2> 
<p><code>torch.nn</code>还有另一个方便的类，可以用来简化我们的代码：<a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential"><code>Sequential</code></a>。 <code>Sequential</code>对象以顺序方式运行其中包含的每个模块。 这是编写神经网络的一种简单方法。</p> 
<p>为了利用这一点，我们需要能够从给定的函数轻松定义<strong>自定义层</strong>。 例如，PyTorch 没有视层，我们需要为我们的网络创建一个层。 <code>Lambda</code>将创建一个层，然后在使用<code>Sequential</code>定义网络时可以使用该层。</p> 
<pre><code class="prism language-py"><span class="token keyword">class</span> <span class="token class-name">Lambda</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> func<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>func <span class="token operator">=</span> func

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> self<span class="token punctuation">.</span>func<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

<span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span>

</code></pre> 
<p>用<code>Sequential</code>创建的模型很简单：</p> 
<pre><code class="prism language-py">model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    Lambda<span class="token punctuation">(</span>preprocess<span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

opt <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

fit<span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token number">0</span> <span class="token number">0.30119081069231035</span>
<span class="token number">1</span> <span class="token number">0.25335356528759</span>

</code></pre> 
<h2>
<a id="DataLoader_823"></a>包装<code>DataLoader</code>
</h2> 
<p>Our CNN is fairly concise, but it only works with MNIST, because:</p> 
<ul>
<li>假设输入为<code>28 * 28</code>长向量</li>
<li>假设 CNN 的最终网格尺寸为<code>4 * 4</code>（因为这是平均值</li>
</ul> 
<p>我们使用的合并核大小）</p> 
<p>让我们摆脱这两个假设，因此我们的模型适用于任何 2d 单通道图像。 首先，我们可以删除初始的 Lambda 层，但将数据预处理移至生成器中：</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">,</span> y

<span class="token keyword">class</span> <span class="token class-name">WrappedDataLoader</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dl<span class="token punctuation">,</span> func<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>dl <span class="token operator">=</span> dl
        self<span class="token punctuation">.</span>func <span class="token operator">=</span> func

    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">return</span> <span class="token builtin">len</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>dl<span class="token punctuation">)</span>

    <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        batches <span class="token operator">=</span> <span class="token builtin">iter</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>dl<span class="token punctuation">)</span>
        <span class="token keyword">for</span> b <span class="token keyword">in</span> batches<span class="token punctuation">:</span>
            <span class="token keyword">yield</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>func<span class="token punctuation">(</span><span class="token operator">*</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span>

train_dl<span class="token punctuation">,</span> valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>train_dl<span class="token punctuation">,</span> preprocess<span class="token punctuation">)</span>
valid_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>valid_dl<span class="token punctuation">,</span> preprocess<span class="token punctuation">)</span>

</code></pre> 
<p>接下来，我们可以将<code>nn.AvgPool2d</code>替换为<code>nn.AdaptiveAvgPool2d</code>，这使我们能够定义所需的<em>输出</em>张量的大小，而不是所需的<em>输入</em>张量的大小。 结果，我们的模型将适用于任何大小的输入。</p> 
<pre><code class="prism language-py">model <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    nn<span class="token punctuation">.</span>AdaptiveAvgPool2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    Lambda<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
<span class="token punctuation">)</span>

opt <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

</code></pre> 
<p>试试看：</p> 
<pre><code class="prism language-py">fit<span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token number">0</span> <span class="token number">0.327303307390213</span>
<span class="token number">1</span> <span class="token number">0.2181092014491558</span>

</code></pre> 
<h2>
<a id="_GPU_890"></a>使用您的 GPU</h2> 
<p>如果您足够幸运地能够使用具有 CUDA 功能的 GPU（可以从大多数云提供商处以每小时 0.50 美元的价格租用一个），则可以使用它来加速代码。 首先检查您的 GPU 是否在 Pytorch 中正常工作：</p> 
<pre><code class="prism language-py"><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token boolean">True</span>

</code></pre> 
<p>然后为其创建一个设备对象：</p> 
<pre><code class="prism language-py">dev <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span>
    <span class="token string">"cuda"</span><span class="token punctuation">)</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cpu"</span><span class="token punctuation">)</span>

</code></pre> 
<p>让我们更新<code>preprocess</code>，将批量移至 GPU：</p> 
<pre><code class="prism language-py"><span class="token keyword">def</span> <span class="token function">preprocess</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span>

train_dl<span class="token punctuation">,</span> valid_dl <span class="token operator">=</span> get_data<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span> valid_ds<span class="token punctuation">,</span> bs<span class="token punctuation">)</span>
train_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>train_dl<span class="token punctuation">,</span> preprocess<span class="token punctuation">)</span>
valid_dl <span class="token operator">=</span> WrappedDataLoader<span class="token punctuation">(</span>valid_dl<span class="token punctuation">,</span> preprocess<span class="token punctuation">)</span>

</code></pre> 
<p>最后，我们可以将模型移至 GPU。</p> 
<pre><code class="prism language-py">model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>dev<span class="token punctuation">)</span>
opt <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span>

</code></pre> 
<p>您应该发现它现在运行得更快：</p> 
<pre><code class="prism language-py">fit<span class="token punctuation">(</span>epochs<span class="token punctuation">,</span> model<span class="token punctuation">,</span> loss_func<span class="token punctuation">,</span> opt<span class="token punctuation">,</span> train_dl<span class="token punctuation">,</span> valid_dl<span class="token punctuation">)</span>

</code></pre> 
<p>输出如下：</p> 
<pre><code class="prism language-py"><span class="token number">0</span> <span class="token number">0.1833980613708496</span>
<span class="token number">1</span> <span class="token number">0.17365939717292786</span>

</code></pre> 
<h2>
<a id="_949"></a>总结</h2> 
<p>现在，我们有了一个通用的数据管道和训练循环，您可以将其用于使用 Pytorch 训练许多类型的模型。 要了解现在可以轻松进行模型训练，请查看<code>mnist_sample</code>示例笔记本。</p> 
<p>当然，您需要添加很多内容，例如数据扩充，超参数调整，监控训练，迁移学习等。 这些功能可在 fastai 库中使用，该库是使用本教程中所示的相同设计方法开发的，为希望进一步推广其模型的从业人员提供了自然的下一步。</p> 
<p>我们承诺在本教程开始时将通过示例分别说明<code>torch.nn</code>，<code>torch.optim</code>，<code>Dataset</code>和<code>DataLoader</code>。 因此，让我们总结一下我们所看到的：</p> 
<blockquote> 
 <ul>
<li>
<code>torch.nn</code> 
   <ul>
<li>
<code>Module</code>：创建一个行为类似于函数的可调用对象，但也可以包含状态（例如神经网络层权重）。 它知道其中包含的 <code>Parameter</code> ，并且可以将其所有坡度归零，遍历它们以进行权重更新等。</li>
<li>
<code>Parameter</code>：张量的包装器，用于告知 <code>Module</code> 具有在反向传播期间需要更新的权重。 仅更新具有<code>require_grad</code>属性集的张量</li>
<li>
<code>functional</code>：一个模块（通常按照惯例导入到 <code>F</code> 名称空间中），其中包含激活函数，损失函数等。 以及卷积和线性层等层的无状态版本。</li>
</ul> </li>
<li>
<code>torch.optim</code>：包含诸如 <code>SGD</code> 的优化程序，这些优化程序在后退步骤</li>
<li>
<code>Dataset</code> 中更新 <code>Parameter</code> 的权重。 具有 <code>__len__</code> 和 <code>__getitem__</code> 的对象，包括 Pytorch 提供的类，例如 <code>TensorDataset</code>
</li>
<li>
<code>DataLoader</code>：获取任何 <code>Dataset</code> 并创建一个迭代器，该迭代器返回批量数据。</li>
</ul> 
</blockquote> 
<p><strong>脚本的总运行时间</strong>：（0 分钟 57.062 秒）</p> 
<p><a href="../_downloads/a6246751179fbfb7cad9222ef1c16617/nn_tutorial.py">下载 Python 源码：<code>nn_tutorial.py</code></a></p> 
<p><a href="../_downloads/5ddab57bb7482fbcc76722617dd47324/nn_tutorial.ipynb">下载 Jupyter 笔记本：<code>nn_tutorial.ipynb</code></a></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>