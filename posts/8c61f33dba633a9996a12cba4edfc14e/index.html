<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Python 爬虫之 urllib 包基本使用 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Python 爬虫之 urllib 包基本使用</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <p>urllib 是一个 python 内置包，不需要额外安装即可使用，包里面包含了以下几个用来处理 url 的模块：</p> 
<ul>
<li> <p>urllib.request，用来打开和读取 url，意思就是可以用它来模拟发送请求，就像在浏览器里输入网址然后敲击回车一样，获取网页响应内容。</p> </li>
<li> <p>urllib.error，用来处理 urllib.request 引起的异常，保证程序的正常执行。</p> </li>
<li> <p>urllib.parse，用来解析 url，可以对 url 进行拆分、合并等。</p> </li>
<li> <p>urllib.robotparse，用来解析 robots.txt 文件，判断网站是否能够进行爬取。</p> </li>
</ul> 
<p>掌握以上四个模块，你就能对网站进行简单的爬虫操作，下面我们逐个介绍。</p> 
<h2>1、urllib.request 模块</h2> 
<p>urllib.request 模块定义了以下几个函数。</p> 
<p>1.1 urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)</p> 
<p>该函数主要用于模拟网站请求，返回一个 HTTPResponse 类型的对象。</p> 
<p>1.1.1 urlopen 函数中参数定义</p> 
<ul>
<li> <p>url，必选参数，是一个 str 字符串或者 Request 对象(后面会介绍)。</p> </li>
<li> <p>data，bytes 类型的可选参数，如果传递的是字典型数据，可以用 urllib.parse.urlencode() 进行编码，返回 str 字符串，再将 str 转换成 bytes 字节流。如果传递 data 参数，urlopen 将使用 HTTP POST 方式请求，否则为 HTTP GET 请求。</p> </li>
<li> <p>timeout，可选参数，设置超时时间(未设置时使用全局默认超时时间)，以秒为单位计时，如果 urlopen 请求超出了设置时间还未得到响应则抛出异常。</p> </li>
<li> <p>cafile 和 capath，可选参数，在 HTTPS 连接请求时指定已认证的 CA 证书以及证书路径。</p> </li>
<li> <p>cadefault，一般可忽略该参数。</p> </li>
<li> <p>context，ssl.SSLContext 类型的可选参数，用来指定 SSL 设置。</p> </li>
</ul> 
<p>1.1.2 urlopen 函数返回类型</p> 
<p>urlopen 函数请求返回一个 HTTPResponse 响应上下文，或者请求异常抛出 URLError 协议错误，一般有如下属性：</p> 
<ul>
<li> <p>geturl()，返回检索的 url，通常用于判定是否进行了重定向。</p> </li>
<li> <p>info()，返回网页的头信息。</p> </li>
<li> <p>getcode()，返回 HTTPResponse 响应的状态码。</p> </li>
</ul> 
<p>1.1.3 urlopen 函数的应用实例​​​​​​</p> 
<pre><code># 创建一个 HTTP GET 请求，输出响应上下文</code><code>from urllib.request import urlopen</code><code>response = urlopen("http://www.python.org")</code><code>print(response.read())</code>​​​​​​</pre> 
<pre><code># 创建一个 HTTP POST 请求，输出响应上下文</code><code>from urllib.request import urlopen</code><code>from urllib.parse import urlencode</code><code>data = {'kw' : 'python'}</code><code>data = bytes(urlencode(data), encoding = 'utf-8')</code><code>response = urlopen("https://fanyi.baidu.com/sug", data)</code><code>print(response.read().decode('unicode_escape'))</code>​​​​​​</pre> 
<pre><code># 创建一个 HTTP GET 请求，设置超时时间为0.1s</code><code>import urllib.request</code><code>import urllib.error</code><code>try:</code><code>    response=urllib.request.urlopen('http://www.python.org',timeout=0.1)</code><code>    print(response.read()) </code><code>except urllib.error.URLError as e:</code><code>    print(e.reason)</code></pre> 
<h3>1.2 urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)</h3> 
<p>该函数主要用于构造一个 url，返回一个 urllib.request.Request 对象。</p> 
<p>1.2.1 Request 函数中参数定义</p> 
<ul>
<li> <p>url，必选参数，请求的 url 地址。</p> </li>
<li> <p>data，bytes 类型的可选参数。</p> </li>
<li> <p>headers，字典类型，有些 HTTP 服务器仅允许来自浏览器的请求，因此通过 headers 来模拟浏览器对 url 的访问，比如模拟谷歌浏览器时使用的 headers："Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36"。可以通过调用 add_header() 来添加 headers 信息。</p> </li>
<li> <p>origin_req_host，请求方的 host 名称或者 IP 地址。</p> </li>
<li> <p>unverifiable，表示这个请求是否无法验证，默认为 False。比如请求一张图片，如果没有权限获取图片那它的值就是 true。</p> </li>
<li> <p>method，是一个字符串，用来指示请求使用的方法，如：GET,POST,PUT 等，默认是 GET 请求。</p> </li>
</ul> 
<p>1.2.2 Request 函数返回类型</p> 
<p>与 urlopen 函数请求返回一样，一般返回一个 HTTPResponse 响应上下文。</p> 
<p>1.2.3 Request 函数的应用实例</p> 
<pre><code># 采用 HTTP GET 请求的方法模拟谷歌浏览器访问网站，输出响应上下文</code><code>from urllib import request,parse</code><code>url = 'http://www.python.org'</code><code>headers = {<!-- --></code><code>    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'</code><code>}</code><code>req = request.Request(url, headers = headers, method = 'GET')</code><code>response = request.urlopen(req) </code><code>print(response.read())</code>​​​</pre> 
<pre><code># 采用 HTTP POST 请求的方法模拟谷歌浏览器访问网站，输出响应上下文</code><code>from urllib import request</code><code>from urllib import parse</code><code>url = 'https://fanyi.baidu.com/sug'</code><code>data = {'kw' : 'python'}</code><code>data = bytes(parse.urlencode(data), encoding = 'utf-8')</code><code>headers = {<!-- --></code><code>    'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36'</code><code>}</code><code>req = request.Request(url, data, headers, method = 'POST')</code><code>response = request.urlopen(req) </code><code>print(response.read().decode('unicode_escape'))</code>​​​</pre> 
<pre><code># 创建一个 HTTP GET 请求，通过 add_header 添加一个 UserAgent</code><code>import urllib.request</code><code>import random</code><code>url = 'http://www.python.org'</code><code>headerUserAgentList = ['Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.132 Safari/537.36',</code><code>'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0']</code><code>randomHeaderUserAgent = random.choice(headerUserAgentList) # 随机选取一个 UserAgent</code><code>req = urllib.request.Request(url) </code><code>req.add_header('User-Agent', randomHeaderUserAgent) # 添加 UserAgent</code><code>response=urllib.request.urlopen(req)</code><code>print(req.get_header('User-agent'))</code><code>print(req.headers) # 打印请求的 header 信息</code></pre> 
<h2>2、 urllib.error 模块</h2> 
<p>urllib.error 模块定义了由 urllib.request 模块引发的异常，异常主要包含 URLError 和 HTTPError。</p> 
<h3>2.1 urllib.error.URLError 异常</h3> 
<p>URLError 类继承自 OSError 类，是 error 异常模块的基类，由request模块产生的异常都可以通过捕获这个类来处理。URLError 只有一个属性 reason，即返回错误的原因。</p> 
<p>应用实例：​​​​​​​</p> 
<pre><code># 在请求连接时候捕获网址错误引发的异常</code><code>from urllib import request, error</code><code>try:</code><code>    response = request.urlopen('https://www,baidu,com')</code><code>except error.URLError as e:</code><code>    print(e.reason)</code></pre> 
<h3>2.2 urllib.error.HTTPError 异常</h3> 
<p>HTTPError 是 URLError 的子类，专门用来处理 HTTP 请求错误，比如认证请求失败，包含以下三个属性：</p> 
<ul>
<li> <p>code：返回 HTTP 响应的状态码，如404页面不存在等。</p> </li>
<li> <p>reason：返回错误的原因。</p> </li>
<li> <p>headers：返回 HTTP 响应头信息。</p> </li>
</ul> 
<p>应用举例：​​​​​​​</p> 
<pre><code># 返回401未授权错误</code><code>from urllib import request,error</code><code>try:</code><code>    response=request.urlopen('http://pythonscraping.com/pages/auth/login.php')</code><code>    print(response.getcode())</code><code>except error.HTTPError as e:</code><code>    print('1.错误原因：n%sn2.状态码：n%sn3.响应头信息：n%s' %(e.reason, e.code, e.headers))</code><code>except error.URLError as e:</code><code>    print(e.reason)</code></pre> 
<h2>3、 urllib.parse 模块</h2> 
<p>urllib.parse 模块定义了一个处理 url 的标准接口，用来实现 url 字符串的抽取、合并以及链接转换。该模块主要用到的函数如下。</p> 
<h3>3.1 urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)</h3> 
<p>用于实现 url 字符串的识别和分段，可以分为六个字符串，分别是 scheme (协议)，netloc (域名)，path (路径)，params (参数)，query (查询条件)和 fragment (锚点)，其结构如下所示：“scheme://netloc/path;parameters?query#fragment”。实际上具体 url 某些字段可能会不存在，比如 “http://www.baidu.com” 只包含了协议和域名。</p> 
<p>3.1.1 urlparse 函数中参数定义</p> 
<ul>
<li> <p>urlstring，待解析的 url 字符串。</p> </li>
<li> <p>scheme，是默认的协议，比如 http 或者 https，url 字符串中如果不携带相关协议，可以通过 scheme 来指定，如果 url 中指定了相关协议则在 url 中生效。</p> </li>
<li> <p>allow_fragments，是否忽略锚点，设置为 False 即 fragment 部分会被忽略，反之不会忽略。</p> </li>
</ul> 
<p>3.1.2 urlparse 的返回类型</p> 
<p>函数返回的是一个 urllib.parse.ParseResult 对象，获取解析出来的 url 六个字段。</p> 
<p>3.1.3 urlparse 应用举例​​​​​​​</p> 
<pre><code># 解析并输出 url 中每个字段的字符串</code><code>import urllib</code><code>url = 'http://www.baidu.com/urllib.parse.html;python?kw=urllib.parse#module-urllib'</code><code>result = urllib.parse.urlparse(url)</code><code>print(result)</code><code>print(result.scheme, result.netloc, result.path, result.params, result.query, result.fragment, sep = 'n')</code></pre> 
<h3>3.2 urllib.parse.urlunparse(parts)</h3> 
<p>与 urlparse 相反，通过列表或者元祖的形式将分段的字符串组合成一个完整的 url 字符串。</p> 
<p>3.2.1 urlunparse 函数中参数定义</p> 
<ul><li> <p>parts，可以是列表或者元组。</p> </li></ul> 
<p>3.2.2 urlunparse 的返回类型</p> 
<p>urlunparse 函数返回一个构造好的 url 字符串。</p> 
<p>3.2.3 应用举例​​​​​​​</p> 
<pre><code># 通过 data 列表或元组构造一个 url 并输出</code><code>import urllib</code><code>dataList = ['http', 'www.baidu.com', '/urllib.parse.html', 'python', 'kw=urllib.parse', 'modul-urllib'] # 六个字符串都必须填写，否则会出现 ValueError 错误，如果某一字符串不存在则填入空字符</code><code>dataTuple = ('http', 'www.baidu.com', '/urllib.parse.html', '', 'kw=urllib.parse', 'modul-urllib') # 六个字符串都必须填写，否则会出现 ValueError 错误，如果某一字符串不存在则填入空字符</code><code>urlList = urllib.parse.urlunparse(dataList)</code><code>urlTuple = urllib.parse.urlunparse(dataTuple)</code><code>print('1.urlList:%sn2.urlTuple:%s' % (urlList, urlTuple))</code></pre> 
<h3>3.3 urllib.parse.urlsplit(urlstring, scheme='', allow_fragments=True)</h3> 
<p>与 urlparse 函数类似，但它只返回 url 字符串的5个字段，把 params 合并到 path 中。</p> 
<p>urlsplit 应用举例​​​​​​​</p> 
<pre><code># 解析并输出 url 中每个字段的字符串，params 会合并到 path 中。</code><code>import urllib</code><code>url = 'http://www.baidu.com/urllib.parse.html;python?kw=urllib.parse#modul-urllib'</code><code>result = urllib.parse.urlsplit(url)</code><code>print(result)</code><code>print(result.scheme, result.netloc, result.path, result.query, result.fragment, sep = 'n')</code></pre> 
<h3>3.4 urllib.parse.urlunsplit(parts)</h3> 
<p>与 urlunparse 函数类似，它也是将 url 中各部分字段组合完整的 url 字符串的方法，唯一的区别是列表或元组的长度必须是5个，因为它把 params 省略了。</p> 
<p>urlunsplit 应用举例​​​​​​​</p> 
<pre><code># 通过 data 列表或元组构造一个 url 并输出</code><code>import urllib</code><code>dataList = ['http', 'www.baidu.com', '/urllib.parse.html;python', 'kw=urllib.parse', 'modul-urllib'] # 五个字符串都必须填写，否则会出现 ValueError 错误，如果某一字符串不存在则填入空字符</code><code>dataTuple = ('http', 'www.baidu.com', '/urllib.parse.html;python', 'kw=urllib.parse', 'modul-urllib') # 五个字符串都必须填写，否则会出现 ValueError 错误，如果某一字符串不存在则填入空字符</code><code>urlList = urllib.parse.urlunsplit(dataList)</code><code>urlTuple = urllib.parse.urlunsplit(dataTuple)</code><code>print('1.urlList:%sn2.urlTuple:%s' % (urlList, urlTuple))</code></pre> 
<h3>3.5 urllib.parse.quote(string, safe='/', encoding=None, errors=None)</h3> 
<p>使用 %xx 转义字符替换字符串中的特殊字符，比如汉字。字母、数字和‘_.-~’字符不会被替换。</p> 
<p>3.5.1 quote 函数中参数定义</p> 
<ul>
<li> <p>string，可以是 str 字符串或 bytes 类型。</p> </li>
<li> <p>safe，可选参数，默认是'/'，指明不应该被替换的附加 ASCII 字符。</p> </li>
<li> <p>encoding 和 errors，可选参数，用来定义如何处理 non-ASCII 字符。一般默认设置编码方法为 encoding='utf-8'，errors='strict'，这意味着编码错误将引发 UnicodeError。如果 string 是 bytes 类型，不能设置 encoding 和 errors，否则将引发 TypeError。</p> </li>
</ul> 
<p>3.5.2 quote 函数的返回类型</p> 
<p>quote 函数返回一个编码后的字符串。</p> 
<p>3.5.3 应用举例​​​​​​​</p> 
<pre><code># 采用 quote 对 url 中的汉字进行编码，输出编码后的结果</code><code>import urllib</code><code>url = 'http://www.baidu.com/爬虫'</code><code>result = urllib.parse.quote(url)</code><code>print(result)</code><code>url = 'http://www.baidu.com/+爬虫'</code><code>result = urllib.parse.quote(url, '+') # 更改 safe 参数</code><code>print(result)</code></pre> 
<h3>3.6 urllib.parse.unquote(string, encoding='utf-8', errors='replace')</h3> 
<p>与 quote 函数相反，把 %xx 转义字符替换成字符。</p> 
<p>3.6.1 unquote 函数的参数定义</p> 
<ul>
<li> <p>string，必须是 str 字符串。</p> </li>
<li> <p>encoding 和 errors，可选参数，定义如何将 %xx 转义字符解码为 Unicode 字符。encoding 默认为 'utf-8'，errors 默认为 'replace'，表示无效的转义字符将会用占位符替换。</p> </li>
</ul> 
<p>3.6.2 unquote 函数的返回类型</p> 
<p>unquote 函数返回一个解码后的字符串。</p> 
<p>3.6.3 应用举例​​​​​​​</p> 
<pre><code># 解码经过 quote 函数处理后的 url，输出解码后的结果。</code><code>import urllib</code><code>url = 'http://www.baidu.com/爬虫'</code><code>result = urllib.parse.quote(url)</code><code>print(result)</code><code>result = urllib.parse.unquote(url)</code><code>print(result)</code></pre> 
<h3>3.7 urllib.parse.urljoin(base, url, allow_fragments=True)</h3> 
<p>该函数用来将基本 url 与另一个 url 组合，更新基本 url 字符串。它会使用 url 对基本 url 中缺失的部分进行补充，比如 scheme (协议)、netloc (域名)和 path (路径)。即根据 url 字符串中带有的字段，对基本 url 中没有的字段进行补充，已存在的字段进行替换。</p> 
<p>3.7.1 urljoin 函数中参数定义</p> 
<ul>
<li> <p>base，是一个基本 url。</p> </li>
<li> <p>url，将 scheme (协议)、netloc (域名)或 path (路径)字段组合进基本 url 的 url。</p> </li>
<li> <p>allow_fragments，是否忽略锚点，设置为 False 即 fragment 部分会被忽略，反之不会忽略。</p> </li>
</ul> 
<p>3.7.2 urljoin 函数返回类型</p> 
<p>返回组合成功的 url 字符串。</p> 
<p>3.7.3 应用举例​​​​​​​</p> 
<pre><code># 基于 url 对 base_url 进行重新组合，并输出组合结果。</code><code>import urllib</code><code>base_url = 'http://www.baidu.com'</code><code>url = 'https://www.google.com/urllib.parse.html;python?kw=urllib.parse#module-urllib'</code><code>result = urllib.parse.urljoin(base_url,url,False)</code><code>print(result)</code></pre> 
<h3>3.8 urllib.parse.urlencode(query, doseq=False, safe='', encoding=None, errors=None, quote_via=quote_plus)</h3> 
<p>urlencode 函数可以将字典转化为 GET 请求中的 query (查询条件)，或者将字典转化为 POST 请求中需要上传的数据。</p> 
<p>3.8.1 urlencode 函数中参数定义</p> 
<ul>
<li> <p>query，字典类型。</p> </li>
<li> <p>doseq，允许字典中一个键对应多个值，编码成 query (查询条件)。</p> </li>
<li> <p>safe、encoding 和 errors，这三个参数由 quote_via 指定。</p> </li>
</ul> 
<p>3.8.2 urlencode 函数返回类型</p> 
<p>urlencode 函数返回 str 字符串。</p> 
<p>3.8.3 应用举例​​​​​​​</p> 
<pre><code># 创建 GET 请求</code><code>import urllib</code><code>params = {'username':'xxx','password':'123'}</code><code>base_url='http://www.baidu.com'</code><code>url=base_url + '?' + urllib.parse.urlencode(params)</code><code>print(url)</code><code>params = {'username':['xxx', 'yyy'], 'password':'123'} # username 键对应多个值</code><code>base_url='http://www.baidu.com'</code><code>url=base_url + '?' + urllib.parse.urlencode(params) # doseq 设置为 False，会解析成乱码</code><code>print(url)</code><code>url=base_url + '?' + urllib.parse.urlencode(params, True) # doseq 设置为 True</code><code>print(url)</code></pre> 
<h2>4、 urllib.robotparse 模块</h2> 
<p>rebotparser 模块提供了一个 RobotFileParser 类，主要用来解析网站上发布的 robots.txt，然后根据解析内容判断爬虫是否有权限来爬取这个网页。</p> 
<h3>4.1 robots.txt 文件</h3> 
<p>robots.txt，存放于网站根目录下，采用 ASCII 编码的文本文件，记录此网站中的哪些内容是不应被爬虫获取的，哪些是可以被爬虫获取的。</p> 
<p>robots.txt 文件内容举例</p> 
<p>User-agent: * Disallow: / Allow: /public/</p> 
<ul>
<li> <p>User-agent，爬虫的名称，将其设置为 * 代表协议对任何爬虫有效，如果设置为 Baiduspider 则代表协议仅对百度爬虫有效，要是有多条则对多个爬虫有效，至少需要指定一条。</p> </li>
<li> <p>Disallow，网页中不允许抓取的目录，上述例子中设置的 / 代表不允许抓取所有的页面。</p> </li>
<li> <p>Allow，一般和 Disallow 一起使用，用来排除单独的某些限制，上述例子中设置为 /public/ 表示所有页面不允许抓取，但可以抓取 public 目录。</p> </li>
</ul> 
<h3>4.2 urllib.robotparser.RobotFileParser(url='') 类及其常用方法</h3> 
<ul>
<li> <p>set_url(url)，设置引用 robots.txt 文件的 url，如果在创建 RobotFileParser 对象时传入了 url，那么就不需要使用这个方法设置 url。</p> </li>
<li> <p>read()，读取 robots.txt 文件并将其提供给解析器，不返回任何内容。</p> </li>
<li> <p>parse(lines)，用来解析 robots.txt 某些行的内容，并安装语法规则来分析内容。</p> </li>
<li> <p>can_fetch(useragent, url)，传入两个参数，用户代理以及要爬取的网站，返回的内容是该用户代理否可以抓取这个网站，结果为 True 或 False。</p> </li>
</ul> 
<p>应用举例​​​​​​​</p> 
<pre><code># 使用两种爬虫代理分别查看是否可以对 'http://www.baidu.com' 网站进行爬取</code><code>from urllib.robotparser import RobotFileParser</code><code>rp = RobotFileParser()</code><code>rp.set_url("http://www.baidu.com/robots.txt")</code><code>rp.read()</code><code>print(rp.can_fetch('Baiduspider', 'http://www.baidu.com')) </code><code>print(rp.can_fetch('*', 'http://www.baidu.com'))</code></pre> 
<h2>总结</h2> 
<p>本节给大家介绍了 Python 中 urllib 包的使用，为 Python 工程师对该包的使用提供了支撑，了解爬取网站所需的一些基本函数操作。</p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>