<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Hive从本质到实战 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Hive从本质到实战</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h1>
<a id="Hive_0"></a>Hive</h1> 
<p><img src="https://images2.imgbox.com/99/65/gMXpbMfx_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/5e/7b/jVxsLRW7_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c2/ea/TAUoDNRM_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/72/f9/o9ssHTZP_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c8/2f/ODpKBT9o_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="_7"></a>本质</h2> 
<h3>
<a id="_HQL__MapReduce__9"></a>将 HQL 转化成 MapReduce 程序</h3> 
<h3>
<a id="HQLMapReduce_11"></a>本人理解是使用HQL去写MapReduce</h3> 
<h3>
<a id="Hive__HDFS_13"></a>Hive 处理的数据存储在 HDFS</h3> 
<h3>
<a id="Hive__MapReduce_15"></a>Hive 分析数据底层的实现是 MapReduce</h3> 
<h3>
<a id="_Yarn__17"></a>执行程序运行在 Yarn 上</h3> 
<h2>
<a id="_19"></a>属性配置</h2> 
<h3>
<a id="_21"></a>配置文件</h3> 
<ul><li>hive-site.xml：用户自定义配置会覆盖默认配置</li></ul> 
<h3>
<a id="_25"></a>命令行参数</h3> 
<ul><li>在命令行添加-hiveconf param=value 来设定参数<br> 仅对本次有效</li></ul> 
<h3>
<a id="_30"></a>参数声明</h3> 
<ul><li>可以在 HQL 中使用 SET 关键字设定参数，仅对本次有效<br> set mapred.reduce.tasks=100;</li></ul> 
<h2>
<a id="Hive_35"></a>Hive的数据类型</h2> 
<h3>
<a id="_37"></a>基本数据类型</h3> 
<ul>
<li>TINYINT</li>
<li>SMALLINT</li>
<li>INT</li>
<li>BIGINT</li>
<li>DOUBLE</li>
<li>FLOAT</li>
<li>STRING</li>
<li>BOOLEAN</li>
<li>TIMESTAMP</li>
<li>BIGANY</li>
</ul> 
<h3>
<a id="_50"></a>集合数据类型</h3> 
<ul>
<li> <p>STRUCT</p> 
  <ul><li> <p>struct&lt;street:string, city:string&gt;</p> 
    <ul><li>“address”: {<!-- --><br> “street”: “hui long guan”,<br> “city”: “beijing”<br> }</li></ul> </li></ul> </li>
<li> <p>MAP</p> 
  <ul><li>map&lt;string, int&gt;</li></ul> </li>
<li> <p>ARRAY</p> 
  <ul><li>array</li></ul> </li>
</ul> 
<h3>
<a id="_Hive__69"></a>基于上述数据结构，我们在 Hive 里创建对应的表，并导入数据</h3> 
<ul>
<li> <p>可以将数据写在txt文件中，MAP，STRUCT 和 ARRAY 里的元素间关系都可以用同一个字符表示，这里用“_”。</p> 
  <ul><li>songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long<br> guan_beijing<br> yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing</li></ul> </li>
<li> <p>创建Hive表的时候在语句最后添加<br> row format delimited fields terminated by ‘,’<br> collection items terminated by ‘_’<br> map keys terminated by ‘:’<br> lines terminated by ‘n’;</p> 
  <ul>
<li>row format delimited fields terminated by ‘,’ ----&gt; 列分隔符</li>
<li>collection items terminated by ‘_’ ----&gt;MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)</li>
<li>map keys terminated by ‘:’ ----&gt;MAP 中的 key 与 value 的分隔符</li>
<li>lines terminated by ‘n’; ----&gt; 行分隔符</li>
</ul> </li>
<li> <p>导入文本数据到测试表</p> 
  <ul><li>load data local inpath ‘/opt/module/hive/datas/test.txt’ into table test;</li></ul> </li>
<li> <p>访问三种集合列里的数据，以下分别是 ARRAY，MAP，STRUCT 的访问方式</p> 
  <ul><li>hive (default)&gt; select friends[1],children[‘xiao song’],address.city from test</li></ul> </li>
</ul> 
<h3>
<a id="_97"></a>类型转换</h3> 
<ul>
<li> <p>隐式转换规则</p> 
  <ul>
<li>（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如 TINYINT 可以转换成INT，INT 可以转换成 BIGINT。</li>
<li>（2）所有整数类型、FLOAT 和 STRING 类型都可以隐式地转换成 DOUBLE。</li>
<li>（3）TINYINT、SMALLINT、INT 都可以转换为 FLOAT。</li>
<li>（4）BOOLEAN 类型不可以转换为任何其它的类型。</li>
</ul> </li>
<li> <p>可以使用 CAST 操作显示进行数据类型转换</p> 
  <ul><li>CAST(‘1’ AS INT)将把字符串’1’ 转换成整数 1</li></ul> </li>
</ul> 
<h2>
<a id="DDL__110"></a>DDL 数据定义</h2> 
<h3>
<a id="_112"></a>创建数据库</h3> 
<ul>
<li>CREATE DATABASE [IF NOT EXISTS] database_name<br> [COMMENT database_comment]<br> [LOCATION hdfs_path]<br> [WITH DBPROPERTIES (property_name=property_value, …)];</li>
<li>创建一个数据库，数据库在 HDFS 上的默认存储路径是/user/hive/warehouse/*.db。</li>
</ul> 
<h3>
<a id="_120"></a>查询数据库</h3> 
<h3>
<a id="_122"></a>修改数据库</h3> 
<ul>
<li>使用 ALTER DATABASE 命令为某个数据库的 DBPROPERTIES 设置键-值对属性值</li>
<li>hive (default)&gt; alter database db_hive set dbproperties(‘createtime’=‘20170830’);</li>
</ul> 
<h3>
<a id="_127"></a>删除数据库</h3> 
<h3>
<a id="_129"></a>创建表</h3> 
<ul>
<li> <p>管理表(内部表)</p> 
  <ul><li>默认建立的表都是管理表，删除的时候会将原始数据删除</li></ul> </li>
<li> <p>外部表</p> 
  <ul><li>external，外部表删除之后，只会删除元数据，其原始数据在hdfs中不会被删除</li></ul> </li>
<li> <p>两表的相互转换</p> 
  <ul>
<li> <p>管理表转换为外部表</p> 
    <ul><li>alter table student2 set tblproperties(‘EXTERNAL’=‘TRUE’);</li></ul> </li>
<li> <p>外部表转换为管理表</p> 
    <ul><li>alter table student2 set tblproperties(‘EXTERNAL’=‘FALSE’);</li></ul> </li>
</ul> </li>
</ul> 
<h3>
<a id="_149"></a>修改表</h3> 
<h2>
<a id="DML__151"></a>DML 数据操作</h2> 
<h3>
<a id="Load_153"></a>向表中装载数据(Load)</h3> 
<ul>
<li>hive&gt; load data [local] inpath ‘数据的 path’ [overwrite] into table student [partition (partcol1=val1,…)];</li>
<li>（1）load data:表示加载数据</li>
<li>（2）local:表示从本地加载数据到 hive 表；否则从 HDFS 加载数据到 hive 表</li>
<li>（3）inpath:表示加载数据的路径</li>
<li>（4）overwrite:表示覆盖表中已有数据，否则表示追加</li>
<li>（5）into table:表示加载到哪张表</li>
<li>（6）student:表示具体的表</li>
<li>（7）partition:表示上传到指定分区</li>
</ul> 
<h3>
<a id="Insert_164"></a>通过查询语句向表中插入数据（Insert）</h3> 
<ul>
<li> <p>hive (default)&gt; insert overwrite table student_par select id, name from student where month=‘201709’;</p> </li>
<li> <p>insert into：以追加数据的方式插入到表或分区，原有数据不会删除</p> </li>
<li> <p>insert overwrite：会覆盖表中已存在的数据</p> </li>
<li> <p>注意：insert 不支持插入部分字段</p> </li>
<li> <p>多表(多分区)插入模式</p> 
  <ul><li>hive (default)&gt; from student<br> insert overwrite table student partition(month=‘201707’)<br> select id, name where month=‘201709’<br> insert overwrite table student partition(month=‘201706’)<br> select id, name where month=‘201709’;</li></ul> </li>
</ul> 
<h3>
<a id="_178"></a>数据导出</h3> 
<ul>
<li> <p>Insert 导出</p> </li>
<li> <p>Hadoop 命令导出到本地</p> </li>
<li> <p>Hive Shell 命令导出</p> </li>
<li> <p>Export 导出到 HDFS 上</p> </li>
<li> <p>Sqoop 导出</p> </li>
<li> <p>清除表中数据（Truncate）</p> 
  <ul><li>Truncate 只能删除管理表，不能删除外部表中数据</li></ul> </li>
</ul> 
<h2>
<a id="_189"></a>查询</h2> 
<h3>
<a id="select_191"></a>select</h3> 
<h3>
<a id="from_193"></a>from</h3> 
<h3>
<a id="where_195"></a>where</h3> 
<h3>
<a id="like_197"></a>like</h3> 
<h3>
<a id="betweeninisnot_199"></a>betweeninisnot</h3> 
<h3>
<a id="andnotor_201"></a>andnotor</h3> 
<h3>
<a id="having_203"></a>having</h3> 
<h3>
<a id="group_by_205"></a>group by</h3> 
<h3>
<a id="join_on_207"></a>join on</h3> 
<h3>
<a id="order_by_209"></a>order by</h3> 
<h3>
<a id="sort_by_211"></a>sort by</h3> 
<h3>
<a id="distribute_by_213"></a>distribute by</h3> 
<h3>
<a id="cluster_by_215"></a>cluster by</h3> 
<h2>
<a id="_217"></a>分区表和分桶表</h2> 
<h3>
<a id="_219"></a>分区表</h3> 
<ul>
<li> <p>一级分区表</p> 
  <ul><li>partitioned by (day string)</li></ul> </li>
<li> <p>二级分区表</p> 
  <ul>
<li> <p>partitioned by (day string, hour string)</p> </li>
<li> <p>分区表与数据产生关联的三种方式</p> 
    <ul>
<li>上传数据后修复</li>
<li>上传数据后添加分区</li>
<li>创建文件夹后load数据到分区</li>
</ul> </li>
</ul> </li>
<li> <p>动态分区调整</p> 
  <ul>
<li> <p>自动会根据分区字段的值，将数据插入到相应的分区中</p> </li>
<li> <p>（1）开启动态分区功能（默认 true，开启）</p> 
    <ul><li>hive.exec.dynamic.partition=true</li></ul> </li>
<li> <p>（2）设置为非严格模式（动态分区的模式，默认 strict，表示必须指定至少一个分区为静态分区，nonstrict 模式表示允许所有的分区字段都可以使用动态分区。）</p> 
    <ul><li>hive.exec.dynamic.partition.mode=nonstrict</li></ul> </li>
<li> <p>（3）在所有执行 MR 的节点上，最大一共可以创建多少个动态分区。默认 1000</p> 
    <ul><li>hive.exec.max.dynamic.partitions=1000</li></ul> </li>
<li> <p>（4）在每个执行 MR 的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即 day 字段有 365 个值，那么该参数就需要设置成大于 365，如果使用默认值 100，则会报错。</p> 
    <ul><li>hive.exec.max.dynamic.partitions.pernode=100</li></ul> </li>
<li> <p>（5）整个 MR Job 中，最大可以创建多少个 HDFS 文件。默认 100000</p> 
    <ul><li>hive.exec.max.created.files=100000</li></ul> </li>
<li> <p>（6）当有空分区生成时，是否抛出异常。一般不需要设置。默认 false</p> <pre><code>- hive.error.on.empty.partition=false
</code></pre> 
    <ul><li> <p>举例：将 dept 表中的数据按照地区（loc 字段），插入到目标表 dept_partition 的相应分区中</p> 
      <ul><li>hive (default)&gt; insert into table dept_partition_dy partition(loc) select deptno, dname, loc from dept;</li></ul> </li></ul> </li>
</ul> </li>
</ul> 
<h3>
<a id="_266"></a>分桶表</h3> 
<ul>
<li>分区针对的是数据的存储路径；分桶针对的是数据文件。</li>
<li>create table stu_buck(id int, name string)<br> clustered by(id)<br> into 4 buckets<br> row format delimited fields terminated by ‘t’;</li>
</ul> 
<h3>
<a id="_274"></a>抽样查询</h3> 
<h2>
<a id="_276"></a>函数</h2> 
<h3>
<a id="_278"></a>内置函数</h3> 
<ul>
<li> <p>nvl</p> </li>
<li> <p>窗口函数</p> 
  <ul>
<li> <p>over</p> 
    <ul>
<li>Current row: 当前行</li>
<li>n preceding：往前n行数据</li>
<li>n following：往后n行数据</li>
<li>unbounded preceding：表示从前面的起点</li>
<li>unbounded following：表示从后面的起点</li>
</ul> </li>
<li> <p>lag(col,n,default_value)</p> </li>
<li> <p>lead(col,n,default_value)</p> </li>
<li> <p>ntile(n)</p> </li>
</ul> </li>
<li> <p>Rank</p> 
  <ul>
<li>RANK() 排序相同时会重复，总数不会变</li>
<li>DENSE_RANK() 排序相同时会重复，总数会减少</li>
<li>ROW_NUMBER() 会根据顺序计算</li>
</ul> </li>
<li> <p>行转列</p> 
  <ul>
<li> <p>CONCAT(string A/col, string B/col…)</p> 
    <ul><li> 
      <blockquote> 
       <p>SELECT concat(‘abc’, ‘def’) FROM src LIMIT 1;<br> ‘abcdef’</p> 
      </blockquote> </li></ul> </li>
<li> <p>CONCAT_WS(separator, str1, str2,…)</p> 
    <ul><li> 
      <blockquote> 
       <p>SELECT concat_ws(’.’, ‘www’, array(‘facebook’, ‘com’)) FROM src LIMIT 1;<br> ‘www.facebook.com’</p> 
      </blockquote> </li></ul> </li>
<li> <p>注意: CONCAT_WS must be "string or array</p> </li>
<li> <p>hive (default)&gt; SELECT<br> t1.c_b,<br> CONCAT_WS("|",collect_set(t1.name))<br> FROM (<br> SELECT<br> NAME ,<br> CONCAT_WS(’,’,constellation,blood_type) c_b<br> FROM person_info<br> )t1<br> GROUP BY t1.c_b</p> </li>
</ul> </li>
<li> <p>列转行</p> 
  <ul>
<li> <p>LATERAL VIEW</p> 
    <ul><li>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li></ul> </li>
<li> <p>SPLIT(string str, string regex):</p> 
    <ul>
<li> <p>按照regex字符串分割str，会返回分割后的字符串数组</p> </li>
<li> 
      <blockquote> 
       <p>SELECT split(‘oneAtwoBthreeC’, ‘[ABC]’) FROM src LIMIT 1;<br> [“one”, “two”, “three”]</p> 
      </blockquote> </li>
</ul> </li>
<li> <p>EXPLODE(col)：</p> 
    <ul><li>将hive一列中复杂的array或者map结构拆分成多行</li></ul> </li>
</ul> </li>
</ul> 
<h3>
<a id="_345"></a>自定义函数</h3> 
<ul>
<li> <p>UDF（User-Defined-Function）–&gt; 一进一出</p> 
  <ul>
<li> <p>继承GenericUDF类</p> 
    <ul>
<li>initialize(ObjectInspector[] arguments)</li>
<li>evaluate(DeferredObject[] arguments)</li>
<li>getDisplayString(String[] children)</li>
</ul> </li>
<li> <p>打成jar包上传到服务器/opt/module/hive/datas/myudf.jar</p> </li>
<li> <p>将jar包添加到hive的classpath</p> 
    <ul><li>hive (default)&gt; add jar /opt/module/hive/datas/myudf.jar;</li></ul> </li>
<li> <p>创建临时函数与开发好的java class关联</p> 
    <ul><li>hive (default)&gt; create temporary function my_len as “com.gis.hive. MyStringLength”;</li></ul> </li>
<li> <p>即可在hql中使用自定义的函数</p> </li>
</ul> </li>
<li> <p>UDAF（User-Defined Aggregation Function） --&gt; 聚合函数，多进一出，类似：count/max/min</p> </li>
<li> <p>UDTF（User-Defined Table-Generating Functions）–&gt; 炸裂函数，一进多出，如：explode()</p> 
  <ul>
<li> <p>继承GenericUDTF</p> 
    <ul>
<li> <p>initialize(StructObjectInspector argOIs)</p> 
      <ul><li>初始化</li></ul> </li>
<li> <p>process(Object[] args)</p> 
      <ul><li>函数逻辑</li></ul> </li>
<li> <p>close()</p> </li>
</ul> </li>
<li> <p>打成jar包上传到服务器/opt/module/hive/data/myudtf.jar</p> </li>
<li> <p>将jar包添加到hive的classpath下</p> 
    <ul><li>hive (default)&gt; add jar /opt/module/hive/data/myudtf.jar;</li></ul> </li>
<li> <p>创建临时函数与开发好的java class关联</p> 
    <ul><li>hive (default)&gt; create temporary function myudtf as “com.atguigu.hive.MyUDTF”;</li></ul> </li>
<li> <p>使用自定义的函数</p> </li>
</ul> </li>
</ul> 
<h2>
<a id="_392"></a>压缩与存储</h2> 
<h3>
<a id="_394"></a>压缩</h3> 
<ul>
<li>map端压缩</li>
<li>reduce端压缩</li>
</ul> 
<h3>
<a id="_399"></a>文件存储</h3> 
<h2>
<a id="_401"></a>调优</h2> 
<h3>
<a id="Explain_403"></a>执行计划Explain</h3> 
<ul>
<li>explain select * from emp;</li>
<li>详细的：explain extended select * from emp;</li>
</ul> 
<h3>
<a id="Fetch_408"></a>Fetch抓取</h3> 
<ul>
<li>Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算</li>
<li>例如SELECT * FROM employees;，是简单的读取输出，不走MapReduce</li>
<li>在hive-default.xml.template文件中：<br> ·hive.fetch.task.conversion默认是more<br> ·老版本hive默认是minimal<br> 该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。</li>
</ul> 
<h3>
<a id="_417"></a>本地模式</h3> 
<ul>
<li>Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短</li>
<li>用户可以通过设置hive.exec.mode.local.auto= true，来让Hive在适当的时候自动启动这个优化</li>
</ul> 
<h3>
<a id="_422"></a>表的优化</h3> 
<ul>
<li> <p>Group By</p> 
  <ul>
<li> <p>是否在Map端进行聚合，默认为True</p> 
    <ul><li>hive(default)&gt; set hive.map.aggr = true</li></ul> </li>
<li> <p>在Map端进行聚合操作的条目数目</p> 
    <ul><li>hive(default)&gt; set hive.groupby.mapaggr.checkinterval = 100000</li></ul> </li>
<li> <p>有数据倾斜的时候进行负载均衡（默认是false）</p> 
    <ul><li>hive(default)&gt; set hive.groupby.skewindata = true</li></ul> </li>
</ul> </li>
<li> <p>合理设置Map和Reduce</p> 
  <ul><li>小文件合并</li></ul> </li>
<li> <p>jvm重用</p> </li>
</ul> 
<h2>
<a id="_444"></a>实战</h2> 
<h3>
<a id="Top10_446"></a>视频观看量Top10</h3> 
<ul><li> <p>select<br> videoId,<br> <code>views</code><br> from gulivideo_orc<br> order by <code>views</code> desc<br> limit 10;</p> 
  <ul><li>select videoId,sum(views) over(partition by videoId) from gulivideo_orc</li></ul> </li></ul> 
<h3>
<a id="Top10_457"></a>统计视频类别热度Top10</h3> 
<ul>
<li> <p>因为一个视频可能属于一个或多个类别，所以还要进行列转行</p> 
  <ul><li>hive(default)&gt; select<br> tmp01.category_col,<br> count(tmp01.videoId) num<br> from (<br> select<br> videoId,<br> category_col<br> from gulivideo_orc<br> lateral view<br> explode(category) t as category_col<br> ) tmp01<br> group by tmp01.category_col<br> order by num desc<br> limit 10;</li></ul> </li>
<li> <p>列转行</p> 
  <ul>
<li> <p>LATERAL VIEW</p> 
    <ul><li>LATERAL VIEW udtf(expression) tableAlias AS columnAlias</li></ul> </li>
<li> <p>SPLIT(string str, string regex):</p> 
    <ul><li>按照regex字符串分割str，会返回分割后的字符串数组</li></ul> </li>
<li> <p>EXPLODE(col)：</p> 
    <ul><li>将hive一列中复杂的array或者map结构拆分成多行</li></ul> </li>
</ul> </li>
</ul> 
<h3>
<a id="20Top20_490"></a>统计出视频观看数最高的20个视频的所属类别以及类别包含Top20视频的个数</h3> 
<ul><li>hive(default)&gt; select<br> table02.categroy_name,<br> count(table02.videoId) num<br> from (<br> select<br> videoId,<br> categroy_name<br> from (<br> select<br> videoId,<br> <code>views</code>,<br> category<br> from gulivideo_orc<br> order by <code>views</code> desc<br> limit 20<br> ) table01<br> lateral view<br> explode(category) tmp as categroy_name<br> ) table02<br> group by table02.categroy_nam;</li></ul> 
<p><img src="https://images2.imgbox.com/c3/c7/ExdHmOei_o.png" alt="在这里插入图片描述"></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>