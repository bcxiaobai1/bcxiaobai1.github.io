<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>NLP 项目：维基百科文章爬虫和分类 - 语料库阅读器 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">NLP 项目：维基百科文章爬虫和分类 - 语料库阅读器</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="htmledit_views">
                    <div> 
 <p id="543d" style="background-color:transparent;margin-left:0px"></p> 
</div> 
<div> 
 <div style="margin-left:auto"></div> 
</div> 
<p class="img-center"><img alt="" height="396" src="https://images2.imgbox.com/7b/a3/pwnh67zX_o.jpg" width="560"></p> 
<p><span><span style="background-color:#ffffff"><span style="color:#242424"><span style="color:#242424"><span><span style="background-color:#ffffff"><span style="color:#242424"><span style="color:#242424"><a class="af ag ah ai aj ak al am an ao ap aq ar hl" href="https://admantium.medium.com/?source=post_page-----5d8d315aa41d--------------------------------" title="塞巴斯蒂安">塞巴斯蒂安</a></span></span></span></span></span></span></span></span></p> 
<div> 
 <div> 
  <div> 
   <div style="margin-left:0"> 
    <div> 
     <div> 
      <div> 
       <div> 
        <h2>一、说明</h2> 
       </div> 
      </div> 
     </div> 
    </div> 
   </div> 
  </div> 
 </div> 
</div> 
<p id="b175" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        自然语言处理是机器学习和人工智能的一个迷人领域。这篇博客文章启动了一个具体的 NLP 项目，涉及使用维基百科文章进行聚类、分类和知识提取。灵感和一般方法源自<a class="af mq" href="https://www.goodreads.com/book/show/32758032-applied-text-analysis-with-python" title="《Applied Text Analysis with Python》">《Applied Text Analysis with Python》</a>一书。</span></span></p> 
<p id="1d65" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        在接下来的文章中，我将展示如何实现维基百科文章爬虫，如何将文章收集到语料库中，如何应用文本预处理、标记化、编码和矢量化，以及最后应用机器学习算法进行聚类和分类。</span></span></p> 
<p id="1e2b" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff"><em>        本文的技术背景是</em><code><em>Python v3.11</em></code><em>和几个附加库，其中最重要的</em><code><em>nltk v3.8.1</em></code><em>是 和</em><code><em>wikipedia-api v0.6.0</em></code><em>。所有示例也应该适用于较新的版本。</em></span></span></p> 
<p id="e8bd" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff"><em>        本文最初出现在我的博客</em><a class="af mq" href="https://admantium.com/blog/nlp06_wikipedia_article_crawler/" title="admantium.com">admantium.com</a>上。</span></span></p> 
<h2 id="36fc" style="background-color:transparent;margin-left:0px"><span style="color:#242424"><span style="background-color:#ffffff">二、项目概要</span></span></h2> 
<p id="415d" style="margin-left:-.46em"><span style="color:#242424"><span style="background-color:#ffffff">        该项目的目标是下载、处理和应用维基百科文章上的机器学习算法。首先，下载并存储来自维基百科的选定文章。其次，生成一个语料库，即所有文本文档的总和。第三，对每个文档文本进行预处理，例如通过删除停用词和符号，然后进行标记化。第四，将标记化文本转换为向量以接收数字表示。最后，应用不同的机器学习算法。</span></span></p> 
<p id="3d96" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        在第一篇文章中，解释了步骤一和步骤二。</span></span></p> 
<h2 id="48ac" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">三、先决条件</span></span></h2> 
<p id="c2bf" style="margin-left:-.46em"><span style="color:#242424"><span style="background-color:#ffffff"><a class="af mq" href="https://jupyter.org/" title="我喜欢在Jupyter Notebook">我喜欢在Jupyter Notebook</a>中工作并使用优秀的依赖管理器<a class="af mq" href="https://python-poetry.org/" title="Poetry">Poetry</a>。在您选择的项目文件夹中运行以下命令以安装所有必需的依赖项并在浏览器中启动 Jupyter 笔记本。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba"># Complete the interactive project creation
poetry init

# Add core dependencies
poetry add nltk@^3.8.1 jupyterlab@^4.0.0 scikit-learn@^1.2.2 wikipedia-api@^0.5.8 matplotlib@^3.7.1 numpy@^1.24.3 pandas@^2.0.1

# Add NLTK dependencies
python3 -c "import nltk; 
    nltk.download('punkt'); 
    nltk.download('averaged_perceptron_tagger'); 
    nltk.download('reuters'); 
    nltk.download('stopwords');"

# Start jupyterhub
poetry run jupyterlab</code></pre> 
<p id="9511" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">浏览器中应该会打开一个新的 Jupyter Notebook。</span></span></p> 
<h2 id="47ce" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">四、Python 库</span></span></h2> 
<p id="1090" style="margin-left:-.46em"><span style="color:#242424"><span style="background-color:#ffffff">在这篇博文中，将使用以下 Python 库：</span></span></p> 
<p id="fd6b" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff"><a class="af mq" href="https://pypi.org/project/Wikipedia-API/" title="维基百科-API">维基百科-API</a>：</span></span></p> 
<ul style="margin-left:0"><li id="1b5c" style="margin-left:30px">
<code>Page</code>代表维基百科文章及其标题、文本、类别和相关页面的对象。</li></ul> 
<p id="ad83" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff"><a class="af mq" href="https://www.nltk.org/api/nltk.html" title="NLTK">NLTK</a></span></span></p> 
<ul style="margin-left:0">
<li id="87c9" style="margin-left:30px">
<code>PlaintextCorpusReader</code>用于提供对文档的访问、提供标记化方法并计算有关所有文件的统计信息的可遍历对象</li>
<li id="2044" style="margin-left:30px">
<code>sent_tokenizer</code>并<code>word_tokenizer</code>用于生成令牌</li>
</ul> 
<h2 id="f5c0" style="background-color:transparent;margin-left:0px"><span style="color:#242424"><span style="background-color:#ffffff">五、（第 1 部分）维基百科文章爬虫</span></span></h2> 
<p id="0bb2" style="margin-left:-.46em"><span style="color:#242424"><span style="background-color:#ffffff">        该项目从创建自定义维基百科爬虫开始。尽管我们可以使用来自各种来源的维基百科语料库数据集（例如 NLTK 中的内置语料库），但自定义爬虫提供了对文件格式、内容和内容现实的最佳控制。</span></span></p> 
<p id="322b" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        下载和处理原始 HTML 可能非常耗时，尤其是当我们还需要从中确定相关链接和类别时。一个非常方便的图书馆可以帮助您。wikipedia <a class="af mq" href="https://pypi.org/project/Wikipedia-API/" title="-api">-api</a>为我们完成了所有这些繁重的工作。在此基础上，我们逐步开发核心功能。</span></span></p> 
<p id="876f" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        首先，我们创建一个基类，定义它自己的 Wikipedia 对象并确定存储文章的位置。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">import os
import re
import wikipediaapi as wiki_api

class WikipediaReader():
    def __init__(self, dir = "articles"):
        self.pages = set()
        self.article_path = os.path.join("./", dir)
        self.wiki = wiki_api.Wikipedia(
                language = 'en',
                extract_format=wiki_api.ExtractFormat.WIKI)
        try:
            os.mkdir(self.article_path)
        except Exception as e:
            pass</code></pre> 
<p id="e5b6" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        这还定义了<code>pages</code>爬虫访问的一组页面对象。该<code>page</code>对象非常有用，因为它可以访问文章标题、文本、类别和其他页面的链接。</span></span></p> 
<p id="85c4" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        其次，我们需要接收文章名称的辅助方法，如果存在，它将<code>page</code>向集合中添加一个新对象。我们需要将调用包装在一个<code>try except</code>块中，因为某些包含特殊字符的文章无法正确处理，例如<code>Add article 699/1000 Tomasz Imieliński</code>. 此外，还有一些我们不需要存储的元文章。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">def add_article(self, article):
    try:
        page = self.wiki.page(self._get_page_title(article))
        if page.exists():
            self.pages.add(page)
            return(page)
    except Exception as e:
        print(e)</code></pre> 
<p id="d2f1" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">第三，我们要提取一篇文章的类别。每篇维基百科文章都在页面底部的两个可见部分（请参阅以下屏幕截图）以及未呈现为 HTML 的元数据中定义类别。因此，最初的类别列表可能听起来令人困惑。看一下这个例子：</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">wr = WikipediaReader()
wr.add_article("Machine Learning")
ml = wr.list().pop()

print(ml.categories)
# {'Category:All articles with unsourced statements': Category:All articles with unsourced statements (id: ??, ns: 14),
#  'Category:Articles with GND identifiers': Category:Articles with GND identifiers (id: ??, ns: 14),
#  'Category:Articles with J9U identifiers': Category:Articles with J9U identifiers (id: ??, ns: 14),
#  'Category:Articles with LCCN identifiers': Category:Articles with LCCN identifiers (id: ??, ns: 14),
#  'Category:Articles with NDL identifiers': Category:Articles with NDL identifiers (id: ??, ns: 14),
#  'Category:Articles with NKC identifiers': Category:Articles with NKC identifiers (id: ??, ns: 14),
#  'Category:Articles with short description': Category:Articles with short description (id: ??, ns: 14),
#  'Category:Articles with unsourced statements from May 2022': Category:Articles with unsourced statements from May 2022 (id: ??, ns: 14),
#  'Category:Commons category link from Wikidata': Category:Commons category link from Wikidata (id: ??, ns: 14),
#  'Category:Cybernetics': Category:Cybernetics (id: ??, ns: 14),
#  'Category:Learning': Category:Learning (id: ??, ns: 14),
#  'Category:Machine learning': Category:Machine learning (id: ??, ns: 14),
#  'Category:Short description is different from Wikidata': Category:Short description is different from Wikidata (id: ??, ns: 14),
#  'Category:Webarchive template wayback links': Category:Webarchive template wayback links (id: ??, ns: 14)}</code></pre> 
<p id="7a1a" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">因此，我们根本不通过应用多个正则表达式过滤器来存储这些特殊类别。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">def get_categories(self, title):
    page = self.add_article(title)
    if page:
        if (list(page.categories.keys())) and (len(list(page.categories.keys())) &gt; 0):
            categories = [c.replace('Category:','').lower() for c in list(page.categories.keys())
                if c.lower().find('articles') == -1
                and c.lower().find('pages') == -1
                and c.lower().find('wikipedia') == -1
                and c.lower().find('cs1') == -1
                and c.lower().find('webarchive') == -1
                and c.lower().find('dmy dates') == -1
                and c.lower().find('short description') == -1
                and c.lower().find('commons category') == -1

            ]
            return dict.fromkeys(categories, 1)
    
    return {}</code></pre> 
<p id="5b71" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">第四，我们现在定义抓取方法。这是一种可定制的广度优先搜索，从一篇文章开始，获取所有相关页面，将这些页面广告到页面对象，然后再次处理它们，直到文章总数耗尽或达到深度级别。说实话：我只用它爬过 1000 篇文章。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">
def crawl_pages(self, article, depth = 3, total_number = 1000):
    print(f'Crawl {total_number} :: {article}')

    page = self.add_article(article)
    childs = set()
    if page:
        for child in page.links.keys():
            if len(self.pages) &lt; total_number:
                print(f'Add article {len(self.pages)}/{total_number} {child}')
                self.add_article(child)
                childs.add(child)
    depth -= 1
    if depth &gt; 0:
        for child in sorted(childs):
            if len(self.pages) &lt; total_number:
                self.crawl_pages(child, depth, len(self.pages))</code></pre> 
<p id="f6a7" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">让我们开始爬取机器学习文章：</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">reader = WikipediaReader()
reader.crawl_pages("Machine Learning")

print(reader.list())
# Crawl 1000 :: Machine Learning
# Add article 1/1000 AAAI Conference on Artificial Intelligence
# Add article 2/1000 ACM Computing Classification System
# Add article 3/1000 ACM Computing Surveys
# Add article 4/1000 ADALINE
# Add article 5/1000 AI boom
# Add article 6/1000 AI control problem
# Add article 7/1000 AI safety
# Add article 8/1000 AI takeover
# Add article 9/1000 AI winter</code></pre> 
<p id="4b17" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        最后，当一组<code>page</code>对象可用时，我们提取它们的文本内容并将它们存储在文件中，其中文件名代表其标题的清理版本。需要注意的是：文件名需要保留其文章名称的投降，否则我们无法再次获取页面对象，因为使用小写文章名称的搜索不会返回结果。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">def process(self, update=False):
    for page in self.pages:
        filename = re.sub('s+', '_', f'{page.title}')
        filename = re.sub(r'[():]','', filename)
        file_path = os.path.join(self.article_path, f'{filename}.txt')
        if update or not os.path.exists(file_path):
            print(f'Downloading {page.title} ...')
            content = page.text
            with open(file_path, 'w') as file:
                file.write(content)
        else:
            print(f'Not updating {page.title} ...')</code></pre> 
<p id="78c5" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">这是该类的完整源代码<code>WikipediaReader</code>。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">import os
import re
import wikipediaapi as wiki_api

class WikipediaReader():
    def __init__(self, dir = "articles"):
        self.pages = set()
        self.article_path = os.path.join("./", dir)
        self.wiki = wiki_api.Wikipedia(
                language = 'en',
                extract_format=wiki_api.ExtractFormat.WIKI)
        try:
            os.mkdir(self.article_path)
        except Exception as e:
            pass

    def _get_page_title(self, article):
        return re.sub(r's+','_', article)

    def add_article(self, article):
        try:
            page = self.wiki.page(self._get_page_title(article))
            if page.exists():
                self.pages.add(page)
                return(page)
        except Exception as e:
            print(e)

    def list(self):
        return self.pages

    def process(self, update=False):
        for page in self.pages:
            filename = re.sub('s+', '_', f'{page.title}')
            filename = re.sub(r'[():]','', filename)
            file_path = os.path.join(self.article_path, f'{filename}.txt')
            if update or not os.path.exists(file_path):
                print(f'Downloading {page.title} ...')
                content = page.text
                with open(file_path, 'w') as file:
                    file.write(content)
            else:
                print(f'Not updating {page.title} ...')
    def crawl_pages(self, article, depth = 3, total_number = 1000):
        print(f'Crawl {total_number} :: {article}')
        page = self.add_article(article)
        childs = set()
        if page:
            for child in page.links.keys():
                if len(self.pages) &lt; total_number:
                    print(f'Add article {len(self.pages)}/{total_number} {child}')
                    self.add_article(child)
                    childs.add(child)
        depth -= 1
        if depth &gt; 0:
            for child in sorted(childs):
                if len(self.pages) &lt; total_number:
                    self.crawl_pages(child, depth, len(self.pages))
    def get_categories(self, title):
        page = self.add_article(title)
        if page:
            if (list(page.categories.keys())) and (len(list(page.categories.keys())) &gt; 0):
                categories = [c.replace('Category:','').lower() for c in list(page.categories.keys())
                   if c.lower().find('articles') == -1
                   and c.lower().find('pages') == -1
                   and c.lower().find('wikipedia') == -1
                   and c.lower().find('cs1') == -1
                   and c.lower().find('webarchive') == -1
                   and c.lower().find('dmy dates') == -1
                   and c.lower().find('short description') == -1
                   and c.lower().find('commons category') == -1
                ]
                return dict.fromkeys(categories, 1)
        return {}</code></pre> 
<p id="cde1" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        让我们使用维基百科爬虫来下载与机器学习相关的文章。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">reader = WikipediaReader()
reader.crawl_pages("Machine Learning")

print(reader.list())
# Downloading The Register ...
# Not updating Bank ...
# Not updating Boosting (machine learning) ...
# Not updating Ian Goodfellow ...
# Downloading Statistical model ...
# Not updating Self-driving car ...
# Not updating Behaviorism ...
# Not updating Statistical classification ...
# Downloading Search algorithm ...
# Downloading Support vector machine ...
# Not updating Deep learning speech synthesis ...
# Not updating Expert system ...</code></pre> 
<h2 id="fa91" style="background-color:transparent;margin-left:0px"><span style="color:#242424"><span style="background-color:#ffffff">六、（第 2 部分）维基百科语料库</span></span></h2> 
<p id="0d21" style="margin-left:-.46em"><span style="color:#242424"><span style="background-color:#ffffff">        所有文章均以文本文件形式下载到<code>article</code>文件夹中。为了提供所有这些单独文件的抽象，NLTK 库提供了不同的语料库阅读器对象。该对象不仅提供对单个文件的快速访问，还可以生成统计信息，例如词汇量、单个标记的总数或单词量最多的文档。</span></span></p> 
<p id="8c9d" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        让我们使用该类<code>PlaintextCorpusReader</code>作为起点，然后初始化它，使其指向文章：</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">import nltk
from  nltk.corpus.reader.plaintext import PlaintextCorpusReader
from time import time

class WikipediaCorpus(PlaintextCorpusReader):
    pass

corpus = WikipediaCorpus('articles', r'[^.ipynb].*', cat_pattern=r'[.*]')
print(corpus.fileids())
# ['2001_A_Space_Odyssey.txt',
#  '2001_A_Space_Odyssey_film.txt',
#  '2001_A_Space_Odyssey_novel.txt',
#  '3D_optical_data_storage.txt',
#  'A*_search_algorithm.txt',
#  'A.I._Artificial_Intelligence.txt',
#  'AAAI_Conference_on_Artificial_Intelligence.txt',
#  'ACM_Computing_Classification_System.txt',</code></pre> 
<p id="792f" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        好的，这已经足够好了。让我们用两种方法来扩展它来计算词汇量和最大单词数。对于词汇，我们将使用 NLTK 辅助类<code>FreqDist</code>，它是一个包含所有单词出现的字典对象，此方法使用简单辅助类消耗所有文本<code>corpus.words()</code>，从中删除非文本和非数字。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">def vocab(self):
    return nltk.FreqDist(re.sub('[^A-Za-z0-9,;.]+', ' ', word).lower() for word in corpus.words())</code></pre> 
<p id="ea32" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        为了得到最大单词数，我们遍历所有带有 的文档<code>fileids()</code>，然后确定 的长度<code>words(doc)</code>，并记录最高值</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">def max_words(self):
    max = 0
    for doc in self.fileids():
        l = len(self.words(doc))
        max = l if l &gt; max else max
    return max</code></pre> 
<p id="2795" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        最后，我们添加一个<code>describe</code>生成统计信息的方法（这个想法也源于上面提到的《<a class="af mq" href="https://www.goodreads.com/book/show/32758032-applied-text-analysis-with-python" title="Applied Text Analysis with Python》">Applied Text Analysis with Python》</a>一书）。</span></span></p> 
<p id="ee86" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        该方法启动一个计时器来记录校园处理持续了多长时间，然后使用语料库阅读器对象的内置方法和刚刚创建的方法来计算文件数、段落数、句子数、单词数、词汇量和文档中的最大字数。</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">def describe(self, fileids=None, categories=None):
    started = time()

    return {
        'files': len(self.fileids()),
        'paras': len(self.paras()),
        'sents': len(self.sents()),
        'words': len(self.words()),
        'vocab': len(self.vocab()),
        'max_words': self.max_words(),
        'time': time()-started
        }
    pass</code></pre> 
<p id="0eac" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        这是最后一<code>WikipediaCorpus</code>堂课：</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">import nltk
from  nltk.corpus.reader.plaintext import PlaintextCorpusReader
from time import time

class WikipediaCorpus(PlaintextCorpusReader):
    def vocab(self):
        return nltk.FreqDist(re.sub('[^A-Za-z0-9,;.]+', ' ', word).lower() for word in corpus.words())
    
    def max_words(self):
        max = 0
        for doc in self.fileids():
            l = len(self.words(doc))
            max = l if l &gt; max else max
        return max
    
    def describe(self, fileids=None, categories=None):
        started = time()
        return {
            'files': len(self.fileids()),
            'paras': len(self.paras()),
            'sents': len(self.sents()),
            'words': len(self.words()),
            'vocab': len(self.vocab()),
            'max_words': self.max_words(),
            'time': time()-started
            }
        pass</code></pre> 
<p id="b9af" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">        在撰写本文时，爬取维基百科有关人工智能和机器学习的文章后，可以获得以下统计数据：</span></span></p> 
<pre class="has" style="margin-left:0"><code class="language-ba">corpus = WikipediaCorpus('articles', r'[^.ipynb].*', cat_pattern=r'[.*]')

corpus.describe()
{'files': 1163,
 'paras': 96049,
 'sents': 238961,
 'words': 4665118,
 'vocab': 92367,
 'max_words': 46528,
 'time': 32.60307598114014}</code></pre> 
<h2 id="988f" style="margin-left:0"><span style="color:#242424"><span style="background-color:#ffffff">七、结论</span></span></h2> 
<p id="c5ec" style="margin-left:-.46em"><span style="color:#242424"><span style="background-color:#ffffff">        本文是 NLP 项目在维基百科文章上下载、处理和应用机器学习算法的起点。本文涵盖了两个方面。首先，创建<code>WikipediaReader</code>通过名称查找文章的类，并可以提取其标题、内容、类别和提到的链接。爬虫由两个变量控制：爬取的文章总数和爬取的深度。其次，<code>WikipediaCorpus</code>NLTK 的扩展<code>PlaintextCorpusReader</code>。该对象可以方便地访问单个文件、句子和单词，以及总语料库数据，例如文件数量或词汇、唯一标记的数量。下一篇文章将继续构建文本处理管道。</span></span></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>