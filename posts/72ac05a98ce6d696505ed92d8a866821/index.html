<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【计算机视觉 | 目标检测 | 图像分割】arxiv 计算机视觉关于目标检测和图像分割的学术速递（7 月 17 日论文合集） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【计算机视觉 | 目标检测 | 图像分割】arxiv 计算机视觉关于目标检测和图像分割的学术速递（7 月 17 日论文合集）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p> 
<div class="toc"> 
 <h3>文章目录</h3> 
 <ul>
<li><a href="#5_1">一、检测相关(5篇)</a></li>
<li>
<ul>
<li><a href="#11_TALL_Thumbnail_Layout_for_Deepfake_Video_Detection_2">1.1 TALL: Thumbnail Layout for Deepfake Video Detection</a></li>
<li><a href="#12_Cloud_Detection_in_Multispectral_Satellite_Images_Using_Support_Vector_Machines_With_Quantum_Kernels_11">1.2 Cloud Detection in Multispectral Satellite Images Using Support Vector Machines With Quantum Kernels</a></li>
<li><a href="#13_Multimodal_Motion_Conditioned_Diffusion_Model_for_Skeletonbased_Video_Anomaly_Detection_19">1.3 Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection</a></li>
<li><a href="#14_Brain_Tumor_Detection_using_Convolutional_Neural_Networks_with_Skip_Connections_28">1.4 Brain Tumor Detection using Convolutional Neural Networks with Skip Connections</a></li>
<li><a href="#15_cOOpD_Reformulating_COPD_classification_on_chest_CT_scans_as_anomaly_detection_using_contrastive_representations_37">1.5 cOOpD: Reformulating COPD classification on chest CT scans as anomaly detection using contrastive representations</a></li>
</ul> 
  </li>
<li><a href="#5_46">二、分割|语义相关(5篇)</a></li>
<li>
<ul>
<li><a href="#21_SynTable_A_Synthetic_Data_Generation_Pipeline_for_Unseen_Object_Amodal_Instance_Segmentation_of_Cluttered_Tabletop_Scenes_47">2.1 SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes</a></li>
<li><a href="#22_FreeCOS_SelfSupervised_Learning_from_Fractals_and_Unlabeled_Images_for_Curvilinear_Object_Segmentation_56">2.2 FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation</a></li>
<li><a href="#23_Adaptive_Region_Selection_for_Active_Learning_in_Whole_Slide_Image_Semantic_Segmentation_65">2.3 Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation</a></li>
<li><a href="#24__AnyStar_Domain_randomized_universal_starconvex_3D_instance_segmentation_74">2.4 AnyStar: Domain randomized universal star-convex 3D instance segmentation</a></li>
<li><a href="#25_Frequency_Domain_Adversarial_Training_for_Robust_Volumetric_Medical_Segmentation_83">2.5 Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation</a></li>
</ul> 
 </li>
</ul> 
</div> 
<p></p> 
<h1>
<a id="5_1"></a>一、检测相关(5篇)</h1> 
<h2>
<a id="11_TALL_Thumbnail_Layout_for_Deepfake_Video_Detection_2"></a>1.1 TALL: Thumbnail Layout for Deepfake Video Detection</h2> 
<p>Tall：用于深度假冒视频检测的缩略图布局</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07494</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/f5/b8/a6f3K5dv_o.png" alt="在这里插入图片描述"><br> deepfake对社会和网络安全的威胁日益严重，引起了公众的极大关注，人们越来越多地致力于deepfake视频检测这一关键话题。现有的视频方法实现了良好的性能，但计算密集型。本文介绍了一种简单而有效的策略–缩略图布局（TALL），该策略将视频片段转换为预定义的布局，以实现空间和时间依赖性的保留。具体地，连续帧在每个帧中的固定位置被掩蔽以改善泛化，然后调整大小为子图像并重新布置为预定义的布局作为缩略图。TALL是模型无关的，而且非常简单，只需修改几行代码即可。受Vision Transformers成功的启发，我们将TALL整合到Swin Transformer中，形成了一种高效的方法TALL-Swin。在数据集内和跨数据集上的大量实验验证了TALL和SOTA TALL-Swin的有效性和优越性。TALL-Swin在具有挑战性的跨数据集任务FaceForensics++ <span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         → 
        
       
      
        to 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.3669em"></span><span class="mrel">→</span></span></span></span></span> Celeb-DF上实现了90.79<span class="katex--inline"><span class="katex"><span class="katex-mathml"> 
     
      
       
       
         % 
        
       
      
        % 
       
      
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8056em;vertical-align: -0.0556em"></span><span class="mord">%</span></span></span></span></span> AUC。该代码可在https://github.com/rainy-xu/TALL4Deepfake获得。</p> 
<h2>
<a id="12_Cloud_Detection_in_Multispectral_Satellite_Images_Using_Support_Vector_Machines_With_Quantum_Kernels_11"></a>1.2 Cloud Detection in Multispectral Satellite Images Using Support Vector Machines With Quantum Kernels</h2> 
<p>基于量子核支持向量机的多光谱卫星云层检测</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07281</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/bb/d7/8BGAgbGm_o.png" alt="在这里插入图片描述"><br> 支持向量机（SVM）是一个成熟的分类器，有效地部署在一系列模式识别和分类任务。在这项工作中，我们考虑扩展经典的支持向量机与量子内核，并将其应用到卫星数据分析。提出了一种量子核支持向量机（混合支持向量机）的设计与实现。它包括量子核估计（QKE）程序与经典的SVM训练例程相结合。像素数据被映射到希尔伯特空间使用ZZ-特征映射作用于参数化的假设状态。优化参数以最大化内核目标对齐。我们探讨了卫星图像数据云检测问题，这是地面和星载卫星图像分析处理链中的关键步骤之一。在基准Landsat-8多光谱数据集进行的实验表明，模拟的混合SVM成功地分类卫星图像的准确性与经典的支持向量机。</p> 
<h2>
<a id="13_Multimodal_Motion_Conditioned_Diffusion_Model_for_Skeletonbased_Video_Anomaly_Detection_19"></a>1.3 Multimodal Motion Conditioned Diffusion Model for Skeleton-based Video Anomaly Detection</h2> 
<p>基于骨架的视频异常检测多模运动条件扩散模型</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07205</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/4e/70/KnLE9J0d_o.png" alt="在这里插入图片描述"><br> 异常是罕见的，因此异常检测通常被框定为一类分类（OCC），即只接受过正常生活训练领先的OCC技术将正常运动的潜在表示限制在有限的体积内，并将外部的任何异常检测为异常，这令人满意地解释了异常的开放性。但是常态具有相同的开集性质，因为人类可以用几种方式执行相同的动作，这是领先的技术所忽视的。我们提出了一种新的生成模型的视频异常检测（VAD），它假设正常和异常是多模态的。我们认为骨架表示和利用国家的最先进的扩散概率模型，以产生多模态未来的人类构成。我们贡献了一个新的空调上的人过去的运动，并利用改进的模式覆盖能力的扩散过程中产生不同的，但似乎合理的未来运动。在对未来模式进行统计聚合时，当所生成的运动集合与实际未来不相关时，检测到异常。我们在4个已建立的基准上验证我们的模型：UBnormal、HR-UBnormal、HR-STC和HR-Avenue，广泛的实验超越了最先进的结果。</p> 
<h2>
<a id="14_Brain_Tumor_Detection_using_Convolutional_Neural_Networks_with_Skip_Connections_28"></a>1.4 Brain Tumor Detection using Convolutional Neural Networks with Skip Connections</h2> 
<p>基于带跳连接的卷积神经网络的脑肿瘤检测</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07503</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/2c/ca/HW25oMP4_o.png" alt="在这里插入图片描述"><br> 在本文中，我们提出了不同的卷积神经网络（CNN）的架构，使用磁共振成像（MRI）技术分析和分类的良性和恶性类型的脑肿瘤。应用不同的CNN架构优化技术，例如网络的加宽和加深以及添加跳过连接，以提高网络的准确性。结果表明，这些技术的子集可以明智地用于优于用于相同目的的基线CNN模型。</p> 
<h2>
<a id="15_cOOpD_Reformulating_COPD_classification_on_chest_CT_scans_as_anomaly_detection_using_contrastive_representations_37"></a>1.5 cOOpD: Reformulating COPD classification on chest CT scans as anomaly detection using contrastive representations</h2> 
<p>COOpD：重新制定胸部CT扫描的COPD分类作为使用对比表示法的异常检测</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07254</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/ac/c4/5sEFcKxM_o.png" alt="在这里插入图片描述"><br> 异质性疾病的分类是具有挑战性的，由于其复杂性，多变的症状和影像学表现。慢性阻塞性肺疾病（COPD）就是一个很好的例子，尽管是第三大死亡原因，但仍被诊断不足。其稀疏，弥漫和异构的计算机断层扫描的外观挑战监督二进制分类。我们将COPD二元分类重新表述为异常检测任务，提出cOOpD：异质病理区域被检测为来自正常同质肺区域的分布外（OOD）。为此，我们采用自监督对比借口模型学习未标记肺区域的表示，可能捕获患病和健康未标记区域的特定特征。生成模型然后学习健康表示的分布，并将异常（源于COPD）识别为偏差。通过汇总区域OOD评分获得患者水平评分。我们表明，cOOpD在两个公共数据集上实现了最佳性能，与以前的监督最先进的技术相比，AUROC增加了8.2%和7.7%。此外，cOOpD产生可解释的空间异常图和患者水平的分数，我们证明这在识别进展早期的个体中具有额外的价值。在人工设计的真实世界患病率设置中的实验进一步支持异常检测是解决coro分类的有力方式。</p> 
<h1>
<a id="5_46"></a>二、分割|语义相关(5篇)</h1> 
<h2>
<a id="21_SynTable_A_Synthetic_Data_Generation_Pipeline_for_Unseen_Object_Amodal_Instance_Segmentation_of_Cluttered_Tabletop_Scenes_47"></a>2.1 SynTable: A Synthetic Data Generation Pipeline for Unseen Object Amodal Instance Segmentation of Cluttered Tabletop Scenes</h2> 
<p>SynTable：一种用于杂乱桌面场景不可见对象非模态实例分割的合成数据生成流水线</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07333</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/b4/78/Vtes4aNP_o.png" alt="在这里插入图片描述"><br> 在这项工作中，我们提出了SynTable，一个统一和灵活的Python数据集生成器，使用NVIDIA的Isaac Sim Replicator Composer构建，用于生成高质量的合成数据集，用于看不见的对象amodal实例分割杂乱桌面场景。我们的数据集生成工具可以渲染包含对象网格、材质、纹理、光照和背景的复杂3D场景。元数据，如模态和模态实例分割掩模，遮挡掩模，深度图，边界框和材料属性，可以生成，以自动注释根据用户的要求的场景。我们的工具消除了在数据集生成过程中手动标记的需要，同时确保数据集的质量和准确性。在这项工作中，我们讨论了我们的设计目标，框架体系结构，和我们的工具的性能。我们演示了使用光线跟踪使用SynTable生成的样本数据集来训练最先进的模型UOAIS-Net。结果表明，显着改善的性能，在模拟到真实的传输时，OSD-Amodal数据集进行评估。我们提供这个工具作为一个开源的，易于使用的，逼真的数据集生成器，用于推进深度学习和合成数据生成的研究。</p> 
<h2>
<a id="22_FreeCOS_SelfSupervised_Learning_from_Fractals_and_Unlabeled_Images_for_Curvilinear_Object_Segmentation_56"></a>2.2 FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation</h2> 
<p>基于自监督学习的曲线目标分割算法</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07245</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/8d/15/uHAsSywV_o.png" alt="在这里插入图片描述"><br> 曲线对象分割对于许多应用是关键的。然而，手动注释曲线对象是非常耗时和容易出错的，产生现有的监督方法和域自适应方法的可用注释数据集不足。本文提出了一种自监督曲线对象分割方法，该方法从分形和未标记图像（FreeCOS）中学习鲁棒性和独特性。主要贡献包括一个新的分形FDA合成（FFS）模块和几何信息对齐（GIA）的方法。FFS基于参数分形L系统生成曲线结构，并将生成的结构集成到未标记的图像中，以通过傅立叶域自适应获得合成训练图像。GIA通过比较给定像素的强度顺序与其附近邻居的值来减少合成图像和未标记图像之间的强度差异。这样的图像对准可以明确地去除对绝对强度值的依赖性，并且增强在合成图像和真实图像两者中共同的固有几何特性。此外，GIA通过预测空间自适应损失（PSAL）和曲线掩模对比损失（CMCL）对齐合成图像和真实图像的特征。在四个公共数据集上的广泛实验结果，即，XCAD，DRIVE，STARE和CrackTree表明，我们的方法优于最先进的无监督方法，自监督方法和传统方法的大幅度提高。该工作的源代码可在https://github.com/TY-Shi/FreeCOS上获得。</p> 
<h2>
<a id="23_Adaptive_Region_Selection_for_Active_Learning_in_Whole_Slide_Image_Semantic_Segmentation_65"></a>2.3 Adaptive Region Selection for Active Learning in Whole Slide Image Semantic Segmentation</h2> 
<p>自适应区域选择在整体幻灯片图像语义分割中的主动学习</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07168</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/90/5d/UzFXgC9u_o.png" alt="在这里插入图片描述"><br> 为了训练监督分割模型而在像素级注释组织学千兆像素大小的全载玻片图像（WSIs）的过程是耗时的。基于区域的主动学习（AL）涉及在有限数量的注释图像区域上训练模型，而不是请求整个图像的注释。这些注释区域被迭代地选择，其目标是在最小化注释区域的同时优化模型性能。区域选择的标准方法评估指定大小的所有正方形区域的信息量，然后选择特定数量的信息量最大的区域。我们发现该方法的效率高度依赖于AL步长的选择（即，区域大小和每个WSI的所选区域的数量的组合），以及次优的AL步长可能导致冗余的注释请求或膨胀的计算成本。本文介绍了一种新的技术，用于自适应地选择注释区域，减轻对这个AL超参数的依赖。具体来说，我们动态地确定每个区域，首先确定一个信息区域，然后检测其最佳的边界框，而不是选择一个统一的预定义的形状和大小的区域，在标准方法。我们使用公共CAMELYON16数据集上的乳腺癌转移分割任务来评估我们的方法，并表明它在各种AL步长中始终实现比标准方法更高的采样效率。只有2.6%的组织区域注释，我们实现了完整的注释性能，从而大大降低了注释WSI数据集的成本。源代码可在https://github.com/DeepMicroscopy/AdaptiveRegionSelection获得。</p> 
<h2>
<a id="24__AnyStar_Domain_randomized_universal_starconvex_3D_instance_segmentation_74"></a>2.4 AnyStar: Domain randomized universal star-convex 3D instance segmentation</h2> 
<p>AnyStar：域随机化通用星凸3D实例分割</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07044</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/53/bb/cglVRV4I_o.png" alt="在这里插入图片描述"><br> 星凸形状以细胞核、结节、转移瘤和其他单位的形式出现在生物显微镜和放射学中。用于这种结构的现有实例分割网络在每个数据集的密集标记的实例上训练，这需要大量且通常不切实际的手动注释工作。此外，当由于对比度、形状、取向、分辨率和密度的变化而呈现新的数据集和成像模态时，需要显著的重新设计或微调。我们提出了AnyStar，这是一个域随机生成模型，它模拟具有随机外观，环境和成像物理的斑点状对象的合成训练数据，以训练通用的星凸实例分割网络。因此，使用我们的生成模型训练的网络不需要来自看不见的数据集的注释图像。在我们的合成数据上训练的单个网络准确地3D分段C。elegans和P.荧光显微镜中的dumerilii核、微CT中的小鼠皮质核、EM中的斑马鱼脑核和人胎儿MRI中的胎盘子叶，所有这些都没有任何再训练、微调、迁移学习或域适应。代码可在https://github.com/neel-dey/AnyStar获得。</p> 
<h2>
<a id="25_Frequency_Domain_Adversarial_Training_for_Robust_Volumetric_Medical_Segmentation_83"></a>2.5 Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation</h2> 
<p>用于健壮体积医学分割的频域对抗性训练</p> 
<pre><code class="prism language-python">https<span class="token punctuation">:</span><span class="token operator">//</span>arxiv<span class="token punctuation">.</span>org<span class="token operator">/</span><span class="token builtin">abs</span><span class="token operator">/</span><span class="token number">2307.07269</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/43/3e/OpnOc2nA_o.png" alt="在这里插入图片描述"><br> 确保深度学习模型在医疗保健等关键应用中的鲁棒性至关重要。虽然深度学习的最新进展提高了体积医学图像分割模型的性能，但由于这些模型容易受到对抗性攻击，因此无法立即部署到现实世界的应用中。我们提出了一个三维频域对抗攻击的体积医学图像分割模型，并证明其优势，传统的输入或体素域攻击。使用我们提出的攻击，我们引入了一种新的频域对抗训练方法，用于优化针对体素和频域攻击的鲁棒模型。此外，我们提出了频率一致性损失来调节我们的频域对抗训练，从而在模型对干净样本和对抗样本的性能之间实现更好的权衡。代码可在https://github.com/asif-hanif/vafa公开获得。</p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>