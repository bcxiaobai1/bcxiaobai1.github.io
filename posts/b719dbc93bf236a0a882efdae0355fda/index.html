<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【视觉SLAM】Visual-Based Semantic SLAM with Landmarks for Large-Scale Outdoor Environment - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【视觉SLAM】Visual-Based Semantic SLAM with Landmarks for Large-Scale Outdoor Environment</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <table>
<thead><tr>
<th>Z. Zhao, Y. Mao, Y. Ding, P. Ren and N. Zheng, “Visual-Based Semantic SLAM with Landmarks for Large-Scale Outdoor Environment,” 2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI), 2019, pp. 149-154, doi: 10.1109/CCHI.2019.8901910.</th>
<th></th>
</tr></thead>
<tbody><tr>
<td>Plain Text引用格式</td>
<td></td>
</tr></tbody>
</table>
<h1>
<a id="_6"></a>?摘要</h1> 
<blockquote> 
 <p><strong>语义SLAM</strong>是自动驾驶和智能代理中的重要领域，它可以使机器人实现高级导航任务，获得简单的认知或推理能力，实现<strong>基于语言的人机交互</strong>。<br> 在本文中，我们构建了一个系统，通过将<strong>ORB SLAM</strong> [1]中的3D点云与来自卷积神经网络模型<strong>PSPNet-101</strong>[3]的语义分割信息相结合，创建语义3D地图，用于大规模环境。此外，还建立了一个新的KITTI[4]序列数据集，其中包含GPS信息和来自序列相关街道的谷歌地图地标的标签。此外，我们找到了一种将现实世界地标与点云地图相关联的方法，<strong>并构建了基于语义地图的拓扑图</strong>。</p> 
</blockquote> 
<h1>
<a id="_11"></a>?一、介绍</h1> 
<p><strong>语义3D环境在多个领域越来越重要，特别是在机器人领域。</strong> 如今，3D建图方法只包含周围环境的里程或几何信息，没有语义意义，无法使机器人为特定任务推断出更多信息，使人机交互变得困难。<strong>具有语义信息的地图允许机器人完全了解其环境，并像人类一样推广其导航能力，并实现更高级别的任务。</strong> 语义信息还将使机器人获得简单的认知或推理能力。语义信息中的机器人感知也使机器人能够实现基于语言的人机交互任务。</p> 
<p><strong>语义同步定位和建图（SLAM）系统主要涉及3D建图和语义分割。</strong> 最近，对语义SLAM的研究主要集中在室内环境或基于激光雷达的SLAM系统。基于视觉的语义SLAM主要是通过使用RGB深度（RGB-D）相机实现的，该摄像头会受到照明条件的极大影响，并且不适合户外环境。<code>激光雷达更适合这样的环境，但它比基于摄像头的SLAM系统成本高得多。而且激光雷达包含的信息比视觉信息少，这使得基于摄像头的语义SLAM系统中的研究更有意义。</code></p> 
<p>我们受到人类视觉导航系统的启发。<code>人类导航系统在很大程度上依赖于视觉感知，因为视觉图像包含大量信息，例如里程学，几何结构和语义含义。我们从一个地方到另一个地方的导航主要基于地标级的语义意义，视觉特征及其拓扑关系。</code>在我们的系统中，我们使用基于单目视觉 SLAM 系统-ORB SLAM2 [1] 的功能。该系统通过使用定向快速和旋转BRIEF（ORB）功能[5]来执行，该功能在运动条件下具有良好的鲁棒性和良好的实时性能。它可用于户外环境的多个场景，在闭环方面具有出色的性能。我们使用 ORB-SLAM 提取视觉特征以进行重新本地化。语义信息由深度神经网络 （DNN） 获取。我们使用PSPNet-101模型[3]进行像素级图像语义分割，具有19种不同的语义标签，包括车辆，建筑物，植被，人行道和道路。然后，<strong>语义信息在像素级别与点云地图相关联。通过语义意义，我们将建筑物地标与语义点云相关联。</strong> 我们将从Google地图获得的地标与我们的语义3D地图相关联，用于城市地区导航。它可以在没有GPS信息的情况下实现基于地标的重新定位。</p> 
<p>本文的贡献总结如下：</p> 
<ul>
<li> <p>我们开发了一个系统，通过将视觉SLAM地图与大型环境的语义分割信息融合在一起来构建语义3D地图。</p> </li>
<li> <p>我们为KITTI [4]序列开发了一个新的数据集，其中包含GPS信息和来自序列相关街道的谷歌地图地标的标签。</p> </li>
<li> <p>我们开发了一种将现实世界地标与点云地图相关联的方法，并构建了基于语义地图的拓扑地图。</p> </li>
</ul> 
<p>本文组织如下：第2节介绍语义分割、SLAM和语义SLAM中的相关工作。第3节描述了我们提出的方法的细节。第4节描述了KITTI数据集中的实验，并分析了我们的实验结果。最后，结论在第5节中得出。</p> 
<h1>
<a id="_28"></a>?二、相关工作</h1> 
<p>语义SLAM的目标是构建语义上有意义的地图，其中语义含义通过组合几何和语义信息附加到实体。SLAM是作为重建未知环境的3D地图的方法实现的，并且语义分割用于提取语义特征。</p> 
<p>SLAM系统依赖于不同种类的传感器提供的输入，用于几何3D地图以及同时估计位置和方向。根据用于定位的传感器，它们主要可分为三类，即基于激光雷达的SLAM，直接提供的测距方法和视觉SLAM。最近的许多研究都集中在仅使用视觉信息上，这被特别称为视觉SLAM。<code>这种方法在计算机视觉、机器人和AR领域被广泛采用[6]。戴维森等人[7]在2007年提出了第一个单目视觉SLAM系统，名为MonoSLAM，它只使用单目相机来估计3D轨迹。</code>为了解决单晶结构中的计算成本问题，PTAM [8]于2015年提出。Mur-Artal和Tards提出了<strong>ORB-SLAM</strong> [1]，[2]，这是具有完全传感器支持和最佳性能的视觉SLAM系统之一，在并行跟踪，建图和环路闭合检测中应用ORB功能，并使用基于姿态图优化和BA[9]的优化。与上面提到的基于特征的方法不同，另一种视觉SLAM系统直接使用图像作为输入，而无需使用描述符或手工制作的特征检测器进行任何抽象，称为直接方法[10]。DTAM [11]，其中通过将输入图像与重建地图生成的合成视图图像相关联来实现跟踪，而LSD-SLAM [12]遵循半密度VO [13]的思想，是直接方法的主要策略。DSO [14]将最小光度误差模型与模型参数的联合优化方法相结合。本文主要基于ORB-SLAM提出的模型。</p> 
<p>语义分割是计算机视觉中另一项具有挑战性的任务。在强大的深度神经网络[15]-[18]的发展的推动下，语义分割取得了巨大的进步，<strong>灵感来自于用分类中的全连接层代替卷积层</strong>[19]。Farabet等人[20]采用多尺度卷积网络从图像金字塔中提取多尺度特征（输入图像的拉普拉斯金字塔版本）。Couprie等人[21]采用了类似的方法来学习具有图像深度信息的多尺度特征。在[22]中，生成了用于对象解析的多尺度补丁，以同时实现每个补丁的分割和分类，并聚合它们以推断对象。随着基于特征的增强方法[23]的开发，该方法提取了多尺度的特征，Zhao等人[3]提出了金字塔场景解析网络（PSPNet）用于语义分割，允许多尺度特征融合。它将特征地图与并行池化层的上采样输出连接起来，并涉及具有不同金字塔比例的信息，在不同子区域之间有所不同。该方法实现了用于最先进的语义分割和场景解析的实用系统，包括所有关键的实现细节。</p> 
<p>语义建图提供了空间的抽象和人类机器人交互的手段。根据[24]，我们的研究可以分为户外解释。为了应对室外环境中语义建图的挑战，提出了多pIe方法。[25]中提出的方法是利用立体视觉和图像分类的早期工作，<code>使用SVM将可遍历和不可遍历的场景分开</code>。此外，[26]中描述的算法生成了具有相关语义标签的高效且准确的密集3D重建。应用条件随机场（CRF）框架对立体图像进行操作，以估计标签并注释3D体积。[27]应用ORB-SLAM获得真实比例的3D可视化地图和CRF-RNN算法进行语义分割。在[28]中，<code>通过将最先进的深度学习算法和基于单目相机的半稠密SLAM相结合</code>，解决了这一挑战。2D 语义信息通过具有空间一致性的连接关键帧之间的对应关系传输到 3D 映射。然而，很少有关于将现实世界地标与语义3D地图相关联的工作，用于基于任务的导航和人机交互。</p> 
<h1>
<a id="_38"></a>?三、方法</h1> 
<h2>
<a id="A__40"></a>?A. 系统概述</h2> 
<p>我们的语义SLAM系统使用单目摄像头作为主要传感器，专注于大型城市地区。如流程图所示，我们的系统不仅可以使用ORB功能重建3D环境，还可以实现GPS数据融合，地图再利用以及实时重新定位和基于地标的定位。整个系统的流程图如图1所示。</p> 
<p>首先，<code>通过基于CNN的分割算法对图像进行分割。</code>像素级语义建图结果和当前帧将被发送到SLAM系统进行环境重建。几何环境由 ORB SLAM 重建，其中点云由当前帧中的拐角 ORB 特征生成。在SLAM系统中，像素级语义信息将使用贝叶斯更新规则与地图点相关联，这将更新帧中每个观测值的每个地图点的概率分布。然后，地标将被投影到SLAM地图中，并与保存在SLAM系统中的最近关键帧相关联。无需 GPS 信息，即可将地图重新用于地标级别的重新定位。<code>我们还提供了为每个地标构建拓扑可到达关系的方法，这将更方便机器人实现地标级的自主导航。</code></p> 
<h2>
<a id="B__45"></a>?B. 语义地图</h2> 
<p><strong>1） 语义分割</strong></p> 
<p>语义分割的目的是针对其语义标签正确分类每个像素。在这项工作中，我们选择PSPNet-101模型[3]进行图像分割，选择TensorRT进行实时推理加速。</p> 
<p><strong>2） ORB SLAM</strong></p> 
<p>3D重建由ORB SLAM [1]实现，这是一个开源的基于视觉特征的最先进的SLAM系统。ORB SLAM具有良好的实时性能，具有出色的闭环效果。我们使用欧氏SLAM进行3D重建和轨迹估计。在ORB SLAM系统中，有三个线程，即跟踪，局部建图和闭环检测，并行运行。</p> 
<p><img src="https://images2.imgbox.com/9c/5a/iO2iRO74_o.png" alt="在这里插入图片描述"><br> <strong>3） 实时数据融合</strong></p> 
<p>数据融合步骤是尝试将语义含义与SLAM系统中的每个地图点相关联。在此步骤中，我们尝试使用贝叶斯更新规则来更新每个地图点的语义标签的概率分布。</p> 
<p>首先，每个像素上超过19个标签的分数将被发送到SLAM系统。在ORB SLAM系统中，良好的特征点将被保存并转换到点云中。3D 点云坐标系和相机坐标系中的这些要素点之间将存在变换关系。3D点云系统与相机坐标的变换关系如下图所示：<br> <img src="https://images2.imgbox.com/16/2b/KAt5Yzgv_o.png" alt="在这里插入图片描述"><br> (xm,ym,zm)是地图点在 3D 地图坐标中的位置。Tpointcloud2camera是将点云的位置传递到相机坐标中的位置的参数矩阵。(uc,vc)是相机坐标中与地图点对应的相机像素(xm,ym,zm).将特征点投影到照相机坐标后，将给出每个特征点的 19 个标签的概率分布，如下所示：<br> <img src="https://images2.imgbox.com/1d/dc/5EwvgHOi_o.png" alt="在这里插入图片描述"><br> Fs是语义分割部分后当前帧中每个标签的概率分布，以及Lm(xm,ym,zm)表示地图点的标签(xm,ym,zm).此外，由于每个特征点都可以在不同的帧中观察到，因此数据融合方法应用于不同的观察。使用贝叶斯更新执行多观测数据融合，如下所示：<br> <img src="https://images2.imgbox.com/73/17/bT0cCjMS_o.png" alt="在这里插入图片描述"><br> Z是归一化常数，并且lmk表示地图点的标签m在框架k. p(lml|F1：kP1：k)分别表示从帧 1 到帧 k 的累积概率分布。在此等式中，概率分布是先前的分布更新的结果，具有新出现的帧和点云。每个特征点的概率分布保存在ORB SLAM系统中。每个地图点的最终标签是通过最大化概率来搜索的，如下面的等式所示。<br> <img src="https://images2.imgbox.com/38/08/pY2CPCOY_o.png" alt="在这里插入图片描述"></p> 
<p>m表示单个地图点，以及lm表示地图点的语义标签m.在实时融合过程中，每个映射点将包含一个语义标签和一个语义概率分布。</p> 
<h2>
<a id="C_GPS_70"></a>?C. GPS融合</h2> 
<p>为了在像素级将建筑地标与点云相关联，生成语义点云，我们需要将Google地图中使用的建筑地标的WGS84坐标转换为与点云相同的坐标系。但是，从谷歌地图API获得的WGS84中的经纬度不适合直接转换。因此，我们首先将坐标转换为笛卡尔坐标，其中单位是米。将关键帧的GPS信息转换为笛卡尔坐标后，我们采用了Besl和McKay[29]提出的方法，将坐标系与点云统一起来。每30帧，我们以当前帧为采样点，并将相应的姿态以及经度和纬度添加到两个全局采样器中。最后，我们使用SVD来计算全局采样器中这两个点集之间的最佳旋转。在这里，我们假设P一个作为笛卡尔坐标系中的点集，PB作为姿势坐标中的点集。centroid一个是 的质心P一个和centroidB是 的质心PB.由于两个坐标的比例不同，因此还需要进行比例变换。旋转矩阵R和翻译矩阵T计算公式为：<br> <img src="https://images2.imgbox.com/45/4c/tRRgrZQC_o.png" alt="在这里插入图片描述"></p> 
<p>哪里λ是比例乘数，因为不同坐标的比例可能不同，其计算公式为：<br> <img src="https://images2.imgbox.com/b3/b2/uKyTd2u7_o.png" alt="在这里插入图片描述"></p> 
<p>获得后R和T，笛卡尔坐标中的每个点（即建筑物地标的位置）都可以转换为点云的坐标系，如下所示：<br> <img src="https://images2.imgbox.com/1e/f5/keXNFSV1_o.png" alt="在这里插入图片描述"></p> 
<p>一个是笛卡尔坐标中的点，表示地标的位置，以及B是点云坐标系中的点。然后我们可以在点云图中找到相应的点，并将语义标签与它融合在一起。<br> <img src="https://images2.imgbox.com/45/0b/EDV1mWId_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="D_85"></a>?D、后处理</h2> 
<p>在实时过程之后，我们将执行后处理以优化结果并获取更结构化的语义信息。在此过程中，聚类方法将应用于对象级语义地图的不同语义标签中。地标也将融合在要素点中，可用于地标级定位和导航。</p> 
<p><strong>1） 地标级数据融合</strong></p> 
<p>我们假设地标和特征点之间的关系是模糊的隶属关系。在一个区域中，地标对人类导航很重要，因为人类导航的目标主要由地标分配。利用地标GPS信息和语义标签，我们可以将地标级数据与3D重建结果进行融合，这将更方便地解决面向任务的导航问题。我们将使用我们的开源代码发布这些数据集。</p> 
<p>我们使用基于模糊数学的方法进行里程碑式的数据融合。在这种方法中，我们将不关注地标位置的准确性，而是关注地标位置的成员资格分布。<code>由于根据人类的认知习惯，地标位置的概念实际上是一个模糊的概念。这允许机器人以人类的方式定义地标的位置。</code>我们尝试基于高斯概率分布来评估位置成员身份。如果该地点在物理上靠近地标，则对于高斯分布而言，该地点的成员资格将更高。成员资格定义如下：<br> <img src="https://images2.imgbox.com/78/8e/wkGi17ry_o.png" alt="在这里插入图片描述"></p> 
<p>其中 m（x， y） 表示位置的成员资格(x，y)。 G（x，y,xl,yl,σ)表示 2D 高斯概率密度函数 （PDF）。(xl,yl)表示地标位置，σ表示高斯分布的标准差。该分布将插入到语义图中，并与轨迹相关联，以便进行基于地标的实时定位。</p> 
<p><img src="https://images2.imgbox.com/05/65/5QhpazO7_o.png" alt="在这里插入图片描述"></p> 
<p><strong>2） 拓扑语义地图</strong></p> 
<pre><code class="prism language-java">语义 SLAM 还可以生成语义拓扑图，其中仅包含地标与其几何关系之间的可访问联系。
语义图中将只有边和节点，更适合全局路径规划。
</code></pre> 
<p>拓扑图是通过以下步骤构建的。首先，在SLAM系统中完成建图过程后，将保存相机的轨迹。地标将与其最近的关键帧相关联。其次，将保存两种关键帧，即与地标关联的关键帧和转弯处的关键帧。第三，如果该地点被访问多次，地图将被优化<code>。如果先前的节点表示相同的位置或地标，则它们将与新节点融合。</code>拓扑语义图如图 3 所示。</p> 
<h1>
<a id="_111"></a>✨四、实验</h1> 
<blockquote> 
 <p>我们主要基于KITTI数据集设计实验，该数据集向公众开放，主要在城市地区记录。根据KITTI原始数据中记录的GPS信息，我们通过谷歌地图记录地标GPS信息。数据集包含经度、纬度和地标的真实名称。我们记录序列 00 到 10 以进行评估和测试。它将很快向公众发布。此外，我们评估系统的实时性能的定量基准。实验是使用ROS和Keras设计的，我们的计算平台涉及英特尔酷睿i7 CPU和英伟达GeForce GTX 1080Ti GPU平台。</p> 
</blockquote> 
<h2>
<a id="A__115"></a>?A. 数据集</h2> 
<p>KITTI序列在城市地区有大量的户外环境。我们选择序列00到10来评估我们系统的整体质量。我们在系统中使用的数据主要是GPS信息和图像。我们使用来自右侧相机的RGB图像来模拟单目相机。我们不完全依赖GPS信息，因为我们只是每30帧使用GPS信息来模拟现实世界中较差的GPS设备。</p> 
<h2>
<a id="B__118"></a>?B. 实施细节</h2> 
<p>首先，我们的实验主要基于机器人操作系统（ROS），这是机器人多进程通信的框架。我们使用ROS节点来模拟相机ROS驱动器和GPS设备驱动器。对于所有实验，点云坐标和相机坐标之间的变换关系都是在ORB SLAM中估计的。在GPS融合和变换中，我们使用采样而不是所有GPS信息来减少相对误差并模拟较差的GPS信号。我们每 30 帧对 GPS 信息进行采样。语义分割在张量流和张量RT中实现。该模型在城市景观数据集中训练。</p> 
<h2>
<a id="C__121"></a>?C. 定性评价</h2> 
<p>为了评估语义SLAM系统，在KITTI数据集中使用了多个大规模的室外序列。我们系统的定性结果如图4所示。每个图都显示了整个地图的多个视图。地标分布和拓扑语义图也如图所示。结果表明，我们的系统能够成功地将语义标签融合到ORB SLAM生成的点云中，从而生成具有19个标签的语义3D点云。此外，标志性水平数据融合是预先形成的，并在不同的序列中获得了良好的拓扑关系。它将适用于基于地标的大型导航任务或人机交互。</p> 
<p><code>实验表明，语义信息将使机器人不仅能够更多地了解环境的无意义特征，还可以了解它们的语义含义。此外，基于语义意义，机器人将使用更强大的功能重新定位自己，例如建筑物，道路，人行道，墙壁上的特征，而不是车辆，树木，人等。</code>例如，我们选择序列 02、05 和 09。结果如图 4 所示。</p> 
<p><img src="https://images2.imgbox.com/9e/a8/rG3ySJCe_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="D__128"></a>?D. 时间分析</h2> 
<p>实验是使用ROS和Keras设计的，我们的计算平台涉及英特尔酷睿i7 CPU和英伟达GeForce GTX 1080Ti GPU平台。</p> 
<p>我们已经测试了它们协同工作时的系统运行时间。在我们的计算系统中，<code>整个系统可以在近1.8Hz的频率下运行</code>。由于我们使用的语义分割模型基于PSPNet-101，这是一个没有加速的大型CNN模型，因此，如果模型在FPGA或TensorRT中加速，我们可以达到更好的性能。我们系统的整体运行时性能如表 I 所示。</p> 
<p><img src="https://images2.imgbox.com/2b/49/OgZAmuJG_o.png" alt="在这里插入图片描述"></p> 
<h1>
<a id="_135"></a>?五、结论</h1> 
<p>本文针对大规模户外定位和导航，开发了一种基于单目摄像头的带地标语义SLAM的系统。现有工作只关注准确性或实时性能，这可能很难真正提高机器人的整体认知水平。我们基于KITTI GPS信息进行了数据集，用于基于地标的语义融合和拓扑语义地图。带有地标信息的3D语义点云是由我们的系统使用我们提到的数据集构建的。它包含地标的真实名称和位置，多个语义标签，这使得基于离线语言的人机交互，面向任务的导航或地标级本地化成为可能。<code>通过使用坐标系变换和贝叶斯更新，将3D地图与相关的语义信息融合在一起。</code>标志性数据融合是通过基于高斯分布的模糊隶属关系实现的，构建拓扑语义地图。</p> 
<p>我们的论文为未来的工作提供了几个引人注目的领域。我们计划改进视觉SLAM系统，使其适应各种照明条件下的定位和导航。此外，我们希望开发一种基于地标拓扑图和人机交互的机器人导航系统。<code>如何利用语义信息提高本地化性能，也是一个值得未来研究的有趣领域。</code></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>