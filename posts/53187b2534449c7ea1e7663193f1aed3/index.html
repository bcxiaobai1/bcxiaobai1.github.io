<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>字节跳动算法岗武功秘籍（上） - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">字节跳动算法岗武功秘籍（上）</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p><strong>45家大厂面经秘籍及参考答案：</strong></p> 
<p>已助力<strong>150+人</strong>进入国内大厂的《<strong>人工智能算法岗江湖武林秘籍</strong>》，<a href="https://www.jiangdabai.com/1653">点击查看</a></p> 
<p></p>
<div class="toc">
 <h3>目录</h3>
 <ul>
<li><a href="#1%09_8">1 字节跳动面经汇总资料</a></li>
<li>
<ul>
<li><a href="#11%09_11">1.1 面经汇总参考资料</a></li>
<li><a href="#12__26">1.2 面经涉及招聘岗位</a></li>
<li><a href="#13__36">1.3 面试流程时间安排</a></li>
<li><a href="#14__46">1.4 字节跳动面试心得汇总</a></li>
</ul>
  </li>
<li><a href="#2__105">2 字节跳动面经涉及基础知识点</a></li>
<li>
<ul>
<li><a href="#21__109">2.1 图像处理基础</a></li>
<li>
<ul>
<li><a href="#211__110">2.1.1 讲解相关原理</a></li>
<li><a href="#212__125">2.1.2 手写算法代码</a></li>
</ul>
   </li>
<li><a href="#22_CNN_133">2.2 深度学习：CNN卷积神经网络方面</a></li>
<li>
<ul>
<li><a href="#221__134">2.2.1 讲解相关原理</a></li>
<li><a href="#2211__135">2.2.1.1 卷积方面</a></li>
<li>
<ul>
<li><a href="#2212__156">2.2.1.2 池化方面</a></li>
<li><a href="#2213__164">2.2.1.3 网络结构方面</a></li>
<li><a href="#2214__188">2.2.1.4 其他方面</a></li>
</ul>
    </li>
<li><a href="#222__209">2.2.2 数学计算</a></li>
<li><a href="#223__220">2.2.3 公式推导</a></li>
<li><a href="#224__225">2.2.4 手写算法代码</a></li>
<li><a href="#225__239">2.2.5 激活函数类</a></li>
</ul>
   </li>
<li><a href="#23_RNN_249">2.3 深度学习：RNN递归神经网络方面</a></li>
<li>
<ul>
<li><a href="#231___250">2.3.1 讲解相关原理</a></li>
<li><a href="#232__272">2.3.2 手绘网络原理</a></li>
</ul>
   </li>
<li><a href="#24_CNNRNN_276">2.4 深度学习：CNN&amp;RNN通用知识点</a></li>
<li>
<ul>
<li><a href="#241__277">2.4.1 基础知识点</a></li>
<li><a href="#242__300">2.4.2 模型评价</a></li>
</ul>
   </li>
<li><a href="#25__318">2.5 传统机器学习方面</a></li>
<li>
<ul>
<li><a href="#251__319">2.5.1 讲解相关原理</a></li>
<li>
<ul>
<li><a href="#2511__320">2.5.1.1 数据准备</a></li>
<li><a href="#2512__324">2.5.1.2 特征工程</a></li>
<li><a href="#2513__348">2.5.1.3 有监督学习-分类和回归方面</a></li>
<li><a href="#2514__479">2.5.1.4 无监督学习-聚类方面</a></li>
<li><a href="#2515__489">2.5.1.5 模型评价</a></li>
</ul>
    </li>
<li><a href="#252__501">2.5.2 手推算法及代码</a></li>
<li>
<ul>
<li><a href="#2521__502">2.5.2.1 手推公式</a></li>
<li><a href="#2522__525">2.5.2.2 手写代码</a></li>
</ul>
   </li>
</ul>
   </li>
<li><a href="#26__529">2.6 深度学习&amp;机器学习面经通用知识点</a></li>
<li>
<ul>
<li><a href="#261__530">2.6.1 损失函数方面</a></li>
<li><a href="#262__545">2.6.2 激活函数方面</a></li>
<li><a href="#263__555">2.6.3 网络优化梯度下降方面</a></li>
<li><a href="#264__580">2.6.4 正则化方面</a></li>
<li><a href="#265__594">2.6.5 压缩&amp;剪枝&amp;量化&amp;加速</a></li>
<li><a href="#266__600">2.6.6 过拟合&amp;欠拟合方面</a></li>
<li><a href="#267__610">2.6.7 其他方面</a></li>
</ul>
  </li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<hr> 
<h1>
<a id="1%09_8"></a>1 字节跳动面经汇总资料</h1> 
<p><img src="https://images2.imgbox.com/01/c9/wgtwoenc_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="11%09_11"></a>1.1 面经汇总参考资料</h2> 
<p>① 参考资料：</p> 
<p>（1）牛客网：字节跳动面经-340篇，<a href="https://www.nowcoder.com/discuss/experience?tagId=645&amp;order=3&amp;companyId=665&amp;phaseId=0">网页链接</a></p> 
<p>（2）知乎面经：<a href="https://www.zhihu.com/search?type=content&amp;q=%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F">点击进入查看</a></p> 
<p>（3）面试圈：<a href="http://www.mianshigee.com/interview/t17">点击进入查看</a></p> 
<p>② 面经框架&amp;答案&amp;目录&amp;心得：</p> 
<p>（1）面经框架及参考答案：<a href="https://docs.qq.com/doc/DWGZGc0ZVYmZCUFNO">点击进入查看</a></p> 
<p>（2）大厂目录及整理心得：<a href="https://docs.qq.com/doc/DWGZGc0ZVYmZCUFNO">点击进入查看</a></p> 
<h2>
<a id="12__26"></a>1.2 面经涉及招聘岗位</h2> 
<p><strong>（1）实习岗位类</strong></p> 
<p>【图像与多媒体算法实习】、【Data搜索部（数据挖掘）实习】、【三维视觉实习】、【自然语言处理实习】、【数据挖掘/搜索/推荐实习】、【效率工程算法实习】、【广告算法实习】、【AI Lab机器学习实习生】、【商业变现部门推荐算法】、【编解码算法工程师实习】</p> 
<p><strong>（2）全职岗位类</strong></p> 
<p>【AI Lab计算机视觉与深度学习岗】、【抖音互娱图形图像算法工程师】、【搜索团队算法工程师】、【研发算法工程师】、【视频基础架构组】，【搜索部门算法工程师】、【广告算法工程师】、【企业应用算法工程师】、【抖音算法工程师图像增强方向】、【飞书算法工程师】、【Data部门算法工程师】、【推荐算法工程师】、【抖音算法工程师】、【NLP算法工程师】、【自然语言处理算法工程师】、【机器学习中台算法工程师】、【多媒体视频算法工程师】、【Data推荐算法工程师】、【字节教育工程师】、【电商NLP算法工程师】、【大数据开发工程师】、【教育部门算法工程师】</p> 
<h2>
<a id="13__36"></a>1.3 面试流程时间安排</h2> 
<p><img src="https://images2.imgbox.com/37/64/AwaR0CxS_o.png" alt="在这里插入图片描述"></p> 
<p>PS：以上流程为大白总结归纳所得，以供参考。</p> 
<p>其他注意点：</p> 
<p>● 在面试的流程中，需要注意的是，有的人是三轮技术面试，有的人是两轮技术面试。</p> 
<h2>
<a id="14__46"></a>1.4 字节跳动面试心得汇总</h2> 
<p>字节跳动的面经超级多，说明机会和发展都是很不错的。以下是大白整理的，几百篇面经中的面试者的心得感悟，将此提炼出来，便于大家体会：</p> 
<p>★ 一面二面的面试官比较偏理论基础，三面的大佬比较偏业务，不过总体来说，头条的面试官总的来说都挺好的，编程题检查时也会指错并引导。</p> 
<p>★ 阿里更注重底层基础和深度，源码级别，头条更注重算法，手撕代码。常常自我介绍完啥也不说基本都先来一道编程题，然后发问，问到最后，再以一个编程题收尾。</p> 
<p>★ 面试时对简历上自己实习或者项目的细节要很清楚，会问的很深。（比如为什么用欧式距离算样本之间的距离）而且涉及的面也会比较广，比如做图像/视频增强的，应该对于超分，去模糊，去噪，去雾，HDR甚至图像translation等问题都应该有一个比较深的了解，希望各位还不急着找工作的同学们能坚持努力。</p> 
<p>★ 注重原理的理解而不是方法看起来有多fancy，原理至上，所以一定要理解透彻。</p> 
<p>★ 一定要回忆所有细节，并站着面试官的角度思考他会问什么问题。项目中你做工作时的流程一定要清楚，比如业界一般是怎样解决该问题的，你是怎样做的，你遇到了什么困难，你如何解决这些困难的。如果有的问题不会，或是只有个模糊的答案，直接说出来或这说不会就行了，没有事的。</p> 
<p>★ 划重点！！！项目一定要挑自己熟悉的说，简历上放一些和岗位相关的项目。</p> 
<p>★ 心态部分，战略上藐视，战术上重视。</p> 
<p>★ 面试官似乎很看重工程能力 而我的经历都很Research 所以除了表达自己算法能力的同时，把工程上的东西说一说也是很加分的，哪怕是脏活累活。</p> 
<p>★ 大家面试的时候放松心态，做足准备就好，谋事在人，成事在天，不必太过紧张。如果面试中遇到思路卡壳可以一点一点解释，不用着急。</p> 
<p>★ 算法工程师的自我修养：数据建模，C/C++，思考、解决问题的方向和逻辑（建立在足够的理论基础和实践基础上）</p> 
<p>★ 多运用到工程，想一想工程方向的优化。在有算法的基础做一做开发对于自己的成长有帮助。</p> 
<p>★ 项目中每一个创新点一定要清楚：为什么用、怎么用的、好处在哪里。</p> 
<p>★ 一些基本的概念一定要熟悉，不能只是知道。比如ROC曲线和PR曲线，面试官的<br> 要求不仅仅是横轴纵轴是什么，往往会有进一步的follow up：比如说样本分布不平衡的话PR和ROC会有差别类似的；不能只关注于某一个概念是什么，往往最基础的follow up是考察的重点。</p> 
<p>★ 各种基本算法除了原理要了解他们的优缺点，这里应该是面试官考察比较核心的地方。平时不能只顾着刷题，而没有去练习怎么在人前把一个算法表述清楚。</p> 
<p>★ 感觉算法岗真的没有那么难，没必要散播焦虑，感觉年轻人有无限可能，大家多花点时间真正去做事情了，基本结果都不会差。</p> 
<p>★ 可以感受一下面试官问问题的思路和感兴趣的角度。前两轮面试更加注重于专业知识，技术细节，了解你做的多深。后两面更注重了解你思考问题的方式以及想做的内容和团队是否match。</p> 
<p>★ 字节跳动是一个很注重基础的公司，他不会要求你有竞赛有paper有多么强的工业界能力，但是基础一定要好，至少对于算法工程师来说现场手撕code的时候 bug free 是必须的，因为面试官不会留太长的时间给你debug，除非能够精准定位bug并快速解决，不然凉的几率很大。</p> 
<p>★ 字节的面试是我面的最硬核的，就是会一直问到底，看你到底掌握到什么程度，如果掌握的不深刻很容易就被问出来了 。</p> 
<p>★ 在面试时，针对项目，面试官会假设他不懂这个项目，将项目从头讲到尾</p> 
<p>★ 春招的笔试也很难（不要以为实习的笔试就简单了）</p> 
<p>★ 早点准备什么时候都不会错的（毕竟机会是留给有准备的人）</p> 
<p>★ 发现形式不对,要快刀斩乱麻；今年算法岗从春招的形式就可以看到秋招的严峻了</p> 
<p>★ 认清自己（这一点确实有点难），但这确实是重中之重</p> 
<p>★ 好好刷题，不要抱着内推免笔试提起批免笔试的心态，即使免笔试，coding也是你没办法跳过的。</p> 
<p>★ 投一个公司之前多问问自己为什么要投，目标一定要明确，不要学我做个无头苍蝇到处乱撞，然后撞的头破血流。</p> 
<p>★ 虽然字节疯狂的招人，但是也请看好招人的部门，并不是每个部分都疯狂招人的，所以还得好好甄别，多看看，别头铁第一天出来就投，浪费机会。</p> 
<p>★ 一面考核的是思想层面, 比如考核面对大数据如何增量训练, 面对多指标如何多任务学习, 从离线实验到部署上线的流程, 线上测试需要关注的信息, 模型选择的依据等。<br> 二面考核的主要是宽度, 几乎都是横向的问, 很少纵向的深挖, 而我恰恰缺乏宽度的积累, 所以面试过程还是有点难受。<br> 三面面试官的考核主要是深度, 一直深挖每一个细节, 基本上问到口述代码的程度.不过所有代码都是自己写的就问题不大。</p> 
<h1>
<a id="2__105"></a>2 字节跳动面经涉及基础知识点</h1> 
<p><img src="https://images2.imgbox.com/f1/82/hwk4BgRY_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="21__109"></a>2.1 图像处理基础</h2> 
<h3>
<a id="211__110"></a>2.1.1 讲解相关原理</h3> 
<p>● 是否了解图像降噪的一些方法？</p> 
<p>● 了解常用图像增强的一些方法吗?</p> 
<p>● 是否了解各种边缘检测算子？<br> 介绍下Sobel算子，sobel核的参数由-1-&gt;2,改变后会发生什么？<br> 面试官的意思：比如实现不同的功能效果，高斯模糊、腐蚀、膨胀、锐化等</p> 
<p>● 了解Hog吗？讲解下Hog特征的原理，步骤流程是什么？</p> 
<p>● 知道图像里面的插值算法有哪些？（三次样条和线性插值），用过什么图像的库函数？</p> 
<p>● 解释下Raw图像和rgb图像的区别？了解其他色彩空间格式吗？或者饱和度、亮度这些吗？</p> 
<h3>
<a id="212__125"></a>2.1.2 手写算法代码</h3> 
<p>● 手写马赛克算法</p> 
<p>● 手写高斯滤波算法</p> 
<p>● 手写均值滤波及优化</p> 
<p>● 手写下中值滤波器</p> 
<h2>
<a id="22_CNN_133"></a>2.2 深度学习：CNN卷积神经网络方面</h2> 
<h3>
<a id="221__134"></a>2.2.1 讲解相关原理</h3> 
<h3>
<a id="2211__135"></a>2.2.1.1 卷积方面</h3> 
<p>● depthwise 卷积</p> 
<p>● 1*1的卷积核有什么用？</p> 
<p>● 反卷积相比其他上采样层（pixelshuffle）的缺点，棋盘格现象怎么产生的?</p> 
<p>● 3D卷积和2D卷积的区别，主要存在问题，如何加速运算，视频理解的sota方法，<br> 还有什么方向可以改进？</p> 
<p>● 卷积核大小如何选取</p> 
<p>● 卷积层减少参数的方法？使用1<em>3,3</em>1代替3*3的原理是什么？</p> 
<p>● 设计一个在CNN卷积核上做dropout的方式</p> 
<p>● 反卷积/转置卷积的实现原理?</p> 
<p>● Dropout的原理？</p> 
<p>● 直接转置卷积和先上采样再卷积的区别？</p> 
<h4>
<a id="2212__156"></a>2.2.1.2 池化方面</h4> 
<p>● maxPooling怎么传递导数？</p> 
<p>● CNN里面池化的作用</p> 
<p>● 反向传播的时候怎么传递pooling的导数</p> 
<p>● 卷积神经网络在maxpooling处怎么反向传播误差</p> 
<h4>
<a id="2213__164"></a>2.2.1.3 网络结构方面</h4> 
<p>● shufflenet的结构</p> 
<p>● 深度网络Attention是怎么加？</p> 
<p>● ResNet的结构特点以及解决的问题是什么？</p> 
<p>● 图神经网络的理解，讲了发展史，应该从基于图谱和基于空间来讲</p> 
<p>● unet结构，为什么要下采样，上采样？</p> 
<p>● ResNet V1到V2的改进有了解吗？<br> 那ResNet的下采样过程是怎么样的？<br> 讲了res-block的跳跃连接，以及连接前后的shape保持（通过padding保持shape不变）</p> 
<p>● fpn的结构</p> 
<p>● roi pooling和roi align的区别</p> 
<p>● Resnet的理解、和全连接相比有什么区别？</p> 
<p>● 简单说一下Alexnet、Vgg、Resnet、Densenet、和GoogleNet，它们的特色是什么?</p> 
<p>● 问了很多轻量级网络，mobileNet v1 v2，shuffleNet v2，Xception，denseNet等等</p> 
<h4>
<a id="2214__188"></a>2.2.1.4 其他方面</h4> 
<p>● 有上过神经网络的课程吗，是自学的吗？了解感受野吗？怎么计算感受野？怎么增加感受野？（增加感受野和网络深度，压缩图像尺寸）</p> 
<p>● 为什么卷积神经网络适用于图像和视频，还能用于其他领域吗？</p> 
<p>● CNN反向传播细节，怎么过全连接层、池化层、卷积层？</p> 
<p>● CNN里面能自然起到防止过拟合的办法</p> 
<p>● CNN中感受野/权值共享是什么意思？</p> 
<p>● BN层的作用，为什么有这个作用？测试和训练时有什么不同，在测试时怎么使<br> 用？</p> 
<p>● BN层做预测的时候，方差均值怎么算，online learning 的时候怎么算？</p> 
<p>● BN机制，BN怎么训练；</p> 
<p>● 发生梯度消失，梯度爆炸问题的原因？如果发生梯度爆炸、梯度消失，怎么解决？</p> 
<p>● 若CNN网络很庞大，在手机上运行效率不高，对应模型压缩方法有了解吗?</p> 
<h3>
<a id="222__209"></a>2.2.2 数学计算</h3> 
<p>● 如何计算卷积的复杂度、卷积层的参数量</p> 
<p>● 计算Feature Map的size</p> 
<p>● 输入为L<em>L，卷积核为k</em>k，还有步长s和padding p，求输出尺寸？（L1 = (L-k+2<em>p)/s + 1）<br> 接上题，求操作的FLOPs？（FLOPS = k</em>k<em>c1</em>c2<em>L1</em>L1）</p> 
<p>● 在同时考虑 pooling， stride， padding 的情况下，计算 depthwise conv 和 pointwise conv 过程中每一步的计算量和feature map的尺寸</p> 
<p>● CNN中给定输入数据维度[c,w,h]，卷积核[k,k]，则输出维度，如何padding=p，输出维度是什么？</p> 
<h3>
<a id="223__220"></a>2.2.3 公式推导</h3> 
<p>● BP神经网络反向传播推导</p> 
<p>● max pooling梯度求导？</p> 
<h3>
<a id="224__225"></a>2.2.4 手写算法代码</h3> 
<p>● 说一些卷积、用代码实现卷积，并再改成有通道的三维卷积</p> 
<p>● 写一个单通道的图像卷积（带padding）</p> 
<p>● 手写前向传播、反向传播代码</p> 
<p>● 面试官轻描淡写地说，BP你会吧，写一下吧，正向传播、反向传播都推了一个遍，交给面试官看了一眼，说用代码实现一下吧，用 numpy 写了一个单层神经元的反向传播，给面试官看了，问他还用不用写完整的传播过程，他说不用了。</p> 
<p>● 让写代码或者数学公式展示BN的内部实现，为什么要用GN，你知道GN，BN，LN和IN的区别吗？（这里BN内部实现回答错了，还好面试官非常nice，一直知指导，最后给我讲了BN的内部实现，豁然开朗，回来看了一下代码，有了进一步的认识）。</p> 
<p>● 用代码展示shuffleNet v2的结构</p> 
<p>● 实现一维数组的maxpool</p> 
<h3>
<a id="225__239"></a>2.2.5 激活函数类</h3> 
<p>● 说一下Softmax多分类器的作用？和二分类相比有什么特点？</p> 
<p>● Softmax的计算公式写一下,并进行解释</p> 
<p>● Softmax的Loss function、写一下损失函数</p> 
<p>● 写一个 Softmax 实现，注意上下溢出问题</p> 
<p>● Softmax在数值计算上可能会出现的上溢和下溢的问题</p> 
<h2>
<a id="23_RNN_249"></a>2.3 深度学习：RNN递归神经网络方面</h2> 
<h3>
<a id="231___250"></a>2.3.1 讲解相关原理</h3> 
<p>● 讲一下隐马、CRF、RNN、LSTM的区别？</p> 
<p>● RNN为什么会出现梯度消失？</p> 
<p>● BPTT的推导？</p> 
<p>● LSTM和GRU和传统RNN的对比？</p> 
<p>● LSTM减弱梯度消失的原理，项目里用了LSTM，问了LSTM的结构，三个门的作用，每个门用什么激活函数？</p> 
<p>● LSTM的输入，输出，遗忘门分别是做什么的，整个计算流程怎么样</p> 
<p>● RNN梯度弥散和爆炸的原因，lstm为什么不会这样</p> 
<p>● RNN/LSTM解释，你知道哪些时间序列预测，举一个例子，写出伪代码(写了HMM)。</p> 
<p>● RNN如何防止梯度爆炸（LSTM原理）。</p> 
<p>● LSTM和RNN的区别， 遗忘门的具体实现？</p> 
<p>● BN和LN的区别，以及BN一般怎么用，LSTM中有没有用BN？</p> 
<h3>
<a id="232__272"></a>2.3.2 手绘网络原理</h3> 
<p>● 手画gru，并解释门的原理</p> 
<p>● 写一下LSTM的公式？</p> 
<h2>
<a id="24_CNNRNN_276"></a>2.4 深度学习：CNN&amp;RNN通用知识点</h2> 
<h3>
<a id="241__277"></a>2.4.1 基础知识点</h3> 
<p>● 详解梯度消失、爆炸原因及其解决方法</p> 
<p>● 你用过dropout么？介绍一下 ,Dropout的作用 ？</p> 
<p>● 梯度消失的表现是什么，该怎么处理</p> 
<p>● 神经网络权重怎么初始化，说一下自己知道的方法</p> 
<p>● dropout机制，为什么dropout能够抑制过拟合？</p> 
<p>● 神经网络中网络权重W初始化为0有什么问题？为什么不能初始权重为0？</p> 
<p>● 如何解决模型不收敛问题 以及如何加快模型的训练速度</p> 
<p>● 你知道哪几种normlize的方法？请着重介绍一种（BatchNormalization）.这个方法<br> 在深度学习网络中有什么用？为什么可以加速模型收敛？</p> 
<p>● Attention怎么做，self-attention怎么做？self-attention原理公式，为什么有效？</p> 
<p>● Encoder-Decoder模型里，如果Decoder是基于Attention做的，该怎么做，是一个什么结构？</p> 
<p>● attention机制是什么解释一下，啥是soft attention 和hard attention?</p> 
<h3>
<a id="242__300"></a>2.4.2 模型评价</h3> 
<p>● 有哪些评价指标？-比如ROC、AUC、F1-Score</p> 
<p>● 解释下深度学习中的评价指标：Map、PR曲线、AUC、Recall?</p> 
<p>● AUC怎么计算？它刻画的是什么?实现求AUC的过程？（输入就是instance的score和对应label）</p> 
<p>● 给你M个正样本，N个负样本，以及他们的预测值P，求AUC。（写完之后接问：AUC究竟在衡量模型什么能力？如果现在所有预测值都*1.2，AUC是否会变化？）这一题印象深刻是因为平时在计算auc的时候，很多同学都知道是roc曲线的面积，但是对auc具体的含义了解不多。</p> 
<p>● ROC曲线的含义和其他评价指标的区别？</p> 
<p>● 分类问题的指标是什么？准确度、召回率、PR曲线</p> 
<p>● 相关系数是怎么计算的？讲一下协方差和它的意义？</p> 
<p>● 做视频用的是何种评价指标？</p> 
<p>● 计算广告中CPM、CPC、ROI的含义，计算方式</p> 
<h2>
<a id="25__318"></a>2.5 传统机器学习方面</h2> 
<h3>
<a id="251__319"></a>2.5.1 讲解相关原理</h3> 
<h4>
<a id="2511__320"></a>2.5.1.1 数据准备</h4> 
<p>● 采样一般有哪些方法？讲一下</p> 
<p>● 解释下MCMC采样？</p> 
<h4>
<a id="2512__324"></a>2.5.1.2 特征工程</h4> 
<p>① 特征降维</p> 
<p>● 看项目中有数据降维的项目，讲一下PCA原理？PCA与SVD的联系与区别？SVD分解是怎么回事？</p> 
<p>● PCA了解吗？怎么推导？SVD怎么求？</p> 
<p>● 简单说一下LDA的思想？并说一下公式</p> 
<p>● T-SNE算法了解吗？</p> 
<p>② 特征选择</p> 
<p>● 特征选择有哪些方法？什么是特征向量与特征值？怎么理解它们代表的意义。（介绍项目时涉及到特征相关性分析，所以问了这个）</p> 
<p>● 介绍下特征选择的Lasso回归？</p> 
<p>● 特征选择里提到的互信息选择，互信息的计算公式是什么？</p> 
<p>● 树模型中分叉的判断有哪些：信息增益，信息增益比，Gini系数；他们有什么区别？</p> 
<p>● 写出信息增益的表达式</p> 
<p>● 在做特征工程时采用了哪些方法呢？ 常见的筛选特征的方法有哪些？</p> 
<h4>
<a id="2513__348"></a>2.5.1.3 有监督学习-分类和回归方面</h4> 
<p>① 分类回归树（集成学习）</p> 
<p>● 机器学习的集成方法有哪些？</p> 
<p>● Boosting与Bagging的原理以及异同点?为什么说bagging降低方差boosting降低偏差？<br> 谁是更关注方差 ，谁是更关注偏差？</p> 
<p>● bagging 中随机有放回采样，假如一共有N个样本 采样了N次，得到N个采样数据，去重后有X个数据 求E（X），我只列出了暴力计算的方法。</p> 
<p>● 决策树，熵的公式、如何分裂，如何剪枝，回归树、分类树的做法</p> 
<p>A.基于bagging：随机森林</p> 
<p>● 随机森林的随机性怎么体现？</p> 
<p>● 为什么bagging能降低方差?</p> 
<p>B.基于boosting：Adaboost、GDBT、XGBoost</p> 
<p>● xgboost与lgbm的原理讲一下？是如何进行调参的？</p> 
<p>● xgboost和gbdt的优势？两者的区别？并行怎么做？他们的应用场景有哪些呢？其他模型的能说一下吗？</p> 
<p>● Xgboost 和 GBDT的区别 以及如何改进和提升Xgboost模型</p> 
<p>● Random Forest 和GDBT、 XGBoost 、LR有什么区别？</p> 
<p>● GBDT的原理，怎么做多分类问题？</p> 
<p>● gbdt的gb是什么意思，如何体现。gbdt里如何知道每个特征的重要性。</p> 
<p>● 因为实习用到了xgb，让写一下 xgb的loss func？问xgb到底是怎么预测的？</p> 
<p>● 问GBDT原理，然后具体问了下每个叶子节点是怎么分裂的，用什么标准决定最优特征，答曰和CART一样，用Gini指数，然后写了下Gini指数的公式。老板看起来不甚满意，估计他本来想让我写的是xgboost那种带正则项的节点分裂方式吧。</p> 
<p>● CART了解吗？怎么做回归和分类的？ 哈希表了解吗？有哪些解决冲突方法？ 堆空间栈空间了解吗？</p> 
<p>● CART树的原理，和ID3以及C4.5有什么区别，回归树与分类树有什么区别。</p> 
<p>● GBDT中G是什么？怎么拟合树的？梯度拟合了怎么和原来的树合并的</p> 
<p>● 为什么XGBOOST在大赛上表现很好/与GBDT相比优势</p> 
<p>● lightgbm GBDT xgb，问的超级细，可能持续了7 8分钟，XGB残差怎么用一次和二次梯度求，分裂点怎么求，思想原理是什么。XGB实际使用中重要的超参数，你们比赛中用的目标函数是什么，为什么lightgbm速度更快，其并行计算如何实现？</p> 
<p>● xgboost的特征重要性怎么计算的？设计能适应测试集里有缺失值的训练集没有的GBDT， 要求不能从填充数据的角度来做？</p> 
<p>● LightGBM和xgboost的区别 ，LightGBM的直方图排序后会比xgboost的效果差吗，为什么？</p> 
<p>● xgb怎么并行运算（除了自带的并行找特征分裂点，还说了一般模型的按数据和按特征并行） ，但是面试官一直追问详细的并行方法</p> 
<p>● xgb与LR各自的优缺点，LR为什么更容易并行？</p> 
<p>② 线性回归</p> 
<p>● 能否详细的讲解一下，线性回归的原理？具体讲解一下线性回归的底层原理，比如说如何训练，如何得到参数，如何调整参数等？</p> 
<p>● 线性回归R^2公式及意义</p> 
<p>③ K近邻（KNN）</p> 
<p>● knn算法了解吗，和传统的LR和SVM有什么区别？</p> 
<p>● 怎么优化knn呢？</p> 
<p>④ 逻辑回归LR</p> 
<p>● LR为什么要用sigmoid？（经过面试官提示，是来自于最大熵模型，建议不明白的同学去查一下，下次面试给面试官露一手）</p> 
<p>● 逻辑回归特征之间关联程度大会有什么问题？</p> 
<p>● 讲解一下逻辑回归的原理？再详细的讲解一下朴素贝叶斯的底层原理，比如说，如何选参数，如何训练模型，如何做分类？</p> 
<p>● 对于LR来说，LR如果多了一维冗余特征，其权重和AUC会怎样变化（权重变为1/2, AUC不会变化）</p> 
<p>● 逻辑斯蒂回归里面，输出的那个0-1之间的值，是概率值吗？你看它又叫对数几率回归，怎么理解几率这个概念？</p> 
<p>● 什么是线性模型？项目为什么使用LR，介绍LR？LR为什么是线性模型？ 如何提升LR的模型性能？</p> 
<p>● FM 与 LR对比一下，FM是否也能起到自动特征选择的作用，为什么?</p> 
<p>● LR和FM的区别?</p> 
<p>● LR的w可不可能是负的，正负样本10：1的情况下？</p> 
<p>● LR一个特征重复会怎么样？</p> 
<p>⑤ SVM（支持向量机）</p> 
<p>● 为什么svm的loss不能直接用梯度下降要用对偶？说说你知道的优化算法。</p> 
<p>● SVM原理，与感知机的区别？还问了SVM如果不用对偶怎么做？</p> 
<p>● SVM对于异常值的处理，敏感程度？</p> 
<p>● Svm和LR的区别和各自优缺点？</p> 
<p>● SVM最后的形表达形式是什么？</p> 
<p>● KKT条件是什么？在SVM中起到什么样的作用；</p> 
<p>● SVM中SMO具体的操作以及原理。</p> 
<p>● 熟悉什么机器学习算法（SVM），写损失函数（hinge+正则）</p> 
<p>● SVM原理？为什么能转化为对偶问题？能不能推导</p> 
<p>● SVM怎么解决不容易找到超平面的问题?</p> 
<p>● SVM有哪些核函数？</p> 
<p>⑥ 朴素贝叶斯（Naive Bayes）</p> 
<p>● 贝叶斯模型知道吗？问贝叶斯网络的原理，贝叶斯估计和极大似然估计原理？朴素贝叶斯公式？</p> 
<p>● 讲一下最大似然的原理？</p> 
<p>● 朴素贝叶斯的算法实现？</p> 
<p>⑦ 决策树（DT）</p> 
<p>● 了解其他机器学习模型吗，说了决策树，为什么用信息熵？</p> 
<p>● 决策树的ID3和C4.5介绍一下？决策树模型的类别？</p> 
<p>● 决策树分裂节点的标准与对应的算法</p> 
<p>● 代码写一个决策树，给定数据，启发函数是信息增益，假设所有特征的值都是数值类型的：定义节点类、构建节点、选取当前节点的最优划分特征（计算所有特征的信息增益）、数据划分、构建子节点、考虑停止划分的条件。花了好长时间写了个代码框架，然后和面试官讲了思路。</p> 
<p>● 写的决策树是几叉树？暂时考虑的是有多少种不同的取值就有多少个分支，意识到这肯定是不对的，优化的话可以将所有取值进行划分，比如二划分就可以改成二叉树。</p> 
<h4>
<a id="2514__479"></a>2.5.1.4 无监督学习-聚类方面</h4> 
<p>① Kmeans均值聚类</p> 
<p>● KMeans和 GMM 联系与区别，kmeans原理，怎么做的，你是怎么并行的？</p> 
<p>● kmeans原理，怎么做的，你是怎么并行的？k-means是否一定收敛？</p> 
<p>② 高斯混合模型（GMM）</p> 
<p>● 高斯混合模型和K-means的区别和联系</p> 
<h4>
<a id="2515__489"></a>2.5.1.5 模型评价</h4> 
<p>● 信息检索中为什么使用Recall和Precision？</p> 
<p>● 机器学习中一般怎么衡量模型效果？AUC值怎么理解？</p> 
<p>● 怎么衡量两个分布的差异？KL散度和交叉熵损失有什么不同？关系是啥？</p> 
<p>● 一些统计学的原理比如t-test, AUC curve的意义是啥，为什么要用AUC去衡量机器学习模型的好坏。</p> 
<p>● 为什么召回的数量级小，排序模型的效果就好</p> 
<p>● 交叉熵 和 相对熵(kl散度)的关系</p> 
<h3>
<a id="252__501"></a>2.5.2 手推算法及代码</h3> 
<h4>
<a id="2521__502"></a>2.5.2.1 手推公式</h4> 
<p>● LR推导，手写LR前向传播和反向传播</p> 
<p>● 写出LR的损失函数（交叉熵损失函数）</p> 
<p>● FM的推导？</p> 
<p>● 上来就让手写个LogisticRegression 你了解元学习吗？说一下你的理解。 权值初<br> 始化方式对LR的收敛有影响吗？ 你对权值初始化有什么了解？怎样才算是好的初始化？</p> 
<p>● 问了xgboost并且手推</p> 
<p>● 问能推哪些算法的公式，只敢说LR，因为没准备SVM。然后就是推LR的梯度下降，接着让我写sigmoid函数，最后就是sigmoid求导。</p> 
<p>● LR、SVM的公式推导</p> 
<p>● 朴素贝叶斯写公式。</p> 
<p>● 介绍一个熟悉的算法（LR），推导sigmoid求导过程</p> 
<p>● 手写LR的实现过程，然后聊了聊L1以及L1的扩展</p> 
<p>● 手推 LR 的损失函数、损失函数怎么来的、梯度如何计算，写成一个完整的类</p> 
<h4>
<a id="2522__525"></a>2.5.2.2 手写代码</h4> 
<p>● 问了adaboost的原理，模型的权重以及数据的权重各自有什么意义，写出adaboost的伪代码。</p> 
<p>● 手写kmeans聚类算法（代码）</p> 
<h2>
<a id="26__529"></a>2.6 深度学习&amp;机器学习面经通用知识点</h2> 
<h3>
<a id="261__530"></a>2.6.1 损失函数方面</h3> 
<p>● 交叉熵，相对熵</p> 
<p>● 回归和分类的常用损失函数</p> 
<p>● Logistic Regression损失函数，怎么来的？</p> 
<p>● 常见损失函数有哪些</p> 
<p>● 逻辑回归中损失函数的实际意义？</p> 
<p>● Smooth l1 loss公式以及为什么是这样的?</p> 
<p>● 为什么分类问题用交叉熵, 怎么来的？</p> 
<h3>
<a id="262__545"></a>2.6.2 激活函数方面</h3> 
<p>● 各种常用激活函数对比下？（sigmoid, tanh, relu, lrelu等）</p> 
<p>● sigmoid的优缺点?</p> 
<p>● sigmoid和relu的区别？</p> 
<p>● 平时用什么用的多？为什么用relu多呢？</p> 
<p>● 写逻辑回归的logloss损失函数</p> 
<h3>
<a id="263__555"></a>2.6.3 网络优化梯度下降方面</h3> 
<p>● SGD每步做什么，为什么能online learning</p> 
<p>● l1是损失函数，有哪些优化方法，能用sgd么？为什么？</p> 
<p>● Adam优化器的迭代公式</p> 
<p>● 4adam用到二阶矩的原理是什么</p> 
<p>● 几种梯度下降的方法和优缺点？</p> 
<p>● 梯度下降系列算法有哪些，有点蒙住了，后来才想起来应该问问 momentum<br> adam 之类的算不算？</p> 
<p>● 讲一下你熟悉的优化器，说一下区别或发展史</p> 
<p>● 有哪些优化算法，Adam的默认参数有哪些?</p> 
<p>● 介绍方向导数和梯度，方向导数和梯度的关系？为什么梯度在机器学习中的优化方法中有效？</p> 
<p>● 神经网络权重初始化方法和优化方法</p> 
<p>● 介绍一下你了解的优化器和各自的优缺点?</p> 
<p>● Adam和Adagrad的区别?</p> 
<h3>
<a id="264__580"></a>2.6.4 正则化方面</h3> 
<p>● 正则化的本质？</p> 
<p>● L1正则化和L2正则化的区别，从数学角度说</p> 
<p>● L1有什么缺点？L2呢？平时用L1多还是用L2多？为什么正则化选L2呢？为什么不选L1？L1为什么产生稀疏解？</p> 
<p>● L1、L2的区别， L1为什么图像是菱形</p> 
<p>● L1范数和L2范数的区别，作用。为什么bias不正则</p> 
<p>● 为什么要用正则化？解释了奥卡姆剃刀</p> 
<p>● L1正则化与L2正则化的区别？解释了参数先验和拉格朗日乘子法</p> 
<h3>
<a id="265__594"></a>2.6.5 压缩&amp;剪枝&amp;量化&amp;加速</h3> 
<p>● 了解模型蒸馏吗?</p> 
<p>● 怎么做模型压缩？-使用知识蒸馏、设计小的网络，得到End-to-End模型。</p> 
<p>● 介绍了量化（8bit,4bit,二值化）的项目?训练后量化和量化感知训练分别是怎么实现的？</p> 
<h3>
<a id="266__600"></a>2.6.6 过拟合&amp;欠拟合方面</h3> 
<p>● 什么是过拟合，过拟合怎么解决？</p> 
<p>● 深层网络容易过拟合还是浅层网络容易过拟合 ？</p> 
<p>● 防止欠拟合的方法？</p> 
<p>● 过拟合要怎么解决？（减少模型参数、早停、正则化、数据增强、GAN合成数据、dropout、few shot learning，等等等等）</p> 
<p>● BN为什么防止过拟合呢？</p> 
<h3>
<a id="267__610"></a>2.6.7 其他方面</h3> 
<p>● 深度学习与机器学习的异同及联系？</p> 
<p>● 数据不均衡的处理方法-过多数据欠采样，过少数据过采样，另外还有一些基于模型的方法比如SMOTE方法等。</p> 
<p>● 样本不均衡怎么解决，我说人为对采样少的样本重复几次，然后他问这样auc会不会变并解释，我说不会变，解释的不太清楚但他好像听懂了</p> 
<p>● 当模型的性能不好时，如何分析模型的瓶颈？</p> 
<p>● 数据不均衡有什么解决方式，从数据，模型选择，以及损失函数选择角度？</p> 
<p><img src="https://images2.imgbox.com/74/9b/uu1sbtQO_o.gif" alt="在这里插入图片描述"></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>