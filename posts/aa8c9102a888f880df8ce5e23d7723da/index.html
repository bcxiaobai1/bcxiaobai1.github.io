<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>YOLOV7训练自己的数据集以及训练结果分析(手把手教你) - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">YOLOV7训练自己的数据集以及训练结果分析(手把手教你)</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <h2>
<a id="YOLOV7_2"></a><strong>YOLOV7训练自己的数据集以及训练结果分析(手把手教你)</strong>
</h2> 
<p><strong>YOLOV7训练自己的数据集整个过程主要包括：环境安装----制作数据集----参数修改----模型测试----模型推理</strong></p> 
<h2>
<a id="_7"></a>一：环境安装</h2> 
<pre><code>conda create -n yolov7 python=3.8
conda activate yolov7
#cuda cudnn torch等版本就不细说了，根据自己的显卡配置自行下载
#v7 guihub代码地址 https://github.com/WongKinYiu/yolov7
git clone https://github.com/WongKinYiu/yolov7.git #下载到本地也行
#cd到v7项目中
conda activate yolov7
pip install -r requirements.txt
</code></pre> 
<h2>
<a id="__18"></a>二： 制作数据集</h2> 
<p>labelme标注的数据格式是VOC，而YOLOv7能够直接使用的是YOLO格式的数据，因此下面将介绍如何将自己的数据集转换成可以直接让YOLOv7进行使用。</p> 
<h3>
<a id="1__20"></a>1. 创建数据集</h3> 
<p>在data目录下新建Annotations, images, ImageSets, labels 四个文件夹<br> images目录下存放数据集的图片文件<br> Annotations目录下存放图片的xml文件（labelImg标注）</p> 
<p><img src="https://images2.imgbox.com/fd/65/qW1GayOd_o.png" alt="在这里插入图片描述"><br> 其中目录结构如下</p> 
<pre><code>.
├── ./data
│   ├── ./data/Annotations
│   │   ├── ./data/Annotations/35.xml
│   │   ├── ./data/Annotations/36.xml
│   │   ├── ...
│   ├── ./data/images
│   │   ├── ./data/images/35.jpg
│   │   ├── ./data/images/36.jpg

│   │   ├── ...
│   ├── ./data/ImageSets
│   └── ./data/labels
│   ├── ./data/coco.yaml
│   ├── ./data/hyp.scratch.p5.yaml
│   ├── ./data/hyp.scratch.p6.yaml
│   ├── ./data/hyp.scratch.tiny.yaml
├── ./cfg
├── ./detect.py
├── ./figure
├── ./hubconf.py
├── ./inference
├── ./models
├── ./README.md
├── ....

</code></pre> 
<h3>
<a id="2__55"></a>2. 按比例划分数据集</h3> 
<p>在yolov7根目录下新建一个文件splitDataset.py<br> 随机分配训练/验证/测试集图片，代码如下所示：</p> 
<pre><code>import os
import random

trainval_percent = 0.9
train_percent = 0.9
xmlfilepath = 'data/Annotations'
txtsavepath = 'data/ImageSets'
total_xml = os.listdir(xmlfilepath)

num = len(total_xml)
list = range(num)
tv = int(num * trainval_percent)
tr = int(tv * train_percent)
trainval = random.sample(list, tv)
train = random.sample(trainval, tr)

ftrainval = open('data/ImageSets/trainval.txt', 'w')
ftest = open('data/ImageSets/test.txt', 'w')
ftrain = open('data/ImageSets/train.txt', 'w')
fval = open('data/ImageSets/val.txt', 'w')

for i in list:
    name = total_xml[i][:-4] + 'n'
    if i in trainval:
        ftrainval.write(name)
        if i in train:
            ftrain.write(name)
        else:
            fval.write(name)
    else:
        ftest.write(name)

ftrainval.close()
ftrain.close()
fval.close()
ftest.close()

</code></pre> 
<h3>
<a id="3_xmlYOLOtxt_99"></a>3. 将xml文件转换成YOLO系列标准读取的txt文件</h3> 
<p>在同级目录下再新建一个文件XML2TXT.py<br> 注意classes = [“…”]一定需要填写自己数据集的类别，在这里我是一个类别"fall"，因此classes = [“ship”]，代码如下所示：<br> 如果数据集中的类别比较多不想手敲类别的，可以使用（4）中的脚本直接获取类别，同时还能查看各个类别的数据量，如果不想可以直接跳过（4）。</p> 
<pre><code># -*- coding: utf-8 -*-
# xml解析包
import xml.etree.ElementTree as ET
import pickle
import os
from os import listdir, getcwd
from os.path import join


sets = ['train', 'test', 'val']
classes = ['fall']


# 进行归一化操作
def convert(size, box): # size:(原图w,原图h) , box:(xmin,xmax,ymin,ymax)
    dw = 1./size[0]     # 1/w
    dh = 1./size[1]     # 1/h
    x = (box[0] + box[1])/2.0   # 物体在图中的中心点x坐标
    y = (box[2] + box[3])/2.0   # 物体在图中的中心点y坐标
    w = box[1] - box[0]         # 物体实际像素宽度
    h = box[3] - box[2]         # 物体实际像素高度
    x = x*dw    # 物体中心点x的坐标比(相当于 x/原图w)
    w = w*dw    # 物体宽度的宽度比(相当于 w/原图w)
    y = y*dh    # 物体中心点y的坐标比(相当于 y/原图h)
    h = h*dh    # 物体宽度的宽度比(相当于 h/原图h)
    return (x, y, w, h)    # 返回 相对于原图的物体中心点的x坐标比,y坐标比,宽度比,高度比,取值范围[0-1]


# year ='2012', 对应图片的id（文件名）
def convert_annotation(image_id):
    '''
    将对应文件名的xml文件转化为label文件，xml文件包含了对应的bunding框以及图片长款大小等信息，
    通过对其解析，然后进行归一化最终读到label文件中去，也就是说
    一张图片文件对应一个xml文件，然后通过解析和归一化，能够将对应的信息保存到唯一一个label文件中去
    labal文件中的格式：calss x y w h　　同时，一张图片对应的类别有多个，所以对应的ｂｕｎｄｉｎｇ的信息也有多个
    '''
    # 对应的通过year 找到相应的文件夹，并且打开相应image_id的xml文件，其对应bund文件
    in_file = open('data/Annotations/%s.xml' % (image_id), encoding='utf-8')
    # 准备在对应的image_id 中写入对应的label，分别为
    # &lt;object-class&gt; &lt;x&gt; &lt;y&gt; &lt;width&gt; &lt;height&gt;
    out_file = open('data/labels/%s.txt' % (image_id), 'w', encoding='utf-8')
    # 解析xml文件
    tree = ET.parse(in_file)
    # 获得对应的键值对
    root = tree.getroot()
    # 获得图片的尺寸大小
    size = root.find('size')
    # 如果xml内的标记为空，增加判断条件
    if size != None:
        # 获得宽
        w = int(size.find('width').text)
        # 获得高
        h = int(size.find('height').text)
        # 遍历目标obj
        for obj in root.iter('object'):
            # 获得difficult ？？
            difficult = obj.find('difficult').text
            # 获得类别 =string 类型
            cls = obj.find('name').text
            # 如果类别不是对应在我们预定好的class文件中，或difficult==1则跳过
            if cls not in classes or int(difficult) == 1:
                continue
            # 通过类别名称找到id
            cls_id = classes.index(cls)
            # 找到bndbox 对象
            xmlbox = obj.find('bndbox')
            # 获取对应的bndbox的数组 = ['xmin','xmax','ymin','ymax']
            b = (float(xmlbox.find('xmin').text), float(xmlbox.find('xmax').text), float(xmlbox.find('ymin').text),
                 float(xmlbox.find('ymax').text))
            print(image_id, cls, b)
            # 带入进行归一化操作
            # w = 宽, h = 高， b= bndbox的数组 = ['xmin','xmax','ymin','ymax']
            bb = convert((w, h), b)
            # bb 对应的是归一化后的(x,y,w,h)
            # 生成 calss x y w h 在label文件中
            out_file.write(str(cls_id) + " " + " ".join([str(a) for a in bb]) + 'n')


# 返回当前工作目录
wd = getcwd()
print(wd)


for image_set in sets:
    '''
    对所有的文件数据集进行遍历
    做了两个工作：
　　　　１．将所有图片文件都遍历一遍，并且将其所有的全路径都写在对应的txt文件中去，方便定位
　　　　２．同时对所有的图片文件进行解析和转化，将其对应的bundingbox 以及类别的信息全部解析写到label 文件中去
    　　　　　最后再通过直接读取文件，就能找到对应的label 信息
    '''
    # 先找labels文件夹如果不存在则创建
    if not os.path.exists('data/labels/'):
        os.makedirs('data/labels/')
    # 读取在ImageSets/Main 中的train、test..等文件的内容
    # 包含对应的文件名称
    image_ids = open('data/ImageSets/%s.txt' % (image_set)).read().strip().split()
    # 打开对应的2012_train.txt 文件对其进行写入准备
    list_file = open('data/%s.txt' % (image_set), 'w')
    # 将对应的文件_id以及全路径写进去并换行
    for image_id in image_ids:
        list_file.write('data/images/%s.jpgn' % (image_id))
        # 调用  year = 年份  image_id = 对应的文件名_id
        convert_annotation(image_id)
    # 关闭文件
    list_file.close()

</code></pre> 
<h3>
<a id="4__213"></a>4. 查看自定义数据集标签类别及数量</h3> 
<p>在同级目录下再新建一个文件ViewCategory.py，将代码复制进去</p> 
<pre><code>import os
from unicodedata import name
import xml.etree.ElementTree as ET
import glob


def count_num(indir):
    label_list = []
    # 提取xml文件列表
    os.chdir(indir)
    annotations = os.listdir('.')
    annotations = glob.glob(str(annotations) + '*.xml')

    dict = {}  # 新建字典，用于存放各类标签名及其对应的数目
    for i, file in enumerate(annotations):  # 遍历xml文件

        # actual parsing
        in_file = open(file, encoding='utf-8')
        tree = ET.parse(in_file)
        root = tree.getroot()

        # 遍历文件的所有标签
        for obj in root.iter('object'):
            name = obj.find('name').text
            if (name in dict.keys()):
                dict[name] += 1  # 如果标签不是第一次出现，则+1
            else:
                dict[name] = 1  # 如果标签是第一次出现，则将该标签名对应的value初始化为1

    # 打印结果
    print("各类标签的数量分别为：")
    for key in dict.keys():
        print(key + ': ' + str(dict[key]))
        label_list.append(key)
    print("标签类别如下：")
    print(label_list)


if __name__ == '__main__':
    # xml文件所在的目录，修改此处
    indir = 'data/Annotations'
    count_num(indir)  # 调用函数统计各类标签数目


</code></pre> 
<p>至此数据集的准备已经就绪，索引文件在data目录下的train.txt/val.txt/test.txt</p> 
<h3>
<a id="5__263"></a>5. 完整的数据集文件</h3> 
<p>images目录下存放数据集的图片文件<br> Annotations目录下存放图片的xml文件（labelImg标注）<br> label:txt文件<br> xx.cache不用在意有没有<br> <img src="https://images2.imgbox.com/a7/09/DFicRWMG_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="_272"></a>三：参数修改</h2> 
<h3>
<a id="1__273"></a>1. 修改模型配置文件</h3> 
<p>进入cfg/training文件夹，选择需要训练的模型配置文件，这里选择yolov7.yaml，将其中的nc修改为自己的类别数量，这里修改为1（根据自己的类别自定义）<br> <img src="https://images2.imgbox.com/a0/e5/dPDwWs0N_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="2__277"></a>2. 修改数据加载配置文件</h3> 
<p>在data/文件夹，新建ship.yaml（根据自己的类被自定义）内容如下：</p> 
<pre><code>train: ./data/train.txt
val: ./data/val.txt
test: ./data/test.txt

# number of classes
nc: 1

# class names
names: ['ship']

</code></pre> 
<p>至此，配置文件修改完成</p> 
<h2>
<a id="_294"></a>四：模型训练</h2> 
<p><strong>yolov7仓库中有两个训练脚本，一个叫train.py，一个叫train_aux.py，前者是训练P5的模型，包含yolov7-tiny、yolov7-tiny-silu、yolov7、yolov7x，后者是训练P6的模型，包含yolov7-w6、yolov7-e6、yolov7-d6、yolov7-e6e。</strong><br> yolov7.yaml 是在3.1修改的<br> ship.yaml是在3.2新建的<br> epochs 不用设置那么高 我对自己的数据集就设置了10轮</p> 
<pre><code># 训练P5模型
python train.py --weights yolov7.pt --data data/ship.yaml --epochs 300 --batch-size 8 --cfg cfg/training/yolov7.yaml --workers 0 --device 0 --img-size 640 640
# 训练P6模型
python train_aux.py --weights yolov7-e6e.pt --data data/fall.yaml --epochs 300 --batch-size 8 --cfg cfg/training/yolov7-e6e.yaml --workers 0 --device 0 --img-size 1280 1280

</code></pre> 
<blockquote> 
 <p>其中，weights是指预训练模型权重，可以去yolov7官方链接下载，指定到相应目录下（推荐），如果没有配置网络可能会存在git不了权重文件<br> epochs：指的就是训练过程中整个数据集将被迭代多少次,显卡不行你就调小点。<br> batch-size：一次看完多少张图片才进行权重更新，梯度下降的mini-batch,显卡不行你就调小点。<br> cfg：存储模型结构的配置文件<br> data：存储训练、测试数据的文件<br> img-size：输入图片宽高,显卡不行你就调小点。<br> data是指数据加载文件路径 –epoch是指模型训练轮次 –batch-size是指一批次输入多少数据一起训练，根据自己显卡的显存决定<br> cfg是指模型加载文件路径，关于–cfg中的training和deploy可以参考这篇文章：training和deploy的区别<br> workers是指dataloader同时读取多少个进程，如果num_worker设为0，意味着每一轮迭代时，dataloader不再有自主加载数据到RAM这一步骤（因为没有worker了），而是在RA中找batch，找不到时再加载相应的batch。缺点当然是速度慢。设置为0可以避免一些错误发生<br> device是指选用几号GPU –img-size是指训练集和测试集图像大小，可选640或1280等<br> rect是指是否采用矩阵推理的方式去训练模型，采用矩阵推理就不要求送入的训练的图片是正方形 –resume断点续训<br> evolve超参数进化，模型提供的默认参数是通过在COCO数据集上使用超参数进化得来的<br> linear-lr利用余弦函数对训练中的学习率进行调整</p> 
</blockquote> 
<p><img src="https://images2.imgbox.com/a8/e3/aZf0fVaI_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="_323"></a>五：模型推理</h2> 
<p>训练结束后会在run/train中有如下结果：<br> <img src="https://images2.imgbox.com/97/08/bThWV2b6_o.png" alt="在这里插入图片描述"></p> 
<p>用自己的模型进行测试</p> 
<pre><code>python detect.py --weights best.pt --source frame22.jpg --device 0
</code></pre> 
<p><img src="https://images2.imgbox.com/a6/fa/Dt5zCNbO_o.jpg" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/54/d2/YB2rUOMb_o.jpg" alt="在这里插入图片描述"></p> 
<h2>
<a id="_333"></a>六：结果分析</h2> 
<h3>
<a id="F1_curvepng_334"></a>F1_curve.png</h3> 
<p>F1分数，它被定义为查准率和召回率的调和平均数</p> 
<p>一些多分类问题的机器学习竞赛，常常将F1-score作为最终测评的方法。<strong>它是精确率和召回率的调和平均数</strong>，最大为1，最小为0。</p> 
<p><strong>F1-Score的值是从0到1的，1是最好，0是最差。</strong></p> 
<p><img src="https://images2.imgbox.com/58/28/X5e4x7CS_o.png" alt="在这里插入图片描述"></p> 
<p><img src="https://images2.imgbox.com/d9/b9/LSgEsLe8_o.png" alt="在这里插入图片描述"><br> 这是100epoch得到的F1_curve，说明在置信度为0.4-0.6区间内得到比较好的F1分数</p> 
<h3>
<a id="PR_curvepng_346"></a>PR_curve.png</h3> 
<p><img src="https://images2.imgbox.com/06/8f/6kYB2BHe_o.png" alt="在这里插入图片描述"><br> PR曲线中的P代表的是<strong>precision（精准率），R代表的是recall（召回率）</strong>，其代表的是精准率与召回率的关系，一般情况下，将recall设置为横坐标，precision设置为纵坐标。PR曲线下围成的面积即AP，所有类别AP平均值即Map.</p> 
<p>如果PR图的其中的一个曲线A完全包住另一个学习器的曲线B，则可断言A的性能优于B，当A和B发生交叉时，可以根据曲线下方的面积大小来进行比较。一般训练结果主要观察精度和召回率波动情况（波动不是很大则训练效果较好）</p> 
<p>Precision和Recall往往是一对矛盾的性能度量指标；及一个的值越高另一个就低一点；<br> 提高Precision &lt;<mark>&gt;提高二分类器预测正例门槛&lt;</mark>&gt; 使得二分类器预测的正例尽可能是真实正例；<br> 提高Recall &lt;<mark>&gt; 降低二分类器预测正例门槛 &lt;</mark> &gt;使得二分类器尽可能将真实的正例挑选</p> 
<h3>
<a id="R_curvepng_355"></a>R_curve.png</h3> 
<p>召回率recall和置信度confidence之间的关系<br> <img src="https://images2.imgbox.com/ec/e5/JiEtKCdF_o.png" alt="在这里插入图片描述"></p> 
<h3>
<a id="resultspng_358"></a>results.png</h3> 
<p><img src="https://images2.imgbox.com/34/d6/bidWPs0q_o.png" alt="在这里插入图片描述"><br> Box：Box推测为GIoU损失函数均值，越小方框越准；<br> Objectness：推测为目标检测loss均值，越小目标检测越准；<br> Classification：推测为分类loss均值，越小分类越准，本实验为一类所以为0；<br> Precision：精度（找对的正类/所有找到的正类）；</p> 
<p>Recall：真实为positive的准确率，即正样本有多少被找出来了（召回了多少）。</p> 
<p>Recall从真实结果角度出发，描述了测试集中的真实正例有多少被二分类器挑选了出来，即真实的正例有多少被该二分类器召回。</p> 
<p>val BOX: 验证集bounding box损失</p> 
<p>val Objectness：验证集目标检测loss均值</p> 
<p>val classification：验证集分类loss均值，本实验为一类所以为0</p> 
<p>mAP是用Precision和Recall作为两轴作图后围成的面积，m表示平均，@后面的数表示判定iou为正负样本的阈值，@0.5:0.95表示阈值取0.5:0.05:0.95后取均值。</p> 
<p>mAP@.5:.95（mAP@[.5:.95]）<br> 表示在不同IoU阈值（从0.5到0.95，步长0.05）（0.5、0.55、0.6、0.65、0.7、0.75、0.8、0.85、0.9、0.95）上的平均mAP。</p> 
<p>mAP@.5：表示阈值大于0.5的平均mAP</p> 
<p>一般训练结果主要观察精度和召回率波动情况（波动不是很大则训练效果较好）<br> 然后观察mAP@0.5 &amp; mAP@0.5:0.95 评价训练结果。</p> 
<h3>
<a id="tensorboard_385"></a>tensorboard</h3> 
<p>tensorboard是可实时观看自己训练数据集效果的可视化工具。</p> 
<pre><code>activate yolov7(自己所配的环境名称)
tensorboard --logdir=训练结果所在的文件夹
</code></pre> 
<p><strong>LOSS</strong><br> <img src="https://images2.imgbox.com/54/03/uzsvAcOq_o.png" alt="在这里插入图片描述"><br> <strong>AP值</strong><br> <img src="https://images2.imgbox.com/18/ed/8pnQ0Y2T_o.png" alt="在这里插入图片描述"></p>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>