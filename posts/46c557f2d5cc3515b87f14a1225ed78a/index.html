<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>《机器学习实战：基于Scikit-Learn、Keras和TensorFlow第2版》-学习笔记（4）：训练模型 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">《机器学习实战：基于Scikit-Learn、Keras和TensorFlow第2版》-学习笔记（4）：训练模型</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="markdown_views prism-atom-one-light">
                    
                        
                    
                    <h1>
<a id="__1"></a>第四章 训练模型</h1> 
<blockquote> 
 <p>· Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition, by Aurélien Géron (O’Reilly). Copyright 2019 Aurélien Géron, 978-1-492-03264-9.<br> · 环境：Anaconda（Python 3.8） + Pycharm<br> · 学习时间：2022.04.16</p> 
</blockquote> 
<p>到目前为止，我们已经探讨了不同机器学习的模型，但是它们各自的训练算法在很大程度上还是一个黑匣子。回顾前几章里的部分案例，你大概感到非常惊讶，在对系统内部一无所知的情况下，居然已经实现了这么多：优化了一个回归系统，改进了一个数字图片分类器，从零开始构建了一个垃圾邮件分类器，所有这些，你都不知道它们实际是如何工作的。确实是这样，在许多情况下，你并不需要了解实施细节。</p> 
<p>但是，很好地理解系统如何工作也是非常有帮助的。针对你的任务，它有助于快速定位到合适的模型、正确的训练算法，以及一套适当的超参数。不仅如此，后期还能让你更高效地执行错误调试和错误分析。最后还要强调一点，本章探讨的大部分主题对于理解、构建和训练神经网络（本书第二部分）是至关重要的。</p> 
<p>本章我们将从最简单的模型之一——线性回归模型，开始介绍两种非常不同的训练模型的方法：</p> 
<ul>
<li>通过“闭式”方程，直接计算出最拟合训练集的模型参数（也就是使训练集上的成本函数最小化的模型参数）；</li>
<li>使用迭代优化的方法，即梯度下降（GD），逐渐调整模型参数直至训练集上的成本函数调至最低，最终趋同于第一种方法计算出来的模型参数。我们还会研究几个梯度下降的变体，包括批量梯度下降、小批量梯度下降以及随机梯度下降。等我们进入到第二部分神经网络的学习时，会频繁地使用这几个的变体。</li>
</ul> 
<p>接着我们将会进入多项式回归的讨论，这是一个更为复杂的模型，更适合非线性数据集。由于该模型的参数比线性模型更多，因此更容易造成对训练数据过拟合，我们将使用学习曲线来分辨这种情况是否发生。然后，再介绍几种正则化技巧，降低过拟合训练数据的风险。</p> 
<p>最后，我们将学习两种经常用于分类任务的模型：Logistic回归和Softmax回归。</p> 
<blockquote> 
 <p>本章将会出现不少数学公式，需要用到线性代数和微积分的一些基本概念。要理解这些方程式，你需要知道什么是向量和矩阵，如何转置向量和矩阵，什么是点积、逆矩阵、偏导数。如果你不熟悉这些概念，请先通过在线补充材料中的Jupyter notebook，进行线性代数和微积分的入门学习。对于极度讨厌数学的读者，还是需要学习这一章，但是可以跳过那些数学公式，希望文字足以让你了解大多数的概念。</p> 
</blockquote> 
<p></p>
<div class="toc">
 <h3>文章目录</h3>
 <ul>
<li><a href="#__1">第四章 训练模型</a></li>
<li>
<ul>
<li><a href="#41__23">4.1 线性回归</a></li>
<li>
<ul>
<li><a href="#411__39">4.1.1 标准方程</a></li>
<li><a href="#412__97">4.1.2 计算复杂度</a></li>
</ul>
   </li>
<li><a href="#42__111">4.2 梯度下降及其算法</a></li>
<li>
<ul>
<li><a href="#421_Batch_Gradient_DescentBGD_144">4.2.1 批量梯度下降（Batch Gradient Descent，BGD）</a></li>
<li><a href="#422_Stochastic_Gradient_DescentSGD_191">4.2.2 随机梯度下降（Stochastic Gradient Descent，SGD）</a></li>
<li><a href="#423_MiniBatch_Gradient_Descent_236">4.2.3 小批量梯度下降（Mini-Batch Gradient Descent）</a></li>
</ul>
   </li>
<li><a href="#43__254">4.3 多项式回归</a></li>
<li><a href="#44__301">4.4 学习曲线</a></li>
<li>
<ul><li><a href="#_369">偏差/方差权衡</a></li></ul>
   </li>
<li><a href="#45__383">4.5 线性模型的正则化</a></li>
<li>
<ul>
<li><a href="#451__391">4.5.1 岭回归</a></li>
<li><a href="#452_Lasso_424">4.5.2 Lasso回归</a></li>
<li><a href="#453__457">4.5.3 弹性网络</a></li>
<li><a href="#454__486">4.5.4 提前停止</a></li>
</ul>
   </li>
<li><a href="#46__518">4.6 逻辑回归</a></li>
<li>
<ul>
<li><a href="#461__524">4.6.1 估计概率</a></li>
<li><a href="#462__540">4.6.2 训练和成本函数</a></li>
<li><a href="#463__556">4.6.3 决策边界（鸢尾植物数据集）</a></li>
<li><a href="#464_Softmax_632">4.6.4 Softmax回归</a></li>
</ul>
   </li>
<li><a href="#47__669">4.7 练习题</a></li>
<li>
<ul>
<li><a href="#_671">问题</a></li>
<li><a href="#_702">答案</a></li>
</ul>
  </li>
</ul>
 </li>
</ul>
</div>
<p></p> 
<h2>
<a id="41__23"></a>4.1 线性回归</h2> 
<p>线性模型就是对输入特征加权求和，再加上一个我们称为偏置项（也称为截距项）的常数，以此进行预测。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         y
        
        
         =
        
        
         
          θ
         
         
          0
         
        
        
         +
        
        
         
          θ
         
         
          1
         
        
        
         
          x
         
         
          1
         
        
        
         +
        
        
         
          θ
         
         
          2
         
        
        
         
          x
         
         
          2
         
        
        
         +
        
        
         …
        
        
         …
        
        
         +
        
        
         
          θ
         
         
          n
         
        
        
         
          x
         
         
          n
         
        
       
       
         y = θ_0 + θ_1x_1 +θ_2x_2+ …… +θ_nx_n 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.84444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.84444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.84444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.66666em;vertical-align: -0.08333em"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.84444em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.151392em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br> 线性回归模型预测（向量化形式）：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        y
       
       
        =
       
       
        
         h
        
        
         θ
        
       
       
        (
       
       
        x
       
       
        )
       
       
        =
       
       
        θ
       
       
        ⋅
       
       
        x
       
      
      
       y = h_θ(x) = θ·x
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mpunct">⋅</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord mathdefault">x</span></span></span></span></span>。</p> 
<p>在机器学习中，向量通常表示为列向量，是有单一列的二维数组。如果<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        θ
       
      
      
       θ
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span></span></span></span></span>和<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        x
       
      
      
       x
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">x</span></span></span></span></span>为列向量，则预测为$y = θ^Tx <span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        ，
       
       
        其
       
       
        中
       
      
      
       ，其中
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0em;vertical-align: 0em"></span><span class="mord cjk_fallback">，</span><span class="mord cjk_fallback">其</span><span class="mord cjk_fallback">中</span></span></span></span></span>θ<sup>T$为$θ$（行向量而不是列向量）的转置，且$θ</sup>Tx<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        为
       
       
        θ
       
      
      
       为θ
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord cjk_fallback">为</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span></span></span></span></span>T<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        和
       
      
      
       和
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0em;vertical-align: 0em"></span><span class="mord cjk_fallback">和</span></span></span></span></span>x$的矩阵乘积。</p> 
<p>这就是线性回归模型，我们该怎样训练线性回归模型呢？回想一下，训练模型就是设置模型参数直到模型最拟合训练集的过程。为此，我们首先需要知道怎么测量模型对训练数据的拟合程度是好还是差。在第2章中，我们了解到回归模型最常见的性能指标是均方根误差（RMSE）。因此，在训练线性回归模型时，你需要找到最小化RMSE的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        θ
       
      
      
       θ
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span></span></span></span></span>值。在实践中，将均方误差（MSE）最小化比最小化RMSE更为简单，二者效果相同（因为使函数最小化的值，同样也使其平方根最小）。</p> 
<p>线性回归模型的MSE成本函数：<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        M
       
       
        S
       
       
        E
       
       
        =
       
       
        (
       
       
        X
       
       
        ,
       
       
        
         h
        
        
         0
        
       
       
        )
       
       
        =
       
       
        
         1
        
        
         m
        
       
       
        
         ∑
        
        
         
          i
         
         
          =
         
         
          1
         
        
        
         m
        
       
       
        (
       
       
        
         θ
        
        
         T
        
       
       
        
         x
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
       
        −
       
       
        
         y
        
        
         
          (
         
         
          i
         
         
          )
         
        
       
       
        
         )
        
        
         2
        
       
      
      
       MSE = (X, h_0) = frac{1}{m}sum^m_{i=1}(θ^Tx^{(i)}-y^{(i)})^2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">M</span><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="mord mathdefault" style="margin-right: 0.05764em">E</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1.233em;vertical-align: -0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.845108em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop"><span class="mop op-symbol small-op">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.804292em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">m</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.29971em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 1.138em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.888em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">i</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>。</p> 
<h3>
<a id="411__39"></a>4.1.1 标准方程</h3> 
<p>为了得到使成本函数最小的θ值，有一个闭式解方法——也就是一个直接得出结果的数学方程，即标准方程。</p> 
<p><span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         θ
        
        
         ′
        
       
       
        =
       
       
        (
       
       
        
         X
        
        
         T
        
       
       
        X
       
       
        
         )
        
        
         
          −
         
         
          1
         
        
       
       
        
         X
        
        
         T
        
       
       
        y
       
      
      
       θ' = (X^TX)^{-1}X^Ty
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.751892em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1.09133em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span></span></span>，我们生成一些线性数据来测试这个公式：</p> 
<pre><code class="prism language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token comment"># 随机生成数据</span>
X <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> <span class="token number">4</span> <span class="token operator">+</span> <span class="token number">3</span> <span class="token operator">*</span> X <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>

X_b <span class="token operator">=</span> np<span class="token punctuation">.</span>c_<span class="token punctuation">[</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> X<span class="token punctuation">]</span>  <span class="token comment"># add x0 = 1 to each instance</span>
theta_best <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>inv<span class="token punctuation">(</span>X_b<span class="token punctuation">.</span>T<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X_b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X_b<span class="token punctuation">.</span>T<span class="token punctuation">)</span><span class="token punctuation">.</span>dot<span class="token punctuation">(</span>y<span class="token punctuation">)</span>  <span class="token comment"># dot（）方法计算矩阵内积</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>theta_best<span class="token punctuation">)</span>
<span class="token comment"># 输出：期待的是θ0=4，θ1=3得到的是θ0=3.6，θ1=3.2。非常接近，噪声的存在使其不可能完全还原为原本的函数</span>

<span class="token comment"># 根据参数做出预测</span>
X_new <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
X_new_b <span class="token operator">=</span> np<span class="token punctuation">.</span>c_<span class="token punctuation">[</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> X_new<span class="token punctuation">]</span>  <span class="token comment"># add x0 = 1 to each instance</span>
y_predict <span class="token operator">=</span> X_new_b<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>theta_best<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>y_predict<span class="token punctuation">)</span>

<span class="token comment"># 绘制模型的预测结果</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_new<span class="token punctuation">,</span> y_predict<span class="token punctuation">,</span> <span class="token string">"r-"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> <span class="token string">"b."</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/4f/0e/Eidp2uiC_o.png" alt=""></p> 
<p>使用Scikit-Learn执行线性回归很简单：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LinearRegression

lin_reg <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 实例化线性模型</span>
lin_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>  <span class="token comment"># 训练模型</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>lin_reg<span class="token punctuation">.</span>intercept_<span class="token punctuation">,</span> lin_reg<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>  <span class="token comment"># 输出模型训练后的参数</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lin_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_new<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 进行预测</span>

<span class="token comment"># LinearRegression类基于scipy.linalg.lstsq（）函数（名称代表“最小二乘”），你可以直接调用它：</span>
theta_best_svd<span class="token punctuation">,</span> residuals<span class="token punctuation">,</span> rank<span class="token punctuation">,</span> s <span class="token operator">=</span> np<span class="token punctuation">.</span>linalg<span class="token punctuation">.</span>lstsq<span class="token punctuation">(</span>X_b<span class="token punctuation">,</span> y<span class="token punctuation">,</span> rcond<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>theta_best_svd<span class="token punctuation">)</span>  <span class="token comment"># 输出最优的参数</span>

<span class="token comment"># LinearRegression()模型的参数和scipy.linalg.lstsq()函数的参数输出是一致的</span>
</code></pre> 
<p><code>scipy.linalg.lstsq()</code>函数计算<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         θ
        
        
         ′
        
       
       
        =
       
       
        
         X
        
        
         +
        
       
       
        y
       
      
      
       θ' = X^+y
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.751892em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.965771em;vertical-align: -0.19444em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span></span></span> ，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         X
        
        
         +
        
       
      
      
       X^+
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.771331em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span></span>是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        X
       
      
      
       X
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span></span></span></span></span>的伪逆。伪逆本身是使用被称为**奇异值分解（Singular Value Decomposition，SVD）**的标准矩阵分解技术来计算的，可以将训练集矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        X
       
      
      
       X
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span></span></span></span></span>分解为三个矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        U
       
       
        Σ
       
       
        
         V
        
        
         T
        
       
      
      
       UΣV^T
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.841331em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">U</span><span class="mord">Σ</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.22222em">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.841331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span></span></span></span></span>的乘积。为了计算矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         Σ
        
        
         +
        
       
      
      
       Σ^+
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.771331em;vertical-align: 0em"></span><span class="mord"><span class="mord">Σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.771331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span></span>，该算法取<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        Σ
       
      
      
       Σ
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.68333em;vertical-align: 0em"></span><span class="mord">Σ</span></span></span></span></span>并将所有小于一个小阈值的值设置为零，然后将所有非零值替换成它们的倒数，最后把结果矩阵转置。再加上它可以很好地处理边缘情况这种，方法比计算标准方程更有效。</p> 
<h3>
<a id="412__97"></a>4.1.2 计算复杂度</h3> 
<p>标准方程计算XT X的逆，XT X是一个（n+1）×（n+1）的矩阵（n是特征数量）。对这种矩阵求逆的计算复杂度通常为O（n2.4）到O（n3）之间，取决于具体实现。换句话说，如果将特征数量翻倍，那么计算时间将乘以大约22.4=5.3倍到23=8倍之间。</p> 
<p>Scikit-Learn的LinearRegression类使用的SVD方法的复杂度约为O（n2）。如果你将特征数量加倍，那计算时间大约是原来的4倍。</p> 
<p>特征数量比较大（例如100 000）时，标准方程和SVD的计算将极其缓慢。好的一面是，相对于训练集中的实例数量（O（m））来说，两个都是线性的，所以能够有效地处理大量的训练集，只要内存足够。</p> 
<p>同样，线性回归模型一经训练（不论是标准方程还是其他算法），预测就非常快速：因为计算复杂度相对于想要预测的实例数量和特征数量来说都是线性的。换句话说，对两倍的实例（或者是两倍的特征数）进行预测，大概需要两倍的时间。</p> 
<p>现在，我们再看几个截然不同的线性回归模型的训练方法，这些方法更适合特征数或者训练实例数量大到内存无法满足要求的场景。</p> 
<h2>
<a id="42__111"></a>4.2 梯度下降及其算法</h2> 
<p>梯度下降是一种非常通用的<strong>优化算法</strong>，能够为大范围的问题找到最优解。梯度下降的中心思想就是迭代地调整参数从而使成本函数最小化。</p> 
<p>**假设你迷失在山上的浓雾之中，你能感觉到的只有你脚下路面的坡度。快速到达山脚的一个策略就是沿着最陡的方向下坡。**这就是梯度下降的做法：<strong>通过测量参数向量θ相关的误差函数的局部梯度，并不断沿着降低梯度的方向调整，直到梯度降为0，到达最小值</strong>！</p> 
<p>具体来说，首先使用一个随机的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        θ
       
      
      
       θ
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span></span></span></span></span>值（这被称为随机初始化），然后逐步改进，每次踏出一步，每一步都尝试降低一点成本函数（如MSE），直到算法收敛出一个最小值（参见图4-3）。</p> 
<p><img src="https://images2.imgbox.com/5e/48/dfQp930m_o.png" alt="在这里插入图片描述"></p> 
<p>梯度下降中一个重要参数是每一步的步长，这取决于超参数学习率。如果学习率太低，算法需要经过大量迭代才能收敛，这将耗费很长时间；反过来说，如果学习率太高，那你可能会越过山谷直接到达另一边，甚至有可能比之前的起点还要高。这会导致算法发散，值越来越大，最后无法找到好的解决方案。</p> 
<p>最后，并不是所有的成本函数看起来都像一个漂亮的碗。有的可能看着像洞、山脉、高原或者各种不规则的地形，导致很难收敛到最小值。下图显示了梯度下降的两个主要挑战：如果随机初始化，算法从左侧起步，那么会收敛到一个局部最小值，而不是全局最小值。如果算法从右侧起步，那么需要经过很长时间才能越过整片高原，如果你停下得太早，将永远达不到全局最小值。</p> 
<p><img src="https://images2.imgbox.com/27/00/QA8vbHa2_o.png" alt="在这里插入图片描述"></p> 
<p>幸好，线性回归模型的MSE成本函数恰好是个凸函数，这意味着连接曲线上任意两点的线段永远不会跟曲线相交。也就是说，不存在局部最小值，只有一个全局最小值。它同时也是一个连续函数，所以斜率不会产生陡峭的变化。这两点保证的结论是：即便是乱走，梯度下降都可以趋近到全局最小值（只要等待时间足够长，学习率也不是太高）。</p> 
<p>成本函数虽然是碗状的，但如果不同特征的尺寸差别巨大，那它可能是一个非常细长的碗。如图4-7所示的梯度下降，左边的训练集上特征1和特征2具有相同的数值规模，而右边的训练集上，特征1的值则比特征2要小得多（注：因为特征1的值较小，所以θ1需要更大的变化来影响成本函数，这就是为什么碗形会沿着θ1轴拉长。）。</p> 
<p><img src="https://images2.imgbox.com/d5/10/IYog4gIY_o.png" alt="在这里插入图片描述"></p> 
<p>正如你所见，左图的梯度下降算法直接走向最小值，可以快速到达。而在右图中，先是沿着与全局最小值方向近乎垂直的方向前进，接下来是一段几乎平坦的长长的山谷。最终还是会抵达最小值，但是这需要花费大量的时间。</p> 
<blockquote> 
 <p>应用梯度下降时，需要保证所有特征值的大小比例都差不多（比如使用Scikit-Learn的<strong>StandardScaler</strong>类），否则收敛的时间会长很多。</p> 
</blockquote> 
<p>上图也说明，训练模型也就是搜寻使成本函数（在训练集上）最小化的参数组合。这是模型参数空间层面上的搜索：模型的参数越多，这个空间的维度就越多，搜索就越难。同样是在干草堆里寻找一根针，在一个三百维的空间里就比在一个三维空间里要棘手得多。幸运的是，线性回归模型的成本函数是凸函数，针就躺在碗底。</p> 
<h3>
<a id="421_Batch_Gradient_DescentBGD_144"></a>4.2.1 批量梯度下降（Batch Gradient Descent，BGD）</h3> 
<p>要实现梯度下降，你需要计算每个模型关于参数<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         θ
        
        
         j
        
       
      
      
       θ_j
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.980548em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span></span></span></span></span>的成本函数的梯度。换言之，你需要计算的是如果改变<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         θ
        
        
         j
        
       
      
      
       θ_j
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.980548em;vertical-align: -0.286108em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.05724em">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.286108em"><span class=""></span></span></span></span></span></span></span></span></span></span>，成本函数会改变多少。这被称为偏导数。</p> 
<p><img src="https://images2.imgbox.com/d9/2a/sZUZznee_o.png" alt="在这里插入图片描述"></p> 
<p>如果不想单独计算这些偏导数，可以使用公式对其进行一次性计算。梯度向量记作<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        ▽
       
       
        θ
       
       
        M
       
       
        S
       
       
        E
       
       
        (
       
       
        θ
       
       
        )
       
      
      
       ▽θMSE(θ)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord">▽</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mord mathdefault" style="margin-right: 0.10903em">M</span><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="mord mathdefault" style="margin-right: 0.05764em">E</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span></span></span></span></span>，包含所有成本函数（每个模型参数一个）的偏导数。</p> 
<p><img src="https://images2.imgbox.com/ef/c5/1O6aVcLz_o.png" alt="在这里插入图片描述"></p> 
<p>请注意，在计算梯度下降的每一步时，都是基于完整的训练集X的。这就是为什么该算法会被称为批量梯度下降：每一步都使用整批训练数据（实际上，全梯度下降可能是个更好的名字）。因此，面对非常庞大的训练集时，算法会变得极慢（不过我们即将看到快得多的梯度下降算法）。但是，梯度下降算法随特征数量扩展的表现比较好。如果要训练的线性模型拥有几十万个特征，使用梯度下降比标准方程或者SVD要快得多。</p> 
<p>一旦有了梯度向量，哪个点向上，就朝反方向下坡。也就是从<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        θ
       
      
      
       θ
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span></span></span></span></span>中减去<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        ▽
       
       
        θ
       
       
        M
       
       
        S
       
       
        E
       
       
        (
       
       
        θ
       
       
        )
       
      
      
       ▽θMSE(θ)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord">▽</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mord mathdefault" style="margin-right: 0.10903em">M</span><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="mord mathdefault" style="margin-right: 0.05764em">E</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span></span></span></span></span>。这时学习率<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        η
       
      
      
       η
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">η</span></span></span></span></span>就发挥作用了：用梯度向量乘以<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        η
       
      
      
       η
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.625em;vertical-align: -0.19444em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">η</span></span></span></span></span>确定下坡步长的大小。</p> 
<p><img src="https://images2.imgbox.com/2d/45/KjakLYX9_o.png" alt="在这里插入图片描述"></p> 
<p>让我们看一下该算法的快速实现：</p> 
<pre><code class="prism language-python">eta <span class="token operator">=</span> <span class="token number">0.1</span>  <span class="token comment"># learning rate</span>
n_iterations <span class="token operator">=</span> <span class="token number">1000</span>
m <span class="token operator">=</span> <span class="token number">100</span>
theta <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># random initialization</span>
<span class="token keyword">for</span> iteration <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_iterations<span class="token punctuation">)</span><span class="token punctuation">:</span>
    gradients <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">/</span>m <span class="token operator">*</span> X_b<span class="token punctuation">.</span>T<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>X_b<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>theta<span class="token punctuation">)</span> <span class="token operator">-</span> y<span class="token punctuation">)</span>
    theta <span class="token operator">=</span> theta <span class="token operator">-</span> eta <span class="token operator">*</span> gradients
    <span class="token keyword">print</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span>
</code></pre> 
<p>下图展现了分别使用三种不同的学习率时，梯度下降的前十步（虚线表示起点）。</p> 
<p><img src="https://images2.imgbox.com/01/e3/CNpdrGnu_o.png" alt="在这里插入图片描述"></p> 
<p>要找到合适的学习率，可以使用⭐<strong>网格搜索</strong>⭐（见第2章）。但是你可能需要限制迭代次数，这样网格搜索可以淘汰掉那些收敛耗时太长的模型。</p> 
<p>你可能会问，要怎么限制迭代次数呢？如果设置太低，算法可能在离最优解还很远时就停了。但是如果设置得太高，模型达到最优解后，继续迭代则参数不再变化，又会浪费时间。一个简单的办法是在开始时设置一个非常大的迭代次数，但是当梯度向量的值变得很微小时中断算法——也就是当它的范数变得低于（称为容差）时，因为这时梯度下降已经（几乎）到达了最小值。</p> 
<blockquote> 
 <p>收敛速度：</p> 
 <p>成本函数为凸函数，并且斜率没有陡峭的变化时（如MSE成本函数），具有固定学习率的批量梯度下降最终会收敛到最佳解，但是你需要等待一段时间：它可以进行O（1/∈）次迭代以在∈的范围内达到最佳值，具体取决于成本函数的形状。换句话说，如果将容差缩小为原来的1/10（以得到更精确的解），算法将不得不运行10倍的时间。</p> 
</blockquote> 
<h3>
<a id="422_Stochastic_Gradient_DescentSGD_191"></a>4.2.2 随机梯度下降（Stochastic Gradient Descent，SGD）</h3> 
<p>批量梯度下降的主要问题是它要用整个训练集来计算每一步的梯度，所以训练集很大时，算法会特别慢。与之相反的极端是随机梯度下降，每一步在训练集中随机选择一个实例，并且仅基于该单个实例来计算梯度。显然，这让算法变得快多了，因为每次迭代都只需要操作少量的数据。它也可以被用来训练海量的数据集，因为每次迭代只需要在内存中运行一个实例即可（SGD可以作为核外算法实现，见第1章）。</p> 
<p>另一方面，由于算法的随机性质，它比批量梯度下降要不规则得多。成本函数将不再是缓缓降低直到抵达最小值，而是不断上上下下，但是从整体来看，还是在慢慢下降。随着时间的推移，最终会非常接近最小值，但是即使它到达了最小值，依旧还会持续反弹，永远不会停止（见图4-9）。所以算法停下来的参数值肯定是足够好的，但不是最优的。</p> 
<p>当成本函数非常不规则时（见图4-6），随机梯度下降其实可以帮助算法跳出局部最小值，所以相比批量梯度下降，它对找到全局最小值更有优势。</p> 
<p>因此，随机性的好处在于可以逃离局部最优，但缺点是永远定位不出最小值。要解决这个困境，有一个办法是逐步降低学习率。开始的步长比较大（这有助于快速进展和逃离局部最小值），然后越来越小，让算法尽量靠近全局最小值。这个过程叫作<strong>模拟退火</strong>，因为它类似于冶金时熔化的金属慢慢冷却的退火过程。确定每个迭代学习率的函数叫作学习率调度。如果学习率降得太快，可能会陷入局部最小值，甚至是停留在走向最小值的半途中。如果学习率降得太慢，你需要太长时间才能跳到差不多最小值附近，如果提早结束训练，可能只得到一个次优的解决方案。</p> 
<p>下面这段代码使用了一个简单的学习率调度实现随机梯度下降：</p> 
<pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">learning_schedule</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> t0 <span class="token operator">/</span> <span class="token punctuation">(</span>t <span class="token operator">+</span> t1<span class="token punctuation">)</span>


theta <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment"># random initialization</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>n_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span><span class="token punctuation">:</span>
        random_index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>m<span class="token punctuation">)</span>
        xi <span class="token operator">=</span> X_b<span class="token punctuation">[</span>random_index<span class="token punctuation">:</span>random_index<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span>
        yi <span class="token operator">=</span> y<span class="token punctuation">[</span>random_index<span class="token punctuation">:</span>random_index<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span>
        gradients <span class="token operator">=</span> <span class="token number">2</span> <span class="token operator">*</span> xi<span class="token punctuation">.</span>T<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>xi<span class="token punctuation">.</span>dot<span class="token punctuation">(</span>theta<span class="token punctuation">)</span> <span class="token operator">-</span> yi<span class="token punctuation">)</span>
        eta <span class="token operator">=</span> learning_schedule<span class="token punctuation">(</span>epoch <span class="token operator">*</span> m <span class="token operator">+</span> i<span class="token punctuation">)</span>
        theta <span class="token operator">=</span> theta <span class="token operator">-</span> eta <span class="token operator">*</span> gradients
        <span class="token keyword">print</span><span class="token punctuation">(</span>theta<span class="token punctuation">)</span>
</code></pre> 
<p>按照惯例，我们进行m个回合的迭代。每个回合称为一个轮次。虽然批量梯度下降代码在整个训练集中进行了1000次迭代，但此代码仅在训练集中遍历了50次，并达到了一个很好的解决方案：</p> 
<blockquote> 
 <p>使用随机梯度下降时，训练实例必须独立且均匀分布（IID），以确保平均而言将参数拉向全局最优值。确保这一点的一种简单方法是在训练过程中对实例进行随机混洗（例如，随机选择每个实例，或者在每个轮次开始时随机混洗训练集）。如果不对实例进行混洗（例如，如果实例按标签排序），那么SGD将首先针对一个标签进行优化，然后针对下一个标签进行优化，以此类推，并且它不会接近全局最小值。</p> 
</blockquote> 
<p>要使用带有Scikit-Learn的随机梯度下降执行线性回归，可以使用<strong>SGDRegressor类</strong>，该类默认优化平方误差成本函数。以下代码最多可运行100000个轮次，或者直到一个轮次期间损失下降小于0.001为止（max_iter=1000，tol=1e-3）。它使用默认的学习调度（与前一个学习调度不同）以0.1（eta0=0.1）的学习率开始。最后，它不使用任何正则化（penalty=None，稍后将对此进行详细介绍）：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> SGDRegressor

sgd_reg <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span>max_iter<span class="token operator">=</span><span class="token number">100000</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> penalty<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> eta0<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>  <span class="token comment"># 实例化SGDRegressor的类</span>
sgd_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 训练模型</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sgd_reg<span class="token punctuation">.</span>intercept_<span class="token punctuation">,</span> sgd_reg<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>  <span class="token comment"># 输出模型的参数</span>
</code></pre> 
<h3>
<a id="423_MiniBatch_Gradient_Descent_236"></a>4.2.3 小批量梯度下降（Mini-Batch Gradient Descent）</h3> 
<p>我们要研究的最后一个梯度下降算法称为小批量梯度下降。只要你了解了批量和随机梯度下降，就很容易理解它：在每一步中，不是根据完整的训练集（如批量梯度下降）或仅基于一个实例（如随机梯度下降）来计算梯度，小批量梯度下降在称为小型批量的随机实例集上计算梯度。小批量梯度下降优于随机梯度下降的主要优点是，你可以通过矩阵操作的硬件优化来提高性能，特别是在使用GPU时。</p> 
<p>与随机梯度下降相比，该算法在参数空间上的进展更稳定，尤其是在相当大的小批次中。结果，小批量梯度下降最终将比随机梯度下降走得更接近最小值，但它<u>可能很难摆脱局部最小值</u>（在受局部最小值影响的情况下，不像线性回归）。下图显示了训练期间参数空间中三种梯度下降算法所采用的路径。它们最终都接近最小值，但是批量梯度下降的路径实际上是在最小值处停止，而随机梯度下降和小批量梯度下降都继续走动。但是，不要忘记批量梯度下降每步需要花费很多时间，如果你使用良好的学习率调度，随机梯度下降和小批量梯度下降也会达到最小值。</p> 
<p><img src="https://images2.imgbox.com/ed/c0/8OfWtSmu_o.png" alt="在这里插入图片描述"></p> 
<p>让我们比较到目前为止讨论的线性回归算法（回想一下，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        m
       
      
      
       m
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">m</span></span></span></span></span>是训练实例的数量，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        n
       
      
      
       n
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">n</span></span></span></span></span>是特征的数量）。</p> 
<p><img src="https://images2.imgbox.com/66/37/x9hJqJGo_o.png" alt="在这里插入图片描述"></p> 
<p>训练后几乎没有区别：所有这些算法最终都具有非常相似的模型，并且以完全相同的方式进行预测。</p> 
<h2>
<a id="43__254"></a>4.3 多项式回归</h2> 
<p>如果你的数据比直线更复杂怎么办？令人惊讶的是，你可以使用线性模型来拟合非线性数据。一个简单的方法就是将每个特征的幂次方添加为一个新特征，然后在此扩展特征集上训练一个线性模型。这种技术称为多项式回归。</p> 
<p>让我们看一个示例。</p> 
<p>首先，让我们基于一个简单的二次方程式（注：二次方程的形式为y=ax2+bx+c。）（加上一些噪声）生成一些非线性数据：</p> 
<pre><code class="prism language-python">m <span class="token operator">=</span> <span class="token number">100</span>
X <span class="token operator">=</span> <span class="token number">6</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>rand<span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">3</span>
y <span class="token operator">=</span> <span class="token number">0.5</span> <span class="token operator">*</span> X<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> X <span class="token operator">+</span> <span class="token number">2</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>m<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/15/07/XQf9aQmv_o.png" alt="在这里插入图片描述"></p> 
<p>显然，一条直线永远无法正确地拟合此数据。因此，让我们使用Scikit-Learn的PolynomialFeatures类来转换训练数据，将训练集中每个特征的平方（二次多项式）添加为新特征（在这种情况下，只有一个特征）：</p> 
<pre><code class="prism language-python"><span class="token comment"># 将每一个特征的平方都变成一个新的特征</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> PolynomialFeatures

poly_features <span class="token operator">=</span> PolynomialFeatures<span class="token punctuation">(</span>degree<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> include_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>
X_poly <span class="token operator">=</span> poly_features<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>X_poly<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre> 
<p>X_poly现在包含X的原始特征以及该特征的平方。现在你可以将LinearRegression模型拟合到此扩展训练数据中：</p> 
<pre><code class="prism language-python"><span class="token comment"># 进行预测</span>
lin_reg <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
lin_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_poly<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lin_reg<span class="token punctuation">.</span>intercept_<span class="token punctuation">,</span> lin_reg<span class="token punctuation">.</span>coef_<span class="token punctuation">)</span>  <span class="token comment"># 输出参数</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/51/ad/KNz11MEC_o.png" alt="在这里插入图片描述"></p> 
<p>请注意，当存在多个特征时，多项式回归能够找到特征之间的关系（这是普通线性回归模型无法做到的）。PolynomialFeatures还可以将特征的所有组合添加到给定的多项式阶数。例如，如果有两个特征a和b，则degree=3的PolynomialFeatures不仅会添加特征a2、a3、b2和 b3，还会添加组合ab、a2b和ab2。</p> 
<blockquote> 
 <p><code>PolynomialFeatures(degree=d)</code>可以将一个包含<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         n
        
       
       
        n
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">n</span></span></span></span></span>个特征的数组转换为包含<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          
           (
          
          
           n
          
          
           +
          
          
           d
          
          
           )
          
          
           !
          
         
         
          
           d
          
          
           !
          
          
           n
          
          
           !
          
         
        
       
       
        frac{(n+d)!}{d!n!}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.355em;vertical-align: -0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.01em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mclose mtight">!</span><span class="mord mathdefault mtight">n</span><span class="mclose mtight">!</span></span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight">n</span><span class="mbin mtight">+</span><span class="mord mathdefault mtight">d</span><span class="mclose mtight">)</span><span class="mclose mtight">!</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.345em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>个特征的数组，其中<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         n
        
        
         !
        
       
       
        n!
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault">n</span><span class="mclose">!</span></span></span></span></span>是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         n
        
       
       
        n
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">n</span></span></span></span></span>的阶乘，等于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         1
        
        
         ×
        
        
         2
        
        
         ×
        
        
         3
        
        
         ×
        
        
         …
        
        
         ×
        
        
         n
        
       
       
        1×2×3×…×n
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">2</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.72777em;vertical-align: -0.08333em"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.66666em;vertical-align: -0.08333em"></span><span class="minner">…</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault">n</span></span></span></span></span>。要小心特征组合的数量爆炸。</p> 
</blockquote> 
<h2>
<a id="44__301"></a>4.4 学习曲线</h2> 
<p>你如果执行高阶多项式回归，与普通线性回归相比，拟合数据可能会更好。</p> 
<p>这种高阶多项式回归模型严重过拟合训练数据，而线性模型则欠拟合。在这种情况下，最能泛化的模型是二次模型，因为数据是使用二次模型生成的。但是总的来说，你不知道数据由什么函数生成，那么如何确定模型的复杂性呢？你如何判断模型是过拟合数据还是欠拟合数据呢？</p> 
<p>在第2章中，你使用交叉验证来估计模型的泛化性能。⭐**如果模型在训练数据上表现良好，但根据交叉验证的指标泛化较差，则你的模型过拟合。如果两者的表现均不理想，则说明欠拟合。**⭐这是一种区别模型是否过于简单或过于复杂的方法。</p> 
<p>还有一种方法是观察学习曲线：这个曲线绘制的是模型在训练集和验证集上关于训练集大小（或训练迭代）的性能函数。要生成这个曲线，只需要在不同大小的训练子集上多次训练模型即可。下面这段代码在给定训练集下定义了一个函数，绘制模型的学习曲线：</p> 
<pre><code class="prism language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>metrics <span class="token keyword">import</span> mean_squared_error
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split


<span class="token keyword">def</span> <span class="token function">plot_learning_curves</span><span class="token punctuation">(</span>model<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
    X_train<span class="token punctuation">,</span> X_val<span class="token punctuation">,</span> y_train<span class="token punctuation">,</span> y_val <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
    train_errors<span class="token punctuation">,</span> val_errors <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    <span class="token keyword">for</span> m <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>X_train<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span><span class="token punctuation">,</span> y_train<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span>
        y_train_predict <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_train<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span>
        y_val_predict <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_val<span class="token punctuation">)</span>
        train_errors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_train<span class="token punctuation">[</span><span class="token punctuation">:</span>m<span class="token punctuation">]</span><span class="token punctuation">,</span> y_train_predict<span class="token punctuation">)</span><span class="token punctuation">)</span>
        val_errors<span class="token punctuation">.</span>append<span class="token punctuation">(</span>mean_squared_error<span class="token punctuation">(</span>y_val<span class="token punctuation">,</span> y_val_predict<span class="token punctuation">)</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>train_errors<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"r-+"</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"train"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>val_errors<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">"b-"</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"val"</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>


<span class="token comment"># 看一下普通线性回归模型的学习曲线</span>
lin_reg <span class="token operator">=</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
plot_learning_curves<span class="token punctuation">(</span>lin_reg<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/e7/4b/rWB2xRvL_o.png" alt="在这里插入图片描述"></p> 
<p>这种欠拟合的模型值得解释一下。首先，让我们看一下在训练数据上的性能：当训练集中只有一个或两个实例时，模型可以很好地拟合它们，这就是曲线从零开始的原因。但是，随着将新实例添加到训练集中，模型就不可能完美地拟合训练数据，这既因为数据有噪声，又因为它根本不是线性的。因此，训练数据上的误差会一直上升，直到达到平稳状态，此时在训练集中添加新实例并不会使平均误差变好或变差。现在让我们看一下模型在验证数据上的性能。当在很少的训练实例上训练模型时，它无法正确泛化，这就是验证误差最初很大的原因。然后，随着模型经历更多的训练示例，它开始学习，因此验证错误逐渐降低。但是，直线不能很好地对数据进行建模，因此误差最终达到一个平稳的状态，非常接近另外一条曲线。</p> 
<p>这些学习曲线是典型的欠拟合模型。两条曲线都达到了平稳状态。它们很接近而且很高。</p> 
<blockquote> 
 <p>如果你的模型欠拟合训练数据，添加更多训练示例将无济于事。你需要使用更复杂的模型或提供更好的特征。</p> 
</blockquote> 
<p>现在让我们看一下在相同数据上的10阶多项式模型的学习曲线：</p> 
<pre><code class="prism language-python"><span class="token comment"># 在相同数据上的10阶多项式模型的学习曲线</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>pipeline <span class="token keyword">import</span> Pipeline

polynomial_regression <span class="token operator">=</span> Pipeline<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"poly_features"</span><span class="token punctuation">,</span> PolynomialFeatures<span class="token punctuation">(</span>degree<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> include_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">"lin_reg"</span><span class="token punctuation">,</span> LinearRegression<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plot_learning_curves<span class="token punctuation">(</span>polynomial_regression<span class="token punctuation">,</span> X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/66/db/SF5fghTM_o.png" alt="在这里插入图片描述"></p> 
<p>这些学习曲线看起来有点像以前的曲线，但是有两个非常重要的区别：</p> 
<ul>
<li> <p>与线性回归模型相比，训练数据上的误差要低得多。</p> </li>
<li> <p>曲线之间存在间隙。这意味着该模型在训练数据上的性能要比在</p> </li>
</ul> 
<p>验证数据上的性能好得多，这是过拟合模型的标志。但是，如果你使用更大的训练集，则两条曲线会继续接近。</p> 
<blockquote> 
 <p>改善过拟合模型的一种方法是向其提供更多的训练数据，直到验证误差达到训练误差为止。</p> 
</blockquote> 
<h3>
<a id="_369"></a>偏差/方差权衡</h3> 
<p>统计学和机器学习的重要理论成果是以下事实：模型的泛化误差可以表示为三个非常不同的误差之和：</p> 
<ul>
<li>偏差：这部分泛化误差的原因在于错误的假设，比如假设数据是线性的，而实际上是二次的。高偏差模型最有可能欠拟合训练数据。</li>
<li>方差：这部分是由于模型对训练数据的细微变化过于敏感。具有许多自由度的模型（例如高阶多项式模型）可能具有较高的方差，因此可能过拟合训练数据。</li>
<li>不可避免的误差这部分误差是因为数据本身的噪声所致。减少这部分误差的唯一方法就是清理数据（例如修复数据源（如损坏的传感器），或者检测并移除异常值）。</li>
</ul> 
<p>增加模型的复杂度通常会显著提升模型的方差并减少偏差。反过来，降低模型的复杂度则会提升模型的偏差并降低方差。这就是为什么称其为权衡。</p> 
<blockquote> 
 <p>不要将这里的偏差概念与线性模型中的偏置项概念弄混。</p> 
</blockquote> 
<h2>
<a id="45__383"></a>4.5 线性模型的正则化</h2> 
<p>正如我们在第1章和第2章中看到的那样，减少过拟合的一个好方法是对模型进行正则化（即约束模型）：它拥有的自由度越少，则过拟合数据的难度就越大。<u>正则化多项式模型的一种简单方法是减少多项式的次数。</u></p> 
<p>对于线性模型，正则化通常是通过约束模型的权重来实现的。现在，我们看一下岭回归、Lasso回归和弹性网络，它们实现了三种限制权重的方法。</p> 
<h3>
<a id="451__391"></a>4.5.1 岭回归</h3> 
<p>岭回归（也称为Tikhonov正则化）是线性回归的正则化版本：将等于<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        α
       
       
        
         ∑
        
        
         
          i
         
         
          =
         
         
          1
         
        
        
         n
        
       
       
        
         θ
        
        
         i
        
        
         2
        
       
      
      
       αsum^n_{i=1}θ_i^2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1.11382em;vertical-align: -0.29971em"></span><span class="mord mathdefault" style="margin-right: 0.0037em">α</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop"><span class="mop op-symbol small-op">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.804292em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.29971em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.814108em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.258664em"><span class=""></span></span></span></span></span></span></span></span></span></span>的正则化项添加到成本函数。这迫使学习算法不仅拟合数据，而且还使模型权重尽可能小。注意：<u>仅在训练期间将正则化项添加到成本函数中</u>。训练完模型后，你要使用非正则化的性能度量来评估模型的性能。</p> 
<blockquote> 
 <p>训练过程中使用的成本函数与用于测试的性能指标不同是很常见的。除正则化外，它们可能不同的另一个原因是好的训练成本函数应该具有对优化友好的导数，而用于测试的性能指标应尽可能接近最终目标。例如，通常使用成本函数（例如对数损失（稍后讨论））来训练分类器，但使用精度/召回率对其进行评估。</p> 
</blockquote> 
<p>超参数α控制要对模型进行正则化的程度。如果α=0，则岭回归仅是线性回归。如果α非常大，则所有权重最终都非常接近于零，结果是一条经过数据均值的平线。公式给出了岭回归成本函数：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         岭
        
        
         回
        
        
         归
        
        
         成
        
        
         本
        
        
         函
        
        
         数
        
        
         ：
        
        
         J
        
        
         (
        
        
         θ
        
        
         )
        
        
         =
        
        
         M
        
        
         S
        
        
         E
        
        
         (
        
        
         θ
        
        
         )
        
        
         +
        
        
         α
        
        
         
          1
         
         
          2
         
        
        
         
          ∑
         
         
          
           i
          
          
           =
          
          
           1
          
         
         
          n
         
        
        
         
          θ
         
         
          i
         
         
          2
         
        
        
        
         闭
        
        
         式
        
        
         解
        
        
         的
        
        
         岭
        
        
         回
        
        
         归
        
        
         ：
        
        
         
          θ
         
         
          ′
         
        
        
         =
        
        
         (
        
        
         
          X
         
         
          T
         
        
        
         X
        
        
         +
        
        
         α
        
        
         A
        
        
         
          )
         
         
          
           −
          
          
           1
          
         
        
        
         
          X
         
         
          T
         
        
        
         y
        
       
       
         岭回归成本函数：J(θ) = MSE(θ) + αfrac{1}{2}sum^n_{i=1}θ^2_i\ 闭式解的岭回归：θ' = (X^TX+αA)^{-1}X^Ty 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord cjk_fallback">岭</span><span class="mord cjk_fallback">回</span><span class="mord cjk_fallback">归</span><span class="mord cjk_fallback">成</span><span class="mord cjk_fallback">本</span><span class="mord cjk_fallback">函</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">：</span><span class="mord mathdefault" style="margin-right: 0.09618em">J</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">M</span><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="mord mathdefault" style="margin-right: 0.05764em">E</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 2.92907em;vertical-align: -1.27767em"></span><span class="mord mathdefault" style="margin-right: 0.0037em">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">2</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.864108em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em"><span class=""></span></span></span></span></span></span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height: 0.801892em;vertical-align: 0em"></span><span class="mord cjk_fallback">闭</span><span class="mord cjk_fallback">式</span><span class="mord cjk_fallback">解</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">岭</span><span class="mord cjk_fallback">回</span><span class="mord cjk_fallback">归</span><span class="mord cjk_fallback">：</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1.14133em;vertical-align: -0.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 1.14133em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.0037em">α</span><span class="mord mathdefault">A</span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.864108em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.03588em">y</span></span></span></span></span></span><br> ⭐在执行岭回归之前缩放数据（例如使用StandardScaler）很重要，因为它对输入特征的缩放敏感。大多数正则化模型都需要如此。</p> 
<p>以下是用Scikit-Learn和闭式解（方程式4-9的一种变体，它使用AndréLouis Cholesky矩阵分解技术）来执行岭回归的方法：</p> 
<pre><code class="prism language-python"><span class="token comment"># 岭回归求解</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> Ridge

<span class="token comment"># 直接使用Ridge()函数</span>
ridge_reg <span class="token operator">=</span> Ridge<span class="token punctuation">(</span>alpha<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> solver<span class="token operator">=</span><span class="token string">"cholesky"</span><span class="token punctuation">)</span>
ridge_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>ridge_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 或者直接使用随机梯度下降法：</span>
sgd_reg <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span>penalty<span class="token operator">=</span><span class="token string">"l2"</span><span class="token punctuation">)</span>  <span class="token comment"># 超参数penalty设置的是使用正则项的类型。</span>
<span class="token comment"># 设为"l2"表示希望SGD在成本函数中添加一个正则项，等于权重向量的l2范数的平方的一半，即岭回归。</span>
sgd_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 需要对y进行一个塑形</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sgd_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="452_Lasso_424"></a>4.5.2 Lasso回归</h3> 
<p>线性回归的另一种正则化叫作<u>最小绝对收缩和选择算子回归</u>（Least Absolute Shrinkage and Selection Operator Regression，简称Lasso回归）。与岭回归一样，它也是向成本函数添加一个正则项，但是它增加的是权重向量的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        l
       
       
        1
       
      
      
       l1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">1</span></span></span></span></span>范数，而不是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        l
       
       
        2
       
      
      
       l2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">2</span></span></span></span></span>范数的平方的一半。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         L
        
        
         a
        
        
         s
        
        
         s
        
        
         o
        
        
         回
        
        
         归
        
        
         成
        
        
         本
        
        
         函
        
        
         数
        
        
         ：
        
        
         J
        
        
         (
        
        
         θ
        
        
         )
        
        
         =
        
        
         M
        
        
         S
        
        
         E
        
        
         (
        
        
         θ
        
        
         )
        
        
         +
        
        
         α
        
        
         
          1
         
         
          2
         
        
        
         
          ∑
         
         
          
           i
          
          
           =
          
          
           1
          
         
         
          n
         
        
        
         ∣
        
        
         
          θ
         
         
          i
         
        
        
         ∣
        
        
       
       
         Lasso回归成本函数：J(θ) = MSE(θ)+αfrac{1}{2}sum^n_{i=1}|θ_i|\ 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault">L</span><span class="mord mathdefault">a</span><span class="mord mathdefault">s</span><span class="mord mathdefault">s</span><span class="mord mathdefault">o</span><span class="mord cjk_fallback">回</span><span class="mord cjk_fallback">归</span><span class="mord cjk_fallback">成</span><span class="mord cjk_fallback">本</span><span class="mord cjk_fallback">函</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">：</span><span class="mord mathdefault" style="margin-right: 0.09618em">J</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">M</span><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="mord mathdefault" style="margin-right: 0.05764em">E</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 2.92907em;vertical-align: -1.27767em"></span><span class="mord mathdefault" style="margin-right: 0.0037em">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">2</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord">∣</span></span><span class="mspace newline"></span></span></span></span></span></p> 
<p><img src="https://images2.imgbox.com/c8/ca/NsCiWTmH_o.png" alt="在这里插入图片描述"></p> 
<p>Lasso回归的一个重要特点是它倾向于完全消除掉最不重要特征的权重（也就是将它们设置为零）。换句话说，Lasso回归会自动执行特征选择并输出一个稀疏模型（即只有很少的特征有非零权重）。</p> 
<blockquote> 
 <p>为了避免在使用Lasso时梯度下降最终在最优解附近反弹，你需要逐渐降低训练期间的学习率（它仍然会在最优解附近反弹，但是步长会越来越小，因此会收敛）。</p> 
</blockquote> 
<p>这是一个使用Lasso类的Scikit-Learn小示例：</p> 
<pre><code class="prism language-python"><span class="token comment"># Lasso回归</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> Lasso

<span class="token comment"># 直接使用Lasso()函数</span>
lasso_reg <span class="token operator">=</span> Lasso<span class="token punctuation">(</span>alpha<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span>
lasso_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>lasso_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 或者使用SGDRegressor（penalty="l1"）</span>
sgd_lasso_reg <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span>penalty<span class="token operator">=</span><span class="token string">'l1'</span><span class="token punctuation">)</span>
sgd_lasso_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 需要对y进行一个塑形</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sgd_lasso_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="453__457"></a>4.5.3 弹性网络</h3> 
<p>弹性网络是介于岭回归和Lasso回归之间的中间地带。正则项是岭和Lasso正则项的简单混合，你可以控制混合比r。当r=0时，弹性网络等效于岭回归，而当r=1时，弹性网络等效于Lasso回归。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         弹
        
        
         性
        
        
         网
        
        
         络
        
        
         成
        
        
         本
        
        
         函
        
        
         数
        
        
         ：
        
        
         J
        
        
         (
        
        
         θ
        
        
         )
        
        
         =
        
        
         M
        
        
         S
        
        
         E
        
        
         (
        
        
         θ
        
        
         )
        
        
         +
        
        
         r
        
        
         α
        
        
         
          1
         
         
          2
         
        
        
         
          ∑
         
         
          
           i
          
          
           =
          
          
           1
          
         
         
          n
         
        
        
         ∣
        
        
         
          θ
         
         
          i
         
        
        
         ∣
        
        
         +
        
        
         
          
           1
          
          
           −
          
          
           r
          
         
         
          2
         
        
        
         α
        
        
         
          ∑
         
         
          
           i
          
          
           =
          
          
           1
          
         
         
          n
         
        
        
         
          θ
         
         
          i
         
         
          2
         
        
       
       
         弹性网络成本函数：J(θ) = MSE(θ)+rαfrac{1}{2}sum^n_{i=1}|θ_i|+frac{1-r}{2}αsum^n_{i=1}θ^2_i 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord cjk_fallback">弹</span><span class="mord cjk_fallback">性</span><span class="mord cjk_fallback">网</span><span class="mord cjk_fallback">络</span><span class="mord cjk_fallback">成</span><span class="mord cjk_fallback">本</span><span class="mord cjk_fallback">函</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">：</span><span class="mord mathdefault" style="margin-right: 0.09618em">J</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.10903em">M</span><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="mord mathdefault" style="margin-right: 0.05764em">E</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 2.92907em;vertical-align: -1.27767em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">r</span><span class="mord mathdefault" style="margin-right: 0.0037em">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">2</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord">∣</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.311664em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mord">∣</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span></span><span class="base"><span class="strut" style="height: 2.92907em;vertical-align: -1.27767em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">2</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.686em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathdefault" style="margin-right: 0.0037em">α</span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.6514em"><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span class=""><span class="pstrut" style="height: 3.05em"></span><span class=""><span class="mop op-symbol large-op">∑</span></span></span><span class="" style="margin-left: 0em"><span class="pstrut" style="height: 3.05em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.27767em"><span class=""></span></span></span></span></span><span class="mspace" style="margin-right: 0.166667em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.864108em"><span class="" style="margin-left: -0.02778em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.247em"><span class=""></span></span></span></span></span></span></span></span></span></span></span><br> 那么什么时候应该使用普通的线性回归（即不进行任何正则化）、岭(Ridge)、Lasso或弹性网络呢？</p> 
<p>⭐通常来说，有正则化——哪怕很小，总比没有更可取一些。所以大多数情况下，你应该避免使用纯线性回归。岭回归是个不错的默认选择，但是如果你觉得实际用到的特征只有少数几个，那就应该更倾向于Lasso回归或是弹性网络，因为它们会将无用特征的权重降为零。一般而言，弹性网络优于Lasso回归，因为当特征数量超过训练实例数量，又或者是几个特征强相关时，Lasso回归的表现可能非常不稳定。⭐</p> 
<p>这是一个使用Scikit-Learn的ElasticNet的小示例（<code>l1_ratio</code>对应于混合比r）：</p> 
<pre><code class="prism language-python"><span class="token comment"># 弹性回归ElasticNet</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> ElasticNet

<span class="token comment"># 使用ElasticNet()函数</span>
elastic_net <span class="token operator">=</span> ElasticNet<span class="token punctuation">(</span>alpha<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> l1_ratio<span class="token operator">=</span><span class="token number">0.5</span><span class="token punctuation">)</span>
elastic_net<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>elastic_net<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token comment"># 使用SGDRegressor（penalty="elasticnet"）</span>
sgd_lasso_reg <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span>penalty<span class="token operator">=</span><span class="token string">'elasticnet'</span><span class="token punctuation">)</span>
sgd_lasso_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">.</span>ravel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># 需要对y进行一个塑形</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>sgd_lasso_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre> 
<h3>
<a id="454__486"></a>4.5.4 提前停止</h3> 
<p>对于梯度下降这一类迭代学习的算法，还有一个与众不同的正则化方法，就是在验证误差达到最小值时停止训练，该方法叫作提前停止法。下图展现了一个用批量梯度下降训练的复杂模型（高阶多项式回归模型）。经过一轮一轮的训练，算法不断地学习，训练集上的预测误差（RMSE）自然不断下降，同样其在验证集上的预测误差也随之下降。但是，一段时间之后，验证误差停止下降反而开始回升。这说明模型开始过拟合训练数据。通过早期停止法，一旦验证误差达到最小值就立刻停止训练。这是一个非常简单而有效的正则化技巧，所以Geoffrey Hinton称其为“美丽的免费午餐”。</p> 
<p>使用随机和小批量梯度下降时，曲线不是那么平滑，可能很难知道你是否达到了最小值。一种解决方案是仅在验证错误超过最小值一段时间后停止（当你确信模型不会做得更好时），然后回滚模型参数到验证误差最小的位置。以下是提前停止法的基本实现：</p> 
<pre><code class="prism language-python"><span class="token comment"># 提前停止法的基本实现：</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>base <span class="token keyword">import</span> clone
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>preprocessing <span class="token keyword">import</span> StandardScaler

<span class="token comment"># prepare the data</span>
poly_scaler <span class="token operator">=</span> Pipeline<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"poly_features"</span><span class="token punctuation">,</span> PolynomialFeatures<span class="token punctuation">(</span>degree<span class="token operator">=</span><span class="token number">90</span><span class="token punctuation">,</span> include_bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">"std_scaler"</span><span class="token punctuation">,</span> StandardScaler<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
X_train_poly_scaled <span class="token operator">=</span> poly_scaler<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>X_train<span class="token punctuation">)</span>  <span class="token comment"># 训练集完成数据处理</span>
X_val_poly_scaled <span class="token operator">=</span> poly_scaler<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>X_val<span class="token punctuation">)</span>  <span class="token comment"># 验证集完成数据处理</span>
sgd_reg <span class="token operator">=</span> SGDRegressor<span class="token punctuation">(</span>max_iter<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> tol<span class="token operator">=</span><span class="token operator">-</span>np<span class="token punctuation">.</span>infty<span class="token punctuation">,</span> warm_start<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> penalty<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> learning_rate<span class="token operator">=</span><span class="token string">"constant"</span><span class="token punctuation">,</span> eta0<span class="token operator">=</span><span class="token number">0.0005</span><span class="token punctuation">)</span>
<span class="token comment"># 请注意，在使用warm_start=True的情况下，当调用fit（）方法时，它将在停止的地方继续训练，而不是从头开始。</span>
minimum_val_error <span class="token operator">=</span> <span class="token builtin">float</span><span class="token punctuation">(</span><span class="token string">"inf"</span><span class="token punctuation">)</span>
best_epoch <span class="token operator">=</span> <span class="token boolean">None</span>
best_model <span class="token operator">=</span> <span class="token boolean">None</span>
<span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    sgd_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X_train_poly_scaled<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>  <span class="token comment"># continues where it left off  在停止的地方继续训练 用训练集训练模型</span>
    y_val_predict <span class="token operator">=</span> sgd_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X_val_poly_scaled<span class="token punctuation">)</span>  <span class="token comment"># 预测验证集</span>
    val_error <span class="token operator">=</span> mean_squared_error<span class="token punctuation">(</span>y_val<span class="token punctuation">,</span> y_val_predict<span class="token punctuation">)</span>   <span class="token comment"># 计算均方误差</span>
    <span class="token keyword">if</span> val_error <span class="token operator">&lt;</span> minimum_val_error<span class="token punctuation">:</span>
        minimum_val_error <span class="token operator">=</span> val_error
        best_epoch <span class="token operator">=</span> epoch
        best_model <span class="token operator">=</span> clone<span class="token punctuation">(</span>sgd_reg<span class="token punctuation">)</span>
</code></pre> 
<h2>
<a id="46__518"></a>4.6 逻辑回归</h2> 
<p>正如第1章中提到过的，一些回归算法也可用于分类（反之亦然）。逻辑回归（Logistic回归，也称为Logit回归）被广泛用于估算一个实例属于某个特定类别的概率。（比如，这封电子邮件属于垃圾邮件的概率是多少？）如果预估概率超过50%，则模型预测该实例属于该类别（称为正类，标记为“1”），反之，则预测不是（称为负类，标记为“0”）。这样它就成了一个二元分类器。</p> 
<h3>
<a id="461__524"></a>4.6.1 估计概率</h3> 
<p>所以逻辑回归是怎么工作的呢？与线性回归模型一样，逻辑回归模型也是计算输入特征的加权和（加上偏置项），但是不同于线性回归模型直接输出结果，它输出的是结果的数理逻辑值。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         逻
        
        
         辑
        
        
         回
        
        
         归
        
        
         模
        
        
         型
        
        
         的
        
        
         估
        
        
         计
        
        
         概
        
        
         率
        
        
         （
        
        
         向
        
        
         量
        
        
         化
        
        
         形
        
        
         式
        
        
         ）
        
        
         ：
        
        
         
          p
         
         
          ′
         
        
        
         =
        
        
         
          h
         
         
          θ
         
        
        
         (
        
        
         x
        
        
         )
        
        
         =
        
        
         σ
        
        
         (
        
        
         
          x
         
         
          T
         
        
        
         θ
        
        
         )
        
        
        
         逻
        
        
         辑
        
        
         函
        
        
         数
        
        
         ：
        
        
         σ
        
        
         (
        
        
         t
        
        
         )
        
        
         =
        
        
         
          1
         
         
          
           1
          
          
           +
          
          
           e
          
          
           x
          
          
           p
          
          
           (
          
          
           −
          
          
           t
          
          
           )
          
         
        
       
       
         逻辑回归模型的估计概率（向量化形式）： p' = h_θ(x) = σ(x^Tθ)\ 逻辑函数：σ(t) = frac{1}{1+exp(-t)} 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.996332em;vertical-align: -0.19444em"></span><span class="mord cjk_fallback">逻</span><span class="mord cjk_fallback">辑</span><span class="mord cjk_fallback">回</span><span class="mord cjk_fallback">归</span><span class="mord cjk_fallback">模</span><span class="mord cjk_fallback">型</span><span class="mord cjk_fallback">的</span><span class="mord cjk_fallback">估</span><span class="mord cjk_fallback">计</span><span class="mord cjk_fallback">概</span><span class="mord cjk_fallback">率</span><span class="mord cjk_fallback">（</span><span class="mord cjk_fallback">向</span><span class="mord cjk_fallback">量</span><span class="mord cjk_fallback">化</span><span class="mord cjk_fallback">形</span><span class="mord cjk_fallback">式</span><span class="mord cjk_fallback">）</span><span class="mord cjk_fallback">：</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.02778em">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 1.14133em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.03588em">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord cjk_fallback">逻</span><span class="mord cjk_fallback">辑</span><span class="mord cjk_fallback">函</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">：</span><span class="mord mathdefault" style="margin-right: 0.03588em">σ</span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 2.25744em;vertical-align: -0.936em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.32144em"><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em"></span><span class="mord mathdefault">e</span><span class="mord mathdefault">x</span><span class="mord mathdefault">p</span><span class="mopen">(</span><span class="mord">−</span><span class="mord mathdefault">t</span><span class="mclose">)</span></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="frac-line" style="border-bottom-width: 0.04em"></span></span><span class=""><span class="pstrut" style="height: 3em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.936em"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span><br> 一旦逻辑回归模型估算出实例x属于正类的概率<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        
         p
        
        
         ′
        
       
      
      
       p'
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.946332em;vertical-align: -0.19444em"></span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span>，就可以做出预测：<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          y
         
         
          ′
         
        
        
         =
        
        
         
          {
         
         
          
           
            
             
              0
             
            
           
           
            
             
              
               如
              
              
               果
              
              
               
                p
               
               
                ′
               
              
              
               &lt;
              
              
               0.5
              
             
            
           
          
          
           
            
             
              1
             
            
           
           
            
             
              
               如
              
              
               果
              
              
               
                p
               
               
                ′
               
              
              
               ≥
              
              
               0.5
              
             
            
           
          
         
        
       
       
         y' = begin{cases}0 &amp; 如果p'&lt;0.5\1 &amp; 如果p'≥0.5end{cases} 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.996332em;vertical-align: -0.19444em"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em">y</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 3.00003em;vertical-align: -1.25003em"></span><span class="minner"><span class="mopen delimcenter"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em"><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord">0</span></span></span><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.69em"><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord cjk_fallback">如</span><span class="mord cjk_fallback">果</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">&lt;</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span><span class=""><span class="pstrut" style="height: 3.008em"></span><span class="mord"><span class="mord cjk_fallback">如</span><span class="mord cjk_fallback">果</span><span class="mord"><span class="mord mathdefault">p</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.751892em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">≥</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 1.19em"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></p> 
<blockquote> 
 <p>分数t通常称为logit。该名称源于以下事实：定义为logit（p）=log（p/（1–p））的logit函数与logistic函数相反。确实，如果你计算估计概率p的对数，则会发现结果为t。对数也称为对数奇数，因为它是正类别的估计概率与负类别的估计概率之比的对数。</p> 
</blockquote> 
<h3>
<a id="462__540"></a>4.6.2 训练和成本函数</h3> 
<p>现在你知道逻辑回归模型是如何估算概率并做出预测了。但是要怎么训练呢？训练的目的就是设置参数向量θ，使模型对正类实例做出高概率估算（y=1），对负类实例做出低概率估算（y=0）。</p> 
<p><img src="https://images2.imgbox.com/aa/33/K7f7c4hB_o.png" alt="在这里插入图片描述"></p> 
<p>这个成本函数是有道理的，因为当t接近于0时，-log（t）会变得非常大，所以如果模型估算一个正类实例的概率接近于0，成本将会变得很高。同理估算出一个负类实例的概率接近1，成本也会变得非常高。那么反过来，当t接近于1的时候，-log（t）接近于0，所以对一个负类实例估算出的概率接近于0，对一个正类实例估算出的概率接近于1，而成本则都接近于0，这不正好是我们想要的吗？整个训练集的成本函数是所有训练实例的平均成本。可以用一个称为对数损失的单一表达式来表示，见公式4-17。</p> 
<p><img src="https://images2.imgbox.com/57/cb/cjNUyqUi_o.png" alt="在这里插入图片描述"></p> 
<p>但是坏消息是，这个函数没有已知的闭式方程（不存在一个标准方程的等价方程）来计算出最小化成本函数的θ值。而好消息是这是个凸函数，所以通过梯度下降（或是其他任意优化算法）保证能够找出全局最小值（只要学习率不是太高，你又能长时间等待）。</p> 
<h3>
<a id="463__556"></a>4.6.3 决策边界（鸢尾植物数据集）</h3> 
<p>这里我们用鸢尾植物数据集来说明逻辑回归。这是一个非常著名的数据集，共有150朵鸢尾花，分别来自三个不同品种（山鸢尾、变色鸢尾和维吉尼亚鸢尾），数据里包含花的萼片以及花瓣的长度和宽度。</p> 
<p>我们试试仅基于花瓣宽度这一个特征，创建一个分类器来检测维吉尼亚鸢尾花。</p> 
<pre><code class="prism language-python"><span class="token comment"># 鸢尾花 逻辑回归</span>
<span class="token keyword">from</span> sklearn <span class="token keyword">import</span> datasets

<span class="token comment"># 导入数据</span>
iris <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_iris<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">list</span><span class="token punctuation">(</span>iris<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token comment"># 输出：['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']</span>
X <span class="token operator">=</span> iris<span class="token punctuation">[</span><span class="token string">"data"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">:</span><span class="token punctuation">]</span>  <span class="token comment"># petal width  花瓣宽度</span>
y <span class="token operator">=</span> <span class="token punctuation">(</span>iris<span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>astype<span class="token punctuation">(</span><span class="token builtin">int</span><span class="token punctuation">)</span>  <span class="token comment"># 1 if Iris virginica, else 0  如果是Iris virginica花，就是1，否则就是0</span>

<span class="token comment"># 训练一个逻辑回归模型</span>
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>linear_model <span class="token keyword">import</span> LogisticRegression

log_reg <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span><span class="token punctuation">)</span>
log_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

<span class="token comment"># 我们来看看花瓣宽度在0到3cm之间的鸢尾花，模型估算出的概率</span>
X_new <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
y_proba <span class="token operator">=</span> log_reg<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_new<span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_new<span class="token punctuation">,</span> y_proba<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"g-"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Iris virginica"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_new<span class="token punctuation">,</span> y_proba<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"b--"</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Not Iris virginica"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p><img src="https://images2.imgbox.com/1a/82/o4sIGTmQ_o.png" alt="在这里插入图片描述"></p> 
<p>上图绘图代码如下：</p> 
<pre><code class="prism language-python"><span class="token comment"># + more Matplotlib code to make the image look pretty</span>
X_new <span class="token operator">=</span> np<span class="token punctuation">.</span>linspace<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span><span class="token builtin">object</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>
y_proba <span class="token operator">=</span> log_reg<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span>X_new<span class="token punctuation">)</span>
decision_boundary <span class="token operator">=</span> X_new<span class="token punctuation">[</span>y_proba<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">&gt;=</span> <span class="token number">0.5</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X<span class="token punctuation">[</span>y <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span>y <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"bs"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X<span class="token punctuation">[</span>y <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> y<span class="token punctuation">[</span>y <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"g^"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span><span class="token punctuation">[</span>decision_boundary<span class="token punctuation">,</span> decision_boundary<span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"k:"</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_new<span class="token punctuation">,</span> y_proba<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"g-"</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Iris virginica"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>X_new<span class="token punctuation">,</span> y_proba<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">"b--"</span><span class="token punctuation">,</span> linewidth<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> label<span class="token operator">=</span><span class="token string">"Not Iris virginica"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>text<span class="token punctuation">(</span>decision_boundary<span class="token operator">+</span><span class="token number">0.02</span><span class="token punctuation">,</span> <span class="token number">0.15</span><span class="token punctuation">,</span> <span class="token string">"Decision  boundary"</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">,</span> color<span class="token operator">=</span><span class="token string">"k"</span><span class="token punctuation">,</span> ha<span class="token operator">=</span><span class="token string">"center"</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>arrow<span class="token punctuation">(</span>decision_boundary<span class="token punctuation">,</span> <span class="token number">0.08</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> head_width<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> head_length<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> fc<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">,</span> ec<span class="token operator">=</span><span class="token string">'b'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>arrow<span class="token punctuation">(</span>decision_boundary<span class="token punctuation">,</span> <span class="token number">0.92</span><span class="token punctuation">,</span> <span class="token number">0.3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> head_width<span class="token operator">=</span><span class="token number">0.05</span><span class="token punctuation">,</span> head_length<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> fc<span class="token operator">=</span><span class="token string">'g'</span><span class="token punctuation">,</span> ec<span class="token operator">=</span><span class="token string">'g'</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">"Petal width (cm)"</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">"Probability"</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span>loc<span class="token operator">=</span><span class="token string">"center left"</span><span class="token punctuation">,</span> fontsize<span class="token operator">=</span><span class="token number">14</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>axis<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.02</span><span class="token punctuation">,</span> <span class="token number">1.02</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre> 
<p>维吉尼亚鸢尾（三角形所示）的花瓣宽度范围为1.4～2.5cm，而其他两种鸢尾花（正方形所示）花瓣通常较窄，花瓣宽度范围为0.1～1.8cm。注意，这里有一部分重叠。对花瓣宽度超过2cm的花，分类器可以很有信心地说它是一朵维吉尼亚鸢尾花（对该类别输出一个高概率值），对花瓣宽度低于1cm以下的，也可以胸有成竹地说其不是（对“非维吉尼亚鸢尾”类别输出一个高概率值）。在这两个极端之间，分类器则不太有把握。但是，如果你要求它预测出类别（使用predict（）方法而不是predict_proba（）方法），它将返回一个可能性最大的类别。也就是说，在大约1.6cm处存在一个决策边界，这里“是”和“不是”的可能性都是50%，如果花瓣宽度大于1.6cm，分类器就预测它是维吉尼亚鸢尾花，否则就预测不是（即使它没什么把握）：</p> 
<pre><code class="prism language-python"><span class="token comment"># 测试预测</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>log_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1.5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 测试1.7和1.5两个值的输出</span>
</code></pre> 
<p>下图还是同样的数据集，但是这次显示了两个特征：花瓣宽度和花瓣长度。经过训练，这个逻辑回归分类器就可以基于这两个特征来预测新花朵是否属于维吉尼亚鸢尾。虚线表示模型估算概率为50%的点，即模型的决策边界。注意这里是一个线性的边界（注：这是点x的集合，使得θ0+θ1x1+θ2x2=0，它定义了一条直线。）。每条平行线都分别代表一个模型输出的特定概率，从左下的15%到右上的90%。根据这个模型，右上线之上的所有花朵都有超过90%的概率属于维吉尼亚鸢尾。</p> 
<p><img src="https://images2.imgbox.com/4b/ae/H1Yl0yIb_o.png" alt="在这里插入图片描述"></p> 
<p>与其他线性模型一样，逻辑回归模型可以用<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        l
       
       
        1
       
      
      
       l1
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">1</span></span></span></span></span>或<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        l
       
       
        2
       
      
      
       l2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">2</span></span></span></span></span>惩罚函数来正则化。Scikit-Learn默认添加的是<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        l
       
       
        2
       
      
      
       l2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">2</span></span></span></span></span>函数。</p> 
<blockquote> 
 <p>控制Scikit-Learn LogisticRegression模型的正则化强度的超参数不是alpha（与其他线性模型一样），而是反值C。C值越高，对模型的正则化越少。</p> 
</blockquote> 
<h3>
<a id="464_Softmax_632"></a>4.6.4 Softmax回归</h3> 
<p>逻辑回归模型经过推广，可以直接支持多个类别，而不需要训练并组合多个二元分类器（如第3章所述）。这就是Softmax回归，或者叫作多元逻辑回归。</p> 
<p>原理很简单：给定一个实例x，Softmax回归模型首先计算出每个类k的分数sk（x），然后对这些分数应用softmax函数（也叫归一化指数），估算出每个类的概率。你应该很熟悉计算sk（x）分数的公式（见公式），因为它看起来就跟线性回归预测的方程一样。<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
     
      
       
        
         类
        
        
         k
        
        
         的
        
        
         S
        
        
         o
        
        
         f
        
        
         t
        
        
         m
        
        
         a
        
        
         x
        
        
         分
        
        
         数
        
        
         ：
        
        
         
          S
         
         
          k
         
        
        
         (
        
        
         x
        
        
         )
        
        
         =
        
        
         
          x
         
         
          T
         
        
        
         
          θ
         
         
          
           (
          
          
           k
          
          
           )
          
         
        
       
       
         类k的Softmax分数：S_k(x) = x^Tθ^{(k)} 
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord cjk_fallback">类</span><span class="mord mathdefault" style="margin-right: 0.03148em">k</span><span class="mord cjk_fallback">的</span><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="mord mathdefault">o</span><span class="mord mathdefault" style="margin-right: 0.10764em">f</span><span class="mord mathdefault">t</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span><span class="mord mathdefault">x</span><span class="mord cjk_fallback">分</span><span class="mord cjk_fallback">数</span><span class="mord cjk_fallback">：</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.05764em">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.336108em"><span class="" style="margin-left: -0.05764em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.277778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em"></span></span><span class="base"><span class="strut" style="height: 0.938em;vertical-align: 0em"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.891331em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right: 0.13889em">T</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.938em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right: 0.03148em">k</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span></span><br> 请注意，每个类都有自己的特定参数向量<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        θ
       
       
        (
       
       
        k
       
       
        )
       
      
      
       θ(k)
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 1em;vertical-align: -0.25em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right: 0.03148em">k</span><span class="mclose">)</span></span></span></span></span>。所有这些向量通常都作为行存储在参数矩阵<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        θ
       
      
      
       θ
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.02778em">θ</span></span></span></span></span>中。</p> 
<p>一旦为实例x计算了每个类的分数，就可以通过softmax函数来估计实例属于类k的概率。该函数计算每个分数的指数，然后对其进行归一化（除以所有指数的总和）。分数通常称为对数或对数奇数（尽管它们实际上是未归一化的对数奇数）。</p> 
<p>就像逻辑回归分类器一样，Softmax回归分类器预测具有最高估计概率的类（简单来说就是得分最高的类）。</p> 
<blockquote> 
 <p>Softmax回归分类器一次只能预测一个类（即它是多类，而不是多输出），因此它只能与互斥的类（例如不同类型的植物）一起使用。你无法使用它在一张照片中识别多个人。</p> 
</blockquote> 
<p>我们来使用Softmax回归将鸢尾花分为三类。当用两个以上的类训练时，Scikit-Learn的LogisticRegressio默认选择使用的是一对多的训练方式，不过将超参数multi_class设置为"multinomial"，可以将其切换成Softmax回归。你还必须指定一个支持Softmax回归的求解器，比如"lbfgs"求解器。默认使用<span class="katex--inline"><span class="katex"><span class="katex-mathml">
    
     
      
       
        l
       
       
        2
       
      
      
       l2
      
     
    </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">2</span></span></span></span></span>正则化，你可以通过超参数C进行控制：</p> 
<pre><code class="prism language-python"><span class="token comment"># softmax预测鸢尾花</span>
X <span class="token operator">=</span> iris<span class="token punctuation">[</span><span class="token string">"data"</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">]</span>  <span class="token comment"># petal length, petal width</span>
y <span class="token operator">=</span> iris<span class="token punctuation">[</span><span class="token string">"target"</span><span class="token punctuation">]</span>
softmax_reg <span class="token operator">=</span> LogisticRegression<span class="token punctuation">(</span>multi_class<span class="token operator">=</span><span class="token string">"multinomial"</span><span class="token punctuation">,</span> solver<span class="token operator">=</span><span class="token string">"lbfgs"</span><span class="token punctuation">,</span> C<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>
softmax_reg<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>X<span class="token punctuation">,</span> y<span class="token punctuation">)</span>

<span class="token comment"># 当你下次碰到一朵鸢尾花，花瓣长5cm宽2cm，你就可以让模型告诉你它的种类，它会回答说：94.2%的概率是维吉尼亚鸢尾（第2类）或者5.8%的概率为变色鸢尾：</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>softmax_reg<span class="token punctuation">.</span>predict<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 预测属于哪一类</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>softmax_reg<span class="token punctuation">.</span>predict_proba<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 得到属于每一类的概率</span>
</code></pre> 
<p>下图展现了由不同背景色表示的决策边界。注意，任何两个类之间的决策边界都是线性的。图中的折线表示属于变色鸢尾的概率（例如，标记为0.45的线代表45%的概率边界）。注意，该模型预测出的类，其估算概率有可能低于50%，比如，在所有决策边界相交的地方，所有类的估算概率都为33%。</p> 
<p><img src="https://images2.imgbox.com/f4/bd/J12CJDD2_o.png" alt="在这里插入图片描述"></p> 
<h2>
<a id="47__669"></a>4.7 练习题</h2> 
<h3>
<a id="_671"></a>问题</h3> 
<ol>
<li> <p>如果训练集具有数百万个特征，那么可以使用哪种线性回归训练算法？</p> </li>
<li> <p>如果训练集里特征的数值大小迥异，哪种算法可能会受到影响？受影响程度如何？你应该怎么做？</p> </li>
<li> <p>训练逻辑回归模型时，梯度下降会卡在局部最小值中吗？</p> </li>
<li> <p>如果你让它们运行足够长的时间，是否所有的梯度下降算法都能得出相同的模型？</p> </li>
<li> <p>假设你使用批量梯度下降，并在每个轮次绘制验证误差。如果你发现验证错误持续上升，可能是什么情况？你该如何解决？</p> </li>
<li> <p>当验证错误上升时立即停止小批量梯度下降是个好主意吗？</p> </li>
<li> <p>哪种梯度下降算法（在我们讨论过的算法中）将最快到达最佳解附近？哪个实际上会收敛？如何使其他的也收敛？</p> </li>
<li> <p>假设你正在使用多项式回归。绘制学习曲线后，你会发现训练误差和验证误差之间存在很大的差距。发生了什么？解决此问题的三种方法是什么？</p> </li>
<li> <p>假设你正在使用岭回归，并且你注意到训练误差和验证误差几乎相等且相当高。你是否会说模型存在高偏差或高方差？你应该增加正则化超参数α还是减小它呢？</p> </li>
<li> <p>为什么要使用：</p> 
  <ul>
<li>岭回归而不是简单的线性回归（即没有任何正则化）？</li>
<li>Lasso而不是岭回归？</li>
<li>弹性网络而不是Lasso？</li>
</ul> </li>
<li> <p>假设你要将图片分类为室外/室内和白天/夜间。你应该实现两个逻辑回归分类器还是一个Softmax回归分类器？</p> </li>
<li> <p>用Softmax回归进行批量梯度下降训练，实现提前停止法（不使用Scikit-Learn）。</p> </li>
</ol> 
<h3>
<a id="_702"></a>答案</h3> 
<ol>
<li> <p>如果你的训练集具有数百万个特征，则可以使用随机梯度下降或小批量梯度下降。如果训练集适合容纳于内存，则可以使用批量梯度下降。但是你不能使用标准方程法或SVD方法，因为随着特征数量的增加，计算复杂度会快速增长（超过二次方）。</p> </li>
<li> <p>如果你的训练集中的特征具有不同的尺寸比例，则成本函数具有细长碗的形状，因此梯度下降算法需要很长时间才能收敛。为了解决这个问题，你应该在训练模型之前缩放数据。请注意，标准方程法或SVD方法无须缩放即可正常工作。此外，如果特征未按比例缩放，则正则化模型可能会收敛至次优解：由于正则化会惩罚较大的权重，因此与具有较大值的特征相比，具有较小值的特征往往会被忽略。</p> </li>
<li> <p>训练逻辑回归模型时，梯度下降不会陷入局部最小值，因为成本函数是凸函数。</p> </li>
<li> <p>如果优化问题是凸的（例如线性回归或逻辑回归），并且假设学习率不是太高，那么所有梯度下降算法都将接近全局最优并最终产生很相似的模型。但是，除非逐步降低学习率，否则随机梯度下降和小批量梯度下降将永远不会真正收敛。相反，它们会一直围绕全局最优值来回跳跃。这意味着即使你让它们运行很长时间，这些梯度下降算法也会产生略微不同的模型。</p> </li>
<li> <p>如果验证错误在每个轮次后持续上升，则一种可能性是学习率过高并且算法在发散。如果训练错误也增加了，那么这显然是问题所在，你应该降低学习率。但是，如果训练错误没有增加，则你的模型已经过拟合训练集，则应该停止训练。</p> </li>
<li> <p>由于随机性，随机梯度下降和小批量梯度下降都不能保证在每次训练迭代中都取得进展。因此，如果在验证错误上升时立即停止训练，则可能在达到最优值之前就停止太早了。更好的选择是按照一定的间隔时间保存模型。然后，当它很长时间没有改善（意味着它可能永远不会超过最优值）时，你可以恢复到保存的最佳模型。</p> </li>
<li> <p>随机梯度下降法具有最快的训练迭代速度，因为它一次只考虑一个训练实例，因此它通常是第一个到达全局最优值附近的（或是很小批量大小的小批量梯度下降）。但是，给定足够的训练时间，实际上只有批量梯度下降会收敛。如前所述，随机梯度下降和小批量梯度下降会在最优值附近反弹，除非你逐渐降低学习率。</p> </li>
<li> <p>如果验证误差远高于训练误差，则可能是因为模型<strong>过拟合</strong>了训练集。解决此问题的一种方法是<strong>降低多项式阶数</strong>：较小自由度的模型不太可能过拟合。另一种方法是<strong>对模型进行正则化</strong>，例如，将<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          l
         
         
          2
         
        
        
         l2
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">2</span></span></span></span></span>（Ridge）或<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          l
         
         
          1
         
        
        
         l1
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">1</span></span></span></span></span>（Lasso）惩罚添加到成本函数。这也会降低模型的自由度。最后，你可以<strong>尝试增加训练集的大小</strong>。</p> </li>
<li> <p>如果训练误差和验证误差几乎相等且相当高，则该模型很可能欠拟合训练集，这意味着它具有很高的偏差。你应该尝试减少正则化超参数<span class="katex--inline"><span class="katex"><span class="katex-mathml">
      
       
        
         
          α
         
        
        
         α
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.43056em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.0037em">α</span></span></span></span></span>。</p> </li>
<li> <p>让我们来看看：</p> 
  <ul>
<li>具有某些正则化的模型通常比没有任何正则化的模型要好，因此，你通常应优先选择岭回归而不是简单的线性回归。</li>
<li>Lasso回归使用<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           l
          
          
           1
          
         
         
          l1
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.69444em;vertical-align: 0em"></span><span class="mord mathdefault" style="margin-right: 0.01968em">l</span><span class="mord">1</span></span></span></span></span>惩罚，这通常会将权重降低为零。这将导致稀疏模型，其中除了最重要的权重之外，所有权重均为零。这是一种自动进行特征选择的方法，<u>如果你怀疑实际上只有很少的特征很重要，那么这是一种很好的方法</u>。如果你不确定，则应首选岭回归。</li>
<li>与Lasso相比，弹性网络通常更受青睐，<strong>因为Lasso在某些情况下可能产生异常（当几个特征强相关或当特征比训练实例更多时）</strong>。但是，它确实增加了额外需要进行调整的超参数。如果你希望Lasso没有不稳定的行为，则可以仅使用<code>l1_ratio</code>接近1的弹性网络。</li>
</ul> </li>
<li> <p>如果你要将图片分类为室外/室内和白天/夜间，因为它们不是排他的</p> </li>
<li> <p>类（即所有四种组合都是可能的），则应训练两个逻辑回归分类器。</p> </li>
<li> <p>请参阅<a href="https://github.com/ageron/handson-ml2">https://github.com/ageron/handson-ml2</a>上的Jupyter notebooks。</p> </li>
</ol>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>