<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>【论文阅读总结】Mask R-CNN翻译总结 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">【论文阅读总结】Mask R-CNN翻译总结</h1>
			
		</header>
		<div class="content post__content clearfix">
			


        
                <div id="content_views" class="markdown_views prism-atom-one-dark">
                    
                        
                    
                    <p></p>
<div class="toc">
 <h3>Mask R-CNN</h3>
 <ul>
<li><a href="#1_3">1.摘要</a></li>
<li>
<ul><li><a href="#Mask_RCNN_4">Mask R-CNN相关介绍与优点</a></li></ul>
  </li>
<li><a href="#2_15">2.引言</a></li>
<li><a href="#3_34">3.文献综述</a></li>
<li>
<ul>
<li><a href="#31_RCNN_35">3.1 R-CNN</a></li>
<li><a href="#32_Instance_Segmentation_40">3.2 Instance Segmentation【实例分割】</a></li>
</ul>
  </li>
<li><a href="#4_Mask_RCNN_50">4. Mask R-CNN介绍</a></li>
<li>
<ul>
<li><a href="#41_Faster_RCNN_55">4.1 Faster R-CNN(相关细节请看相关文章)</a></li>
<li><a href="#42_Mask_RCNN_60">4.2 Mask R-CNN</a></li>
<li><a href="#43_Mask_Representation_76">4.3 Mask Representation【遮罩表示法】</a></li>
<li><a href="#44_RoIAlign_83">4.4 RoIAlign【感兴趣区域对齐】</a></li>
<li><a href="#441_RoIPool_84">4.4.1 RoIPool【感兴趣区域池化】</a></li>
<li><a href="#442_RoIAlign_89">4.4.2 RoIAlign提出【解决不对齐问题】</a></li>
<li><a href="#45_Network_Architecture_97">4.5 Network Architecture【网络体系结构】</a></li>
</ul>
  </li>
<li><a href="#5__115">5. 算法训练过程</a></li>
<li>
<ul>
<li><a href="#51_train_117">5.1 train【训练】</a></li>
<li><a href="#52_Inference_128">5.2 Inference【推断】</a></li>
</ul>
  </li>
<li><a href="#5__134">5. 实验</a></li>
<li>
<ul>
<li><a href="#51_Instance_Segmentation_135">5.1 Instance Segmentation【实例分割】</a></li>
<li><a href="#52_Ablation_Experiments_146">5.2 Ablation Experiments【消融实验】</a></li>
<li><a href="#53__Bounding_Box_Detection_Results_177">5.3 Bounding Box Detection Results【边界框检测结果】</a></li>
<li><a href="#54_Timing_188">5.4 Timing【调速】</a></li>
</ul>
  </li>
<li><a href="#6__200">6. 总结</a></li>
<li>
<ul>
<li><a href="#61_FPN_201">6.1 FPN(特征金字塔网络)</a></li>
<li><a href="#62__212">6.2 候选框生成</a></li>
<li><a href="#63_RPN_216">6.3 RPN(区域建议网络)</a></li>
<li><a href="#64_ROI_220">6.4 ROI(感兴趣区域)</a></li>
<li><a href="#65_DetectionTargetLayer_225">6.5 DetectionTargetLayer【检测目标层】</a></li>
<li><a href="#66_Network_Heads_229">6.6 Network Heads【网络头部】</a></li>
<li><a href="#67__232">6.7 开始建模</a></li>
</ul>
  </li>
<li><a href="#7_233">7.阅读总结</a></li>
</ul>
</div>
<p></p> 
<ul><li>论文链接 
  <ul><li><a href="https://arxiv.org/pdf/1703.06870.pdf">https://arxiv.org/pdf/1703.06870.pdf</a></li></ul> </li></ul> 
<h1>
<a id="1_3"></a>1.摘要</h1> 
<h2>
<a id="Mask_RCNN_4"></a>Mask R-CNN相关介绍与优点</h2> 
<ul>
<li>一个概念上简单、灵活和通用的<code>对象实例分割框架</code>。</li>
<li>有效地检测图像中的对象，同时为每个实例生成高质量的分割掩码。</li>
<li>通过添加一个用于预测对象掩码的分支，与现有的边界框识别分支<code>并行</code>，扩展了Faster R-CNN。</li>
<li>易于实现和训练，这有助于广泛的灵活架构设计。仅为<code>Faster R-CNN</code>增加了一小部分开销，以每秒5帧的速度运行。</li>
<li>很容易推广到其他任务，例如：允许我们在同一框架中估计人体姿势。</li>
<li>通过添加一个分支来预测每个<code>感兴趣区域（RoI）</code>上的分割掩码，并与现有的分类和边界框回归分支并行，从而扩展了<code>Faster R-CNN</code> 。掩模分支是应用于每个<code>RoI</code>的小<code>FCN</code>，以<code> pixel-to-pixel</code>的方式预测分割掩模。</li>
<li>掩码分支只增加了少量的计算开销，从而实现了快速系统和快速实验</li>
<li>
<code>Mask R-CNN</code>可以被更广泛地视为实例级识别的灵活框架，并且可以很容易地扩展到更复杂的任务。</li>
<li>展示了COCO系列挑战的所有三个方面的顶级结果，包括<code>实例分割</code>、<code>边界框对象检测</code>和<code>人物关键点检测</code>。</li>
<li>在没有钟声和哨声的情况下，<code>Mask R-CNN</code>在每项任务上都优于所有现有的单一模型参赛作品。</li>
</ul> 
<h1>
<a id="2_15"></a>2.引言</h1> 
<ul>
<li>视觉社区在短时间内快速改进了对象检测和语义分割结果。很大程度上，这些进步是由强大的基线系统驱动的。 
  <ul>
<li>例如：分别用于<code>对象检测</code>和<code>语义分割</code>的 <code>Fast/Faster R-CNN</code> 和<code>全卷积网络（FCN）框架</code>。</li>
<li>这些方法在概念上是直观的，提供了<strong>灵活性</strong>和<strong>鲁棒性</strong>，以及快速的训练和推理时间。</li>
</ul> </li>
<li>
<mark>算法目标</mark>：为实例分割开发一个相对可行的框架。</li>
<li>实例分割具有挑战性，因为它需要正确检测图像中的所有对象，同时还需要精确分割每个实例。因此，它结合了经典的计算机视觉目标检测任务中的元素，目标是对单个目标进行分类，并使用边界框对每个目标进行定位和语义分割，目标是将每个像素分类为一组固定的类别，而不区分对象实例。发现一个令人惊讶的简单、灵活和快速的系统可以超越现有的最先进的实例分割结果。</li>
<li>
<code>Mask R-CNN</code>是<code>Faster R-CNN</code>的直观扩展，但正确构建掩码分支对于获得良好结果至关重要。</li>
<li>
<code>RoIAlign</code>： 
  <ul><li>
<code>Faster R-CNN</code>不是为网络输入和输出之间的<code> pixel-to-pixel</code>对齐而设计的。这在<code>RoIPool</code> （用于处理实例的事实上的核心操作）如何为特征提取执行粗略的空间量化中最为明显。为了修复错位，提出了一个简单的无量化层<code>【RoIAlign】</code>，它忠实地保留了精确的空间位置(<strong>减小预测误差，相对于原图像素位置</strong>)。</li></ul> </li>
<li>
<code>RoIAlign</code>微小变化，具有很大影响： 
  <ul>
<li>将<strong>掩模精度提高了10%到50%</strong>，在更严格的定位标准下显示出更大的增益。</li>
<li>必须将掩码和类预测解耦： 
    <ul><li>我们独立地为每个类预测一个二进制掩码，而不需要类之间的竞争 <strong>【类与类之间掩码损失无关性，互不影响】</strong>，并依赖网络的<code>RoI</code>分类分支来预测类别。</li></ul> </li>
<li>相比之下，<code>FCN</code>通常执行 <code>per-pixel</code>多分类，这将分割和分类结合起来，并且根据我们的实验，对于实例分割效果不佳</li>
</ul> </li>
<li>模型结果 
  <ul>
<li>在COCO实例分割任务中，<code>Mask R-CNN</code>超越了之前所有最先进的单一模型结果 。</li>
<li>算法在COCO对象检测任务上也很出色。在消融实验中，我们评估了多个基本实例，这使我们能够证明其<strong>鲁棒性</strong>并分析核心因素的影响。</li>
<li>模型可以在<code>GPU</code>上以每帧<code>200</code>毫秒的速度运行，快速的训练和测试速度，加上框架的<strong>灵活性和准确性</strong>，将有利于并简化未来对实例分割的研究。</li>
<li>通过COCO关键点数据集上的人类姿态估计任务展示了我们框架的<strong>通用性</strong> 。通过将每个关键点视为一个热二进制掩码，通过最小的修改，可以应用<code>Mask R-CNN</code>来检测特定于实例的姿势。</li>
</ul> </li>
</ul> 
<h1>
<a id="3_34"></a>3.文献综述</h1> 
<h2>
<a id="31_RCNN_35"></a>3.1 R-CNN</h2> 
<ul>
<li>
<code>Region-based CNN (R-CNN)</code> 方法用于边界盒对象检测，是关注可管理数量的候选对象区域 ，并独立于每个<code>RoI</code>评估卷积网络 。</li>
<li>
<code>R-CNN</code>被扩展 ，以允许使用<code>RoIPool</code>处理特征地图上的<code>RoI</code>，从而<strong>提高速度和准确性</strong>。</li>
<li>
<code>Faster R-CNN</code>通过学习<code>区域建议网络（RPN）</code>的注意力机制，推进了这一流程。</li>
<li>
<code>Faster R-CNN</code>对许多后续改进 ,具有灵活性和鲁棒性，是当前几个基准测试中的领先框架。</li>
</ul> 
<h2>
<a id="32_Instance_Segmentation_40"></a>3.2 Instance Segmentation【实例分割】</h2> 
<ul>
<li>在<code>R-CNN</code>有效性的推动下，许多实例分割方法都基于分段建议。早期的方法采用自下而上的分段 。<code>DeepMask</code>和后续工作学习提出细分候选，然后由<code>Fast R-CNN</code>分类。</li>
<li>在这些方法中，分割先于识别，这是缓慢且不太准确的。</li>
<li>Dai等人提出了一种复杂的多级级联，该级联从边界框建议中预测分段建议，然后进行分类。</li>
<li>
<strong>我们的方法基于掩码和类标签的并行预测，这更简单、更灵活</strong>。</li>
<li>Li等人将片段建议系统和对象检测系统结合起来，用于<code>“全卷积实例分割”（FCIS)</code>。</li>
<li>思想是完全卷积地预测一组位置敏感的输出信道。这些通道同时处理对象类、框和掩码，使系统快速运行。但是<code>FCIS</code>在重叠实例上表现出系统性错误，并产生虚假边缘 ，这表明它受到分割实例的基本困难的挑战。<br> <img src="https://images2.imgbox.com/93/fb/aDiMiSWk_o.png" alt="在这里插入图片描述">
</li>
<li>实例分割的另一系列解决方案 是由语义分割的成功驱动的。从每像素分类结果（例如，<code>FCN</code>输出）开始，这些方法尝试将同一类别的像素分割成不同的实例。</li>
<li>与这些方法的分段优先策略不同，<code>Mask R-CNN</code>基于实例优先策略。我们预计，未来将研究这两种战略的更深入结合。</li>
</ul> 
<h1>
<a id="4_Mask_RCNN_50"></a>4. Mask R-CNN介绍</h1> 
<ul>
<li>
<code>Mask R-CNN</code>在概念上很简单： 
  <ul>
<li>
<code>Faster R-CNN</code>为每个候选对象提供两个输出，一个类标签和一个边界框偏移；为此，我们添加了输出对象掩码的第三个分支。</li>
<li>
<code>Mask R-CNN</code>是一个自然而直观的想法。但附加的掩码输出与类和框输出不同，需要提取更精细的对象空间布局。</li>
</ul> </li>
<li>我们将介绍<code>Mask R-CNN</code>的关键元素，包括<code> pixel-to-pixel</code>的对齐，这是<code>Fast/Faster R-CNN</code>的主要缺失部分。</li>
</ul> 
<h2>
<a id="41_Faster_RCNN_55"></a>4.1 Faster R-CNN(相关细节请看相关文章)</h2> 
<ul>
<li>简要回顾了<code>Faster R-CNN</code>探测器。<code>Faster R-CNN</code>包括两个阶段。 
  <ul>
<li>第一阶段称为<code>区域建议网络（RPN</code>），提出候选对象边界框。</li>
<li>第二阶段本质上是<code>Fast R-CNN</code>，使用<code>RoIPool</code>从每个候选框中提取特征，并执行分类和边界框回归。</li>
</ul> </li>
<li>两个阶段使用的特征可以共享，以便更快地推断。</li>
</ul> 
<h2>
<a id="42_Mask_RCNN_60"></a>4.2 Mask R-CNN</h2> 
<ul>
<li>
<code>Mask R-CNN</code>采用相同的两阶段程序。 
  <ul>
<li>具有相同的第一阶段（即<code>RPN</code>）。</li>
<li>在第二阶段，在预测类和边框偏移的同时，<code>Mask R-CNN</code>还为每个<code>RoI</code>输出二进制掩码。这与最新的系统形成对比，其中分类取决于掩码预测 。</li>
</ul> </li>
<li>
<code>Mask R-CNN</code>遵循<code>Fast R-CNN</code>的精神，即<strong>并行应用边界框分类和回归</strong>（结果证明，这在很大程度上简化了原始<code>R-CNN</code> 的多级管道）</li>
<li>训练期间，每个采样RoI的多任务损失定义为<br> <span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml">
      
       
        
         
          L
         
         
          =
         
         
          
           L
          
          
           
            c
           
           
            l
           
           
            s
           
          
         
         
          +
         
         
          
           L
          
          
           
            b
           
           
            o
           
           
            x
           
          
         
         
          +
         
         
          
           L
          
          
           
            m
           
           
            a
           
           
            s
           
           
            k
           
          
         
        
        
         L=L_{cls}+L_{box}+L_{mask}
        
       
      </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6833em"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right: 0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em"></span></span><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span></span> 
  <ul>
<li>分类损失<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           
            L
           
           
            
             c
            
            
             l
            
            
             s
            
           
          
         
         
          L_{cls}
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right: 0.0197em">l</span><span class="mord mathnormal mtight">s</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>和边界框损失<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           
            L
           
           
            
             b
            
            
             o
            
            
             x
            
           
          
         
         
          L_{box}
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">b</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>与Fast R-CNN中定义的相同。</li>
<li>掩码分支对每个<code>RoI</code>都有一个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           K
          
          
           
            m
           
           
            2
           
          
         
         
          Km^2
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8141em"></span><span class="mord mathnormal" style="margin-right: 0.0715em">K</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8141em"><span class="" style="margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span>维输出，它编码分辨率为<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           m
          
          
           ∗
          
          
           m
          
         
         
          m*m
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4653em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span>的<code>K</code>个二进制掩码，<code>k</code>表示有<code>k</code>个类别,<code>m</code>表示特征图大小。</li>
<li>我们应用<code>per-pixel sigmoid</code>，并将<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           
            L
           
           
            
             m
            
            
             a
            
            
             s
            
            
             k
            
           
          
         
         
          L_{mask}
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>定义为平均二进制交叉熵损失。</li>
<li>对于与 <code>ground-truth</code> 【背景真实值】类别<code>k</code>相关的<code>RoI</code>，<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           
            L
           
           
            
             m
            
            
             a
            
            
             s
            
            
             k
            
           
          
         
         
          L_{mask}
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>仅在第<code>k</code>个掩码上定义（其他掩码输出不影响损失）<strong>【只在一个特征图上有损失，与其他特征图无关】</strong>
</li>
</ul> </li>
<li>
<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          L
         
         
          
           m
          
          
           a
          
          
           s
          
          
           k
          
         
        
       
       
        L_{mask}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span> 
  <ul>
<li>
<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           
            L
           
           
            
             m
            
            
             a
            
            
             s
            
            
             k
            
           
          
         
         
          L_{mask}
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>定义允许网络为每个类生成掩码，而无需类别之间的竞争**【类别之间无相关性】**；我们依靠专门的分类分支来预测用于选择输出掩码的类标签。这使掩码和类预测解耦。</li>
<li>这与将<code>FCN</code>应用于语义分割时的常见做法不同</li>
<li>语义分割通常使用<code>per-pixel softmax</code>和多项式交叉熵损失。在这种情况下，不同分类的掩码会竞争；</li>
<li>它们没有使用，对于<code>per-pixel sigmoid</code>和二进制损失。通过实验表明，该公式<strong>是获得良好实例分割结果的关键。</strong>
</li>
</ul> </li>
</ul> 
<h2>
<a id="43_Mask_Representation_76"></a>4.3 Mask Representation【遮罩表示法】</h2> 
<ul>
<li>掩码对输入对象的空间布局进行编码。提取掩模的空间结构可以通过卷积提供的<code>pixel-to-pixel</code>的对应来自然地解决。 
  <ul><li>与全连接<code>（fc）</code>层不可避免地折叠成短输出向量的类标签或盒偏移不同 <strong>（全连接会导致不同特征叠加，不能获得<code>pixel-to-pixel</code>结构）</strong>。</li></ul> </li>
<li>使用<code>FCN</code>从每个<code>RoI</code>预测<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         m
        
        
         ∗
        
        
         m
        
       
       
        m*m
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4653em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span>掩码。 
  <ul>
<li>允许遮罩分支中的每个层保持明确的<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           m
          
          
           ∗
          
          
           m
          
         
         
          m*m
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4653em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span>对象空间布局，而不会将其折叠为缺少空间维度的矢量表示。</li>
<li>与先前方法不同，全卷积参数更少，并且如实验所证明的那样更准确，这些方法使用<code>fc</code>层进行掩模预测。</li>
</ul> </li>
<li>
<code>pixel-to-pixel</code> 的行为要求我们的<code>RoI</code>特征（它们本身是小的特征图）很好地对齐，以忠实地保持每像素的明确空间对应关系。这促使我们开发了以下<code>RoIAlign</code>层，该层在掩模预测中起着关键作用。</li>
</ul> 
<h2>
<a id="44_RoIAlign_83"></a>4.4 RoIAlign【感兴趣区域对齐】</h2> 
<h2>
<a id="441_RoIPool_84"></a>4.4.1 RoIPool【感兴趣区域池化】</h2> 
<ul>
<li>
<code>RoIPool</code>是从每个<code>RoI</code>中提取小特征图（例如，7×7）的标准操作。</li>
<li>
<code>RoIPool</code>首先将浮点数<code>RoI</code>量化为特征图的离散粒度，然后将该量化的<code>RoI</code>细分为自己量化的空间仓，最后聚合每个仓覆盖的特征值 <strong>（就是最大池）</strong>。 
  <ul>
<li>这些量化引入了<code>RoI</code>和提取的特征之间的不对齐 <strong>【与最大池化前的特征图没有<code>pixel-to-pixel</code>之间对齐关系，不能找到原图位置的像素点】</strong>。</li>
<li>虽然这可能不会影响分类，因为分类对于小的平移是鲁棒的，但<strong>它对预测像素准确的掩码有很大的负面影响。【预测边框准确位置会有很大偏差】</strong>
</li>
</ul> </li>
</ul> 
<h2>
<a id="442_RoIAlign_89"></a>4.4.2 RoIAlign提出【解决不对齐问题】</h2> 
<ul>
<li>提出了一个<code>RoIAlign</code>层，它消除了·RoIPool·的苛刻量化，将提取的特征与输入正确对齐。</li>
<li>提出的改变很简单： 
  <ul>
<li>避免了<code>RoI</code>边界或容器的任何量化。</li>
<li>我们使用<code>双线性插值</code>计算每个<code>RoI</code>容器中四个规则采样位置的输入特征的精确值，并聚合结果（使用最大值或平均值）。注意到:只要不进行量化，结果对精确的采样位置或采样的点数不敏感。<br> <img src="https://images2.imgbox.com/86/70/NEksQRL8_o.png" alt="在这里插入图片描述">
</li>
</ul> </li>
<li>
<code>RoIAlign</code>带来了巨大改进。我们还与论文 <em>(Instance-aware semantic segmen-tation via multi-task network cascades.)</em> 中提出的<code>RoIWarp</code>操作进行了比较。</li>
<li>与<code>RoIAlign</code>不同，<code>RoIWarp</code>忽略了对齐问题，并实现效果与<code>RoIPool</code>一样量化RoI。因此，尽管<code>RoIWarp</code>也采用了激励的双线性重采样，但如实验所示，它的性能与<code>RoIPool</code>相当，<mark>证明了对齐的关键作用</mark>。</li>
</ul> 
<h2>
<a id="45_Network_Architecture_97"></a>4.5 Network Architecture【网络体系结构】</h2> 
<ul>
<li>为了证明我们方法的通用性，我们使用多个架构实例化<code>Mask R-CNN</code>。</li>
<li>为了清楚起见，我们区分了： 
  <ul>
<li>（i）用于整个图像特征提取的卷积主干架构。</li>
<li>（ii）分别应用于每个<code>RoI</code>的边界框识别（分类和回归）和掩码预测的网络头。</li>
</ul> </li>
<li>使用网络深度特征来表示主干架构。 
  <ul>
<li>评估了深度为<code>50</code>或<code>101</code>层的<code>ResNet</code>和<code>ResNeXt</code>网络。</li>
<li>
<code>Faster R-CNN</code>与<code>ResNets</code>的原始实现从第<code>4</code>阶段的<code>最终卷积层</code>提取了特征，我们称之为<code>C4</code>。 
    <ul><li>例如，具有<code>ResNet-50</code>的主干由<code>ResNet-50-C4</code>表示。</li></ul> </li>
</ul> </li>
<li>还探索另一种更有效的主干，称为<em><strong>特征金字塔网络（FPN）</strong></em>。 
  <ul>
<li>
<code>FPN</code>使用具有<em><strong>横向连接的自顶向下</strong></em>架构，从单个规模的输入构建网络内特征金字塔。</li>
<li>具有<code>FPN</code>主干的<code>Faster R-CNN </code>根据其规模从特征金字塔的不同级别提取<code>RoI</code>特征，但其他方法类似于<code>vanilla ResNet</code>。</li>
<li>使用<code>ResNet FPN</code>主干与<code>Mask RCNN</code>进行特征提取，在<strong>精度和速度方面都获得了优异的提高</strong>。</li>
</ul> </li>
<li>
<code>network head</code> 
  <ul>
<li>对于网络头部，我们严格遵循先前工作中提出的架构，在其中添加了全卷积掩码预测分支。</li>
<li>扩展了<code>ResNet</code>和<code>FPN</code>论文中的<code>Faster R-CNN</code>箱头。左/右面板显示了<code>ResNet C4</code>和<code>FPN</code>主干的头部，其中添加了掩码分支。 
    <ul>
<li>
<code>ResNet-C4</code>主干上的头部包括<code>ResNet</code>的第<code>5</code>阶段（即9层“res5”）。</li>
<li>对于<code>FPN</code>，主干已经包括<code>res5</code>，因此允许使用更少滤波器的更高效的头部 <img src="https://images2.imgbox.com/7a/52/tsUIg2BV_o.png" alt="在这里插入图片描述">
</li>
</ul> </li>
</ul> </li>
</ul> 
<h1>
<a id="5__115"></a>5. 算法训练过程</h1> 
<ul><li>根据现有的<code>Fast/Faster R-CNN</code>工作设置了超参数。使用迁移学习作为超参数对我们的实例分割系统对它们是鲁棒的。</li></ul> 
<h2>
<a id="51_train_117"></a>5.1 train【训练】</h2> 
<ul>
<li>与<code>Fast R-CNN</code>一样，如果<code>RoI</code>的<code>IoU</code>至少为<code>0.5</code>，则<code>RoI</code>被认为是正的，否则<code>RoI</code>被视为负的。</li>
<li>掩码损失<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         
          L
         
         
          
           m
          
          
           a
          
          
           s
          
          
           k
          
         
        
       
       
        L_{mask}
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.8333em;vertical-align: -0.15em"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em"><span class="" style="margin-left: 0em;margin-right: 0.05em"><span class="pstrut" style="height: 2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">s</span><span class="mord mathnormal mtight" style="margin-right: 0.0315em">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em"><span class=""></span></span></span></span></span></span></span></span></span></span>仅在正<code>RoI</code>上定义。掩码标签是<code>RoI</code>与其相关 <code>ground-truth</code> (真实背景)之间的交叉点。</li>
<li>采用以图像为中心的训练。调整图像的大小，使其比例（较短的边缘）为<code>800</code>像素。</li>
<li>每个<code>mini-batch</code>每个<code>GPU</code>有<code>2</code>个图像，每个图像有<code>N</code>个采样<code>RoI</code>，正负比为<code>1:3</code>。</li>
<li>对于<code>C4</code>主干，<code>N</code>为<code>64</code>；对于<code>FPN</code>，<code>N</code>为<code>512</code>。</li>
<li>在<code>8</code>个<code>GPU</code>（因此有效的小批量大小为<code>16</code>）上训练<code>160k</code>次迭代，学习率为<code>0.02</code>，在<code>120k</code>次迭代时降低了<code>10</code>。</li>
<li>使用<code>0.0001</code>的重量衰减和<code>0.9</code>的动量。</li>
<li>使用<code>ResNeXt</code>
</li>
<li>使用每个<code>GPU</code> <code>1</code>个图像和相同的迭代次数进行训练，初始学习率为<code>0.01</code>
</li>
<li>
<code> RPN</code>锚范围<code>5</code>个比例和<code>3</code>个纵横比**【5个不同面积的框，每个框有3个不同形状(面积相同，长宽比例不同)】**。为了方便消融，<code>RPN</code>单独训练，除非另有规定，否则不会与<code>Mask R-CNN</code>共享功能。<code>RPN</code>和<code>Mask R-CNN</code>具有相同的主干，因此它们是可共享的。</li>
</ul> 
<h2>
<a id="52_Inference_128"></a>5.2 Inference【推断】</h2> 
<ul>
<li>在测试时，<code>C4</code>主干的建议数为<code>300</code>，<code>FPN</code>的建议数是<code>1000</code>。</li>
<li>我们对这些建议运行框预测分支，然后是<strong>非最大抑制</strong>。</li>
<li>然后将掩码分支应用于最高得分的<code>100</code>个检测框。尽管这与训练中使用的并行计算不同，但它加快了推理速度并提高了准确性（由于使用了更少、更准确的<code>RoI</code>）。</li>
<li>掩码分支可以预测每个<code>RoI</code>的<code>K</code>个掩码，但我们只使用第<code>K</code>个掩码，其中<code>K</code>是分类分支预测的类。然后将<span class="katex--inline"><span class="katex"><span class="katex-mathml">
     
      
       
        
         m
        
        
         ∗
        
        
         m
        
       
       
        m*m
       
      
     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.4653em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span>浮点数掩码输出调整为<code>Ro</code>I大小，并在阈值<code>0.5</code>处进行二值化。</li>
<li>由于我们只在前<code>100</code>个检测框上计算掩码，<code>Mask R-CNN</code>为其<code>Fatser R-CNN</code>对应项增加了一小部分开销（例如，在典型模型上约为<code>20%</code>）。</li>
</ul> 
<h1>
<a id="5__134"></a>5. 实验</h1> 
<h2>
<a id="51_Instance_Segmentation_135"></a>5.1 Instance Segmentation【实例分割】</h2> 
<ul>
<li>在COCO数据集上对<code>Mask R-CNN</code>与现有技术进行了彻底比较，并进行了全面消融。</li>
<li>报告了标准COCO指标，包括<code>AP</code>（在<code>IoU</code>阈值上的平均值）、<code>AP50、AP75和APS、APM、APL</code>（不同尺度的<code>AP</code>）。除非另有说明，<code>AP</code>正在使用掩码<code>IoU</code>进行评估。与之前的工作一样，我们使用<code>80k</code>列车图像和<code>35k val</code>图像子集<code>（trainval35k）</code>的联合进行训练，并报告剩余<code>5k val</code>图像<code>（minival）</code>的消融情况。</li>
<li>结果 
  <ul>
<li>将<code>Mask R-CNN</code>与表1中最先进的实例分割方法进行了比较。我们模型的所有实例都优于先前最先进模型的基线变体。</li>
<li>在没有钟声和哨声的情况下，具有<code>ResNet-101-FPN</code>主干的<code>Mask R-CNN</code>优于<code>FCIS++</code>，后者包括多尺度训练/测试、水平翻转测试和在线硬示例挖掘<code>（OHEM)</code>。虽然不在这项工作的范围内，但我们预计许多此类改进将适用于我们的工作.<br> <img src="https://images2.imgbox.com/6b/bd/EoC0ZqlK_o.png" alt="在这里插入图片描述">
</li>
</ul> </li>
<li>结论 
  <ul>
<li>在具有挑战性的条件下，<code>Mask R-CNN</code>也能取得良好的效果。</li>
<li>比较了<code>Mask R-CNN</code>基线和<code>FCIS++</code>。<code>FCIS+++</code>在重叠实例上表现出系统性的伪影，这表明它受到实例分割的基本困难的挑战。<code>Mask R-CNN</code>没有显示此类伪影<br> <img src="https://images2.imgbox.com/fe/62/DJ1QtOig_o.png" alt="在这里插入图片描述">
</li>
</ul> </li>
</ul> 
<h2>
<a id="52_Ablation_Experiments_146"></a>5.2 Ablation Experiments【消融实验】</h2> 
<ul>
<li>架构： 
  <ul>
<li>如图显示具有各种主干的<code>Mask R-CNN</code>。</li>
<li>它得益于更深的网络<code>（50对101）</code>和先进的设计，包括<code>FPN和ResNeXt</code>。</li>
<li>注意到，<strong>并非所有框架都能自动从更深层次或更高级的网络中受益</strong><br> <img src="https://images2.imgbox.com/49/8c/by99YvMG_o.png" alt="在这里插入图片描述">
</li>
</ul> </li>
<li>
<code>Multinomial vs. Independent Masks</code>【多项式与独立掩码】 
  <ul><li>
<code>Mask R-CNN</code>将掩码和类预测解耦： 
    <ul>
<li>当现有的框分支预测类标签时，我们为每个类生成一个掩码，而类之间没有竞争（通过 <code>per-pixel sigmoid</code> 和二进制损失）。将其与使用<code>per-pixel softmax</code> 和多项式损失（如<code>FCN</code>中常用的）进行了比较。</li>
<li>这种替代方案将掩码和类预测的任务相结合，并导致掩码<code>AP</code>的严重损失（5.5分）</li>
<li>
<strong>表明：一旦实例被分类为一个整体（通过盒分支），就足以预测二元掩码，而不必考虑类别，这使得模型更容易训练</strong><br> <img src="https://images2.imgbox.com/6e/de/sKSbKOAk_o.png" alt="在这里插入图片描述">
</li>
</ul> </li></ul> </li>
<li>
<code>Class-Specific vs. Class-Agnostic Masks</code>【阶级特定与阶级不可知掩码】 
  <ul>
<li>默认实例化预测类特定的掩码，即<code>1</code>个<span class="katex--inline"><span class="katex"><span class="katex-mathml">
       
        
         
          
           m
          
          
           ×
          
          
           m
          
         
         
          m×m
         
        
       </span><span class="katex-html"><span class="base"><span class="strut" style="height: 0.6667em;vertical-align: -0.0833em"></span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right: 0.2222em"></span><span class="mbin">×</span><span class="mspace" style="margin-right: 0.2222em"></span></span><span class="base"><span class="strut" style="height: 0.4306em"></span><span class="mord mathnormal">m</span></span></span></span></span>每个类的掩码。</li>
<li>具有类不可知掩码的<code>Mask R-CNN</code>（即，无论类别如何预测单个<code>m×m</code>输出）几乎同样有效：它具有<code>29.7</code>个掩码<code>AP</code>，而<code>ResNet-50-C4</code>上的类特定掩码<code>AP</code>为<code>30.3</code>。</li>
<li>这进一步突出了我们方法中的分工，<strong>该方法在很大程度上分离了分类和分割</strong>
</li>
</ul> </li>
<li>
<code>RoIAlign</code> 
  <ul>
<li> <p>我们使用<code>ResNet50-C4</code>主干，它的步幅为<code>16</code>。<strong>RoIAlign比RoIPool提高AP约3个点</strong>，大部分增益来自高<code>IoU（AP75）</code>。<code>RoIAlign</code>对最大/平均池不敏感；我们在论文的其余部分使用平均值</p> </li>
<li> <p>与<code>MNC</code>中提出的同样采用双线性抽样的<code>RoIWarp</code>进行了比较。<code>RoIWarp</code>仍然对<code>RoI</code>进行量化，失去了与输入的一致性。<strong>RoIWarp的性能与RoIPool相当，但比RoIAlign差得多。</strong><br> <img src="https://images2.imgbox.com/c1/a6/Kox3nXC2_o.png" alt="在这里插入图片描述"></p> </li>
<li> <p>评估了具有<code>ResNet-50-C5</code>主干的<code>RoIAlign</code>，该主干具有更大的<code>32</code>像素步幅。我们使用与图4（右）相同的头部，因为<code>res5</code>头部不适用。表2d显示，<strong>RoIAlign将掩模AP提高了7.3点，掩模AP75提高了10.5点（相对改善50%）</strong><br> <img src="https://images2.imgbox.com/aa/81/jnmzg6nN_o.png" alt="在这里插入图片描述"></p> </li>
<li> <p>使用<code>RoIAlign</code>，使用<code>stride-32 C5 features (30.9 AP)比tride-16 C4 features (30.3 AP , Table 2c).</code>更准确。</p> </li>
<li> <p><strong>RoIAlign在很大程度上解决了使用大步长特征进行检测和分割的长期挑战</strong>。</p> </li>
<li> <p>当与<code>FPN</code>一起使用时，<code>RoIAlign</code>显示出<code>1.5</code>掩模<code>AP</code>和<code>0.5</code>盒<code>AP</code>的增益，其具有更精细的多级跨步。对于需要更精细对齐的关键点检测，即使使用<code>FPN</code>，<code>RoIAlign</code>也显示出较大的增益（表6）。<br> <img src="https://images2.imgbox.com/72/01/rPMfXz2E_o.png" alt="在这里插入图片描述"></p> </li>
</ul> </li>
<li>
<code>Mask Branch</code>【掩码分支】 
  <ul>
<li>分割是一个<code>pixel-to-pixel</code>的任务，通过使用<code>FCN</code>来利用掩模的空间布局。</li>
<li>使用<code>ResNet-50-FPN</code>主干比较了多层感知器<code>（MLP）</code>和<code>FCN</code>。<strong>使用FCN比MLP提供2.1掩码AP增益</strong>。为了与<code>MLP</code>进行公平比较，我们选择了该主干，以使<code>FCN</code>头部的对流层未经过预训练<br> <img src="https://images2.imgbox.com/6b/b2/vxGS9w7A_o.png" alt="在这里插入图片描述">
</li>
</ul> </li>
</ul> 
<h2>
<a id="53__Bounding_Box_Detection_Results_177"></a>5.3 Bounding Box Detection Results【边界框检测结果】</h2> 
<ul>
<li>
<code>Mask R-CNN</code>与表3中最先进的COCO边界框对象检测进行了比较。</li>
<li>对于结果，即使训练了全<code>Mask R-CNN</code>模型，在推断时也只使用分类和框输出（掩码输出被忽略）。</li>
<li>
<strong>使用ResNet-101FPN的Mask R-CNN优于所有先前最先进模型的基础变体，包括GRMI的单模型变体</strong>。</li>
<li>使用<code>ResNeXt-101-FPN</code>，<code>Mask R-CNN</code>进一步改进了结果，与使用<code>Inception-ResNet-v2-TDM</code>中的最佳先前单一模型条目相比，框<code>AP</code>的裕度<code>为3.0</code>点。</li>
<li>训练了一个<code>Mask R-CNN</code>版本，但没有掩码分支，在表3中用<code>“Faster R-CNN，RoIAlign”</code>表示。由于<code>RoIAlign</code>，<strong>该模型的性能优于[Feature pyramid networks for object detection.]中提出的模型。另一方面，它比Mask R-CNN低0.9分。</strong>
</li>
<li><strong>Mask R-CNN在盒子检测方面的差距仅是由于多任务训练的好处</strong></li>
<li>
<code>Mask R-CNN</code>在其掩码和框AP之间获得了一个小的差距：例如，在<code>37.1</code>（掩码，表1）和<code>39.8</code>（框，表3）之间的<code>2.7</code>点。</li>
<li>
<strong>这表明我们的方法在很大程度上缩小了对象检测和更具挑战性的实例分割任务之间的差距</strong>。<br> <img src="https://images2.imgbox.com/b8/7f/uiezX8gB_o.png" alt="在这里插入图片描述"><br> <img src="https://images2.imgbox.com/c6/dc/syjZmh5F_o.png" alt="在这里插入图片描述">
</li>
</ul> 
<h2>
<a id="54_Timing_188"></a>5.4 Timing【调速】</h2> 
<ul>
<li> <p><code>Inference</code>【推理】</p> 
  <ul>
<li>训练了一个<code>ResNet-101-FPN</code>模型，该模型在<code>RPN</code>和<code>Mask R-CNN</code>阶段之间共享特征，随后对<code>Faster R-CNN</code>进行了<code>4</code>步训练。</li>
<li>该模型在<code>Nvidia Tesla M40 GPU</code>上以每幅图像<code>195</code>毫秒的速度运行（外加<code>15</code>毫秒的<code>CPU</code>时间将输出调整为原始分辨率），并在统计上实现了与非共享图像相同的掩模<code>AP</code>。</li>
<li>
<code>ResNet-101-C4</code>变体需要约<code>400ms</code>的时间，因为它具有更重的箱头（图4），<strong>因此不建议在实践中使用C4变体。</strong>
</li>
<li>尽管<code>Mask R-CNN</code>速度很快，<strong>但是我们的设计并未针对速度进行优化，可以通过改变图像大小和提案数量等方式实现更好的速度/精度权衡，这超出了本文的范围</strong>。<br> <img src="https://images2.imgbox.com/58/f0/BjS5Q92y_o.png" alt="在这里插入图片描述">
</li>
</ul> </li>
<li> <p><code>Training</code>【训练】</p> 
  <ul>
<li><strong>Mask R-CNN训练速度也很快。</strong></li>
<li>在COCO <code>trainval35k</code>上使用<code>ResNet-50-FPN</code>进行培训，在我们的同步<code>8-GPU</code>实施中需要<code>32</code>小时（每<code>16</code>个图像小批量<code>0.72s</code>），使用<code>ResNet-101-FPN</code>需要<code>44</code>小时。</li>
<li>事实上，在训练集上训练时，快速原型制作可以在不到一天的时间内完成。我们希望这样的快速培训将消除这一领域的一个主要障碍，并鼓励更多的人对这一具有挑战性的课题进行研究。</li>
</ul> </li>
</ul> 
<h1>
<a id="6__200"></a>6. 总结</h1> 
<h2>
<a id="61_FPN_201"></a>6.1 FPN(特征金字塔网络)</h2> 
<ul>
<li>特征金字塔用来提取特征</li>
<li>自下而上层 
  <ul><li>使用<code>Resnet</code>深度残差算法主干结构提取特征，返回每个阶段最后一层的数据。</li></ul> </li>
<li>自上而下层 
  <ul><li>使用<code>线性插值</code>与此阶段的前一个阶段进行特征融合，以此类推，返回特征融合后每个阶段的值</li></ul> </li>
</ul> 
<blockquote> 
 <p>为什么要特征融合？【自己理解的，不知道对不对】</p> 
 <blockquote> 
  <p>在特征提取前期阶段提取出的特征也很重要，前期特征语义性虽然弱，但是提取出的零碎特征在图片上数据比较明显<br> 比如：一张图片中有一个人，前期可能会提取出人的主要特征部位(腿等位置)，与后期的特征在进行融合，会使腿这个特征学习的更好</p> 
 </blockquote> 
</blockquote> 
<h2>
<a id="62__212"></a>6.2 候选框生成</h2> 
<ul>
<li>对提取的特征图进行区域金字塔网络【就是设置很多不同的框，在不同的框中提取特征】</li>
<li>
<code>RPN</code>会以每个像素点为中心，设置<code>5</code>个不同大小的<code>scales</code>，每个<code>scales</code>有<code>3</code>个不同的<code>roatis</code>，进行生成 
  <ul><li>例如；<code>scales:(32, 64, 128, 256, 512),roatis([0.5, 1, 2]）</code>，所以每个像素点会生成<code>15</code>个不同的框</li></ul> </li>
</ul> 
<h2>
<a id="63_RPN_216"></a>6.3 RPN(区域建议网络)</h2> 
<ul><li>将生成的候选框做前景和背景二分类 
  <ul><li>返回分类得分，分类概率，区域框数据</li></ul> </li></ul> 
<h2>
<a id="64_ROI_220"></a>6.4 ROI(感兴趣区域)</h2> 
<ul><li>筛选候选框 
  <ul>
<li>根据得分值选择<code>top-pre_nms_limit</code>个前景（<code>top n</code>的得分最高的前景）</li>
<li>根据阈值筛选候选框重叠比例大于阈值的，再选出得分最高的框</li>
</ul> </li></ul> 
<h2>
<a id="65_DetectionTargetLayer_225"></a>6.5 DetectionTargetLayer【检测目标层】</h2> 
<ul>
<li>获取正负样本数据集</li>
<li>根据阈值设置正负样本</li>
<li>数据集正负比例为<code>1:3</code>
</li>
</ul> 
<h2>
<a id="66_Network_Heads_229"></a>6.6 Network Heads【网络头部】</h2> 
<ul>
<li>将所有特征图大小统一一个尺寸，方便后期全连接层</li>
<li>
<code>ROI Align</code>:将特征图进行线性插值下采样方法</li>
</ul> 
<h2>
<a id="67__232"></a>6.7 开始建模</h2> 
<h1>
<a id="7_233"></a>7.阅读总结</h1> 
<ul>
<li>
<code>Mask R-CNN</code>是对于以前的<code>R-NN</code>算法的更新迭代</li>
<li>相关知识点在前期都有涉及</li>
<li>此算法主要就是使用<code>FPN</code>，特征金字塔对特征进行融合</li>
<li>使用线性插值对特征进行上采样融合与下采样特征进入全连接层</li>
<li>只理解这么多东西。。。</li>
</ul>
                </div>
                
                

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>