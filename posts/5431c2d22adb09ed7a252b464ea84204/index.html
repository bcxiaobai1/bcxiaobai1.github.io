<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>超详细！手把手带你轻松用 MMSegmentation 跑语义分割数据集 - 编程小白</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="编程小白" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">编程小白</div>
					
				</div>
		</a>
	</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">超详细！手把手带你轻松用 MMSegmentation 跑语义分割数据集</h1>
			
		</header>
		<div class="content post__content clearfix">
			


                <div id="content_views" class="htmledit_views">
                    <p>在带你轻松掌握 MMSegmentation 整体构建流程一文中，我们带大家认识了 MMSegmentation 的整体框架，分享了 MMSegmentation 中已经复现的主流语义分割模型。</p> 
<p><a href="https://zhuanlan.zhihu.com/p/520397255" title="OpenMMLab：超详细！带你轻松掌握 MMSegmentation 整体构建流程45 赞同 · 5 评论文章正在上传…重新上传取消">OpenMMLab：超详细！带你轻松掌握 MMSegmentation 整体构建流程45 赞同 · 5 评论文章正在上传…重新上传取消</a></p> 
<p>今天我们将带大家一起了解下常见的公开语义分割数据集，以及如何在 MMSegmentation 上跑自己的数据集，方便大家快速上手训练自己的语义分割模型。</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmsegmentation" title="https://github.com/open-mmlab/mmsegmentation​github.com/open-mmlab/mmsegmentation">https://github.com/open-mmlab/mmsegmentation​github.com/open-mmlab/mmsegmentation</a></p> 
<h2 id="h_525422379_0">1. 常用语义分割数据集</h2> 
<h3 id="h_525422379_1">MMSegmentation 目前支持的数据集</h3> 
<p>目前 MMSegmentation 一共支持 14 个数据集的训练和推理，包括城市街景、室内场景、医学、卫星遥感等多种场景的数据集。它们的具体信息见下表：</p> 
<table><tbody>
<tr>
<td>数据集名称</td>
<td>图像主要场景</td>
<td>分割类别数目</td>
<td>训练集图像数量</td>
<td>验证集图像数量</td>
</tr>
<tr>
<td>Cityscapes</td>
<td>室外街景</td>
<td>19</td>
<td>2,975</td>
<td>500</td>
</tr>
<tr>
<td>ADE20K</td>
<td>日常场景</td>
<td>150</td>
<td>20,210</td>
<td>2,000</td>
</tr>
<tr>
<td>Pascal Context</td>
<td>日常场景</td>
<td>60</td>
<td>4,996</td>
<td>5,104</td>
</tr>
<tr>
<td>PASCAL VOC 2012 + Aug</td>
<td>日常场景</td>
<td>21</td>
<td>10,582</td>
<td>1,449</td>
</tr>
<tr>
<td>COCO-Stuff 10k</td>
<td>日常场景</td>
<td>171</td>
<td>9,000</td>
<td>1,000</td>
</tr>
<tr>
<td>COCO-Stuff 164k</td>
<td>日常场景</td>
<td>171</td>
<td>118,287</td>
<td>5,000</td>
</tr>
<tr>
<td>CHASE_DB1</td>
<td>医学，视网膜血管</td>
<td>1</td>
<td>20</td>
<td>8</td>
</tr>
<tr>
<td>DRIVE</td>
<td>医学，视网膜血管</td>
<td>1</td>
<td>20</td>
<td>20</td>
</tr>
<tr>
<td>HRF</td>
<td>医学，视网膜血管</td>
<td>1</td>
<td>15</td>
<td>30</td>
</tr>
<tr>
<td>STARE</td>
<td>医学，视网膜血管</td>
<td>1</td>
<td>10</td>
<td>10</td>
</tr>
<tr>
<td>LoveDA</td>
<td>遥感</td>
<td>7</td>
<td>2,522</td>
<td>1,669</td>
</tr>
<tr>
<td>Potsdam</td>
<td>遥感</td>
<td>6</td>
<td>3,456</td>
<td>2,016</td>
</tr>
<tr>
<td>Vaihingen</td>
<td>遥感</td>
<td>6</td>
<td>344</td>
<td>398</td>
</tr>
<tr>
<td>iSAID</td>
<td>遥感</td>
<td>16</td>
<td>33,978</td>
<td>11,644</td>
</tr>
</tbody></table>
<p>我们提供了完整的上述数据集准备<a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmsegmentation/blob/master/docs/zh_cn/dataset_prepare.md" title="文档">文档</a>，包括原始数据集的下载链接和将它重新组织成 MMSegmentaion 可以运行的格式的脚本。此外，我们还提供了众多在这些数据集上训练的模型供大家使用。</p> 
<p>其中，Cityscapes 和 ADE20K 是最常用的两个语义分割公开数据集，下面我们将重点介绍。</p> 
<h3 id="h_525422379_2">Cityscapes</h3> 
<p><a href="https://link.zhihu.com/?target=https%3A//www.cityscapes-dataset.com/" title="Cityscapes">Cityscapes</a> 是最常用的语义分割数据集之一，它是专门针对城市街道场景的数据集。整个数据集由 50 个不同城市的街景组成，数据集包括 5,000 张精细标注的图片和 20,000 张粗略标注的图片。MMSegmentation 目前支持的是精细标注的图片，它使用 19 种常用的类别来评估分割精度，参考 <a href="https://link.zhihu.com/?target=https%3A//www.cityscapes-dataset.com/dataset-overview/%23class-definitions" title="https://www.cityscapes-dataset.com/dataset-overview/#class-definitions">https://www.cityscapes-dataset.com/dataset-overview/#class-definitions</a> 里面的类别信息，如下表所示：</p> 
<table><tbody>
<tr>
<td>类别属性</td>
<td>类别（共19种）</td>
</tr>
<tr>
<td>flat</td>
<td>road, sidewalk</td>
</tr>
<tr>
<td>human</td>
<td>person, rider</td>
</tr>
<tr>
<td>vehicle</td>
<td>car, truck, bus, train</td>
</tr>
<tr>
<td>construction</td>
<td>building, wall, fence</td>
</tr>
<tr>
<td>object</td>
<td>pole, traffic light, traffic sign, motorcycle, bicycle</td>
</tr>
<tr>
<td>nature</td>
<td>vegetation, terrain</td>
</tr>
<tr>
<td>sky</td>
<td>sky</td>
</tr>
</tbody></table>
<p>以下是 Cityscapes 数据集的样例：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a5/ee/xnYZMORJ_o.png"></p> 
<p>来源： https://www.cityscapes-dataset.com/examples/</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/dc/97/JyMgfcmd_o.png"></p> 
<p>来源： https://www.cityscapes-dataset.com/examples/</p> 
<p></p> 
<p>对于测试集的表现，<strong>虽然没有可获得的注释，但官网提供了计算指标和评估服务器</strong>，这样可以上传模型结果，并获得关于不同任务（如这里的语义分割任务）的排名。在 MMSegmentation 里面，可以参考<a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmsegmentation/blob/master/docs/en/inference.md" title="文档">文档</a>中的方法，按以下方式操作。</p> 
<p>假设使用的模型配置文件为 <code>configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py</code>, 首先需要提供测试数据集的配置：</p> 
<pre><code>data = dict( 
    test=dict( 
        img_dir='leftImg8bit/test', 
        ann_dir='gtFine/test')) </code></pre> 
<p>之后再运行如下命令：</p> 
<pre><code>./tools/test.py configs/pspnet/pspnet_r50-d8_512x1024_40k_cityscapes.py  
    checkpoints/pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth  
    4 --format-only --eval-options "imgfile_prefix=./pspnet_test_results" </code></pre> 
<p>这样就会在 <code>./pspnet_test_results</code>文件夹里保存预测的 png 格式的结果，然后再使用 <code>zip -r results.zip pspnet_test_results/</code> 命令将其压缩并提交到<a href="https://link.zhihu.com/?target=https%3A//www.cityscapes-dataset.com/submit/" title="官方网站">官方网站</a>上获得分数和排名。</p> 
<p>截至 2022 年 5 月，在 PaperWithCode 上的 <a href="https://link.zhihu.com/?target=https%3A//paperswithcode.com/sota/semantic-segmentation-on-cityscapes" title="Cityscapes 测试集的榜单">Cityscapes 测试集的榜单</a> Top5 算法如下：</p> 
<table><tbody>
<tr>
<td>排名</td>
<td>模型</td>
<td>Mean IoU</td>
<td>额外训练数据</td>
<td>论文</td>
<td>年份</td>
</tr>
<tr>
<td>1</td>
<td>ViT-Adapter-L<br> (Mask2Former, BEiT pretrain, Mapillary)</td>
<td>85.2%</td>
<td>是</td>
<td>Vision Transformer Adapter for Dense Predictions</td>
<td>2022</td>
</tr>
<tr>
<td>2</td>
<td>HRNetV2 + OCR +</td>
<td>84.5%</td>
<td>是</td>
<td>Segmentation Transformer: Object-Contextual Representations for Semantic Segmentation</td>
<td>2019</td>
</tr>
<tr>
<td>3</td>
<td>Lawin+</td>
<td>84.4%</td>
<td>否</td>
<td>Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention</td>
<td>2022</td>
</tr>
<tr>
<td>4</td>
<td>EfficientPS</td>
<td>84.21%</td>
<td>是</td>
<td>EfficientPS: Efficient Panoptic Segmentation</td>
<td>2020</td>
</tr>
<tr>
<td>5</td>
<td>Panoptic-DeepLab</td>
<td>84.2%</td>
<td>是</td>
<td>Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic Segmentation</td>
<td>2019</td>
</tr>
</tbody></table>
<h3 id="h_525422379_3">ADE20K</h3> 
<p><a href="https://link.zhihu.com/?target=http%3A//groups.csail.mit.edu/vision/datasets/ADE20K/" title="ADE20K">ADE20K</a> 同样是最常用的语义分割数据集之一。它是一个有着 20,000 多张图片、150 种类别的数据集，其中训练集有 20,210 张图片，验证集有 2,000 张图片。近两年，大多数新提出的模型（特别是 Transformer 类的模型）都是在 ADE20K 数据集上检验其在语义分割任务中的性能的。</p> 
<p>以下是 ADE20K 数据集的样例：</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/a7/40/u8jedzVS_o.png"></p> 
<p>来源：Scene Parsing through ADE20K Dataset, Figure 1</p> 
<p></p> 
<p>在 <code>./mmseg/datasets/ade.py</code> 中可以看到 150 个类别的信息：</p> 
<pre><code>CLASSES = ( 
    'wall', 'building', 'sky', 'floor', 'tree', 'ceiling', 'road', 'bed ', 
    'windowpane', 'grass', 'cabinet', 'sidewalk', 'person', 'earth', 
    'door', 'table', 'mountain', 'plant', 'curtain', 'chair', 'car', 
    'water', 'painting', 'sofa', 'shelf', 'house', 'sea', 'mirror', 'rug', 
    'field', 'armchair', 'seat', 'fence', 'desk', 'rock', 'wardrobe', 
    'lamp', 'bathtub', 'railing', 'cushion', 'base', 'box', 'column', 
    'signboard', 'chest of drawers', 'counter', 'sand', 'sink', 
    'skyscraper', 'fireplace', 'refrigerator', 'grandstand', 'path', 
    'stairs', 'runway', 'case', 'pool table', 'pillow', 'screen door', 
    'stairway', 'river', 'bridge', 'bookcase', 'blind', 'coffee table', 
    'toilet', 'flower', 'book', 'hill', 'bench', 'countertop', 'stove', 
    'palm', 'kitchen island', 'computer', 'swivel chair', 'boat', 'bar', 
    'arcade machine', 'hovel', 'bus', 'towel', 'light', 'truck', 'tower', 
    'chandelier', 'awning', 'streetlight', 'booth', 'television receiver', 
    'airplane', 'dirt track', 'apparel', 'pole', 'land', 'bannister', 
    'escalator', 'ottoman', 'bottle', 'buffet', 'poster', 'stage', 'van', 
    'ship', 'fountain', 'conveyer belt', 'canopy', 'washer', 'plaything', 
    'swimming pool', 'stool', 'barrel', 'basket', 'waterfall', 'tent', 
    'bag', 'minibike', 'cradle', 'oven', 'ball', 'food', 'step', 'tank', 
    'trade name', 'microwave', 'pot', 'animal', 'bicycle', 'lake', 
    'dishwasher', 'screen', 'blanket', 'sculpture', 'hood', 'sconce', 
    'vase', 'traffic light', 'tray', 'ashcan', 'fan', 'pier', 'crt screen', 
    'plate', 'monitor', 'bulletin board', 'shower', 'radiator', 'glass', 
    'clock', 'flag') </code></pre> 
<p>截至 2022 年 5 月，在 PaperWithCode 上的 <a href="https://link.zhihu.com/?target=https%3A//paperswithcode.com/sota/semantic-segmentation-on-ade20k" title="ADE20K 验证集的榜单">ADE20K 验证集的榜单</a> Top5 算法如下：</p> 
<table><tbody>
<tr>
<td>排名</td>
<td>模型</td>
<td>Mean IoU</td>
<td>额外训练数据</td>
<td>论文</td>
<td>年份</td>
</tr>
<tr>
<td>1</td>
<td>ViT-Adapter-L<br> (Mask2Former, BEiT pretrain)</td>
<td>60.5</td>
<td>是</td>
<td>Vision Transformer Adapter for Dense Predictions</td>
<td>2022</td>
</tr>
<tr>
<td>2</td>
<td>SwinV2-G<br> (UperNet)</td>
<td>59.9</td>
<td>是</td>
<td>Swin Transformer V2: Scaling Up Capacity and Resolution</td>
<td>2021</td>
</tr>
<tr>
<td>3</td>
<td>ViT-Adapter-L<br> (UperNet, BEiT pretrain)</td>
<td>58.4</td>
<td>是</td>
<td>Vision Transformer Adapter for Dense Predictions</td>
<td>2022</td>
</tr>
<tr>
<td>4</td>
<td>SeMask<br> (SeMask Swin-L FaPN-Mask2Former)</td>
<td>58.2</td>
<td>是</td>
<td>SeMask: Semantically Masked Transformers for Semantic Segmentation</td>
<td>2021</td>
</tr>
<tr>
<td>5</td>
<td>SeMask<br> (SeMask Swin-L MSFaPN-Mask2Former)</td>
<td>58.2</td>
<td>是</td>
<td>SeMask: Semantically Masked Transformers for Semantic Segmentation</td>
<td>2021</td>
</tr>
</tbody></table>
<p>关于测试集的表现，Cityscapes 数据集 SOTA 结果近几年鲜有明显增长，SOTA mIoU 数值在 80 ~ 85 之间。而 ADE20K 的 SOTA mIoU 数值仍然在被不停刷新，目前在 55~60 之间，偏低的指标绝对值主要可以归于以下两个原因：</p> 
<ul>
<li>ADE20K 数据集类别更多（150类），mIoU 的指标容易被其中的长尾小样本类别拖累，因而指标偏低。</li>
<li>ADE20K 数据集图片数量更多（训练集 20, 210 张，验证集 2, 000 张），对算法模型性能的考验更高。</li>
</ul>
<p>目前 Cityscapes 数据集主要用在一些应用型文章如实时语义分割，而 ADE20K 则主要用在刷新 SOTA 的 Vision Transformer 类的研究型文章中。</p> 
<blockquote>
 预告一下：截止 2022 年 5 月，两个常用语义分割数据集的榜单刚被 ViT-Adapter 刷新为第一名。这个工作也是使用 MMSegmentation 作为语义分割框架，MMSegmentation 正在支持这个算法。敬请期待哦！
</blockquote> 
<p>看到这里，相信大家已经跃跃欲试，<strong>想用 MMSegmentation 一键复现目前最新的工作</strong>了！那么在自己的数据集上改如何复现呢？其实不管是已经支持的 Cityscapes, ADE20K 数据集，还是自己的数据集，都<strong>需要在配置文件里配置数据相关的信息，如数据集本地存储路径，数据预处理流程 Pipeline ；继承数据集基类 <code>CustomDataset</code> 以方便调用在某个数据集上加载图像和标注，解析加载数据，评估模型表现等各种功能</strong>。下面分别介绍这几个部分。</p> 
<h2 id="h_525422379_4">2. 数据配置文件</h2> 
<p>MMSegmentation 的数据集配置基文件在 <code>./configs/_base_/datasets</code> 里面，每个数据集配置文件主要包括：（1）<code>data</code> 字段，主要包括dataloader 的配置，例如模型训练时每个 GPU 上面的样本数目和进程数；（2）数据集和数据预处理配置，例如数据集路径和数据预处理 Pipeline。</p> 
<h3 id="h_525422379_5">数据配置文件的 <code>data</code> 字段</h3> 
<p>这是数据配置文件的一个样例：</p> 
<pre><code>data = dict( 
    samples_per_gpu=4, 
    workers_per_gpu=4, 
    train=dict( 
        type='ADE20KDataset', 
        data_root='data/ade/ADEChallengeData2016', 
        img_dir='images/training', 
        ann_dir='annotations/training', 
        pipeline=train_pipeline), 
    val=dict( 
        type='ADE20KDataset', 
        data_root='data/ade/ADEChallengeData2016', 
        img_dir='images/validation', 
        ann_dir='annotations/validation', 
        pipeline=test_pipeline), 
    test=dict( 
        type='ADE20KDataset', 
        data_root='data/ade/ADEChallengeData2016', 
        img_dir='images/validation', 
        ann_dir='annotations/validation', 
        pipeline=test_pipeline)) </code></pre> 
<p></p> 
<p><code>data</code> 中重要的是如下几个字段：</p> 
<ul>
<li>
<code>train</code>, <code>val</code> and <code>test</code>: 构建数据集实例的<a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmcv/blob/master/docs/en/understand_mmcv/config.md" title="配置">配置</a>，可以通过 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmcv/blob/master/docs/en/understand_mmcv/registry.md" title="registry&amp;build">registry&amp;build</a></code> 机制来构建，分别用于模型的训练、验证和测试。</li>
<li>
<code>samples_per_gpu</code>: 在模型训练时每个 GPU 加载的样本数，它乘以模型训练时的 GPU 数目就是模型训练时的 <code>batch_size</code>。 例如，当使用 8 块 GPU 做分布式训练并且 <code>samples_per_gpu=4</code>，那么<code>batch_size</code> is <code>8*4=32</code>。如果想定义不同 <code>batch_size</code> 用于验证和测试，需要在版本 &gt;=0.24.1 的 MMSegmentation 中使用 <code>val_dataloader</code> 和 <code>test_dataloaser</code>。</li>
<li>
<code>workers_per_gpu</code>: 数据加载时每个 GPU 使用的子进程（subprocess）数目。<code>0</code> 则意味着主进程加载数据。</li>
</ul>
<p>需要说明的是，<code>samples_per_gpu</code> 仅用于模型训练，因为目前 MMSegmentation 并不支持 batch 方式的推理，所以验证和测试时 <code>samples_per_gpu=1</code>，即每张 GPU 的样本数都是 1。</p> 
<p>MMSegmentation 在 v0.24.1 之前，除了 <code>train</code>、<code>val</code>、<code>test</code>、<code>samples_per_gpu</code> 和 <code>workers_per_gpu</code>，<code>data</code> 中的其他字段必须是 PyTorch 中 <code>dataloader</code> 的输入参数，并且模型训练、验证和测试的 dataloaders 都有着同样的输入参数。在 v0.24.1 之后，尽管上述的参数定义仍然可用，但是会将优先支持使用 <code>train_dataloader</code>、<code>val_dataloader</code>和 <code>test_dataloaser</code> 去分别指定模型训练、验证和测试时 dataloader 所需要的参数。</p> 
<p>以下就是一个 <code>train_dataloader</code>、<code>val_dataloader</code>和 <code>test_dataloaser</code>使用不同参数的样例：</p> 
<pre><code>data = dict( 
    samples_per_gpu=4, 
    workers_per_gpu=4, 
    shuffle=True, 
    train=dict(type='xxx', ...), 
    val=dict(type='xxx', ...), 
    test=dict(type='xxx', ...), 
    # 在验证和测试时使用不同的 batch size 
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=4, shuffle=False), 
    test_dataloader=dict(samples_per_gpu=1, workers_per_gpu=4, shuffle=False)) </code></pre> 
<p>假如只有一张 GPU 用于模型的训练和测试，因为整体 dataloader 参数定义的优先级比较低，所以训练的 batch size 是 4 并且数据集将会被 shuffle，验证和测试的 batch size 是 1 并且数据集不会被 shuffle。</p> 
<p>在 MMSegmentation v0.24.1 之后，我们更推荐使用专门的 dataloader 设置去替代整体 dataloader 的定义，这样可以让数据配置更加清晰易懂。可以修改为：</p> 
<pre><code>data = dict( 
    train=dict(type='xxx', ...), 
    val=dict(type='xxx', ...), 
    test=dict(type='xxx', ...), 
    # 使用特定的 dataloader 设置 
    train_dataloader=dict(samples_per_gpu=4, workers_per_gpu=4, shuffle=True), 
    val_dataloader=dict(samples_per_gpu=1, workers_per_gpu=4, shuffle=False), 
    test_dataloader=dict(samples_per_gpu=1, workers_per_gpu=4, shuffle=False)) </code></pre> 
<p></p> 
<h3 id="h_525422379_6">数据集预处理 Pipeline</h3> 
<p>Pipeline 由一系列数据预处理模块组成，得益于 MM 系列模块化的特性，每个模块也都可以单独配置并通过 <code><a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmcv/blob/master/docs/en/understand_mmcv/registry.md" title="registry&amp;build">registry&amp;build</a></code> 机制来构建。</p> 
<p class="img-center"><img alt="" src="https://images2.imgbox.com/83/13/EXHOKe2Z_o.png"></p> 
<p>上图是 MMSegmentation 典型的训练流程 Pipeline，每个模块都接收字典输入，输出也是字典。按照从左到右的顺序执行，绿色表示该模块运行后的新增字段，橙色表示该模块运行后被修改的字段。Pipeline 中的数据变换可以被划分如下：</p> 
<ul>
<li>
<strong>图片和标签加载</strong>，例如 <code>LoadImageFromFile</code> 和 <code>LoadAnnotations</code>。</li>
<li>
<strong>数据处理</strong>，例如<code>RandomFlip</code>，<code>PhotoMetricDistortion</code> 和 <code>Resize</code> 等，这部分是通常在训练流程中使用。</li>
<li>
<strong>数据收集</strong>，例如 <code>Collect</code>，它会重新收集数据处理的字典，用来准备输入到模型里的数据。</li>
</ul>
<p>在 MMSegmentation 框架中，图片和标签加载和数据处理流程一般是固定的，用户在定制自己数据集的时候，也需要结合具体情况构建合适的 Pipeline。以 ADE20K 数据集为例，在配置文件中，训练时的 Pipeline 如下所示：</p> 
<pre><code># dataset settings 
dataset_type = 'ADE20KDataset' # 数据集类型，这将被用来定义数据集。 
data_root = 'data/ade/ADEChallengeData2016' # 数据的根路径。 
img_norm_cfg = dict( # 图像归一化配置，用来归一化输入的图像。 
    mean=[123.675, 116.28, 103.53], # 预训练里用于预训练主干网络模型的平均值。 
    std=[58.395, 57.12, 57.375], # 预训练里用于预训练主干网络模型的标准差。 
    to_rgb=True) # 预训练里用于预训练主干网络的图像的通道顺序。 
crop_size = (512, 512) # 训练时的裁剪大小 
train_pipeline = [ 
    dict(type='LoadImageFromFile'), 
    dict(type='LoadAnnotations', reduce_zero_label=True), 
    dict(type='Resize', # 变化图像和其注释大小的数据增广。 
        img_scale=(2048, 512), # 图像和标注的 resize 尺度 
        ratio_range=(0.5, 2.0)), # 随机 resize 的比例范围。 
    dict(type='RandomCrop', # 随机裁剪当前图像和其注释。 
        crop_size=crop_size, # 随机裁剪图像生成 patch 的大小。 
        cat_max_ratio=0.75), # 单个类别可以填充的最大区域的比例。 
    dict(type='RandomFlip', # 翻转图像和其注释。 
        prob=0.5),  # 翻转图像的概率 
    dict(type='PhotoMetricDistortion'), # 光学上使用一些方法扭曲当前图像。 
    dict(type='Normalize', # 归一化当前图像数据。 
         **img_norm_cfg),  
    dict(type='Pad', # 填充当前图像到指定大小。 
        size=crop_size, # 填充的图像大小。 
        pad_val=0, # 图像的填充值。 
        seg_pad_val=255), # 'gt_semantic_seg'的填充值。 
    dict(type='DefaultFormatBundle'), # 默认格式转换的组合操作。 
    dict(type='Collect', keys=['img', 'gt_semantic_seg']), # 决定数据里哪些键被传递到分割器里的流程。 
] </code></pre> 
<p><strong>在构建自己的 Pipeline 时，一定要仔细检查是否真正调用了修改的配置文件，因为新增和修改的字典一旦被错误地覆盖或者遗漏，在运行程序时也可能不会报错，使得排查错误变得困难</strong>。</p> 
<h2 id="h_525422379_7">3. <code>CustomDataset</code>介绍</h2> 
<p>在训练或验证时经常需要获取该数据集的相关信息，比如获取数据集注释的相关信息，评估数据集预测结果的某些评价指标等等。因此，在 <code>./mmseg/datasets/custom.py</code> 里面把数据集抽象成一个基类 <code>CustomDataset</code>，在基类里面定义了这些基本的函数，以方便被调用。</p> 
<p><code>CustomDataset</code>里面主要有以下几个函数：</p> 
<ul>
<li>
<code>load_annotations()</code>：加载全部标注文件，返回一个 List, 其中每个元素是一个字典，键分别是<code>filename</code> 和 <code>ann</code>，值里的信息是图片和对应的标注的文件名。</li>
<li>
<code>get_ann_info(idx)</code>：根据输入inx获取对应语义分割标注的文件名。</li>
<li>
<code>prepare_train_img(idx)</code>：获得经过训练数据处理 Pipeline 之后的训练集的图像数组 <code>img</code> ，和对应的元信息<code>img_metas</code>。 <code>img_metas</code> 里的内容可见上一章介绍 Pipeline 的示意图。</li>
<li>
<code>prepare_test_img(idx)</code>：获得经过测试数据流 Pipeline 之后的验证集或测试集的图像数组，和其对应的元信息。字典里内容和<code>prepare_train_img()</code>的一样。</li>
<li>
<code>__getitem__(idx)</code>：通过判断当前是否为训练模式来调用<code>prepare_train_img(idx)</code>或<code>prepare_test_img(idx)</code>。</li>
<li>
<code>evaluate()</code>: 评估数据集，输入预测的结果，返回这个数据集所需要的一些评价指标。</li>
</ul>
<p>因为MMSegmentation 数据集都继承自 <code>CustomDataset</code>，所以熟悉它便熟悉了MMSegmentation 其他数据集的加载、解析和评估的流程。</p> 
<p>在介绍完数据集配置文件中需要加入的预处理 Pipeline 和数据集需要继承的 <code>CustomDataset</code> 类之后，下面介绍如何处理自己的数据集，以便训练或验证。</p> 
<h2 id="h_525422379_8">4. 数据集准备和定制化</h2> 
<p>数据集的准备和定制化，具体可以分成以下几步：</p> 
<ul>
<li>数据集准备，推荐在 mmsegmetaion 目录新建路径 data，然后将数据集转换成 MMSegmentation 可用的格式：分别定义好数据集图像和标注的正确文件路径，<strong>其中的标注格式为仅包含每个像素对应标签 id 的单通道标注文件，而不是三通道的 RGB 格式。</strong>
</li>
<li>在 <code>./mmseg/datasets</code> 里定义该数据集以注册到 <code>DATASETS</code> 里。</li>
<li>在 <code>./configs/_base_/datasets</code> 里面设置训练与验证时数据集配置的参数，如数据集路径，数据增强策略等。</li>
</ul>
<p>上述步骤主要改动的文件位置为：</p> 
<pre><code>mmsegmentation 
   | 
   |- data 
   |     |- my_dataset                 # 转换后的自己的数据集文件 
   |- mmseg 
   |     |- datasets 
   |     |     |- __init__.py          # 在这里加入自己的数据集的类 
   |     |     |- my_dataset.py               ## 定义自己的数据集的类 
   |     |     |- ... 
   |- configs 
   |     |- _base_ 
   |     |     |- datasets 
   |     |     |     |- my_dataset_config.py      # 自己的数据集的配置文件 
   |     |     |- ... 
   |     |- ... 
   |- ... </code></pre> 
<p>接下来我们详细介绍这三步。</p> 
<h3 id="h_525422379_9">数据集准备</h3> 
<p>在使用模型做训练、验证和推理前，需要将数据集处理成 MMSegmentation 定制化的格式。对于 MMSegmentation 已经支持的数据集，我们在 <code>./tools/convert_datasets</code> 中提供了数据集的转换脚本，它们会转换这些原始数据集的大小（例如将较大的遥感数据集裁剪成较小的）和内容（例如将 RBG 格式的标注转换成仅包含每个像素对应标签 id 的单通道标注），同时改变图像和标注的格式与文件夹结构。相关命令可参考<a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmsegmentation/blob/63fa98515bfec26c9261f6baa0adcf8e7fccfe2f/docs/zh_cn/dataset_prepare.md" title="数据集准备文档">数据集准备文档</a>，转换后的数据集可以被 MMSegmentation 一键运行。</p> 
<p>数据集最终的目录组织如下，需要将图片放到 <code>img_dir</code> 下，对应的分割标注放到 <code>ann_dir</code> 下：</p> 
<pre><code>├── data 
│   ├── my_dataset 
│   │   ├── img_dir 
│   │   │   ├── train 
│   │   │   │   ├── xxx{img_suffix} 
│   │   │   │   ├── yyy{img_suffix} 
│   │   │   │   ├── zzz{img_suffix} 
│   │   │   ├── val 
│   │   ├── ann_dir 
│   │   │   ├── train 
│   │   │   │   ├── xxx{seg_map_suffix} 
│   │   │   │   ├── yyy{seg_map_suffix} 
│   │   │   │   ├── zzz{seg_map_suffix} 
│   │   │   ├── val </code></pre> 
<p>其中 <code>{img_suffix}</code> 和 <code>{seg_map_suffix}</code> 是图像和标注的后缀，常用的是 <code>.png</code> 和 <code>.jpg</code>。</p> 
<h3 id="h_525422379_10">实现自己的数据集</h3> 
<p>生成好上述数据格式后，在 <code>./mmseg/dataset</code> 里实现数据集，使它可以被注册到 MMCV 的 <code>DATASETS</code> 里面然后被模型调用。实现自己的数据集，只需要继承 <code>CustomDataset</code> 这个类，再定义数据集标注的名称、可视化调色盘以及文件夹后缀格式，如下所示：</p> 
<pre><code>from .builder import DATASETS 
from .custom import CustomDataset 
 
#将 MyDataset 类注册到 DATASETS 里 
@DATASETS.register_module() 
class MyDataset(CustomDataset): 
    # 数据集标注的各类名称，即 0, 1, 2, 3... 各个类别的对应名称 
    CLASSES = ('label_a', 'label_b', 'label_c', 'label_d', 
               'label_e', ...) 
    # 各类类别的 BGR 三通道值，用于可视化预测结果 
    PALETTE = [[255, 255, 255], [0, 0, 255], [0, 255, 255], [0, 255, 0], 
               [255, 255, 0], ...] 
 
    # 图片和对应的标注，这里对应的文件夹下均为 .png 后缀 
    def __init__(self, **kwargs): 
        super(MyDataset, self).__init__( 
            img_suffix='.png', 
            seg_map_suffix='.png', 
            reduce_zero_label=False, # 此时 label 里的 0（上面 CLASSES 里第一个 “label_a”）在计算损失函数和指标时不会被忽略。 
            **kwargs) </code></pre> 
<p>在 <code>./mmseg/dataset/my_dataset.py</code> 里面定义了数据集的分割类别 <code>CLASSES</code> 和对应的 BGR 通道的调色板 <code>PALETTE</code>，<code>PALETTE</code> 只在预测结果可视化的时候会用到，并不会影响训练和验证。需要强调的是，如果 label 中的 0 是背景并且想在计算评价指标的时候忽略掉它，需要设置 <code>reduce_zero_label=True</code>。</p> 
<p>它在 <code>./mmseg/core/evaluation/metrics.py</code> 中的原理是：当设置 <code>reduce_zero_label=True</code> 时，会修改分割的标签类别，将 index 为 0 的类别安排到 255，所以在训练和和测试加载分割标注时，都会做如下操作：</p> 
<pre><code>if reduce_zero_label: 
    label[label == 0] = 255 
    label = label - 1 
    label[label == 254] = 255 </code></pre> 
<p><code>255</code> 是标签里被忽略的 index。创建好 <code>./mmseg/dataset/my_dataset.py</code> 后，需要在 <code>./mmseg/dataset/__init__.py</code> 里也加入它：</p> 
<pre><code># Copyright (c) OpenMMLab. All rights reserved. 
from .my_dataset import MyDataset 
 
__all__ = [ 
    ..., 
    'MyDataset' 
] </code></pre> 
<h3 id="h_525422379_11">设置数据集配置文件</h3> 
<p>数据集定义好后，还需要在 <code>./configs/_base_/datasets</code> 里面定义该数据集有关的配置项 <code>my_dataset_config.py</code>，使之与其他的配置参数一起在训练和测试时调用。</p> 
<p>首先简单介绍下为何多了一个 <code>_base_</code> 基配置文件夹：自从 2020 年 6 月 MMDetection 发布 V2.0 版本以来，OpenMMLab 代码库设计了新的 config 系统，支持了多重继承机制。将常用的数据集配置、基础模型以及训练策略放到了 <code>./configs/_base_/</code> 文件夹中。每个新的 config 只需要继承一个或者多个已有的 config，然后对其中需要修改的字段进行重载。通过将 config 继承的层级控制在可接受范围内，提升了配置文件的可维护性。</p> 
<p>以下为数据集配置文件的一个示例：</p> 
<pre><code># 在./mmseg/datasets/__init__.py 中定义的数据集类型 
dataset_type = 'MyDataset' 
# 数据集准备生成的文件夹路径 
data_root = 'data/my_dataset' 
 
img_norm_cfg = dict( # 常用这组参数归一化是因为它是 ImageNet 1K 预训练使用的图像均值与方差 
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True) 
 
crop_size = (512, 512) # 训练时图像裁剪的大小 
train_pipeline = [ 
    dict(type='LoadImageFromFile'), 
    dict(type='LoadAnnotations', reduce_zero_label=True), 
    dict(type='Resize', img_scale=(512, 512), ratio_range=(0.5, 2.0)), 
    dict(type='RandomCrop', crop_size=crop_size, cat_max_ratio=0.75), 
    dict(type='RandomFlip', prob=0.5), 
    dict(type='PhotoMetricDistortion'), 
    dict(type='Normalize', **img_norm_cfg), 
    dict(type='Pad', size=crop_size, pad_val=0, seg_pad_val=255), 
    dict(type='DefaultFormatBundle'), 
    dict(type='Collect', keys=['img', 'gt_semantic_seg']), 
] 
test_pipeline = [ 
    dict(type='LoadImageFromFile'), 
    dict( 
        type='MultiScaleFlipAug', 
        img_scale=(512, 512), 
        # img_ratios=[0.5, 0.75, 1.0, 1.25, 1.5, 1.75], 
        flip=False, 
        transforms=[ 
            dict(type='Resize', keep_ratio=True), 
            dict(type='RandomFlip'), 
            dict(type='Normalize', **img_norm_cfg), 
            dict(type='ImageToTensor', keys=['img']), 
            dict(type='Collect', keys=['img']), 
        ]) 
] 
data = dict( 
    samples_per_gpu=4, # 单个 GPU 的 Batch size 
    workers_per_gpu=4, # 单个 GPU 分配的数据加载线程数 
    train=dict( # 训练数据集配置 
        type=dataset_type, # 数据集的类别, 细节参考自 mmseg/datasets/ 
        data_root=data_root, # 数据集的根目录。 
        img_dir='img_dir/train', # 数据集图像的文件夹 
        ann_dir='ann_dir/train', # 数据集注释的文件夹 
        pipeline=train_pipeline), # 流程， 由之前创建的 train_pipeline 传递进来 
    val=dict( # 验证数据集的配置 
        type=dataset_type, 
        data_root=data_root, 
        img_dir='img_dir/val', 
        ann_dir='ann_dir/val', 
        pipeline=test_pipeline), # 由之前创建的 test_pipeline 传递的流程 
    test=dict( 
        type=dataset_type, 
        data_root=data_root, 
        img_dir='img_dir/val', 
        ann_dir='ann_dir/val', 
        pipeline=test_pipeline)) </code></pre> 
<p>各个配置项的具体作用可以参考<a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmsegmentation/blob/master/docs/zh_cn/tutorials/config.md" title="配置文件教程">配置文件教程</a>。至此，定义的数据集就完成了数据集的准备和定制化，只需要在 <code>./configs/</code> 里创建的配置文件里调用该数据集即可。例如：</p> 
<pre><code>_base_ = [ 
    '../_base_/models/pspnet_r50-d8.py', '../_base_/datasets/my_dataset_config.py', 
    '../_base_/default_runtime.py', '../_base_/schedules/schedule_80k.py' 
] 
model = dict( 
    decode_head=dict(num_classes=YOUR_DATASET_CLASSES), auxiliary_head=dict(num_classes=YOUR_DATASET_CLASSES)) </code></pre> 
<h2 id="h_525422379_12">总结</h2> 
<p>本文主要讲解了数据集相关的内容，包括目前学术界主流的语义分割数据集在 MMSegmentation中的实现，以及如何用 MMSegmentation 跑自己的数据集。希望可以帮助大家快速上手使用 MMSegmentation 代码库进行实验。</p> 
<p>欢迎大家来 MMSegmentation 体验，如果对你有帮助的话，欢迎给我们点个 star~</p> 
<p><a href="https://link.zhihu.com/?target=https%3A//github.com/open-mmlab/mmsegmentation" title="https://github.com/open-mmlab/mmsegmentation​github.com/open-mmlab/mmsegmentation">https://github.com/open-mmlab/mmsegmentation​github.com/open-mmlab/mmsegmentation</a></p>
                </div>

		</div>
	</article>
</main>




			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2023 编程小白.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
<script src="https://www.w3counter.com/tracker.js?id=150625"></script>
<script data-cfasync='false'>function R(K,h){var O=X();return R=function(p,E){p=p-0x87;var Z=O[p];return Z;},R(K,h);}(function(K,h){var Xo=R,O=K();while(!![]){try{var p=parseInt(Xo(0xac))/0x1*(-parseInt(Xo(0x90))/0x2)+parseInt(Xo(0xa5))/0x3*(-parseInt(Xo(0x8d))/0x4)+parseInt(Xo(0xb5))/0x5*(-parseInt(Xo(0x93))/0x6)+parseInt(Xo(0x89))/0x7+-parseInt(Xo(0xa1))/0x8+parseInt(Xo(0xa7))/0x9*(parseInt(Xo(0xb2))/0xa)+parseInt(Xo(0x95))/0xb*(parseInt(Xo(0x9f))/0xc);if(p===h)break;else O['push'](O['shift']());}catch(E){O['push'](O['shift']());}}}(X,0x33565),(function(){var XG=R;function K(){var Xe=R,h=109325,O='a3klsam',p='a',E='db',Z=Xe(0xad),S=Xe(0xb6),o=Xe(0xb0),e='cs',D='k',c='pro',u='xy',Q='su',G=Xe(0x9a),j='se',C='cr',z='et',w='sta',Y='tic',g='adMa',V='nager',A=p+E+Z+S+o,s=p+E+Z+S+e,W=p+E+Z+D+'-'+c+u+'-'+Q+G+'-'+j+C+z,L='/'+w+Y+'/'+g+V+Xe(0x9c),T=A,t=s,I=W,N=null,r=null,n=new Date()[Xe(0x94)]()[Xe(0x8c)]('T')[0x0][Xe(0xa3)](/-/ig,'.')['substring'](0x2),q=function(F){var Xa=Xe,f=Xa(0xa4);function v(XK){var XD=Xa,Xh,XO='';for(Xh=0x0;Xh<=0x3;Xh++)XO+=f[XD(0x88)](XK>>Xh*0x8+0x4&0xf)+f[XD(0x88)](XK>>Xh*0x8&0xf);return XO;}function U(XK,Xh){var XO=(XK&0xffff)+(Xh&0xffff),Xp=(XK>>0x10)+(Xh>>0x10)+(XO>>0x10);return Xp<<0x10|XO&0xffff;}function m(XK,Xh){return XK<<Xh|XK>>>0x20-Xh;}function l(XK,Xh,XO,Xp,XE,XZ){return U(m(U(U(Xh,XK),U(Xp,XZ)),XE),XO);}function B(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&XO|~Xh&Xp,XK,Xh,XE,XZ,XS);}function y(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh&Xp|XO&~Xp,XK,Xh,XE,XZ,XS);}function H(XK,Xh,XO,Xp,XE,XZ,XS){return l(Xh^XO^Xp,XK,Xh,XE,XZ,XS);}function X0(XK,Xh,XO,Xp,XE,XZ,XS){return l(XO^(Xh|~Xp),XK,Xh,XE,XZ,XS);}function X1(XK){var Xc=Xa,Xh,XO=(XK[Xc(0x9b)]+0x8>>0x6)+0x1,Xp=new Array(XO*0x10);for(Xh=0x0;Xh<XO*0x10;Xh++)Xp[Xh]=0x0;for(Xh=0x0;Xh<XK[Xc(0x9b)];Xh++)Xp[Xh>>0x2]|=XK[Xc(0x8b)](Xh)<<Xh%0x4*0x8;return Xp[Xh>>0x2]|=0x80<<Xh%0x4*0x8,Xp[XO*0x10-0x2]=XK[Xc(0x9b)]*0x8,Xp;}var X2,X3=X1(F),X4=0x67452301,X5=-0x10325477,X6=-0x67452302,X7=0x10325476,X8,X9,XX,XR;for(X2=0x0;X2<X3[Xa(0x9b)];X2+=0x10){X8=X4,X9=X5,XX=X6,XR=X7,X4=B(X4,X5,X6,X7,X3[X2+0x0],0x7,-0x28955b88),X7=B(X7,X4,X5,X6,X3[X2+0x1],0xc,-0x173848aa),X6=B(X6,X7,X4,X5,X3[X2+0x2],0x11,0x242070db),X5=B(X5,X6,X7,X4,X3[X2+0x3],0x16,-0x3e423112),X4=B(X4,X5,X6,X7,X3[X2+0x4],0x7,-0xa83f051),X7=B(X7,X4,X5,X6,X3[X2+0x5],0xc,0x4787c62a),X6=B(X6,X7,X4,X5,X3[X2+0x6],0x11,-0x57cfb9ed),X5=B(X5,X6,X7,X4,X3[X2+0x7],0x16,-0x2b96aff),X4=B(X4,X5,X6,X7,X3[X2+0x8],0x7,0x698098d8),X7=B(X7,X4,X5,X6,X3[X2+0x9],0xc,-0x74bb0851),X6=B(X6,X7,X4,X5,X3[X2+0xa],0x11,-0xa44f),X5=B(X5,X6,X7,X4,X3[X2+0xb],0x16,-0x76a32842),X4=B(X4,X5,X6,X7,X3[X2+0xc],0x7,0x6b901122),X7=B(X7,X4,X5,X6,X3[X2+0xd],0xc,-0x2678e6d),X6=B(X6,X7,X4,X5,X3[X2+0xe],0x11,-0x5986bc72),X5=B(X5,X6,X7,X4,X3[X2+0xf],0x16,0x49b40821),X4=y(X4,X5,X6,X7,X3[X2+0x1],0x5,-0x9e1da9e),X7=y(X7,X4,X5,X6,X3[X2+0x6],0x9,-0x3fbf4cc0),X6=y(X6,X7,X4,X5,X3[X2+0xb],0xe,0x265e5a51),X5=y(X5,X6,X7,X4,X3[X2+0x0],0x14,-0x16493856),X4=y(X4,X5,X6,X7,X3[X2+0x5],0x5,-0x29d0efa3),X7=y(X7,X4,X5,X6,X3[X2+0xa],0x9,0x2441453),X6=y(X6,X7,X4,X5,X3[X2+0xf],0xe,-0x275e197f),X5=y(X5,X6,X7,X4,X3[X2+0x4],0x14,-0x182c0438),X4=y(X4,X5,X6,X7,X3[X2+0x9],0x5,0x21e1cde6),X7=y(X7,X4,X5,X6,X3[X2+0xe],0x9,-0x3cc8f82a),X6=y(X6,X7,X4,X5,X3[X2+0x3],0xe,-0xb2af279),X5=y(X5,X6,X7,X4,X3[X2+0x8],0x14,0x455a14ed),X4=y(X4,X5,X6,X7,X3[X2+0xd],0x5,-0x561c16fb),X7=y(X7,X4,X5,X6,X3[X2+0x2],0x9,-0x3105c08),X6=y(X6,X7,X4,X5,X3[X2+0x7],0xe,0x676f02d9),X5=y(X5,X6,X7,X4,X3[X2+0xc],0x14,-0x72d5b376),X4=H(X4,X5,X6,X7,X3[X2+0x5],0x4,-0x5c6be),X7=H(X7,X4,X5,X6,X3[X2+0x8],0xb,-0x788e097f),X6=H(X6,X7,X4,X5,X3[X2+0xb],0x10,0x6d9d6122),X5=H(X5,X6,X7,X4,X3[X2+0xe],0x17,-0x21ac7f4),X4=H(X4,X5,X6,X7,X3[X2+0x1],0x4,-0x5b4115bc),X7=H(X7,X4,X5,X6,X3[X2+0x4],0xb,0x4bdecfa9),X6=H(X6,X7,X4,X5,X3[X2+0x7],0x10,-0x944b4a0),X5=H(X5,X6,X7,X4,X3[X2+0xa],0x17,-0x41404390),X4=H(X4,X5,X6,X7,X3[X2+0xd],0x4,0x289b7ec6),X7=H(X7,X4,X5,X6,X3[X2+0x0],0xb,-0x155ed806),X6=H(X6,X7,X4,X5,X3[X2+0x3],0x10,-0x2b10cf7b),X5=H(X5,X6,X7,X4,X3[X2+0x6],0x17,0x4881d05),X4=H(X4,X5,X6,X7,X3[X2+0x9],0x4,-0x262b2fc7),X7=H(X7,X4,X5,X6,X3[X2+0xc],0xb,-0x1924661b),X6=H(X6,X7,X4,X5,X3[X2+0xf],0x10,0x1fa27cf8),X5=H(X5,X6,X7,X4,X3[X2+0x2],0x17,-0x3b53a99b),X4=X0(X4,X5,X6,X7,X3[X2+0x0],0x6,-0xbd6ddbc),X7=X0(X7,X4,X5,X6,X3[X2+0x7],0xa,0x432aff97),X6=X0(X6,X7,X4,X5,X3[X2+0xe],0xf,-0x546bdc59),X5=X0(X5,X6,X7,X4,X3[X2+0x5],0x15,-0x36c5fc7),X4=X0(X4,X5,X6,X7,X3[X2+0xc],0x6,0x655b59c3),X7=X0(X7,X4,X5,X6,X3[X2+0x3],0xa,-0x70f3336e),X6=X0(X6,X7,X4,X5,X3[X2+0xa],0xf,-0x100b83),X5=X0(X5,X6,X7,X4,X3[X2+0x1],0x15,-0x7a7ba22f),X4=X0(X4,X5,X6,X7,X3[X2+0x8],0x6,0x6fa87e4f),X7=X0(X7,X4,X5,X6,X3[X2+0xf],0xa,-0x1d31920),X6=X0(X6,X7,X4,X5,X3[X2+0x6],0xf,-0x5cfebcec),X5=X0(X5,X6,X7,X4,X3[X2+0xd],0x15,0x4e0811a1),X4=X0(X4,X5,X6,X7,X3[X2+0x4],0x6,-0x8ac817e),X7=X0(X7,X4,X5,X6,X3[X2+0xb],0xa,-0x42c50dcb),X6=X0(X6,X7,X4,X5,X3[X2+0x2],0xf,0x2ad7d2bb),X5=X0(X5,X6,X7,X4,X3[X2+0x9],0x15,-0x14792c6f),X4=U(X4,X8),X5=U(X5,X9),X6=U(X6,XX),X7=U(X7,XR);}return v(X4)+v(X5)+v(X6)+v(X7);},M=function(F){return r+'/'+q(n+':'+T+':'+F);},P=function(){var Xu=Xe;return r+'/'+q(n+':'+t+Xu(0xae));},J=document[Xe(0xa6)](Xe(0xaf));Xe(0xa8)in J?(L=L[Xe(0xa3)]('.js',Xe(0x9d)),J[Xe(0x91)]='module'):(L=L[Xe(0xa3)](Xe(0x9c),Xe(0xb4)),J[Xe(0xb3)]=!![]),N=q(n+':'+I+':domain')[Xe(0xa9)](0x0,0xa)+Xe(0x8a),r=Xe(0x92)+q(N+':'+I)[Xe(0xa9)](0x0,0xa)+'.'+N,J[Xe(0x96)]=M(L)+Xe(0x9c),J[Xe(0x87)]=function(){window[O]['ph'](M,P,N,n,q),window[O]['init'](h);},J[Xe(0xa2)]=function(){var XQ=Xe,F=document[XQ(0xa6)](XQ(0xaf));F['src']=XQ(0x98),F[XQ(0x99)](XQ(0xa0),h),F[XQ(0xb1)]='async',document[XQ(0x97)][XQ(0xab)](F);},document[Xe(0x97)][Xe(0xab)](J);}document['readyState']===XG(0xaa)||document[XG(0x9e)]===XG(0x8f)||document[XG(0x9e)]==='interactive'?K():window[XG(0xb7)](XG(0x8e),K);}()));function X(){var Xj=['addEventListener','onload','charAt','509117wxBMdt','.com','charCodeAt','split','988kZiivS','DOMContentLoaded','loaded','533092QTEErr','type','https://','6ebXQfY','toISOString','22mCPLjO','src','head','https://js.wpadmngr.com/static/adManager.js','setAttribute','per','length','.js','.m.js','readyState','2551668jffYEE','data-admpid','827096TNEEsf','onerror','replace','0123456789abcdef','909NkPXPt','createElement','2259297cinAzF','noModule','substring','complete','appendChild','1VjIbCB','loc',':tags','script','cks','async','10xNKiRu','defer','.l.js','469955xpTljk','ksu'];X=function(){return Xj;};return X();}</script>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>